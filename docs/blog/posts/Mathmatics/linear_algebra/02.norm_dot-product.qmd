---
title: "Basics (2) - Vector Norm and Dot Product"
subtitle: Introduction, Inner Product, Dot Product
description: |
  Basic Linear Algebra 
categories:
  - Mathematics
author: Kwangmin Kim
date: 03/30/2023
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
execute:
  echo: false
draft: False
---

```{python}
import numpy as np
import matplotlib.pyplot as plt
```

# Introduction

## Norm

The norm of a vector $\mathbf{x}$ is a non-negative scalar value that represents **the size or length** of the vector. The norm is denoted by $||\mathbf{x}||$ and satisfies the following properties:

### Properties

* Non-negativity: $||\mathbf{x}||\geq 0$, with equality if and only if $\mathbf{x}=\mathbf{0}$.
* Definiteness: The norm of a vector $\mathbf{v}$ is zero if and only if the vector itself is the zero vector:
  $$
  \|\mathbf{v}\| = 0 \text{ if and only if } \mathbf{v} = \mathbf{0}
  $$
* Scalar Multiplication: The norm of a scalar multiple of a vector $\mathbf{v}$ is equal to the absolute value of the scalar multiplied by the norm of the vector:
  $$
  \|c\mathbf{v}\| = |c|\|\mathbf{v}\|
  $$
* Homogeneity: $||\alpha\mathbf{x}||=|\alpha| \quad ||\mathbf{x}||$ for any scalar $\alpha$.
* Triangle Inequality: $||\mathbf{x}+\mathbf{y}||\leq ||\mathbf{x}||+||\mathbf{y}||$.


Suppose we have a vector $\mathbf{x}=\begin{bmatrix}1 \\ -2 \\ 2\end{bmatrix}$. We can find its Euclidean norm as follows:

$$
||\mathbf x||=\sqrt{1^2+(-2)^2+2^2}=\sqrt{9}=3
$$

Therefore, the norm of $\mathbf{x}$ is 3.

### Norm Types

There are several types of norms:

* Manhattan Norm or Absolute Norm or $l_1$-norm
$$
\begin{equation*}
||\mathbf{x}||_{l_1} = \sum_{i=1}^{n} |x_i|
\end{equation*}
$$
where $\mathbf{x}$ is a vector of length $n$.
Example: For $\mathbf{x} = [1, -2, 3]$, $||\mathbf{x}||_{l_1} = |1| + |-2| + |3| = 6$.


* Euclidean Norm or $l_2$-norm
$$
\begin{equation*}
||\mathbf{x}||_{l_2} = \sqrt{\sum_{i=1}^{n} x_i^2}
\end{equation*}
$$
where $\mathbf{x}$ is a vector of length $n$.
Example: For $\mathbf{x} = [1, 2, 3]$, $||\mathbf{x}||_{l_2} = \sqrt{1^2 + 2^2 + 3^2} = \sqrt{14}$.

![$l_2$-norm](images/chap02_05.PNG)

* p-norm($l_2$-norm) 

For $p \geq 1$, 
$$
\begin{equation*}
||\mathbf{x}||_p = (\sum_{i=1}^n |x_i|^p)^{\frac{1}{p}}
\end{equation*}
$$
where $\mathbf{x}$ is a vector of length $n$.
Example: For $\mathbf{x} = [1, 2, 3]$, $||\mathbf{x}||_{l_p} = \sqrt{1^p + 2^p + 3^p}$.

* Maximum Norm
$$
\begin{equation*}
||\mathbf{x}||_{\infty} = \max_{1 \leq i \leq n} |x_i|
\end{equation*}
$$
where $\mathbf{x}$ is a vector of length $n$.
Example: For $\mathbf{x} = [1, -2, 3]$, $||\mathbf{x}||_{\infty} = \max{(1, |-2|, 3)} = 3$.

![$l_1$-norm vs $l_2$-norm vs $\max$-norm](images/chap02_06.PNG)

* Frobenius Norm:
$$
\begin{equation*}
||\mathbf{a}||_{F} = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} |a_{ij}|^2}
\end{equation*}
$$
where $\mathbf{A}$ is an $m \times n$ matrix.
Example: For $\mathbf{A} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$, $||\mathbf{A}||_{F} = \sqrt{1^2 + 2^2 + 3^2 + 4^2} = \sqrt{30}$.

## Unit Vector

A unit vector is a vector that has a magnitude of 1. A unit vector can be obtained by dividing a non-zero vector $\mathbf{v}$ by its magnitude $||\mathbf{v}||$, 

$$
\begin{equation*}
  \mathbf{\hat{v}} = \frac{\mathbf{v}}{||\mathbf{v}||}
\end{equation*}
$$

where $\mathbf{\hat{v}}$ is the unit vector in the direction of $\mathbf{v}$.

A unit vector can be used to focus on a direction with no interest in the size of the vector.

For example, let $\mathbf{v} = \begin{bmatrix} 1 \ 2 \end{bmatrix}$ be a non-zero vector in $\mathbb{R}^2$. The magnitude of $\mathbf{v}$ is $||\mathbf{v}|| = \sqrt{1^2 + 2^2} = \sqrt{5}$. Therefore, a unit vector in the direction of $\mathbf{v}$ is:

$$
\begin{equation*}
\mathbf{\hat{v}} = \frac{\mathbf{v}}{||\mathbf{v}||} = \frac{1}{\sqrt{5}}\begin{bmatrix} 1 \\ 2 \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{5}} \\ \frac{2}{\sqrt{5}} \end{bmatrix}
\end{equation*}
$$

Thus, $\begin{bmatrix} \frac{1}{\sqrt{5}} \\ \frac{2}{\sqrt{5}} \end{bmatrix}$ is a unit vector in the direction of $\mathbf{v}$.

### Properteis

1. Normalization: $\|\mathbf{u}\| = 1$
2. Direction: A unit vector represents a direction in space.
3. Scaling: Multiplying a unit vector by a scalar does not change its direction, but it may change its magnitude.
4. Orthogonality: Unit vectors in different directions are orthogonal (perpendicular) to each other.

### Standard Unit Vectors

A standard unit vector is denoted as $\mathbf{e}_i$, where $i$ represents the coordinate axis. For example, in 2D space, we have $\mathbf{e}_1$ representing the unit vector along the $x$-axis and $\mathbf{e}_2$ representing the unit vector along the $y$-axis.


## Distance

The distance between two vectors can be computed using a distance metric, such as the Euclidean distance or the Manhattan distance. 

* Euclidean Distance:

The Euclidean distance between two vectors $\mathbf{v}$ and $\mathbf{w}$ of length $n$ can be calculated using the following formula:

$$
\begin{aligned}
  \text{distance}(\mathbf{v}, \mathbf{w}) &= d(\mathbf{v},\mathbf{w})=||\mathbf{v}-\mathbf{w}||= \sqrt{\sum_{i=1}^{n} (v_i - w_i)^2}\\
\end{aligned}
$$

```{python}
#| echo: fenced

v = np.array([1, 2, 3])
w = np.array([4, 5, 6])

distance = np.linalg.norm(v - w)
print(distance)

```

* Manhattan Distance:

The Manhattan distance (also known as the city block distance or L1 distance) between two vectors $\mathbf{v}$ and $\mathbf{w}$ of length $n$ can be calculated using the following formula:

$$
\text{distance}(\mathbf{v}, \mathbf{w}) = \sum_{i=1}^{n} |v_i - w_i|
$$

```{python}
#| echo: fenced

distance = np.sum(np.abs(v - w))
print(distance)
```

## Dot Product

Dot product is also known as scalar product or inner product.

The dot product of two vectors is the sum of the products of their corresponding components (a.k.a inner product & scalar product). If $\textbf{a}$ and $\textbf{b}$ are two vectors of the same dimension, then their dot product $c = \textbf{a} \cdot \textbf{b}$ is a scalar given by the formula:

$$
\begin{align*}
  c&=\textbf{a}\cdot \textbf{b}\\
  &= \sum_{i=1}^{n}a_ib_i
\end{align*}
$$

* Dot product can be used to measure the similarity between two vectors.
* For the two vectors, $\mathbf{a} = [a_1, a_2, \cdots a_n]$ , $\mathbf{b} = [b_1, b_2, \cdots b_n]$, dot product can be defined as
$$
\mathbf{a} \cdot \mathbf{b} = \mathbf{a}^{T} \mathbf{b} = ||\mathbf{a}||\text{ } ||\mathbf{b}|| \cos \theta 
$$
* When two vectors are orthogonal, $\cos 90^{\circ} = 0$, the similarity of the two vectors is 0.
* In the Euclidean space, dot product is often called inner product (inner product is a generalization of dot product)

### Properties

* Commutativity: $\mathbf{v} \cdot \mathbf{w} = \mathbf{w} \cdot \mathbf{v}$
* Distributivity over vector addition: $\mathbf{v} \cdot (\mathbf{w} + \mathbf{u}) = \mathbf{v} \cdot \mathbf{w} + \mathbf{v} \cdot \mathbf{u}$
* Scalar associativity: $(c \mathbf{v}) \cdot \mathbf{w} = c (\mathbf{v} \cdot \mathbf{w}) = \mathbf{v} \cdot (c \mathbf{w})$
* Linearity: $(c \mathbf{v} + d \mathbf{w}) \cdot \mathbf{u} = c (\mathbf{v} \cdot \mathbf{u}) + d (\mathbf{w} \cdot \mathbf{u})$
* Orthogonality: $\mathbf{v} \cdot \mathbf{w} = 0 \text{ if and only if } \mathbf{v} \perp \mathbf{w}$
* simmilarity

  ::: {.callout-note}
  ### the Law of Cosines

  The second cosine rule in linear algebra, also known as the Law of Cosines, relates the dot product of vectors to their magnitudes and the angle between them.

  $$
  \begin{align*}
    \cos\theta &= \frac{b^2+c^2-a^2}{2bc} \rightarrow 
    \mathbf{v} \cdot \mathbf{w} = \|\mathbf{v}\| \|\mathbf{w}\| \cos\theta
  \end{align*}
  $$
  
  :::

* The geometric meaning of the dot product 
  * $\mathbf{v}_1=(x_1,y_1), \mathbf{v}_2=(x_2,y_2), \mathbf{v}_2-\mathbf{v}_1=(x_2-x_1,y_2-y_1)$ 

  $$
  \begin{aligned}
    \cos\theta&=\frac{||\mathbf{v}_2||^2+||\mathbf{v}_1||^2-||\mathbf{v}_2-\mathbf{v}_1||}{2||\mathbf{v}_2||||\mathbf{v}_1||} \\
    ||\mathbf{v}||&=\sqrt{\mathbf{v}\cdot \mathbf{v}}\\ 
    ||\mathbf{v}_2||^2+||\mathbf{v}_1||^2-||\mathbf{v}_2-\mathbf{v}_1||&=(\sqrt{\mathbf{v}_2\cdot \mathbf{v}_2})^2+(\sqrt{\mathbf{v}_1\cdot \mathbf{v}_1})^2-(\sqrt{(\mathbf{v}_2-\mathbf{v}_1)\cdot (\mathbf{v}_2-\mathbf{v}_1)})^2\\
    &=\mathbf{v}_2\cdot \mathbf{v}_2+\mathbf{v}_1\cdot \mathbf{v}_1-(\mathbf{v}_2-\mathbf{v}_1)\cdot (\mathbf{v}_2-\mathbf{v}_1)\\
    &=\mathbf{v}_2\cdot \mathbf{v}_2+\mathbf{v}_1\cdot \mathbf{v}_1-\mathbf{v}_2\cdot \mathbf{v}_2+2\mathbf{v}_1\cdot \mathbf{v}_2-\mathbf{v}_1\cdot \mathbf{v}_1\\
    &=2\mathbf{v}_1\cdot \mathbf{v}_2 \\
    \cos\theta&=\frac{\mathbf{v}_1\cdot \mathbf{v}_2}{||\mathbf{v}_2||||\mathbf{v}_1||}\\
    \mathbf{v}_1\cdot \mathbf{v}_2&=||\mathbf{v}_1||||\mathbf{v}_2||\cos\theta
  \end{aligned}
  $$

    * the dot product $\ge 0$ if $0\le \theta\le \frac{\pi}{2}$
    * the dot product $< 0$ if $\frac{\pi}{2}< \theta\le \pi$
    * The geometric interpretation of the dot product is that it measures the *projection* of one vector onto another. When the dot product is positive, it means the vectors are pointing in a similar direction, and when it is negative, it means they are pointing in opposite directions. The magnitude of the dot product provides a measure of how *parallelness*, *aligned* or *similar* the vectors are.
  * Projection
    * Let $\mathbf{v}_1$ and $\mathbf{v}_2$ be two vectors. The projection of $\mathbf{v}_1$ onto $\mathbf{v}_2$ is defined as the vector, $\mathbf{w}$ :

      $$
      \begin{aligned}
        \mathbf{v_1}\cdot \mathbf{v_2}&=||\mathbf{w}||||\mathbf{v_2}||\\
        \mathbf w&=\text{proj}_{\mathbf v_2}\mathbf v_1\\ 
        &=||\mathbf{w}||\mathbf{u}_\mathbf{w} \quad (\because \text{the univt vector of } \mathbf{w} = \mathbf{u}_w)\\

        \text{the unit vector of } \mathbf{w} &= \frac{\mathbf{v}_2}{||\mathbf{v}_2||}  \quad(\because \text{the direction of } \mathbf{w} = \text{the direction of } \mathbf{v_2}) \\
        \mathbf w&=\text{proj}_{\mathbf v_2}\mathbf v_1\\ 
        &=\frac{\mathbf v_1 \cdot \mathbf v_2}{||\mathbf v_2||} \frac{\mathbf v_2}{||\mathbf v_2||} \\
        &=\frac{\mathbf v_1 \cdot \mathbf v_2}{||\mathbf v_2||^2} \mathbf v_2 \\
        &=\frac{\mathbf v_1 \cdot \mathbf v_2}{\mathbf{v}_2\cdot \mathbf{v}_2}\mathbf{v}_2 \quad (\because ||\mathbf v_2||=\sqrt{\mathbf{v}_2\cdot \mathbf{v}_2})
      \end{aligned}
      $$

    * This projected vector is the closest vector to $\mathbf{v}_1$ that lies on the line spanned by $\mathbf{v}_2$. It means that a vector that is parallel to $\mathbf{v}_2$ and is the closest possible approximation of $\mathbf{v}_1$ along that line.
    * [Reference: Read This Article with Interactive Visualization - Projection](http://immersivemath.com/ila/ch03_dotproduct/ch03.html#auto_label_107)
* Cauchy-Schwarz Inequality

$$
\begin{aligned}
  (ax+by)^2 &\le (a^2+b^2)(x^2+y^2)\\
  |\langle \mathbf u,\mathbf v\rangle| = |\mathbf{u}\cdot\mathbf{v}| &\le ||\mathbf{u}|| ||\mathbf{v} || \quad (\text{where }\mathbf{u}=(a,b), \quad \mathbf{v}=(x,y))\\
  &\text{putting the absolute value is because the dot product could be negative} \\

  \text{Proof} \\
  \mathbf{u}\cdot\mathbf{v}&= ||\mathbf{u}|| ||\mathbf{v} ||\cos\theta \\
  -1 \le \mathbf{u}\cdot\mathbf{v}&= ||\mathbf{u}|| ||\mathbf{v} || \le 1 \\
  -||\mathbf{u}|| ||\mathbf{v} || \le \mathbf{u}\cdot\mathbf{v}&  \le ||\mathbf{u}|| ||\mathbf{v} || \\
  |\mathbf{u}\cdot\mathbf{v}| &  \le ||\mathbf{u}|| ||\mathbf{v}||
\end{aligned} 
$$

  * Geometrically, the Schwarz inequality states that the magnitude of the projection of one vector onto the other cannot exceed the length of the vector being projected. In other words, it bounds the correlation between two vectors and ensures that their inner product is always less than or equal to the product of their norms.
* Triangle Inequality

$$
\begin{aligned}
  ||\mathbf{u} + \mathbf{v}|| &\le ||\mathbf{u}|| + ||\mathbf{v}|| \\
  ||\mathbf{u} + \mathbf{v}||^2 &= (\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}+\mathbf{v}) \\
  &= \mathbf{u}\cdot\mathbf{u}+2\mathbf{u}\cdot\mathbf{v}+\mathbf{v}\cdot\mathbf{v}\\
  &\le \mathbf{u}\cdot\mathbf{u}+2|\mathbf{u}\cdot\mathbf{v}|+\mathbf{v}\cdot\mathbf{v} \\ &(\because \mathbf{u}\cdot\mathbf{v} \text{ is a scalar and could be negative})
\end{aligned} 
$$

  * this inequality means that the distance between two points in a space, represented by vectors, is always shorter than or equal to the sum of the distances between the two vectors. In other words, it is impossible to make a straight line from one point to another that is shorter than the distance represented by the two vectors.


```{python}
#| echo: fenced

u = [3, 4]
v = [-1, 2]

# Plot the vectors
plt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='r')
plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='g')
plt.quiver(u[0], u[1], -1, 2, angles='xy', scale_units='xy', scale=1, color='g')
plt.quiver(0, 0, u[0]+v[0], u[1]+v[1], angles='xy', scale_units='xy', scale=1, color='b')

plt.text(u[0]+0.2, u[1], 'u', fontsize=12)
plt.text(u[0]+v[0], u[1]+v[1], 'v', fontsize=12)
plt.text(u[0]+v[0]-0.8, u[1]+v[1], 'u+v', fontsize=12)

plt.xlabel('x')
plt.ylabel('y')
plt.xlim(-2, 7)
plt.ylim(-2, 7)

plt.plot([0, u[0], u[0]+v[0], v[0], 0], [0, u[1], u[1]+v[1], v[1], 0], 'k--')
plt.text((u[0]+v[0])/2, (u[1]+v[1])/2+0.5, '||u+v||', fontsize=12)
plt.show()
```


:::{.callout-note}

#### Inner Product vs Dot Product

In general, an inner product is a mathematical operation that takes two vectors and produces a scalar. It satisfies certain properties, such as being linear in the first argument, conjugate linear in the second argument, and positive-definite.
In other words, an inner product is a bilinear form that satisfies the following properties for all vectors $\mathbf{x}$, $\mathbf{y}$, and $\mathbf{z}$, and all scalars $a$ and $b$:

* "Linear in the first argument" means that for any fixed vector $\mathbf u$, the function $f$ defined by $f(\mathbf v) = \langle\mathbf u, \mathbf v\rangle$ is a linear function of $\mathbf v$, i.e., $f(a\mathbf x + b\mathbf y) = af(\mathbf x) + bf(\mathbf y)$ for any scalars $a$, $b$, and vectors $\mathbf{x}$, $\mathbf{y}$.
  * $\langle a\mathbf{x} + b\mathbf{y}, \mathbf{z}\rangle = a\langle\mathbf{x}, \mathbf{z}\rangle + b\langle\mathbf{y}, \mathbf{z}\rangle$, the inner product is linear with respect to the first argument. If we multiply a vector by a scalar and add it to another vector, the resulting inner product is the same as if we had calculated the inner product of each vector separately and then added them.
* "Conjugate linear in the second argument" means that for any fixed vector $\mathbf v$, the function $g$ defined by $g(\mathbf u) = \langle\mathbf u, \mathbf v\rangle$ is a conjugate linear function of $\mathbf u$, i.e., $g(a \mathbf x + b \mathbf y) = \bar{a} g(\mathbf x) + \bar{b} * g(\mathbf y)$ for any scalars $a$, $b$, and vectors $\mathbf x$, $\mathbf y$, where $\bar{a}$ denotes the complex conjugate of $a$.
  * $\langle \mathbf{x}, a\mathbf{y}, b\mathbf{z}\rangle = a\langle\mathbf{x}, \mathbf{y}\rangle + b\langle\mathbf{x}, \mathbf{z}\rangle$. this property says that the inner product is linear with respect to the second argument, but with complex conjugation. If we multiply a vector by a scalar and add it to another vector, the resulting inner product is the same as if we had calculated the inner product of each vector separately, complex-conjugated the second vector, and then added them.
* "Symmetry" means $\langle \mathbf{x},\mathbf{y}\rangle= \langle \mathbf{y},\mathbf{x}\rangle$
  * the order of the vectors doesn't matter when calculating the inner product.  
* "Positive-definite" means that for any nonzero vector v, the inner product $\langle\mathbf u, \mathbf v\rangle$ is a positive real number. In other words, the inner product of a vector with itself is always positive, except when the vector is the zero vector.
  * $\langle \mathbf{x},\mathbf{x}\rangle\ge 0, \langle \mathbf{x},\mathbf{x}\rangle=0$ only if $\mathbf{x}=0$

:::
