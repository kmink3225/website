---
title: "Basics (1) - Vector Operations"
subtitle: Motivation, Scalr, Vector, Addition, Scalar Multiplication, Dot Product, Norm, Unit Vector, Projection, Cross Product, Column Vector, Row Vector, Linear Combination of Vectors, Outer Product
description: |
  Basic Linear Algebra 
categories:
  - Mathematics
author: Kwangmin Kim
date: 03/30/2023
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

# Introduction Linear Algebra to Deep Learning

Deep learning is a pile of neural networks that are made up of layers of interconnected nodes or neurons, and the weights of the connections between them are learned through a process called backpropagation.

Linear algebra is fundamental to deep learning because many of the computations involved in training neural networks can be expressed as linear algebra operations. For example, matrix multiplication is used to compute the output of each layer in a neural network, and the gradients of the loss function with respect to the weights are computed using the chain rule of calculus, which involves matrix multiplication and vector operations.

In addition to matrix multiplication, other linear algebra concepts such as eigenvectors, eigenvalues, and singular value decomposition (SVD) are also important in deep learning. For example, SVD can be used to reduce the dimensionality of a dataset or to compute principal components, which are useful for data visualization and feature extraction.

Linear algebra libraries such as Numpy, Scipy, and PyTorch provide efficient implementations of these operations, which are essential for training large-scale neural networks on GPUs. Without these libraries, implementing deep learning algorithms would be much more difficult and time-consuming.

[Reference: Motivation to Learn Linear Algebra](https://nbviewer.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb)

## Scalar

A scalar is a single mathematical quantity, usually a real number, which can be represented by a single value. Scalars are typically denoted by lowercase letters, such as $a, b, c,$ and so on.

## Vector

A vector $\textbf{v}$ is a mathematical object that represents a quantity with both a magnitude and a direction. In $n$-dimensional Euclidean space $\mathbb{R}^n$, a vector $\textbf{v}$ is typically represented as an ordered list of $n$ real numbers:

* the magnitude of $\mathbf{v} =\begin{bmatrix} x\\  y \end{bmatrix}$ is $||\mathbf{v}|| = \sqrt{x^{2} + y^{2}}$
* the direction of it is the angle with the x axis, $tan^{-1}(\frac{y}{x})$
* If magnitude and vector are equal, then they are equal vectors

$$
\textbf{v}=
\begin{bmatrix}
  v_1 \\
  v_2 \\
  \vdots \\
  v_n
\end{bmatrix}
$$

where $v_1, v_2, \ldots, v_n$ are the components of the vector $\textbf{v}$.

### Plotting Vectors on the Coordinate Plane

Example 

Map $\begin{bmatrix} 3\\  2 \end{bmatrix}$ into $x=3$, $y=2$ on the Coordinate Plane
```{python}
import numpy as np
import matplotlib.pyplot as plt

plt.quiver(0, 0, 3, 2, color='b', angles='xy', scale_units='xy', scale=1)
plt.xlim(-0.5, 3.5)
plt.ylim(-0.5, 2.5)
plt.grid()
plt.show()

```

[Reference: Read This Article with Interactive Visualization - Points and Vectors](http://immersivemath.com/ila/ch02_vectors/ch02.html#auto_label_33)

## Basic Vector Operations

### Addition of Vectors

The addition of two vectors is the process of adding their corresponding components. If $\textbf{a}$ and $\textbf{b}$ are two vectors of the same dimension, then their sum $\textbf{c} = \textbf{a} + \textbf{b}$ is a vector whose $i$-th component is the sum of the $i$-th components of $\textbf{a}$ and $\textbf{b}$. 

$$
\begin{align*}
  \textbf{c}&=\textbf{a}+\textbf{b}\\
  c_i &= a_i + b_i
\end{align*}
$$

For example, if $\textbf{a} = [1, 2, 3]$ and $\textbf{b} = [4, 5, 6]$, then their sum $\textbf{c} = [5, 7, 9]$.

::: {layout-ncol=3}
![](images/chap02_01.PNG)

![](images/chap02_02.PNG)

![](images/chap02_03.PNG)
:::

### Subtraction of Vectors

The subtraction of two vectors is the process of subtracting their corresponding components. If $\textbf{a}$ and $\textbf{b}$ are two vectors of the same dimension, then their difference $\textbf{c} = \textbf{a} - \textbf{b}$ is a vector whose $i$-th component is the difference between the $i$-th components of $\textbf{a}$ and $\textbf{b}$. The formal definition is:

$$
\begin{align*}
  \textbf{c}&=\textbf{a} - \textbf{b}\\
  c_i &= a_i - b_i
\end{align*}
$$

For example, if $\textbf{a} = [1, 2, 3]$ and $\textbf{b} = [4, 5, 6]$, then their difference $\textbf{c} = [-3, -3, -3]$.

### Scalar Multiplication of Vectors

The scalar multiplication of a vector is the process of multiplying each component of the vector by a scalar. If $\textbf{a}$ is a vector and $k$ is a scalar, then the scalar multiple $\textbf{c} = k\textbf{a}$ is a vector whose $i$-th component is $k$ times the $i$-th component of $\textbf{a}$. The formal definition is:

$$
\begin{align*}
  \textbf{c}&=k\textbf{a}\\
  c_i &= ka_i
\end{align*}
$$

For example, if $\textbf{a} = [1, 2, 3]$ and $k = 2$, then their scalar multiple $\textbf{c} = [2, 4, 6]$.

[Reference: Read This Article with Interactive Visualization - Properties of Vector Arithmetic](http://immersivemath.com/ila/ch02_vectors/ch02.html#sec_vec_arithmetic)

### Dot Product of Vectors

The dot product of two vectors is the sum of the products of their corresponding components (a.k.a dot product & scalar product). If $\textbf{a}$ and $\textbf{b}$ are two vectors of the same dimension, then their dot product $c = \textbf{a} \cdot \textbf{b}$ is a scalar given by the formula:

$$
\begin{align*}
  c&=\textbf{a}\cdot \textbf{b}\\
  &= \sum_{i=1}^{n}a_ib_i
\end{align*}
$$

* Dot product can be used to measure the similarity between two vectors.
* For the two vectors, $\mathbf{a} = [a_1, a_2, \cdots a_n]$ , $\mathbf{b} = [b_1, b_2, \cdots b_n]$, dot product can be defined as
$$
\mathbf{a} \cdot \mathbf{b} = \mathbf{a}^{T} \mathbf{b} = ||\mathbf{a}||\text{ } ||\mathbf{b}|| \cos \theta 
$$
* When two vectors are orthogonal, $\cos 90^{\circ} = 0$, the similarity of the two vectors is 0.
* In the Euclidean space, dot product is often called inner product (inner product is a generalization of dot product)

:::{.callout-note}

#### Inner Product vs Dot Product

In general, an inner product is a mathematical operation that takes two vectors and produces a scalar. It satisfies certain properties, such as being linear in the first argument, conjugate linear in the second argument, and positive-definite.

* "Linear in the first argument" means that for any fixed vector $\mathbf u$, the function $f$ defined by $f(\mathbf v) = \langle\mathbf u, \mathbf v\rangle$ is a linear function of $\mathbf v$, i.e., $f(a\mathbf x + b\mathbf y) = af(\mathbf x) + bf(\mathbf y)$ for any scalars $a$, $b$, and vectors $\mathbf{x}$, $\mathbf{y}$.
* "Conjugate linear in the second argument" means that for any fixed vector $\mathbf v$, the function $g$ defined by $g(\mathbf u) = \langle\mathbf u, \mathbf v\rangle$ is a conjugate linear function of $\mathbf u$, i.e., $g(a \mathbf x + b \mathbf y) = \bar{a} g(\mathbf x) + \bar{b} * g(\mathbf y)$ for any scalars $a$, $b$, and vectors $\mathbf x$, $\mathbf y$, where $\bar{a}$ denotes the complex conjugate of $a$.
* "Positive-definite" means that for any nonzero vector v, the inner product $\langle\mathbf u, \mathbf v\rangle$ is a positive real number. In other words, the inner product of a vector with itself is always positive, except when the vector is the zero vector.

A dot product is a specific type of inner product that is defined for Euclidean spaces, which are spaces with a notion of distance or length. The dot product of two vectors is defined as the sum of the products of their corresponding components. In other words, if $\mathbf a = [a_1, a_2, ..., a_n]$ and $\mathbf b = [b_1, b_2, ..., b_n]$ are two vectors in $\mathbb R^n$, then their dot product is given by:

$$
\mathbf a \cdot \mathbf b = a_1b_1 + a_2b_2 + ... + a_nb_n
$$

The dot product satisfies some of the properties of an inner product, such as being linear in the first argument and symmetric. However, it is not conjugate linear in the second argument, and it is not positive-definite in general.

So, while a dot product is a specific type of inner product, not all inner products are dot products.
:::

For example, if $\textbf{a} = [1, 2, 3]$ and $\textbf{b} = [4, 5, 6]$, then their dot product $c = 1\cdot 4 + 2\cdot 5 + 3\cdot 6 = 32$.


:::{.callout-note}
#### Norm

The norm of a vector $\mathbf{x}$ is a non-negative scalar value that represents **the size or length** of the vector. The norm is denoted by $||\mathbf{x}||$ and satisfies the following properties:

* Non-negativity: $||\mathbf{x}||\geq 0$, with equality if and only if $\mathbf{x}=\mathbf{0}$.
* Homogeneity: $||\alpha\mathbf{x}||=|\alpha| \text{ }||\mathbf{x}||$ for any scalar $\alpha$.
* Triangle Inequality: $||\mathbf{x}+\mathbf{y}||\leq ||\mathbf{x}||+||\mathbf{y}||$.

Here is an example of finding the Euclidean norm of a vector:
Suppose we have a vector $\mathbf{x}=\begin{bmatrix}1 \\ -2 \\ 2\end{bmatrix}$. We can find its norm as follows:

$$
||\mathbf x||=\sqrt{1^2+(-2)^2+2^2}=\sqrt{9}=3
$$

Therefore, the norm of $\mathbf{x}$ is 3.

There are several types of norms:

* Manhattan Norm or Absolute Norm or $l_1$-norm
$$
\begin{equation*}
||\mathbf{x}||_{l_1} = \sum_{i=1}^{n} |x_i|
\end{equation*}
$$
where $\mathbf{x}$ is a vector of length $n$.
Example: For $\mathbf{x} = [1, -2, 3]$, $||\mathbf{x}||_{l_1} = |1| + |-2| + |3| = 6$.


* Euclidean Norm or $l_2$-norm
$$
\begin{equation*}
||\mathbf{x}||_{l_2} = \sqrt{\sum_{i=1}^{n} x_i^2}
\end{equation*}
$$
where $\mathbf{x}$ is a vector of length $n$.
Example: For $\mathbf{x} = [1, 2, 3]$, $||\mathbf{x}||_{l_2} = \sqrt{1^2 + 2^2 + 3^2} = \sqrt{14}$.

![$l_2$-norm](images/chap02_05.PNG)

* p-norm($l_2$-norm) 

For $p \geq 1$, 
$$
\begin{equation*}
||\mathbf{x}||_p = (\sum_{i=1}^n |x_i|^p)^{\frac{1}{p}}
\end{equation*}
$$
where $\mathbf{x}$ is a vector of length $n$.
Example: For $\mathbf{x} = [1, 2, 3]$, $||\mathbf{x}||_{l_p} = \sqrt{1^p + 2^p + 3^p}$.

* Maximum Norm
$$
\begin{equation*}
||\mathbf{x}||_{\infty} = \max_{1 \leq i \leq n} |x_i|
\end{equation*}
$$
where $\mathbf{x}$ is a vector of length $n$.
Example: For $\mathbf{x} = [1, -2, 3]$, $||\mathbf{x}||_{\infty} = \max{(1, |-2|, 3)} = 3$.

![$l_1$-norm vs $l_2$-norm vs $\max$-norm](images/chap02_06.PNG)

* Frobenius Norm:
$$
\begin{equation*}
||\mathbf{a}||_{F} = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} |a_{ij}|^2}
\end{equation*}
$$
where $\mathbf{A}$ is an $m \times n$ matrix.
Example: For $\mathbf{A} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$, $||\mathbf{A}||_{F} = \sqrt{1^2 + 2^2 + 3^2 + 4^2} = \sqrt{30}$.

:::


:::{.callout-note}

#### Projection

Let $\mathbf{u}$ and $\mathbf{v}$ be two vectors. The projection of $\mathbf{u}$ onto $\mathbf{v}$ is defined as the vector:

This vector is the closest vector to $\mathbf{u}$ that lies on the line spanned by $\mathbf{v}$.
$$
\text{proj}_{\mathbf v}\mathbf u =\frac{\mathbf u \mathbf v}{||\mathbf v||^2} \mathbf v
$$
![Projection](images/chap02_04.PNG)

* $\mathbf{w} = ||\mathbf{w}||\mathbf{v} = ||\mathbf{u}|| \cos \theta \mathbf{v}$
* $\mathbf{u}^T \mathbf{u} = ||\mathbf{u}|| ||\mathbf{u}|| = ||\mathbf{u}||^2$
* the magnitude of $\mathbf{u}$  = $||\mathbf{u}|| = \sqrt{\mathbf{u}^T \mathbf{u}}$ 
* unit vector: a normalized vector by dividing it by its magnitude, so the magnitude of a unit vector is 1
$$
  \hat{\mathbf{u}} = \frac{\mathbf{u}}{||\mathbf{u}||} = \frac{\mathbf{u}}{\sqrt{\mathbf{u}^T \mathbf{u}}}
$$
* Projected vector, $\mathbf{w}$
  * the product of $\frac{\mathbf{u}^T \mathbf{v}}{||\mathbf{v}||}$ and a unit vector of $\mathbf{v}$
$$
\frac{\mathbf{u}^T \mathbf{v}}{||\mathbf{v}||} \frac{\mathbf{v}}{||\mathbf{v}||} = \frac{\mathbf{u}^T \mathbf{v}}{||\mathbf{v}||^2}\mathbf{v}
$$

For example, let $\mathbf{u} = \begin{bmatrix}2 \ 3\end{bmatrix}$ and $\mathbf{v} = \begin{bmatrix}1 \\ 1\end{bmatrix}$. Then, the projection of $\mathbf{u}$ onto $\mathbf{v}$ is:

$$
\text{proj}_{\mathbf v}\mathbf u =\frac{\mathbf u \mathbf v}{||\mathbf v||^2} \mathbf v =\frac{\begin{bmatrix}2 \\ 3\end{bmatrix}\begin{bmatrix}1 \\ 1\end{bmatrix}}{\bigg{|}\bigg{|}\begin{bmatrix}1 \\ 1\end{bmatrix}\bigg{|}\bigg{|}^2}=\frac{5}{2}\begin{bmatrix}1 \\ 1\end{bmatrix}=\begin{bmatrix}5 \\ 2\end{bmatrix}
$$

This vector is the closest vector to $\mathbf{u}$ that lies on the line spanned by $\mathbf{v} = \begin{bmatrix}1 \\ 1\end{bmatrix}$.

[Reference: Read This Article with Interactive Visualization - Projection](http://immersivemath.com/ila/ch03_dotproduct/ch03.html#auto_label_107)
:::


:::{.callout-note}
#### Cauchy-Schwarz Inequality

a fundamental result in mathematics that relates to inner products and norms. It states that for any vectors $\mathbf{u}$ and $\mathbf{v}$ in an inner product space, the following inequality holds:
$$
  |\langle \mathbf u,\mathbf v\rangle|\le ||\mathbf u|| ||\mathbf v ||
$$

where $\langle \mathbf{u}, \mathbf{v}\rangle$ denotes the inner product of vectors $\mathbf{u}$ and $\mathbf{v}$, and $|\mathbf{u}|$ and $|\mathbf{v}|$ denote their respective norms.
In terms of the cosine formula, the Schwarz inequality can be written as:
$$
\cos \theta \le 1
$$

where $\theta$ is the angle between vectors $\mathbf{u}$ and $\mathbf{v}$, and $\cos{\theta} = \frac{\langle \mathbf{u}, \mathbf{v}\rangle}{|\mathbf{u}| |\mathbf{v}|}$.

Geometrically, the Schwarz inequality states that the magnitude of the projection of one vector onto the other cannot exceed the length of the vector being projected. In other words, it bounds the correlation between two vectors and ensures that their inner product is always less than or equal to the product of their norms. This inequality has numerous applications in mathematics, physics, and engineering, and is used in many different fields, including linear algebra, signal processing, and quantum mechanics.
:::

:::{.callout-note}
#### Triangle Inequality

The triangle inequality states that for any two vectors $\mathbf{u}$ and $\mathbf{v}$, the length of the sum of the vectors is less than or equal to the sum of the lengths of the vectors themselves. In terms of the cosine formula, this can be expressed as:
$$
 ||\mathbf u + \mathbf v||^2 \le ||\mathbf u||^2 + 2||\mathbf u ||||\mathbf v || + ||\mathbf v ||^2
$$

equivalently,

$$
 ||\mathbf u + \mathbf v|| \le ||\mathbf u|| + ||\mathbf v ||
$$

this inequality means that the distance between two points in a space, represented by vectors, is always shorter than or equal to the sum of the distances between each point and a third reference point. In other words, it is impossible to make a straight line from one point to another that is shorter than the distance represented by the two vectors.

:::


### Unit Vector

A unit vector is a vector that has a magnitude of 1. A unit vector can be obtained by dividing a non-zero vector $\mathbf{v}$ by its magnitude $||\mathbf{v}||$, 

$$
\begin{equation*}
  \mathbf{\hat{v}} = \frac{\mathbf{v}}{||\mathbf{v}||}
\end{equation*}
$$

where $\mathbf{\hat{v}}$ is the unit vector in the direction of $\mathbf{v}$.

For example, let $\mathbf{v} = \begin{bmatrix} 1 \ 2 \end{bmatrix}$ be a non-zero vector in $\mathbb{R}^2$. The magnitude of $\mathbf{v}$ is $||\mathbf{v}|| = \sqrt{1^2 + 2^2} = \sqrt{5}$. Therefore, a unit vector in the direction of $\mathbf{v}$ is:

$$
\begin{equation*}
\mathbf{\hat{v}} = \frac{\mathbf{v}}{||\mathbf{v}||} = \frac{1}{\sqrt{5}}\begin{bmatrix} 1 \\ 2 \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{5}} \\ \frac{2}{\sqrt{5}} \end{bmatrix}
\end{equation*}
$$

Thus, $\begin{bmatrix} \frac{1}{\sqrt{5}} \\ \frac{2}{\sqrt{5}} \end{bmatrix}$ is a unit vector in the direction of $\mathbf{v}$.

### Cross Product of Vectors

The cross product of two vectors is a vector that is perpendicular to both of them. If $\textbf{a}$ and $\textbf{b}$ are two vectors in $\mathbb{R}^3$, then their cross product $\textbf{c} = \textbf{a} \times \textbf{b}$ is a vector given by the formula

$$
\textbf{c} = \textbf{a} \times \textbf{b} \\
          = ||\textbf{a}|| ||\textbf{b}||\sin(\theta) \mathbf n           
$$

where:

* $\theta$ is the angle between $\textbf{a}$ and $\textbf{b}$ in the plane containing them (hence, it $0 \le \theta \le \pi$)
* $||\textbf{a}||$ and $||\textbf{b}||$ are the magnitudes of vectors $||\textbf{a}||$ and $||\textbf{b}||$
* and $||\textbf{n}||$ is a unit vector perpendicular to the plane containing $||\textbf{a}||$ and $||\textbf{a}||$, with direction such that the ordered set ($||\textbf{a}||$, $||\textbf{b}||$, $||\textbf{n}||$) is positively-oriented.

If the vectors $\textbf{a}$ and $\textbf{b}$ are parallel (that is, $\theta$ between them is either $0$ or $\pi$), by the above formula, the cross product of $\textbf{a}$ and $\textbf{b}$ is the zero vector 0.

[Reference: read the explanations in wiki](https://en.wikipedia.org/wiki/Cross_product)

::: {layout-ncol=2}
![By User:Acdx - Self-made, based on Image:Crossproduct.png, Public Domain](../linear_algebra/images/Cross_product_vector.svg.png)
![Right_hand_rule_cross_product](../linear_algebra/images/Right_hand_rule_cross_product.svg)
:::

For example, 
$$
\textbf{c} = \textbf{a} \times \textbf{b} = [a_2b_3 - a_3b_2, a_3b_1 - a_1b_3, a_1b_2 - a_2b_1]           
$$

If $\textbf{a} = [1, 2, 3]$ and $\textbf{b} = [4, 5, 6]$, then their cross product $\textbf{c} = [-3, 6, -3]$.

### Column Vector & Row Vector

A column vector $\mathbf{u}$ with $n$ elements is an $m \times 1$ matrix, which can be represented as:
$$
\mathbf{u} =
\begin{bmatrix}
u_{1} \\
u_{2} \\
\vdots \\
u_{m}
\end{bmatrix}
$$

In an $m \times n$ matrix, the column vectors can be represented as:

$$
\mathbf U  = \begin{bmatrix}  \mathbf u_{1} &\mathbf u_{2} & \dots &\mathbf u_{n} \end{bmatrix} \\
=
\begin{bmatrix} 
  u_{11} & u_{12} & \cdots & u_{1n} \\
  u_{21} & u_{22} & \cdots & u_{2n} \\
  \vdots & \vdots & \ddots & \vdots \\
  u_{m1} & u_{m2} & \cdots & u_{mn}
\end{bmatrix} 
$$


where $u_i$ is the $i$-th element of the column vector $\mathbf{u}$, $n$ is the number of columns, and $m$ is the number of rows in the matrices.


A row vector $\mathbf{u}$ with $m$ elements is a $1 \times n$ matrix, which can be represented as:
$$
\mathbf{u} = 
\begin{bmatrix}
u_{1} & u_{2} & \cdots & u_{m}
\end{bmatrix}
$$

In an $m \times n$ matrix, the row vectors can be represented as:

$$
\mathbf U  = \begin{bmatrix}  \mathbf u_{1} \\\mathbf u_{2} \\ \vdots \\\mathbf u_{m} \end{bmatrix} \\
=
\begin{bmatrix} 
  u_{11} & u_{12} & \cdots & u_{1n} \\
  u_{21} & u_{22} & \cdots & u_{2n} \\
  \vdots & \vdots & \ddots & \vdots \\
  u_{m1} & u_{m2} & \cdots & u_{mn}
\end{bmatrix} 
$$


where $u_i$ is the $i$-th element of the row vector $\mathbf{u}$ and $n$ is the number of columns in the matrix.

### Linear Combination of vectors

A linear combination of vectors $\mathbf{v}_1,\mathbf{v}_2,\dots,\mathbf{v}_n$ in a vector space $V$ over a field $\mathbb{F}$ is a vector of the form:
$$
a_1\mathbf{v_1}+a_2\mathbf{v_2}+\dots+a_n\mathbf{v_n}
$$

where $a_1,a_2,\dots,a_n\in\mathbb{F}$.

For example, suppose we have two vectors $\mathbf{v}_1=\begin{bmatrix} 1 \ 2 \ 3 \end{bmatrix}$ and $\mathbf{v}_2=\begin{bmatrix} 4 \ 5 \ 6 \end{bmatrix}$ in $\mathbb{R}^3$. Then, a linear combination of $\mathbf{v}_1$ and $\mathbf{v}_2$ is of the form:

$$
a_1\begin{bmatrix}1\\2\\3\end{bmatrix}+a_2\begin{bmatrix}4\\5\\6\end{bmatrix}=\begin{bmatrix}a_1+4a_2\\2a_1+5a_2\\3a_1+6a_2\end{bmatrix}
$$

Here, $a_1$ and $a_2$ are scalar coefficients that determine the resulting linear combination vector.

### Outer Product

The outer product of two vectors $\mathbf{u} = [u_1, u_2, \dots, u_m]^T$ and $\mathbf{v} = [v_1, v_2, \dots, v_n]^T$ is a matrix $\mathbf{u} \mathbf{v}^T$ of size $m \times n$, defined by:

$$
\begin{aligned}
\mathbf{u} \otimes \mathbf{v} &= 
\begin{bmatrix}
u_1v_1 &u_1v_2& \dots & u_1v_n \\
u_2v_1 &u_2v_2& \dots & u_2v_n \\
\vdots &\vdots& \ddots & u_1v_n \\
u_mv_1 &u_mv_2& \dots & u_mv_n \\

\end{bmatrix} \\
 
(\mathbf{u} \otimes \mathbf{v})_{i,j} &= u_i v_j
\end{aligned}
$$

where $\mathbf{u} = [u_1, u_2, \dots, u_m]$ and $\mathbf{v} = [v_1, v_2, \dots, v_n]$.

The outer product is also called the tensor product, and it is a type of binary operation between two vectors that results in a matrix. It is important in linear algebra and other fields such as physics and engineering.

Here is an example: Let $\mathbf{u} = [2, 4, 6]^T$ and $\mathbf{v} = [1, 3]^T$. The outer product of $\mathbf{u}$ and $\mathbf{v}$ is:


So the outer product of $\mathbf{u}$ and $\mathbf{v}$ is a $3 \times 2$ matrix.

[What is a matrix? Go to the Next Blog](../linear_algebra/02.basic_matrix.qmd)