---
title: LDA - EDA
subtitle: Exploratory Data Analysis
description: |
  template
categories:
  - Statistics
author: Kwangmin Kim
date: 04/23/2023
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
execute: 
  echo: false
  warning: false
draft: true
---
```{r}
library(tidyverse)
library(lme4)
rm(list=ls())
#unzip("C:/Users/kmkim/Desktop/projects/data/LDA.zip",list=T)
spruce_data<-read.table("C:/Users/kmkim/Desktop/projects/data/LDA/spruce_data.txt")
milk_data<-read.table("C:/Users/kmkim/Desktop/projects/data/LDA/milk_modified.tsv")
names(milk_data)<-c('trt','id','time','protein')
```
<ul class="nav nav-pills" id="language-tab" role="tablist">
  <li class="nav-item" role="presentation">
    <button class="nav-link active" id="Korean-tab" data-bs-toggle="tab" data-bs-target="#Korean" type="button" role="tab" aria-controls="Korean" aria-selected="true">Korean</button>
  </li>
  <li class="nav-item" role="presentation">
    <button class="nav-link" id="English-tab" data-bs-toggle="tab" data-bs-target="#English" type="button" role="tab" aria-controls="knitr" aria-selected="false">English</button>
  </li>

<div class="tab-content" id="language-tabcontent">

<div class="tab-pane fade  show active" id="Korean" role="tabpanel" aria-labelledby="Korean-tab">

::: {#Korean .tab-pane .fade .show .active role="tabpanel" aria-labelledby="Korean-tab"}

mean function, covariance structure, variogram

## Mean Function Estimation Using Smoothing Methods

response variable의 trend estimation을 smoothing을 통해 수행한다.

Visualization should be performed to capture the characteristics of data and to support the statistical inference results.

* 시간에 따라 변화하는 반응 변수의 평균 패턴 인식
* 시간에 따라 변화하는 개인별 패턴 인식
* 그룹간의 반응 변수와의 관계 인식
* 이상점 또는 특이치를 판독 


### Recognition of Average Patterns in Response Variables Changing over Time

#### Spaghetti Plot

Spaghetti Plot: individual trends of a response variable

```{r}
ggplot(data=milk_data,aes(x=time,y=protein,group=id))+
geom_line()

ggplot(data=milk_data,aes(x=time,y=protein,group=id,col=factor(trt)))+
geom_line()+
facet_wrap(.~trt,ncol=1)
```


#### Spaghetti Plot with Smoothing

Spaghetti plot에 mean function을 추가해 더 more informative visualtion

$$
Y(t)=\mu(t)+\epsilon
$$

###### Kernel Estimation

Kernel estimation is a nonparametric method used to estimate the underlying probability density function of a random variable. In kernel estimation, the density estimate is calculated at each point by placing a kernel function around that point, and the values of all kernel functions are added up to estimate the density.

:::{.callout}
In statistics and data analysis, a kernel is a mathematical function that weights data points in a certain way to estimate a target function, such as a probability density function or a regression function. The idea is to assign weights to neighboring data points based on their distance to the target point, with the weights determined by the kernel function. The kernel function is typically a symmetric, non-negative function that integrates to 1, such as the Gaussian or Epanechnikov kernel.
:::

In the case of estimating the conditional mean function $\mu(t)=\operatorname{E}(Y|T=t)$, we can use kernel estimation with a smoothing kernel function to estimate the mean at each point $t$. The kernel function is used to assign weights to the data points near each point $t$ based on their distance from $t$, and the weighted average of the $Y$ values for these nearby data points gives the estimated value of $\mu(t)$.

t시점을 중심으로 window에 포함된 반응변수 값에 대해 적절한 가중치를 적용하여 mean function을 추정.

$$
\mu(t)=\operatorname{E}(Y|T=t)=\int y f(y|t)dy=\int y \frac{f(t.y)}{}
$$


$$
\hat{\mu}(t) = \frac{\sum_{i=1}^n K\left(\frac{t-t_i}{h}\right) y_i}{\sum_{i=1}^n K\left(\frac{t-t_i}{h}\right)} =\hat{\mu}_{NW}(t)
$$

where $\hat{\mu}(t)$ is the estimate of the mean function at time point $t$, $y_i$ is the response variable for the $i$ th observation, $t_i$ is the time point for the $i$ th observation, $K_h$ is the kernel function with bandwidth parameter $h$, $n$ is the number of observations, and $\hat{\mu}_{NW}(t)$ is the Nadarian-Watson estimator.

$$
\hat{\mu}(t) = \frac{\sum\limits_{i=1}^n y_i K_h(t-T_i)}{\sum\limits_{i=1}^n K_h(t-T_i)}=\frac{\sum\limits_{i=1}^n y_iw(t,t_i,h)}{\sum\limits_{i=1}^n w(t,t_i,h)}

$$

where $w(t,t_i,h)=\frac{K(t-t_i)}{h}$ 

The smaller the bandwith parameter $h$, the smoothing line more wiggly.

The kernel function $K(\cdot)$ is usually a symmetric probability density function that integrates to one, such as the Gaussian kernel and Epanechnikov kernel. The Gassuian kernel is most commonly chosen:

**Gaussian kernel**
$$
K(u) = \frac{1}{\sqrt{2\pi}}\exp{\left(-\frac{1}{2}u^2\right)}
$$

**Epanechnikov kernel**

$$
K(u) = \begin{cases}
\dfrac{3}{4}(1-u^2), & \text{if } |u|<1 \\
0, & \text{otherwise}
\end{cases}
$$


```{r}
#| eval: false

# Nadaraya-Watson estimator using Gaussian kernel
gaussian_kernel_density <- function(x, x_i, h) {
  dnorm((x - x_i) / h) / h
}

gaussian_nadaraya_watson <- function(x, y, t, h) {
  numerator <- sum(y * gaussian_kernel_density(t, x, h))
  denominator <- sum(gaussian_kernel_density(t, x, h))
  return(numerator / denominator)
}

# Nadaraya-Watson estimator using Epanechnikov kernel
Epanechnikov_kernel_density <- function(x, x_i, h) {
  ifelse(abs((x - x_i) / h) > 1, 0, 0.75 * (1 - ((x - x_i) / h) ^ 2) / h)
}

Epanechnikov_nadaraya_watson <- function(x, y, t, h) {
  numerator <- sum(y * Epanechnikov_kernel_density(t, x, h))
  denominator <- sum(Epanechnikov_kernel_density(t, x, h))
  return(numerator / denominator)
}

# long_milk_data<-milk_data%>%
#   mutate(gaussian_wt_h0.1=sapply(milk_data$time, 
#   function(t) gaussian_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.1)),
#   Epanechnikov_wt_h0.1=sapply(milk_data$time, 
#   function(t) Epanechnikov_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.1)),
#   gaussian_wt_h0.3=sapply(milk_data$time, 
#   function(t) gaussian_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.3)),
#   Epanechnikov_wt_h0.3=sapply(milk_data$time, 
#   function(t) Epanechnikov_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.3)),
#   gaussian_wt_h0.6=sapply(milk_data$time, 
#   function(t) gaussian_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.6)),
#   Epanechnikov_wt_h0.6=sapply(milk_data$time, 
#   function(t) Epanechnikov_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.6)),
#   gaussian_wt_h0.9=sapply(milk_data$time, 
#   function(t) gaussian_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.9)),
#   Epanechnikov_wt_h0.9=sapply(milk_data$time, 
#   function(t) Epanechnikov_nadaraya_watson(milk_data$time, milk_data$protein, t, h=0.9)))%>%
#   gather(key=kernels,value=smoothed,protein:Epanechnikov_wt_h0.9)

bandwidths <- c(0.1, 0.3, 0.6, 0.9)
kernels <- c("gaussian", "Epanechnikov")

smoothed_data <- map_dfc(bandwidths, function(h) {
  map_dfc(kernels, function(kernel) {
    col_name <- paste0(kernel, "_wt_h", h)
    smoothed_values <- sapply(milk_data$time, function(t) {
      if (kernel == "gaussian") {
        gaussian_nadaraya_watson(milk_data$time, milk_data$protein, t, h)
      } else {
        Epanechnikov_nadaraya_watson(milk_data$time, milk_data$protein, t, h)
      }
    })
    tibble(!!col_name := smoothed_values)
  })
})

milk_data <- bind_cols(milk_data, smoothed_data)

# Generate fake data
set.seed(123)
n <- 100
x <- seq(0, 1, length.out = n)
y <- sin(2*pi*x) + rnorm(n, sd = 0.2)

# Estimate mean function using Gaussian Nadaraya-Watson estimator
t_grid <- seq(0, 1, length.out = 100)
h <- 0.1
gaussian_mu_hat_0.1 <- sapply(t_grid, function(t) gaussian_nadaraya_watson(x, y, t, 0.1))
gaussian_mu_hat_0.3 <- sapply(t_grid, function(t) gaussian_nadaraya_watson(x, y, t, 0.3))
gaussian_mu_hat_0.6 <- sapply(t_grid, function(t) gaussian_nadaraya_watson(x, y, t, 0.6))
gaussian_mu_hat_0.9 <- sapply(t_grid, function(t) gaussian_nadaraya_watson(x, y, t, 0.9))

# Plot results
plot(x, y, main = "Gaussian Nadaraya-Watson estimator", xlab = "x", ylab = "y", ylim = c(-2, 2))
lines(t_grid, gaussian_mu_hat_0.1 , col = "red", lwd = 2,lty=2)
lines(t_grid, gaussian_mu_hat_0.3 , col = "green", lwd = 2,lty=3)
lines(t_grid, gaussian_mu_hat_0.6 , col = "blue", lwd = 2,lty=4)
lines(t_grid, gaussian_mu_hat_0.9 , col = "purple", lwd = 2,lty=5)


```

##### Tuning Hyperparameter `h`

To tune the hyperparameter `h`, we can use and estimate PSE (average predicted squared error) reflecting both bias and variance using cross-validation. 

$$
\operatorname{PSE}(h)=\frac{1}{n}\sum_{i=1}^{n}\operatorname{E}(Y_i^{*}-\hat{\mu}(t,h))^2
$$

$Y_i^{*}$ typically denotes a transformed version of the response variable $Y_i$. It is used to make the distribution of $Y_i^{*}$ more symmetric or more normal, which can be helpful in some statistical analyses.

$$
\operatorname{CV}(h)=\sum_{i=1}^{n}\operatorname{E}(y_i-\hat{\mu}^{-i}(t,h))^2
$$

where $\hat{\mu}^{-i}$ is the mean estimator estimated excluding the ith observation.

```{r}
#| eval: false

set.seed(123)

# Generate fake data
n <- 100
x <- seq(0, 10, length.out = n)
y <- rnorm(n, mean = sin(x))
milk_data <- data.frame(time = x, protein = y)

# PSE function
PSE <- function(h, x, y, K) {
  n <- length(y)
  Y_hat_star <- rep(0, n)
  for (i in 1:n) {
    Y_hat_star[i] <- sum(K((x - x[i]) / h) * y) / sum(K((x - x[i]) / h))
  }
  return(mean((Y_hat_star - y)^2))
}

# CV function
CV <- function(h, x, y, K) {
  n <- length(y)
  Y_hat_minus_i <- rep(0, n)
  for (i in 1:n) {
    Y_hat_minus_i[i] <- sum(K((x[-i] - x[i]) / h) * y[-i]) / sum(K((x[-i] - x[i]) / h))
  }
  return(sum((y - Y_hat_minus_i)^2))
}

# Bandwidth tuning using PSE
tune_bandwidth_PSE <- function(x, y, K) {
  n <- length(y)
  h_seq <- seq(0.01, 1, length = 100)
  PSE_seq <- sapply(h_seq, PSE, x = x, y = y, K = K)
  h_opt <- h_seq[which.min(PSE_seq)]
  return(h_opt)
}

# Bandwidth tuning using CV
tune_bandwidth_CV <- function(x, y, K) {
  n <- length(y)
  h_seq <- seq(0.01, 1, length = 100)
  CV_seq <- sapply(h_seq, CV, x = x, y = y, K = K)
  h_opt <- h_seq[which.min(CV_seq)]
  return(h_opt)
}

h_opt_PSE <- tune_bandwidth_PSE(milk_data$time, milk_data$protein, dnorm)

h_opt_CV <- tune_bandwidth_CV(milk_data$time, milk_data$protein, function(x) ifelse(abs(x) > 1, 0, 0.75 * (1 - x^2)))

# Plotting CV sequence against bandwidth values
CV_plot <- function(x, y, K) {
  n <- length(y)
  h_seq <- seq(0.01, 1, length = 100)
  CV_seq <- sapply(h_seq, CV, x = x, y = y, K = K)
  df <- data.frame(h = h_seq, CV = CV_seq)
  ggplot(df, aes(x = h, y = CV)) +
    geom_line() +
    geom_vline(xintercept = tune_bandwidth_CV(x, y, K), linetype = "dashed", color = "red") +
    labs(x = "Bandwidth", y = "Cross-validation score", title = "Bandwidth tuning using CV")
}

CV_plot(milk_data$time, milk_data$protein, function(x) ifelse(abs(x) > 1, 0, 0.75 * (1 - x^2)))

# Bandwidth tuning using CV, returning CV sequence for all bandwidth values
tune_bandwidth_CV_table <- function(x, y, K) {
  n <- length(y)
  h_seq <- seq(0.01, 1, length = 100)
  CV_seq <- sapply(h_seq, CV, x = x, y = y, K = K)
  df <- data.frame(h = h_seq, CV = CV_seq)
  return(df)
}

CV_table <- tune_bandwidth_CV_table(milk_data$time, milk_data$protein, function(x) ifelse(abs(x) > 1, 0, 0.75 * (1 - x^2)))

knitr::kable(CV_table)
```



###### LOESS

LOESS (locally estimated scatterplot smoothing or LOcal regrESSion) is a nonparametric regression method used for modeling the relationship between a response variable $Y$ and a predictor variable $T$. The goal of LOESS is to estimate the conditional mean function $\mu(t) = \mathbb{E}(Y|T = t)$ using a weighted polynomial regression model.

LOESS involves fitting a separate polynomial regression model to the data in each local neighborhood of the predictor variable $T$. The size of the local neighborhood is controlled by a tuning parameter called the smoothing parameter. For each observation $i$, the model is fit using a weighted least squares method, with weights given by a kernel function that assigns higher weights to observations closer to $i$ in the predictor variable $T$. The polynomial order of the regression model is chosen by the user, with a typical choice being a second-order polynomial.

The loess method first selects a subset of data points near a target point $t$ using a kernel function. A weighted linear regression model is then fit to the data points in the subset, giving more weight to points closer to the target point $t$. The degree of smoothing is controlled by a bandwidth parameter, which determines the size of the subset of data points used in the regression.

The estimated mean function $\hat{\mu}(t)$ is obtained by repeating this process at a large number of target points along the range of $t$ values. The final smooth function is obtained by connecting these estimated mean values.

Loess is particularly useful for estimating smooth nonlinear functions and can handle heteroscedasticity (non-constant variance) and nonlinearity in the data. It is commonly used in applications such as time series analysis, epidemiology, and environmental science.

:::{.callout-tip}
**weighted least square**

The weighted least squares (WLS) solution can be obtained by minimizing the sum of squared weighted residuals, given by:

$$
\operatorname{minimize} \sum_{i=1}^{n} w_i(y_i - f(x_i))^2
$$

The WLS solution is given by:
$$
\beta_{WLS} = (X^TWX)^{-1}X^TWy

$$

where $X$ is the design matrix, $W$ is a diagonal weight matrix with $w_i$ on the $i$th diagonal element, and $y$ is the vector of responses. The predicted response $\hat{y}$ can be obtained as $\hat{y} = X\hat{\beta}$.

Note that the OLS solution is a special case of WLS when all weights are equal to 1.


1. Define the weighted design matrix, $\mathbf{W}$, as a diagonal matrix of weights, where each diagonal element corresponds to the weight for the corresponding observation.
1. Define the weighted response vector, $\mathbf{y}_{w}$, as a vector of the response values multiplied by the square root of the corresponding weight.
1. Define the weighted parameter estimates, $\hat{\beta}_{w}$, as the solution to the weighted least squares problem:
$$
\hat{\beta}_w = \operatorname*{arg\,min}_{\beta} (y_w - X\beta)^T W (y_w - X\beta)
$$
where $\mathbf{X}$ is the design matrix of predictor variables.
4. The estimated model can be obtained by substituting the weighted parameter estimates, $\hat{\beta}_{w}$, into the regression equation:

$$
\hat{y}=\mathbf{X}\hat{\beta}_w
$$

Let's start by defining the problem: we have a set of m data points, represented as a matrix X with dimensions m x p, where p is the number of independent variables. We also have a corresponding vector y with m elements, representing the dependent variable. We want to fit a linear function of the form y = Xβ + ε to the data points, where β is a vector of coefficients to be determined and ε is the residual error.

To perform weighted least squares, we define a weight matrix W with dimensions m x m, where the diagonal elements w(i) are the weights for each data point i. Weights are typically chosen to be proportional to the inverse of the variance of the data point, so that data points with smaller variances are given more weight.

Using this weight matrix, the objective function for weighted least squares is defined as follows:
$$
\begin{aligned}
\text{minimize } S &= (y - X\beta)^TW(y - X\beta) \\
&= y^TWy - \beta^TX^TWy - y^TWX\beta + \beta^TX^TWX\beta \\
\frac{\partial S}{\partial \beta} &= -2X^TWy + 2X^TWX\beta = 0 \\
X^TWX\beta &= X^TWy \\
\beta &= (X^TWX)^{-1}X^TWy
\end{aligned}

$$

:::
the LOESS model can be expressed as:
$$
\hat{\mu}(t_i)=\sum_{j=1}^{n}w_{ij}(t_i)y_j
$$

where $\hat{\mu}(t_i)$ is the estimated mean response at predictor value $t_i$, $y_j$ is the response value at predictor value $t_j$, and $w_{ij}(t_i)$ is the weight assigned to the $j$th observation in the local neighborhood of $t_i$. The weights are defined by a kernel function $K$, such that:

$$
w_{ij}(t_i)=K\left(\frac{t_i-t_j}{h}\right)
$$

where $h$ is the smoothing parameter, controlling the size of the local neighborhood. A common choice for the kernel function is the tri-cube kernel:

$$
K(x) = \begin{cases} 
         \left(1 - |x|^3\right)^3, & \text{if } |x| < 1 \\
         0, & \text{otherwise}
      \end{cases}
$$

```{r}
#| eval: false

tri_cube_kernel <- function(x) {
  sapply(x, function(x_i) {
    ifelse(abs(x_i) <= 1, (1 - abs(x_i)^3)^3, 0)
  })
}

# Define LOESS function
loess <- function(x, y, span, degree) {
  n <- length(x)
  weights <- matrix(0, n, n)
  for (i in 1:n) {
    weights[i,] <- tri_cube_kernel((x - x[i]) / span)
  }
  fit <- lm(y ~ poly(x, degree), weights = weights)
  return(fit)
}

# Generate fake data
set.seed(123)
n <- 100
x <- seq(0, 10, length.out = n)
y <- rnorm(n, mean = sin(x) + 0.1 * x)
df <- data.frame(x = x, y = y)

# Fit LOESS
fit <- loess(x = df$x, y = df$y, span = 0.2, degree = 2)
# Predict on new data
new_x <- seq(0, 10, length.out = 1000)
pred <- predict(fit, newdata = data.frame(x = new_x))

# Plot results
library(ggplot2)
ggplot(data = df, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(x = new_x, y = pred), color = "red", size = 1) +
  theme_minimal()


# Fit loess model
loess_fit <- loess(y ~ x, data = sim_data, span = 0.5)

# Plot data and fitted curve
plot(sim_data, pch = 16, col = "blue", main = "Simulated Data with Loess Fit")
lines(sim_data$x, loess_fit$fitted, col = "red", lwd = 2)
```


```{r}


ggplot(milk_data, aes(x = time, y = protein)) +
geom_point()+
geom_smooth(method = "loess", formula = "y~x", se = FALSE, span = 0.1,color='red')+
geom_smooth(method = "loess", formula = "y~x", se = FALSE, span = 0.3,color='green')+
geom_smooth(method = "loess", formula = "y~x", se = FALSE, span = 0.6,color='blue')+
geom_smooth(method = "loess", formula = "y~x", se = FALSE, span = 0.9,color='purple')+
scale_color_manual(values=c('red','green','blue','purple'),
labels = c("Span = 0.1", "Span = 0.3", "Span = 0.6", "Span = 0.9"))
```

```{r}
ggplot(data=milk_data,aes(x=time,y=protein,group=id))+
geom_line()+
geom_smooth(aes(group=1),method='loess',formula=y~x)+
stat_summary(aes(x = 19, yintercept = ..y.., group = 1), fun = "median", color = "red", geom = "hline")

ggplot(data=milk_data,aes(x=time,y=protein,group=id,col=factor(trt)))+
geom_line()+
geom_smooth(aes(group=1),method='loess',formula=y~x,color='black')+
stat_summary(aes(x = 19, yintercept = ..y.., group = 1), fun = "median", color = "red", geom = "hline")+
facet_wrap(.~trt,ncol=3)
```

### Recognition of Individual Patterns Changing over Time

### Recognition of Relationships with Response Variables between Groups

### Recognition of Outliers or Anomaly Data 


:::
</div>

<div class="tab-pane fade" id="English" role="tabpanel" aria-labelledby="English-tab">

::: {#English .tab-pane .fade role="tabpanel" aria-labelledby="English-tab"}

:::


</div>

# Go to Project Content List

[Project Content List](./docs/projects/index.qmd)

# Go to Blog Content List

[Blog Content List](../../content_list.qmd)