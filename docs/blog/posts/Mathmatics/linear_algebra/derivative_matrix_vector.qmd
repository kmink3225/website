---
title: "Matrix Calculus (1) - Matrix to Vector Derivatives"
subtitle: the sum of squares, covariance matrix, and correlation matrix
description: |
  template
categories:
  - Mathematics
author: Kwangmin Kim
date: 04/02/2023
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: false
execute:
  echo: false
---

## Matrix to Vector Derivatives

Matrix-to-vector derivatives refer to the derivatives of a matrix function with respect to a vector argument. Let $\mathbf{f}(\mathbf{x})$ be a matrix-valued function of a vector $\mathbf{x} \in \mathbb{R}^n$. The matrix-to-vector derivative is denoted as follows:

$$
\frac{\partial \mathbf{f}(\mathbf{x})}{\partial \mathbf{x}} =
\begin{bmatrix}
\frac{\partial \mathbf{f}(\mathbf{x})}{\partial x_1} &
\frac{\partial \mathbf{f}(\mathbf{x})}{\partial x_2} & \cdots &
\frac{\partial \mathbf{f}(\mathbf{x})}{\partial x_n}
\end{bmatrix}
$$

Here, the matrix-to-vector derivative is a matrix whose $i$th column is the partial derivative of $\mathbf{f}$ with respect to the $i$th component of $\mathbf{x}$.

For example, let $\mathbf{A}$ and $\mathbf{x}$ be matrices in $\mathbb{R}^{n\times n}$ and $\mathbb{R}^{n\times 1}$, respectively. Consider the function $\mathbf{f}(\mathbf{x}) = \mathbf{Ax}$, which is a matrix-vector product. The matrix-to-vector derivative of $\mathbf{f}(\mathbf{x})$ with respect to $\mathbf{x}$ is given by:
$$
\frac{\partial \mathbf{f}(\mathbf{x})}{\partial \mathbf{x}} = \mathbf{A}
$$
Here, the derivative is a matrix whose rows are the rows of $\mathbf{A}$.

### Differentiation of Quadratic Form

This is because the output of this differentiation is a vector (with respect to $\mathbf{x}$), rather than a scalar.

The differentiation of a quadratic form is the process of finding the gradient of a quadratic form with respect to its input vector.

Given a quadratic form $f(\mathbf{x})=\mathbf{x}^T \mathbf{A} \mathbf{x}$, where $\mathbf{x}$ is an $n$-dimensional column vector, $\mathbf{b} \in \mathbb{R}^n$ is a vector, and $\mathbf{A}$ is an $n \times n$ symmetric matrix, the derivative of $f(\mathbf{x})$ with respect to $\mathbf{x}$ is given by:

$$
\nabla_{\mathbf x} f(\mathbf x) = (A + A^T)\mathbf x + b
$$

In this expression, $\mathbf{A}^T$ is the transpose of $\mathbf{A}$.

$\mathbf A+\mathbf A^T$ is written instead of $2\mathbf A$ when calculating the gradient of a quadratic form. It is because in general, the matrix $\mathbf A$ might not be symmetric, so $\mathbf A\neq \mathbf A^T$. However, for any matrix $\mathbf A$, we have $\mathbf A+\mathbf A^T = (\mathbf A+\mathbf A^T)^T$, which is a symmetric matrix. Therefore, by writing the gradient as $\nabla_x f(x) = (\mathbf A+\mathbf A^T)x$, we ensure that the gradient is always a symmetric matrix, even if $\mathbf A$ is not symmetric. This is useful in many applications where symmetric matrices are preferred. But if $\mathbf A$ is constrained to be a symmetric matrix, $2\mathbf A$ can be written.

### When $\mathbf{A}$ is Symmetric

As an example, consider the quadratic form $f(\mathbf{x})=x_1^2+2x_1x_2+3x_2^2$, which can be written in the form $\mathbf{x}^T \mathbf{A} \mathbf{x}$, where:

$$
\mathbf x=\begin{bmatrix} x_1 \ x_2 \end{bmatrix}, \mathbf A=\begin{bmatrix} 1 & 1 \\ 1 & 3 \end{bmatrix}
$$


The derivative of $f(\mathbf{x})$ with respect to $\mathbf{x}$ is then:

$$
\frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}=(\mathbf{A}+\mathbf{A}^T)\mathbf{x}=\begin{bmatrix}2 & 2\\2 & 6\end{bmatrix}\begin{bmatrix}x_1 \\ x_2\end{bmatrix}=\begin{bmatrix}2x_1+2x_2\\2x_1+6x_2\end{bmatrix}
$$

This represents the gradient vector of $f(\mathbf{x})$ at any point $\mathbf{x}$.

### When $\mathbf{A}$ is Not Symmetric

Let $\mathbf{A} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$ and $\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$.

Then, we have $\mathbf{x}^T \mathbf{A} \mathbf{x} = \begin{bmatrix} x_1 & x_2 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} x_1 & x_2 \end{bmatrix} \begin{bmatrix} x_1 + 2x_2 \\ 3x_1 + 4x_2 \end{bmatrix} = x_1^2 + 5x_1x_2 + 4x_2^2$.

To find the gradient of this quadratic form, we can take the partial derivatives of $x_1$ and $x_2$ with respect to each variable:

$$
\frac{\partial}{\partial x_1} (\mathbf{x}^T \mathbf{A} \mathbf{x}) = 2x_1 + 5x_2
$$

$$
\frac{\partial}{\partial x_2} (\mathbf{x}^T \mathbf{A} \mathbf{x}) = 5x_1 + 8x_2
$$

$$
\frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}=(\mathbf{A}+\mathbf{A}^T)\mathbf{x}=\begin{bmatrix}1+1 & 2+3\\3+2 & 4+4\end{bmatrix}\begin{bmatrix}x_1 \\ x_2\end{bmatrix}=\begin{bmatrix}2 & 5\\5 & 8\end{bmatrix}\begin{bmatrix}x_1 \\ x_2\end{bmatrix} = \begin{bmatrix} 2x_1 + 5x_2 \\ 5x_1 + 8x_2 \end{bmatrix}
$$

So the gradient of $\mathbf{x}^T \mathbf{A} \mathbf{x}$ is $\nabla_x f(x) = \begin{bmatrix} 2x_1 + 5x_2 \\ 5x_1 + 8x_2 \end{bmatrix}$.


### Ordinary Least Square 

For the  $n \times 1$ vector $\mathbf{y}$, the  $n \times k$ matrix $\mathbf{X}$, the  $k \times 1$ vector $\mathbf{\beta}$, when $L=(\mathbf{y}-\mathbf{X}\mathbf{\beta})^T(\mathbf{y}-\mathbf{X}\mathbf{\beta})$, what is $\frac{\partial L}{\partial \mathbf{\beta}}$?

$$
\begin{aligned}
L &= (\mathbf{y}-\mathbf{X}\mathbf{\beta})^T(\mathbf{y}-\mathbf{X}\mathbf{\beta}) \\
&= \mathbf{y}^T\mathbf{y} - \mathbf{y}^T\mathbf{A}\mathbf{\beta} - \mathbf{\beta}^T\mathbf{X}^T\mathbf{y} + \mathbf{\beta}^T\mathbf{X}^T\mathbf{X}\mathbf{\beta} \\
&= \mathbf{y}^T\mathbf{y} - 2\mathbf{\beta}^T\mathbf{X}^T\mathbf{y} + \mathbf{\beta}^T\mathbf{X}^T\mathbf{X}\mathbf{\beta}
\end{aligned}
$$

Now, we can take the derivative of $L$ with respect to $\mathbf{\beta}$:

$$
\begin{aligned}
\frac{\partial L}{\partial \mathbf{\beta}} &= \frac{\partial}{\partial \mathbf{\beta}} (\mathbf{y}^T\mathbf{y} - 2\mathbf{\beta}^T\mathbf{X}^T\mathbf{y} + \mathbf{\beta}^T\mathbf{X}^T\mathbf{X}\mathbf{\beta}) \\
&= \frac{\partial}{\partial \mathbf{\beta}} (- 2\mathbf{\beta}^T\mathbf{X}^T\mathbf{y} + \mathbf{\beta}^T\mathbf{X}^T\mathbf{X}\mathbf{\beta}) \\
&= - 2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\mathbf{\beta}
\end{aligned}
$$

$\hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$ is the solution to the optimization problem by taking its derivative with respect to \mathbf{\beta} and setting it equal to zero.

Starting with the expression for $L$:

$$
L=(\mathbf{y}-\mathbf{X}\mathbf{\beta})^T(\mathbf{y}-\mathbf{X}\mathbf{\beta})
$$

Expanding the quadratic term gives:

$$
L=\mathbf{y}^T\mathbf{y}-\mathbf{\beta}^T\mathbf{X}^T\mathbf{y}-\mathbf{y}^T\mathbf{X}\mathbf{\beta}+\mathbf{\beta}^T\mathbf{X}^T\mathbf{X}\mathbf{\beta}
$$

Taking the derivative of $L$ with respect to $\mathbf{\beta}$ and setting it to zero gives:

$$
\frac{\partial L}{\partial \mathbf{\beta}} = -2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\mathbf{\beta} = 0
$$

Solving for $\mathbf{\beta}$ gives:

$$
\mathbf{X}^T\mathbf{X}\mathbf{\beta}=\mathbf{X}^T\mathbf{y}
$$

$$
\mathbf{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
$$
Multiplying both sides of the equation by $(\mathbf{X}^T\mathbf{X})$ gives:

$$
(\mathbf{X}^T\mathbf{X})\mathbf{\beta}=\mathbf{X}^T\mathbf{y}
$$

Therefore, we have verified that 
$$
\hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
$$ is the solution to the optimization problem of OLS (Ordinary Least Square).