---
title: "Orthogonality"
subtitle: Orthogonality of the Four Subspaces, Gram-Schmidt, QR Decomposition,
description: |
  template
categories:
  - Mathematics
author: Kwangmin Kim
date: 04/21/2023
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
execute:
  warning: false
---

```{python}
import numpy as np
import matplotlib.animation as animation
import matplotlib_inline
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import sympy as sym # for RREF
import scipy.linalg # for LU
import matplotlib.gridspec as gridspec # used to create non-regular subplots
from scipy.linalg import lstsq # for least square example
```

# Orthogonality

* Orthogonality of the Four Subspaces
  * Orthogonal Vectors
  * Orthogonal Subspaces
    * Orthogonal Components
  * Orthogonal Bases
  * Orthogonal Matrices
* Orthogonal Vector Decomposition,
* QR decomposition
  * 'Q' stands for an orthogonal matrix, and 
  * 'R' stands for an upper triangular matrix. 
* Gram-Schmidt Decomposition, 
* Eigen Decomposition, and 
* Singular Value Decomposition

## Orthogonality of the Four Subspaces

The four subspaces: vectors, subspaces, orthogonal bases, and orthogonal matrices

### Orthogonal Vectors

:::{#def-orthogonalVec}
Two vectors $\mathbf{v}$ and $\mathbf{w}$ in $\mathbb{R}^n$ are said to be orthogonal if their dot product is zero:

$$
\mathbf{v} \cdot \mathbf{w} = \sum_{i=1}^n v_i w_i = 0 \text{ and }||\mathbf{v}||^2+||\mathbf{w}||^2=||\mathbf{v}+\mathbf{w}||^2
$$
:::

Geometrically, two vectors are orthogonal if they are perpendicular to each other.

Orthogonality is an important concept in linear algebra and has many applications, including in the construction of orthonormal bases and in least-squares regression.

#### Examples

**Example1**
$\mathbf{v} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$ and $\mathbf{w} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}$ are orthogonal because $\mathbf{v} \cdot \mathbf{w} = 1 \cdot 0 + 0 \cdot 1 + 0 \cdot 0 = 0$. These vectors are also perpendicular to each other in 3D space.

**Example2**
$\mathbf{v} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$ and $\mathbf{w} = \begin{bmatrix} 1 \\ -2 \\ 1 \end{bmatrix}$ are orthogonal because $\mathbf{v} \cdot \mathbf{w} = 1 \cdot 1 + 1 \cdot (-2) + 1 \cdot 1 = 0$. These vectors are also perpendicular to each other in 3D space.

**Example3**
$\mathbf{v} = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$ and $\mathbf{w} = \begin{bmatrix} -2 \\ 1 \end{bmatrix}$ are orthogonal because $\mathbf{v} \cdot \mathbf{w} = 1 \cdot (-2) + 2 \cdot 1 = 0$. These vectors are also perpendicular to each other in 2D space.

```{python}
# Example 1
v1 = np.array([1, 0, 0])
w1 = np.array([0, 1, 0])
print(np.dot(v1, w1))  # Output: 0

# Example 2
v2 = np.array([1, 1, 1])
w2 = np.array([1, -2, 1])
print(np.dot(v2, w2))  # Output: 0

# Example 3
v3 = np.array([1, 2])
w3 = np.array([-2, 1])
print(np.dot(v3, w3))  # Output: 0

# Example 1
v2 = np.array([1, 1, 1])
w2 = np.array([1, -2, 1])
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.quiver(0, 0, 0, v2[0], v2[1], v2[2], colors='b', arrow_length_ratio=0.1)
ax.quiver(0, 0, 0, w2[0], w2[1], w2[2], colors='r', arrow_length_ratio=0.1)
ax.set_xlim([-1, 2])
ax.set_ylim([-1, 2])
ax.set_zlim([-1, 2])
ax.set_title("Example 1: Orthogonal Vectors")
ax.legend(["v", "w"])
plt.show()

# Example 2
v1 = np.array([1, 0])
w1 = np.array([0, 1])
plt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='b')
plt.quiver(0, 0, w1[0], w1[1], angles='xy', scale_units='xy', scale=1, color='r')
plt.xlim(-1, 2)
plt.ylim(-1, 2)
plt.title("Example 2: Orthogonal Vectors")
plt.legend(["v", "w"])
plt.grid(True)
plt.show()


# Example 3
v3 = np.array([1, 2])
w3 = np.array([-2, 1])
plt.quiver(0, 0, v3[0], v3[1], angles='xy', scale_units='xy', scale=1, color='b')
plt.quiver(0, 0, w3[0], w3[1], angles='xy', scale_units='xy', scale=1, color='r')
plt.xlim(-3, 3)
plt.ylim(-3, 3)
plt.title("Example 3: Orthogonal Vectors")
plt.legend(["v", "w"])
plt.grid(True)
plt.show()
```

#### Properties

* Orthogonal vectors have a dot product of zero:
$$
\mathbf{v} \cdot \mathbf{w} = 0
$$

* The magnitude (length) of the projection of a vector onto an orthogonal vector is given by:
$$
\text{proj}_{\mathbf{w}}(\mathbf{v}) = \frac{\mathbf{v} \cdot \mathbf{w}}{\|\mathbf{w}\|^2} \mathbf{w} = 0
$$

* The Pythagorean theorem holds for orthogonal vectors:
$$
\|\mathbf{v} + \mathbf{w}\|^2 = \|\mathbf{v}\|^2 + \|\mathbf{w}\|^2
$$

* The angle between two orthogonal vectors is $\frac{\pi}{2}$ radians or $90$ degrees:
$$
\theta = \frac{\pi}{2}
$$

* Orthogonal vectors are linearly independent, which means that no vector in the span of one vector can be expressed as a linear combination of the other vector:
$$
\text{span}\{\mathbf{v}\} \cap \text{span}\{\mathbf{w}\} = \{\mathbf{0}\}
$$

![Orthogonal Space-Gilbert Strang: Introduction to Linear Algebra](../../../../../images/linear_algebra/orthogonal_space.PNG)

* The row space is perpendicular to the nullspace
* The column space is perpendicular to the nullspace of $\mathbf{A}^T$.
  * This peroperty $\mathbf{A}$ plays a key role in solving the equation $\mathbf{Ax=b}$ but $\mathbf{b}$ is outside the column space (meaning we can't solve the equation directly). In this case, we use the nullspace of $\mathbf{A}^T$ to find the "least-squares" solution, which gives us the smallest possible error $\mathbf{e = b - Ax}$ in the solution.

:::{.callout-note}
When $\mathbf{b}$ is outside the column space of $\mathbf{A}$, there is no exact solution to the equation $\mathbf{Ax = b}$. Instead, we seek a solution that minimizes the error $\mathbf{e = b - Ax}$. The least-squares solution achieves this by finding the projection of $\mathbf{b}$ onto the column space of $\mathbf{A}$. It turns out that the projection of $\mathbf{b}$ onto the column space of $\mathbf{A}$ is exactly equal to the solution of the equation $\mathbf{A}^T\mathbf{Ax} = \mathbf{A}^T\mathbf{b}$, which can be solved using the nullspace of $\mathbf{A}^T$.

In summary, the statement "the column space is perpendicular to the nullspace of $\mathbf{A}^T$" tells us that the column space and nullspace are orthogonal (i.e., perpendicular) subspaces, and this fact allows us to use the nullspace of $\mathbf{A}^T$ to find the least-squares solution to $\mathbf{Ax = b}$.
:::

##### Least Square Example

Suppose we have a system of equations $\mathbf{Ax} = \mathbf{b}$ where $\mathbf{A}$ is an $m \times n$ matrix and $\mathbf{b}$ is an $m \times 1$ vector, and we want to find the least squares solution to this system (i.e., the solution that minimizes the residual $|\mathbf{Ax} - \mathbf{b}|$). If $\mathbf{A}$ has linearly independent columns, then we can solve for $\mathbf{x}$ using the formula $\mathbf{x} = (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b}$. 

However, if $\mathbf{A}$ does not have linearly independent columns, then we can use the fact that the column space of $\mathbf{A}$ is perpendicular to the nullspace of $\mathbf{A}^T$ to find the least squares solution.

To do this, we first find a basis for the column space of $\mathbf{A}$ and a basis for the nullspace of $\mathbf{A}^T$. Let $\mathbf{P}$ be the projection matrix onto the column space of $\mathbf{A}$, given by $\mathbf{P} = \mathbf{A}(\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T$. Then the least squares solution to $\mathbf{Ax} = \mathbf{b}$ is given by $\mathbf{x} = \mathbf{P}\mathbf{b}$, and the residual $\mathbf{e} = \mathbf{b} - \mathbf{Ax}$ is in the nullspace of $\mathbf{A}^T$.


```{python}
#| echo: false
#| eval: false

# Define the matrix A and the vector b
A = np.array([[1, 2, 3],
              [4, 5, 6],
              [7, 8, 9],
              [10, 11, 12]])
b = np.array([1, 2, 3, 4])

# Compute the least-squares solution using lstsq from SciPy
x, res, rank, s = lstsq(A, b)

# Compute the error e = b - Ax
e = b - A @ x # matrix multiplication

# Compute the projection of b onto the column space of A
Projected_b = A @ x

# Compute the projection of b onto the orthogonal complement of the column space of A
Projected_b_perp = b - Projected_b

# Compute the projection of e onto the nullspace of A^T
Projected_e = np.linalg.pinv(A.T).T @ e

'''
The expected value of Projected_e = np.linalg.pinv(A.T) @ e is the projection of the true value x 
onto the column space of A. 
This is because the least squares solution x_hat is the orthogonal projection of the vector b onto the column space of A, 
which is given by x_hat = A @ np.linalg.lstsq(A, b)[0].

Since e = b - Ax_hat is the error in the least squares solution, 
np.linalg.pinv(A.T) @ e computes the projection of this error vector onto the nullspace of A^T. 
Therefore, the expected value of Projected_e is zero, since the error e is orthogonal to the column space of A and 
its projection onto the nullspace of A^T is also orthogonal to the column space of A.

In other words, Projected_e is the component of the error e that lies in the nullspace of A^T, and 
since the nullspace of A^T is orthogonal to the column space of A, the expected value of Projected_e is zero.
'''

# Compute the projection of e onto the orthogonal complement of the nullspace of A^T
Projected_e_perp = e - A @ Projected_e

# Verify that the column space is perpendicular to the nullspace of A^T
assert np.allclose(A @ Projected_e, np.zeros((A.shape[0],)))
#assert np.allclose(Projected_e_perp @ x, np.zeros((x.shape[0],)))

'''
In this example, we define the matrix `A` and the vector `b`, and use the lstsq function from SciPy to compute the least-squares solution `x`. We then compute the error `e = b - Ax`, and project `b` onto the column space of `A` to obtain `Projected_b`, and onto the orthogonal complement of the column space of `A` to obtain `Projected_b_perp`. We also project `e` onto the nullspace of `A^T` to obtain `Projected_e`, and onto the orthogonal complement of the nullspace of `A^T` to obtain `Projected_e_perp`. Finally, we verify that the column space of `A` is perpendicular to the nullspace of `A^T` by checking that `A^T` `Projected_e = 0` and `Projected_e_perp @ x = 0`.
'''
```



```{python}
#| eval: false
#| echo: false

# define the matrix A and vector b
A = np.array([[1, 1], [1, -1], [2, 1], [2, -1]])
b = np.array([3, 1, 5, 3])

# calculate the least-squares solution
x = np.linalg.lstsq(A, b, rcond=None)[0]

# create a figure with a 3D axes object
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# plot the points in the column space of A
x1 = np.linspace(-1, 3, 10)
x2 = np.linspace(-1, 3, 10)
X1, X2 = np.meshgrid(x1, x2)
Y = x[0]*X1 + x[1]*X2
ax.plot_surface(X1, X2, Y, alpha=0.2)

# plot the points in the nullspace of A^T
x1 = np.linspace(-1, 3, 10)
x3 = np.linspace(-1, 3, 10)
X1, X3 = np.meshgrid(x1, x3)
Y = np.zeros_like(X1)
Z = x[0]*X1 + x[1]*Y + x[2]*X3
ax.plot_surface(X1, Y, X3, color='red', alpha=0.2)

# plot the data points and the least-squares solution
ax.scatter(A[:,0], A[:,1], b, color='blue', marker='o', s=50)
ax.scatter(x[0], x[1], x[2], color='green', marker='*', s=100)

# set the axis labels and limits
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('b')
ax.set_xlim(-1, 3)
ax.set_ylim(-1, 3)
ax.set_zlim(0, 6)

# show the plot
plt.show()
```

### Orthogonal Subspaces

:::{#def-orthogonalSubspaces}
Two subspaces $U$ and $V$ of a vector space $W$ are said to be orthogonal subspaces if every vector in $U$ is orthogonal to every vector in $V$. Symbolically, we write $U \perp V$ if and only if $\mathbf{u} \cdot \mathbf{v} = 0$ for all $\mathbf{u} \in U$ and $\mathbf{v} \in V$.

$$
\mathbf u^T \mathbf v = 0
$$
:::

Every vector $\mathbf{x}$ in the nullspace is perpendicular to every row of $\mathbf{A}$, because $\mathbf{Ax=0}$. 
The $\operatorname{null}(\mathbf{A})$ and the row space $\operatorname{Col}(\mathbf{A}^T)$ are orthogonal subspaces of $\mathbb{R}^n$

Every vector $\mathbf{y}$ in the nullspace of $\mathbf{A}^T$ is perpendicular to every column of $\mathbf{A}$. The left $\operatorname{null}(\mathbf{A}^T)$ and the column space $\operatorname{Col}(\mathbf{A})$ are orthogonal subspaces in $\mathbb{R}^n$

![Null(A) $\perp$ Col(A^T)-Gilbert Strang: Introduction to Linear Algebra](../../../../../images/linear_algebra/Null(A)_Col(A%5ET)_orthogonal.PNG)


#### Examples

**Example1**
Let $\mathbf{v} = \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}$ and $\mathbf{u} = \begin{bmatrix} 0 \\ 1 \\ -1 \end{bmatrix}$ be two vectors in $\mathbb{R}^3$. Then the subspaces $U = \text{span}\{\mathbf{u}\}$ and $V = \text{span}\{\mathbf{v}\}$ are orthogonal subspaces, since $\mathbf{u} \cdot \mathbf{v} = 0$.

$$
U = \text{span}\{\mathbf{u}\} = \text{span}\left\{\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}\right\}, \quad
V = \text{span}\{\mathbf{v}\} = \text{span}\left\{\begin{bmatrix} 0 \\ 1 \\ -1 \end{bmatrix}\right\}
$$

**Example2**
Let $U$ be the subspace of $\mathbb{R}^3$ spanned by the vectors $\begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix}$ and $\begin{bmatrix} 1 \\ 0 \\ -2 \end{bmatrix}$, and let $V$ be the subspace spanned by the vector $\begin{bmatrix} 2 \\ -1 \\ 1 \end{bmatrix}$. Then $U$ and $V$ are orthogonal subspaces, since every vector in $U$ is orthogonal to every vector in $V$.

$$
U = \text{span}\left\{\begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \\ -2 \end{bmatrix}\right\}, \quad
V = \text{span}\left\{\begin{bmatrix} 2 \\ -1 \\ 1 \end{bmatrix}\right\}
$$

**Example3**
Let $U$ and $V$ be the subspaces of $\mathbb{R}^2$ spanned by the vectors $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$ and $\begin{bmatrix} 1 \\ -1 \end{bmatrix}$, respectively. Then $U$ and $V$ are orthogonal subspaces, since $\begin{bmatrix} 1 & 1 \end{bmatrix} \cdot \begin{bmatrix} 1 \\ -1 \end{bmatrix} = 0$.

$$
U = \text{span}\left\{\begin{bmatrix} 1 \\ 1 \end{bmatrix}\right\}, \quad
V = \text{span}\left\{\begin{bmatrix} 1 \\ -1 \end{bmatrix}\right\}
$$

```{python}
u = np.array([1, 1])
v = np.array([1, -1])

# plot vectors
plt.figure()
plt.plot([0, u[0]], [0, u[1]], 'b', label=r'$\mathbf{u}$')
plt.plot([0, v[0]], [0, v[1]], 'r', label=r'$\mathbf{v}$')
plt.legend()

# plot subspaces
plt.axline((0, 0), slope=u[1]/u[0], color='b', linestyle='--', label=r'$U$')
plt.axline((0, 0), slope=v[1]/v[0], color='r', linestyle='--', label=r'$V$')

plt.xlim(-1.5, 1.5)
plt.ylim(-1.5, 1.5)
plt.gca().set_aspect('equal', adjustable='box')
plt.show()
```

The vectors $\mathbf{u}$ and $\mathbf{v}$ are in blue and red, respectively, and the subspaces $U$ and $V$ as dashed lines with corresponding colors. Since $\mathbf{u} \cdot \mathbf{v} = 0$, the subspaces are orthogonal.

### Orthogonal Complements

Given a subspace $V$ of a vector space $W$, we can decompose any vector $\mathbf{w} \in W$ into two orthogonal components, one in $V$ and one in the orthogonal complement of $V$.

:::{#def-orthogonalComplements}
Let $V$ be a subspace of a vector space $W$. The orthogonal complement of $V$, denoted by $V^\perp$, is the set of all vectors in $W$ that are orthogonal to every vector in $V$. That is, the orthogonal complement of $V$ is the set of vectors $\mathbf w \in W$ such that the inner product between $\mathbf w$ and any vector $\mathbf v \in V$ is equal to zero:
$$
V^\perp = \{ \mathbf w \in W | \langle \mathbf w,\mathbf v \rangle = 0, \forall \mathbf v \in V \}.
$$

where  $V^\perp$ represents the orthogonal complement of the subspace $V$, $w$ and $v$ are vectors in the subspaces $W$ and $V$ respectively, and $\langle \rangle$ denote the inner product between two vectors.

In other words, The orthogonal complement of a subspace $V$ contains every vector that is perpendicular to $V$. This orthogonal subspace is denoted by $V^\perp$.
:::

We can then decompose any vector $\mathbf{w} \in W$ into two orthogonal components as follows:
$$
\mathbf w = \mathbf w_{V} +\mathbf w_{V^{\perp}}  
$$

where $\mathbf{w}_{V}$ is the orthogonal projection of $\mathbf{w}$ onto $V$, and $\mathbf{w}_{V^\perp}$ is the orthogonal projection of $\mathbf{w}$ onto $V^\perp$.

If $\mathbf v$ is orthogonal to the nullspace, it must be in the row space.

:::{#thm-orthogonalComplement}
Let $\mathbf A$ be a matrix and let $\mathbf W = \operatorname{Col}(A)$. Then, $\mathbf W^{\perp} = \operatorname{Null}(\mathbf A)$.
:::
By the proposition, computing the orthogonal complement of a span means solving a system of linear equations.

#### Example

Let $W = \mathbb{R}^3$ and $V$ be the subspace spanned by the vectors $\mathbf{v}_1 = (1,0,0)$ and $\mathbf{v}_2 = (0,1,1)$. Then, we can find a basis for $V^\perp$ by solving the system of equations

$$
\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 1  \end{bmatrix} \begin{bmatrix} x \\ y \\ z \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
$$

Compute $W^{\perp}$, where $W=\operatorname{Span}\left\{ \begin{bmatrix}1 \\ 0 \\ 0 \end{bmatrix},\begin{bmatrix}0 \\ 1 \\ 1 \end{bmatrix}  \right\}$

Compute $\operatorname{Null}(\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 1  \end{bmatrix})$
$$
\begin{align*}
x &= 0 \\
y + z &= 0\\
W^{\perp}&=\operatorname{\begin{bmatrix} 0 \\ -z \\ z\end{bmatrix}}
\end{align*}
$$,
which has the unique solution $x = 0$, $y = -z$. Therefore, the subspace $V^\perp$ is spanned by the vector $\begin{bmatrix} 0 \\ -1 \\ 1 \end{bmatrix}$.

To see this, note that any vector $\begin{bmatrix} x \\ y \\ z \end{bmatrix}$ in $V^\perp$ must satisfy $\begin{bmatrix} x \\ y \\ z \end{bmatrix} \cdot \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} = 0$ and $\begin{bmatrix} x \\ y \\ z \end{bmatrix} \cdot \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix} = 0$. These conditions can be rewritten as the equations $x = 0$ and $y = -z$, respectively. Therefore, any vector in $V^\perp$ must have the form $\begin{bmatrix} 0 \\ -z \\ z \end{bmatrix}$, and it is easy to check that $\begin{bmatrix} 0 \\ -1 \\ 1 \end{bmatrix}$ satisfies this equation and is linearly independent from $\mathbf{v}_1$ and $\mathbf{v}_2$, so it is a basis for $V^\perp$.

Therefore, we have $V^\perp = \operatorname{span}\left(\begin{bmatrix} 0 \\ -1 \\ 1 \end{bmatrix}\right)$.

Given any vector $\mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ w_3 \end{bmatrix} \in \mathbb{R}^3$, we can decompose it into two orthogonal components as

$$
\mathbf w = \mathbf w_{V} +\mathbf w_{V^{\perp}}  
$$

where $\mathbf{w}_V$ is the projection of $\mathbf{w}$ onto $V$, and $\mathbf{w}_{V^\perp}$ is the projection of $\mathbf{w}$ onto $V^\perp$.

To compute $\mathbf{w}_V$, note that $\mathbf{w}$ can be written as a linear combination of $\mathbf{v}_1$ and $\mathbf{v}_2$ as

$$
\begin{align*}
\mathbf{w} &= \frac{\mathbf{w} \cdot \mathbf{v}_1}{||\mathbf{v}_1||^2} \mathbf{v}_1 + \frac{\mathbf{w} \cdot \mathbf{v}_2}{||\mathbf{v}_2||^2} \mathbf{v}_2 \\
&= \frac{1}{1^2}\begin{bmatrix}1 \\ 0 \\ 0 \end{bmatrix}+ \frac{1}{2}\begin{bmatrix}0 \\ 1 \\ 1 \end{bmatrix} \\
&= \begin{bmatrix}1 \\ \frac{1}{2} \\ \frac{1}{2}\end{bmatrix}
\end{align*}
$$

Therefore, the subspace spanned by $\mathbf{w}$ is the line passing through the point $\begin{bmatrix}1 \\ \frac{1}{2} \\ \frac{1}{2}\end{bmatrix}$ in the direction of $\mathbf{w}$, which is $\text{span}\left \{\begin{bmatrix}1 \\ \frac{1}{2} \\ \frac{1}{2}\end{bmatrix}\right \}$.

The projection of $\mathbf{w}$ onto $V$ is given by

$$
\mathbf{w}_V = \operatorname{proj}_{V}(\mathbf{w}) = \frac{\langle \mathbf{w},\mathbf{v}_1\rangle}{\|\mathbf{v}_1\|^2} \mathbf{v}_1 + \frac{\langle \mathbf{w},\mathbf{v}_2\rangle}{\|\mathbf{v}_2\|^2} \mathbf{v}_2 = \frac{1}{1^2+0^2+0^2} \begin{bmatrix} 1 \\ 0 \\ 0\end{bmatrix} =\begin{bmatrix} 1 \\ 0 \\ 0\end{bmatrix}
$$

The projection of $\mathbf{w}$ onto $V^\perp$ is given by

$$
\mathbf w = \mathbf w_{V} -\mathbf w_{V^{\perp}}  = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} - \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} = \begin{bmatrix} 0 \\ 2 \\ 3 \end{bmatrix}
$$

Therefore, we have decomposed $\mathbf{w}$ into two orthogonal components as $\mathbf{w} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} + \begin{bmatrix} 0 \\ 2 \\ 3 \end{bmatrix}$.

:::{#thm-fundamentralThm}
The Fundamental Theorem.  
Let $\mathbf A$ be an $m\times n$ matrix over $\mathbb{R}$. Then,

* The column space of $\mathbf A$, denoted $\operatorname{Col}(\mathbf A)$, is a subspace of $\mathbb{R}^m$.
* The null space of $\mathbf A$, denoted $\operatorname{Null}(\mathbf A)$, is a subspace of $\mathbb{R}^n$.
* The orthogonal complement of $\operatorname{Col}(\mathbf A)$, denoted $\operatorname{Col}(\mathbf A)^\perp$, is equal to $\operatorname{Null}(\mathbf A)$.
* The orthogonal complement of $\operatorname{Null}(\mathbf A)$, denoted $\operatorname{Null}(\mathbf A)^\perp$, is equal to $\operatorname{Col}(\mathbf A)$.

In other words, we have the following orthogonal decomposition of $\mathbb{R}^n$:
$$
\mathbb R^n = N(\mathbf A) \oplus C(\mathbf A)^\perp
$$

and the following orthogonal decomposition of $\mathbb{R}^m$:

$$
\mathbb R^m = C(\mathbf A) \oplus N(\mathbf A)^\perp
$$

where $\oplus$ denotes the direct sum of subspaces.

:::

The Fundamental Theorem states that for a given matrix $\mathbf A$, the column space of $\mathbf A$ and the null space of $\mathbf A$ are orthogonal complements of each other. In other words, every vector in the null space of $\mathbf A$ is orthogonal to every vector in the column space of $\mathbf A$, and vice versa. This means that any vector in the domain of $\mathbf A$ can be uniquely decomposed as the sum of a vector in the column space and a vector in the null space.

The point of **complements** is that every $\mathbf x$ can be split into a row space component $\mathbf x^r$ and a nullspace component $\mathbf x^n$. When $\mathbf  A$ multiplies $\mathbf x = \mathbf x^r + \mathbf x^n$, Figure 4.3 shows what happens:

![Null Space Complement-Gilbert Strang: Introduction to Linear Algebra](../../../../../images/linear_algebra/nullspace_complement.PNG)

* The nullspace component goes to zero: $\mathbf{Ax}_n = \mathbf{0}$.
* The row space component goes to the column space: $\mathbf{Ax}_r = \mathbf{Ax}$ Ax r = Ax.
* Every vector $\mathbf b$ in the column space comes from one and only one vector in the row space.
* pseudoinverse: there is an $r$ by $r$ invertible matrix hiding inside $\mathbf A$, if we throwaway the two nullspaces. From the row space to the column space, $\mathbf A$ is invertible.

#### Exmaple

**Example1**
Let $\mathbf A$ be an $m \times n$ matrix with rank $r$. Then, $\mathbb{R}^n$ can be decomposed as $\mathbb{R}^n = N(\mathbf A) \oplus N(\mathbf A)^{\perp}$, where $N(\mathbf A)$ is the null space of $\mathbf A$, $N(\mathbf A)^{\perp}$ is its orthogonal complement, and $\oplus$ denotes the direct sum. This means that any vector $\mathbf{v} \in \mathbb{R}^n$ can be written uniquely as $\mathbf{v} = \mathbf{v}_1 + \mathbf{v}_2$, where $\mathbf{v}_1 \in N(\mathbf A)$ and $\mathbf{v}_2 \in N(\mathbf A)^{\perp}$.

**Example2**
Let $\mathbf A$ be an $m \times n$ matrix with rank $r$. Then, the column space of $\mathbf A$, denoted $C(\mathbf A)$, is equal to the orthogonal complement of the null space of $\mathbf A^T$, i.e., $C(\mathbf A) = N(\mathbf A^T)^{\perp}$.

**Example3**
Let $\mathbf A$ be an $m \times n$ matrix with rank $r$, and let $\mathbf{b} \in \mathbb{R}^m$ be a vector. Then, the system of linear equations $A\mathbf{x} = \mathbf{b}$ has a solution if and only if $\mathbf{b} \in C(\mathbf A)$. Moreover, if $\mathbf{x}_0$ is a particular solution to $\mathbf A\mathbf{x} = \mathbf{b}$, then the set of all solutions is given by ${\mathbf{x}_0 + \mathbf{v} : \mathbf{v} \in N(\mathbf A)}$, i.e., it is the affine space consisting of $\mathbf{x}_0$ plus the null space of $\mathbf A$.

**Example4**
Every diagonal matrix $\mathbf D$ has a diagonal submatrix consisting of its first $r$ diagonal entries that is $r \times r$ and invertible for any $r$ between $1$ and the size of $\mathbf D$. For example, consider the diagonal matrix $\mathbf D = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 4 \end{bmatrix}$. The $2 \times 2$ diagonal submatrix consisting of the first two diagonal entries, $\mathbf D' = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}$, is invertible since its diagonal entries are nonzero. Similarly, the $3 \times 3$ diagonal submatrix consisting of all the diagonal entries, $\mathbf D'' = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 4 \end{bmatrix}$, is also invertible since all of its diagonal entries are nonzero. This example illustrates the fact that every diagonal matrix has an invertible diagonal submatrix of any size between $1$ and the size of the matrix.

**Example5**
Consider the matrix $\mathbf A=\begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}$. We want to find the right bases for $\mathbb{R}^2$ and $\mathbb{R}^3$ such that $\mathbf A$ becomes a diagonal matrix.

We begin by computing $\mathbf A^T\mathbf A$:

$$
\mathbf A^T \mathbf A = \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix}\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}  = \begin{bmatrix} 17 & 22 & 27 \\ 22 & 29 & 36 \\ 27 & 36 & 45\end{bmatrix}
$$

The eigenvalues of $\mathbf A^T\mathbf A$ are $\lambda_1 = 0$, $\lambda_2 = 1$, and $\lambda_3 = 90$. We can find the corresponding eigenvectors as follows:

- For $\lambda_1 = 0$, we solve $(\mathbf A^T\mathbf A - \lambda_1\mathbf I)\mathbf v = 0$, which gives us the equation $17x + 22y + 27z = 0$. One possible eigenvector is $\begin{bmatrix} -2 \\ 1 \\ 0 \end{bmatrix}$.
- For $\lambda_2 = 1$, we solve $(\mathbf A^T\mathbf A - \lambda_2\mathbf I)\mathbf v = 0$, which gives us the equation $16x + 20y + 24z = 0$. One possible eigenvector is $\begin{bmatrix} 3 \\ -2 \\ 0 \end{bmatrix}$.
- For $\lambda_3 = 90$, we solve $(\mathbf A^T\mathbf A - \lambda_3 \mathbf I)\mathbf v = 0$, which gives us the equation $-2x + y + z = 0$. One possible eigenvector is $\begin{bmatrix} 1 \\ 2 \\ -1 \end{bmatrix}$.

We normalize these eigenvectors to obtain an orthonormal basis for $\mathbb{R}^3$:

$$
\mathbf v_1 = \frac{1}{\sqrt{5}}\begin{bmatrix} -2 \\ 1 \\ 0 \end{bmatrix}, \quad \mathbf v_2 = \frac{1}{\sqrt{13}}\begin{bmatrix} 3 \\ -2 \\ 0 \end{bmatrix}, \quad \mathbf v_3 = \frac{1}{\sqrt{6}}\begin{bmatrix} 1 \\ 2 \\ -1 \end{bmatrix}
$$

Next, we compute $\mathbf{Av}_i$ for each $i=1,2,3$:

$$
\mathbf{Av}_1 = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}\begin{bmatrix} \frac{-2}{\sqrt{5}} \\ \frac{1}{\sqrt{5}} \\ 0 \end{bmatrix} = \begin{bmatrix} \frac{-2}{\sqrt{5}} \\ \frac{8}{\sqrt{5}} \end{bmatrix} = \frac{2}{\sqrt{5}}\begin{bmatrix} -1 \\ 2 \end{bmatrix}
$$

$$
\mathbf{Av}_2 = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}\begin{bmatrix} -0.6931 \\ -0.1184 \\ 0.7107 \end{bmatrix} \approx \begin{bmatrix} -3.1623 \\ -7.4162 \end{bmatrix} \approx -3.1623v_1,
$$

and

$$
\mathbf{Av}_3 = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}\begin{bmatrix} -0.6931 \\ 0.6646 \\ -0.2774 \end{bmatrix} \approx \begin{bmatrix} -4.7246 \\ 4.6707 \end{bmatrix} \approx 4.6707v_1,
$$

where $\mathbf{v}_1=\begin{bmatrix} 0.2673 \\ 0.5345 \\ 0.8018 \end{bmatrix}$.

Therefore, we can take $\mathbf{v}_1$ as the first column of the matrix $\mathbf{V}$, and the normalized eigenvectors $\mathbf v_2$ and $\mathbf v_3$ as the second and third columns of $\mathbf V$, respectively. Then we can define $\mathbf{U=AV\Sigma}^{-1}$, where $\mathbf \Sigma$ is the diagonal matrix with the square roots of the nonzero eigenvalues of $\mathbf A^T\mathbf A$ as its entries.

< 여기서 부터 다시 볼것>
Thus, we have

$$
\mathbf{A} = \mathbf{U\Sigma V}^T = \begin{bmatrix} -0.231 \ \ \ 0.9730 \\ -0.5253 \ \ \ 0.0806 \\ -0.8196 -0.9195 \end{bmatrix} \begin{bmatrix} 9.4868 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} -0.2673 \ \ \ 0.5345 \ \ \ 0.8018 \\ -0.6931 \ \ \ -0.1184 \ \ \ 0.7107 \\ -0.6646 \ \ \ 0.7774 \ \ \ -0.2774 \end{bmatrix}.
$$
This gives us the diagonalization $\mathbf A=\mathbf{QDQ}^{-1}$, where $\mathbf{Q}=\mathbf{U\Sigma}$ and $\mathbf{D}=\mathbf{V}^T$. Therefore, by choosing the appropriate bases for $\mathbb{R}^2$ and $\mathbb{R}^3$ given by the columns of $\mathbf Q$, we can make $\mathbf A$ a diagonal matrix.

< 여기까지>

$$
\mathbf A = \mathbf{U\Sigma V}^T = \begin{bmatrix} -0.231 & 0.973 & 0 \\ 0.732 & 0.182 & -0.655 \\ 0.641 & 0.136 & 0.755 \end{bmatrix} \begin{bmatrix} 3.89 & 0 & 0 \\ 0 & 1.27 & 0 \\ 0 & 0 & 0.43 \end{bmatrix} \begin{bmatrix} -0.227 & -0.592 & -0.773 \\ -0.904 & 0.275 & 0.329 \\ 0.361 & 0.758 & -0.541 \end{bmatrix}
$$

This is known as the Singular Value Decomposition (SVD) of $\mathbf A$. The diagonal matrix $\mathbf \Sigma$ contains the singular values of $\mathbf A$, which are the square roots of the eigenvalues of $\mathbf A^T\mathbf A$. These values represent the importance of the corresponding singular vectors in the matrix $\mathbf A$.

The SVD can be used for a variety of applications, including data compression, dimensionality reduction, and image processing. It is also used in machine learning and data science for tasks such as collaborative filtering, recommender systems, and principal component analysis.

### Combining bases from Subspaces

**Any $n$ independent vectors in $\mathbf R^n$ must span $\mathbf R^n$. So, they are a basis.**

In $\mathbf R^n$, a set of $n$ independent vectors is said to span $\mathbf R^n$ if any vector in $\mathbf R^n$ can be expressed as a linear combination of these $n$ vectors. This means that the $n$ vectors are sufficient to represent any vector in $\mathbf R^n$.

To see why this is the case, consider that any vector in $\mathbf R^n$ can be represented as a column vector with $n$ entries. By definition, each entry can be written as a linear combination of the entries of the $n$ independent vectors. Therefore, the entire column vector can be expressed as a linear combination of the $n$ independent vectors. Since this is true for any vector in $\mathbf R^n$, the set of $n$ independent vectors must span $\mathbf R^n$.

Moreover, if a set of $n$ independent vectors spans $\mathbf R^n$, then they are a basis for $\mathbf R^n$. This means that the $n$ vectors are linearly independent and also span $\mathbf R^n$. By definition, a basis is a set of vectors that can be used to represent any vector in a space and that is linearly independent. So, any set of $n$ independent vectors that spans $\mathbf R^n$ is a basis for $\mathbf R^n$.

**Any $n$ vectors that span $\mathbf R^n$ must be independent. So, they are a basis.**

Let ${v_1,v_2,\dots,v_n}$ be a set of $n$ vectors that span $\mathbf R^n$. This means that any vector $\mathbf x$ in $\mathbf R^n$ can be expressed as a linear combination of the vectors in ${v_1,v_2,\dots,v_n}$, i.e., there exist scalars $a_1,a_2,\dots,a_n$ such that $\mathbf x = a_1v_1+a_2v_2+\cdots+a_nv_n$.

Now suppose that the vectors in ${v_1,v_2,\dots,v_n}$ are not independent. Then there exist scalars $b_1,b_2,\dots,b_n$, not all zero, such that $b_1v_1+b_2v_2+\cdots+b_nv_n=\mathbf 0$, where $\mathbf 0$ denotes the zero vector in $\mathbf R^n$.

We can rewrite this equation as $a_1v_1+a_2v_2+\cdots+a_nv_n=\mathbf 0$, where $a_i=-b_i$ for $i=1,2,\dots,n$. But this implies that the vector $\mathbf x=\mathbf 0$ can be expressed as a nontrivial linear combination of the vectors in ${v_1,v_2,\dots,v_n}$, which contradicts the assumption that these vectors span $\mathbf R^n$.

Therefore, the vectors ${v_1,v_2,\dots,v_n}$ must be independent. Since they span $\mathbf R^n$, they form a basis for $\mathbf R^n$.

**If the $n$ columns of $\mathbf A$ are independent, they span $\mathbf R^n$. So Ax=b is solvable**

If the $n$ columns of a matrix $\mathbf A$ are independent, then they span $\mathbf R^n$, which means that any vector $\mathbf b$ in $\mathbf R^n$ can be expressed as a linear combination of the columns of $\mathbf A$.

Suppose we have a system of linear equations $\mathbf{Ax} = \mathbf{b}$. If the columns of $\mathbf A$ are independent, then we can find a unique linear combination of the columns that equals $\mathbf b$. In other words, we can solve the system of equations for $\mathbf x$. This means that $\mathbf{Ax} = \mathbf{b}$ is solvable for any vector $\mathbf b$ in $\mathbf R^n$.

Therefore, if the columns of $\mathbf A$ are independent, the equation $\mathbf{Ax} = \mathbf{b}$ is solvable for any $\mathbf b \in \mathbf R^n$, and the columns of $\mathbf A$ form a basis for $\mathbf R^n$.

**If the $n$ columns span $\mathbf R^n$, they are independent. So $\mathbf{Ax=b}$ has only one solution.**

If the $n$ columns of $\mathbf A$ span $\mathbf R^n$, it means that any vector in $\mathbf R^n$ can be expressed as a linear combination of those columns. Mathematically, if we denote the $n$ columns of $\mathbf A$ as $\mathbf a_1, \mathbf a_2, \dots, \mathbf a_n$, then for any vector $\mathbf b \in \mathbf R^n$, there exist scalars $x_1, x_2, \dots, x_n$ such that:

$$
\mathbf{b} = x_1 \mathbf{a}_1 + x_2 \mathbf{a}_2 + \cdots + x_n \mathbf{a}_n
$$


Now, let's assume that the columns of $\mathbf A$ are not independent. This means that there exist scalars $x_1, x_2, \dots, x_n$, not all zero, such that:

$$
x_1 \mathbf{a}_1 + x_2 \mathbf{a}_2 + \cdots + x_n \mathbf{a}_n = \mathbf{0}
$$

This implies that the homogeneous system $\mathbf{Ax}=\mathbf 0$ has a nontrivial solution, since we can choose $\mathbf x = \begin{bmatrix} x_1 \ x_2 \ \vdots \ x_n \end{bmatrix} \neq \mathbf 0$ as a solution.

However, this contradicts the assumption that the columns of $\mathbf A$ span $\mathbf R^n$. If there exists a nontrivial solution $\mathbf x$ to $\mathbf{Ax}=\mathbf 0$, it means that the columns of $\mathbf A$ do not span the entire $\mathbf R^n$ space, because they are not able to generate the zero vector. Therefore, the assumption that the columns of $\mathbf A$ are not independent leads to a contradiction.

Hence, we conclude that the columns of $\mathbf A$ must be independent if they span $\mathbf R^n$. This also implies that $\mathbf A$ is invertible, since the equation $\mathbf{Ax}=\mathbf b$ has a unique solution for any $\mathbf b \in \mathbf R^n$.

### Example

$$
\mathbf v_1=\begin{bmatrix} 1 \\0\\1 \end{bmatrix} \quad \mathbf v_2=\begin{bmatrix} 0 \\1\\1 \end{bmatrix} \quad  \mathbf v_3=\begin{bmatrix} 1 \\1\\0 \end{bmatrix}
$$

$$
\begin{align*}
q_1 &= \frac{v_1}{|v_1|} = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}, \\
u_2 &= v_2 - \langle v_2, q_1 \rangle q_1 = \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix} - \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix} = \begin{bmatrix} -\frac{1}{\sqrt{2}} \\ 1 \\ \frac{1}{\sqrt{2}} \end{bmatrix}, \\
q_2 &= \frac{u_2}{|u_2|} = \frac{1}{\sqrt{2}} \begin{bmatrix} -1 \\ 1 \\ 1 \end{bmatrix}, \\
u_3 &= v_3 - \langle v_3, q_1 \rangle q_1 - \langle v_3, q_2 \rangle q_2 = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} - \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix} - \frac{1}{\sqrt{6}}\begin{bmatrix} -1 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}} \ -\frac{1}{\sqrt{3}} \end{bmatrix}, \\
q_3 &= \frac{u_3}{|u_3|} = \frac{1}{\sqrt{3}} \begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix}.
\end{align*}
$$

Therefore, the orthogonal matrix $\mathbf{Q}$ is:

$$
\mathbf{Q}=
\begin{bmatrix} 
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} & 0\\ 
0 & \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}}\\ 
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{3}}
\end{bmatrix}
$$


### Orthogonal Matrices


:::{#def-orthogonalMatrix}
An $n\times n$ matrix $\mathbf{Q}$ is orthogonal if its columns $\mathbf q$ form an orthonormal set. That is, the columns $\mathbf q$ of $\mathbf{Q}$ satisfy 

$$
\langle\mathbf{q}_i,\mathbf{q}_j\rangle =
\begin{cases}
0 \text{ if } i \ne j \\
1 \text{ if } i = j 
\end{cases}
$$

We can organize all of the dot products amongst all pairs of columns by premultiplying the matrix by its transpose. Since matrix multiplication is defined as dot products between all rows of the left matrix with all columns of the right matrix, 

$$
\mathbf{Q}\mathbf{Q}^T=\mathbf{Q}^T\mathbf{Q}=\mathbf{I}
$$

where $\mathbf{I}$ is the $n\times n$ identity matrix.
:::

### Properties

* Orthogonal columns: all columns are pair-wise orthogonal
* Unit-norm columns: the norm (geometric length) of each column is exactly 1.
* $\mathbf{Q}^T\mathbf{Q}=\mathbf{Q}\mathbf{Q}^T=\mathbf{I}$, where $\mathbf{I}$ is the identity matrix of appropriate size.
* $\mathbf{Q}^T=\mathbf{Q}^{-1}$
  * Great propoerty because the matrix inverse is tedious and prone to numerical inaccuracies, whereas the matrix transpose is fast and accurate.
* The determinant of an orthogonal matrix is either $1$ or $-1$.
* If $\mathbf{Q}$ is orthogonal, then its columns form an orthonormal set, i.e., the columns are pairwise orthogonal and each column has unit length.
* Orthogonal matrices preserve lengths and angles. If $\mathbf{x}$ and $\mathbf{y}$ are two vectors, then $||\mathbf{Qx}||=||\mathbf{x}||$ and $\mathbf{x}^T\mathbf{y}=(\mathbf{Qx})^T(\mathbf{Qy})$.

### Example

**Example1** 
Orthogonal matrices include rotation matrices and reflection matrices. 

the $2\times 2$ matrix and the $3\times 3$ matrix:
$$
\begin{bmatrix}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{bmatrix}
\quad
\begin{bmatrix}
\cos \theta & -\sin \theta & 0 \\
\sin \theta & \cos \theta & 0 \\
0 & 0 & 1 
\end{bmatrix}
$$

is an orthogonal matrix that rotates a vector counterclockwise by an angle $\theta$ regardless of the
rotation angle (as long as the same rotation angle is used in all matrix elements). 

```{python}
# Pure rotation matrix

# angle to rotate by
th = np.pi/5

# transformation matrix
T = np.array([ 
              [ np.cos(th),np.sin(th)],
              [-np.sin(th),np.cos(th)]
            ])


# original dots are a vertical line
x = np.linspace(-1,1,20)
origPoints = np.vstack( (np.zeros(x.shape),x) )


# apply the transformation
transformedPoints = T @ origPoints


# plot the points
plt.figure(figsize=(6,6))
plt.plot(origPoints[0,:],origPoints[1,:],'ko',label='Original')
plt.plot(transformedPoints[0,:],transformedPoints[1,:],'s',color=[.7,.7,.7],label='Transformed')

plt.axis('square')
plt.xlim([-1.2,1.2])
plt.ylim([-1.2,1.2])
plt.legend()
plt.title(f'Rotation by {np.rad2deg(th):.0f} degrees.')
plt.show()

# Animating transformations
# function to update the axis on each iteration
def aframe(ph):

  # create the transformation matrix
  T = np.array([
                 [  1, 1-ph ],
                 [  0, 1    ]
                ])
  
  # apply the transformation to the points using matrix multiplication
  P = T@points

  # update the dots
  plth.set_xdata(P[0,:])
  plth.set_ydata(P[1,:])

  # export the plot handles
  return plth


# define XY points
theta  = np.linspace(0,2*np.pi,100)
points = np.vstack((np.sin(theta),np.cos(theta)))


# setup figure
fig,ax = plt.subplots(1,figsize=(12,6))
plth,  = ax.plot(np.cos(x),np.sin(x),'ko')
ax.set_aspect('equal')
ax.set_xlim([-2,2])
ax.set_ylim([-2,2])

# define values for transformation (note: clip off the final point for a smooth animation loop)
phi = np.linspace(-1,1-1/40,40)**2

# run animation!
animation.FuncAnimation(fig, aframe, phi, interval=100, repeat=True)

```

$$
\begin{align*}
\mathbf{Q}&=\begin{bmatrix}
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}\\
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
\end{bmatrix}
\end{align*}
$$

$\mathbf{Q}$ is orthogonal by computing $\mathbf{Q}^T\mathbf{Q}$:
$$
\begin{align*}
\mathbf{Q}^T\mathbf{Q}&=\begin{bmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
-\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
\end{bmatrix}\begin{bmatrix}
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}\\
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
\end{bmatrix}\\
&=\begin{bmatrix}
\frac{1}{2}+\frac{1}{2} & -\frac{1}{2}+\frac{1}{2}\\
-\frac{1}{2}+\frac{1}{2} & \frac{1}{2}+\frac{1}{2}
\end{bmatrix}\\
&=\begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}\\
&=\mathbf{I}
\end{align*}
$$

Therefore, $\mathbf{Q}$ is an orthogonal matrix.

**Example 2**
$$
\begin{bmatrix}
1 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & -1 
\end{bmatrix}
$$,

which is an orthogonal matrix that reflects a vector across the $x$-axis.

**Example 3**
the identity matrix is an example of an orthogonal matrix

**Exmaple 4**
Permutation matrices are also orthogonal. Permutation matrices are used to exchange rows of a matrix.


## Orthogonal Bases

An orthogonal basis is a set of vectors that are pairwise orthogonal (perpendicular) and each vector is non-zero.

:::{#def-orthogonalBases}
A set of vectors ${\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n}$ is said to be an orthogonal basis of a vector space $V$ if:
1. Each vector $\mathbf{v}_i$ is non-zero.
2. Each vector $\mathbf{v}_i$ is orthogonal (perpendicular) to every other vector $\mathbf{v}_j$, $i \neq j$. In other words, $\mathbf{v}_i \cdot \mathbf{v}_j = 0$ for $i \neq j$.
:::

### Example

**Example1**
Let $\mathbf{v} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$ and $\mathbf{u} = \begin{bmatrix} -1 \ 0 \ 1 \end{bmatrix}$. We can check if these vectors form an orthogonal basis by computing their dot product:

$$
\mathbf{v} \cdot \mathbf{u} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} \cdot \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix} = (-1) \cdot 1 + (0) \cdot 2 + (1) \cdot 3 = 2
$$

Since the dot product is not zero, we can conclude that $\mathbf{v}$ and $\mathbf{u}$ do not form an orthogonal basis.

**Example2**

Let $\mathbf{v} = \begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix}$ and $\mathbf{u} = \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix}$. To check if they form an orthogonal basis, we need to compute their dot product:

$$
\mathbf{v} \cdot \mathbf{u} = (1)(-2) + (2)(1) + (0)(1) = -2 + 2 + 0 = 0
$$

Since the dot product is zero, we know that $\mathbf{v}$ and $\mathbf{u}$ are orthogonal. We can also check that they are both nonzero and linearly independent by computing their norms:

$$
||\mathbf{v}|| = \sqrt{1^2 + 2^2 + 0^2} = \sqrt{5} \ne 0
$$

$$
||\mathbf{w}|| = \sqrt{(-2)^2 + 1^2 + 1^2} = \sqrt{6} \ne 0
$$

Therefore, $\mathbf{v}$ and $\mathbf{w}$ form an orthogonal basis for $\mathbb{R}^3$.

## Gram-Schmidt (GS or G-S)

The Gram-Schmidt procedure is a method of transforming a nonorthogonal matrix into an orthogonal matrix by orthonormalizing a set of linearly independent vectors in an inner product space, usually the Euclidean space $\mathbb{R}^n$. The process takes a sequence of vectors $\mathbf v_1, \mathbf v_2, \dots, \mathbf v_n$ and constructs an orthonormal sequence $\mathbf q_1, \mathbf q_2, \dots, \mathbf q_n$ that spans the same subspace as the original sequence.

The Gram-Schmidt procedure is useful for understanding orthogonal vector decomposition when programming and implementing the QR decomposition algorithm, and GS is the right way to conceptualize how and why QR decomposition works even if the low-level implementation is slightly different.

:::{#def-gramSchmidt}
Let $\mathbf v_1, \mathbf v_2, \dots, \mathbf v_n$ be a sequence of linearly independent vectors in $\mathbb{R}^n$. Define $\mathbf q_1$ to be the unit vector in the direction of $\mathbf v_1$, i.e., $\mathbf q_1 = \frac{\mathbf v_1}{|\mathbf v_1|}$. For $k = 2, 3, \dots, n$, define $\mathbf q_k$ as follows:

$$
\mathbf q_k = \frac{\mathbf u_k}{|\mathbf u_k|}
$$

where $\mathbf u_k=\mathbf v_k-\sum_{j=1}^{k-1}\langle \mathbf v_k,\mathbf q_j \rangle\mathbf q_j$ 
:::

The vector $\mathbf u_k$ is the projection of $\mathbf v_k$ onto the subspace orthogonal to $\text{span}{\mathbf q_1, \mathbf q_2, \dots, \mathbf q_{k-1}}$.

The Gram-Schmidt process produces an orthonormal basis $\mathbf q_1, \mathbf q_2, \dots, \mathbf q_n$ for $\text{span}{\mathbf v_1, \mathbf v_2, \dots, \mathbf v_n}$. The matrix whose columns are $\mathbf q_1, \mathbf q_2, \dots, \mathbf q_n$ is an orthogonal matrix $\mathbf{Q}$.

$\mathbf{V}$ is transformed into $\mathbf{Q}$ according to the following algorithm:

For all column vectors $\mathbf{v} \in V$ starting from the first (leftmost) and moving systematically to the last (rightmost):

1. Orthogonalize $\mathbf v_k$ to all previous columns in matrix $\mathbf Q$ using orthogonal vector decomposition. That is, compute the component of $\mathbf v_k$ that is perpendicular to $\mathbf q_{k-1}, \mathbf q_{k-2}$, and so on down to $\mathbf q_{1}$. The orthogonalized vector is called $\mathbf v^{*}_k$.
:::{.callout-note}
The first column vector is not orthogonalized because there are no preceeding vectors; therefore, you begin
with the following normalization step.
:::
2. Normalize $\mathbf v^{*}_k$ to unit length. This is now $\mathbf q_{k}$, the $k$ th column in matrix $\mathbf Q$.

```{python}

# # Define a 4x4 random matrix
# A = np.random.rand(4,4)
# 
# # Gram-Schmidt procedure
# Q = np.zeros_like(A)
# for j in range(A.shape[1]):
#     v = A[:,j]
#     for i in range(j):
#         q = Q[:,i]
#         R[i,j] = np.dot(q,v)
#         v -= R[i,j]*q
#     R[j,j] = np.linalg.norm(v)
#     Q[:,j] = v/R[j,j]
# 
# # Check answer against Q from np.linalg.qr
# Q_np, R_np = np.linalg.qr(A)
# diff = Q - Q_np
# sum_Q = Q + Q_np
# 
# print("Q matrix (Gram-Schmidt):\n", Q)
# print("Q matrix (numpy):\n", Q_np)
# print("Difference between Q and Q_np:\n", diff)
# print("Sum of Q and Q_np:\n", sum_Q)


# create the matrix 
m = 4
n = 4
A = np.random.randn(m,n)

# initialize
Q = np.zeros((m,n))


# the GS algo
for i in range(n):
    
    # initialize
    Q[:,i] = A[:,i]
    
    # orthogonalize
    a = A[:,i] # convenience
    for j in range(i): # only to earlier cols
        q = Q[:,j] # convenience
        Q[:,i]=Q[:,i]-np.dot(a,q)/np.dot(q,q)*q
    
    # normalize
    Q[:,i] = Q[:,i] / np.linalg.norm(Q[:,i])

    
# "real" QR decomposition for comparison
Q_np,R = np.linalg.qr(A)

# note the possible sign differences.
# seemingly non-zero columns will be 0 when adding
print("Q matrix (Gram-Schmidt):\n", Q)
print("Q matrix (numpy):\n", Q_np)
print("Difference between Q and Q_np:\n", np.round( Q-Q_np ,10) ), print(' ')
print("Sum of Q and Q_np:\n", np.round( Q+Q_np ,10) )

```

## QR Decomposition

QR decomposition is a factorization of a matrix $\mathbf{A}$ into the product of an orthogonal matrix $\mathbf{Q}$ and an upper triangular matrix $\mathbf{R}$:

$$
\mathbf{A}=\mathbf{QR}
$$ 

where $\mathbf{Q}$ has orthonormal columns and $\mathbf{R}$ is an upper triangular matrix.

The process of finding the QR decomposition of $\mathbf{A}$ involves the Gram-Schmidt orthogonalization process, which produces an orthonormal basis for the columns of $\mathbf{A}$ as mentioned earlier. The columns of $\mathbf{Q}$ are the orthonormal basis vectors, and $\mathbf{R}$ is the matrix that expresses the columns of $\mathbf{A}$ in terms of the orthonormal basis vectors.

$$
\begin{align*}
\mathbf{A}&=\mathbf{QR}\\
\mathbf{Q}^T \mathbf{A}&=\mathbf{Q}^T\mathbf{QR}\\
\mathbf{Q}^T\mathbf{A}&=\mathbf{R}
\end{align*}
$$

The algorithm for computing the QR decomposition of a matrix $\mathbf{A}$ is as follows:

1. Apply the Gram-Schmidt orthogonalization process to the columns of $\mathbf{A}$ to obtain an orthonormal basis for the column space of $\mathbf{A}$. Let the resulting matrix be denoted by $\mathbf{Q}$.
2. Compute the matrix $\mathbf{R}$ such that $\mathbf{A} = \mathbf{Q}\mathbf{R}$. This can be done by solving the linear system $\mathbf{R} = \mathbf{Q}^T\mathbf{A}$, which expresses the columns of $\mathbf{A}$ in terms of the orthonormal basis vectors.
```{python}


# create a random matrix
A = np.random.randn(6,6)

# QR decomposition
Q,R = np.linalg.qr(A)



# show the matrices
fig = plt.figure(figsize=(10,6))
axs = [0]*5
c = 1.5 # color limits

gs1 = gridspec.GridSpec(2,6)
axs[0] = plt.subplot(gs1[0,:2])
axs[0].imshow(A,vmin=-c,vmax=c,cmap='gray')
axs[0].set_title('A',fontweight='bold')

axs[1] = plt.subplot(gs1[0,2:4])
axs[1].imshow(Q,vmin=-c,vmax=c,cmap='gray')
axs[1].set_title('Q',fontweight='bold')

axs[2] = plt.subplot(gs1[0,4:6])
axs[2].imshow(R,vmin=-c,vmax=c,cmap='gray')
axs[2].set_title('R',fontweight='bold')

axs[3] = plt.subplot(gs1[1,1:3])
axs[3].imshow(A - Q@R,vmin=-c,vmax=c,cmap='gray')
axs[3].set_title('A - QR',fontweight='bold')

axs[4] = plt.subplot(gs1[1,3:5])
axs[4].imshow(Q.T@Q,cmap='gray')
axs[4].set_title(r'$\mathbf{Q}^{T}\mathbf{Q}$',fontweight='bold')

# remove ticks from all axes
for a in axs:
  a.set_xticks([])
  a.set_yticks([])

plt.tight_layout()
plt.show()
```



### Examples

1. Consider the matrix $\mathbf{A} = \begin{bmatrix} 1 & 2 \\ 2 & 3 \\ 3 & 4 \end{bmatrix}$. To find its QR decomposition, we first apply the Gram-Schmidt process to obtain an orthonormal basis for its column space:

$$
\mathbf q_1 = \frac{1}{\sqrt{14}}\begin{bmatrix} 1 \\ 2 \\ 3\end{bmatrix} \quad \mathbf q_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} -2 \\ 1 \\ 0\end{bmatrix}
$$

The resulting orthogonal matrix is 
$$\begin{align*}
\mathbf{Q} = \begin{bmatrix} 
\frac{1}{\sqrt{14}} & -\frac{2}{\sqrt{28}} \\
\frac{2}{\sqrt{14}} & \frac{1}{\sqrt{2}} \\
\frac{3}{\sqrt{14}} & 0 
\end{bmatrix}
\end{align*}
$$.

Next, we compute the upper triangular matrix $\mathbf{R}$ by solving $\mathbf{R} = \mathbf{Q}^T\mathbf{A}$. This gives 
$$
\mathbf{R} = \begin{bmatrix}
\sqrt{14} & \frac{11}{\sqrt{14}} \\
0 & \frac{\sqrt{2}}{\sqrt{7}}
\end{bmatrix}
$$


Therefore, the QR decomposition of $\mathbf{A}$ is given by $\mathbf{A} = \mathbf{Q}\mathbf{R}$.

2. Consider the matrix $\mathbf{A} = \begin{bmatrix} 1 & -1 & 0 \\ 1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix}$. Applying the Gram-Schmidt process to its columns yields the orthonormal basis vectors:

$$
\begin{align*}
\mathbf q_1 &= \frac{1}{\sqrt{3}} \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}, \\
\mathbf u_2 &= \mathbf v_2 - \langle \mathbf v_2, \mathbf q_1 \rangle \mathbf q_1 = \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix} - \frac{1}{3}\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} -\frac{4}{3} \\ -\frac{1}{3} \\ \frac{2}{3} \end{bmatrix}, \\
\mathbf q_2 &= \frac{u_2}{|u_2|} = \frac{1}{\sqrt{2}} \begin{bmatrix} -2 \\ -1 \\ 1 \end{bmatrix}, \\
\mathbf u_3 &= \mathbf v_3 - \langle \mathbf v_3, \mathbf q_1 \rangle \mathbf q_1 - \langle \mathbf v_3, \mathbf q_2 \rangle \mathbf q_2 = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} - \frac{1}{3}\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} - \frac{1}{2}\begin{bmatrix} -2 \\ -1 \\ 1 \end{bmatrix} = \begin{bmatrix} \frac{1}{3} \\ -\frac{1}{3} \\ -\frac{1}{3} \end{bmatrix}, \\
\mathbf q_3 &= \frac{\mathbf u_3}{|\mathbf u_3|} = \frac{1}{\sqrt{3}} \begin{bmatrix} 1 \\ -1 \\ -1 \end{bmatrix}.
\end{align*}
$$

So the QR decomposition of $\mathbf{A}$ is $\mathbf{A} = \mathbf{QR}$ where

$$
\mathbf{A=QR} = \begin{bmatrix}
\frac{1}{\sqrt{3}} & -\frac{1}{\sqrt{6}} & \frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{6}} & -\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{3}} & \frac{2}{\sqrt{6}} & 0 \\
\end{bmatrix}
$$

and

$$
\mathbf{R} = \begin{bmatrix}
\sqrt{3} & \frac{1}{\sqrt{3}} & \sqrt{3} \\
0 & \frac{2}{\sqrt{6}} & -\frac{1}{\sqrt{2}} \\
0 & 0 & \frac{1}{\sqrt{2}} \\
\end{bmatrix}
$$.

So, 
$$
\mathbf{A} = 
\begin{bmatrix}
 1 & -1 & 0 \\
 1 & 0  & 1 \\
 1 & 1  & 0 
\end{bmatrix} =
\begin{bmatrix}
\frac{1}{\sqrt{3}} & -\frac{1}{\sqrt{6}} & \frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{6}} & -\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{3}} & \frac{2}{\sqrt{6}} & 0 \
\end{bmatrix}
\begin{bmatrix}
\sqrt{3} & \frac{1}{\sqrt{3}} & \sqrt{3} \\
0 & \frac{2}{\sqrt{6}} & -\frac{1}{\sqrt{2}} \\
0 & 0 & \frac{1}{\sqrt{2}} 
\end{bmatrix}
=
\mathbf{QR}
$$


In Python,

```{python}

A = np.array([[1, -1, 0], [1, 0, 1], [1, 1, 0]])
Q, R = np.linalg.qr(A)

print("Q =\n", Q)
print("R =\n", R)
print("QR =\n", Q @ R)

```

The QR decomposition of a matrix is not unique, So the $\mathbf Q$ and $\mathbf R$ could be different from the latex ones.

### Sizes of $\mathbf Q$ and $\mathbf R$

The sizes of $\mathbf Q$ and $\mathbf R$ depend on the size of the matrix $\mathbf A$ and on whether the QR decomposition is *reduced* or *full*.

For a tall matrix $(m > n)$, do we create a $\mathbf Q$ matrix with n columns or m columns? 

* Economy or Reduced: $\mathbf Q_{m\times n}$ (tall $\mathbf Q$)
* Full or Complete: $\mathbf Q_{m\times m}$ (square $\mathbf Q$) 
  * $\mathbf Q$ can be square when $\mathbf A$ is tall ($\mathbf Q$ can have more columns than $\mathbf A$)
  * In python, the option of `np.linalg.qr(A,'complete')` is 'complete', which produces a full QR decomposition.
  * The option of `np.linalg.qr(A,'reduced')` is 'reduced', which is the default, gives the economy-mode QR decomposition, in which $\mathbf Q$ is the same size as $\mathbf A$.
* Likewise, the rank of $\mathbf Q$ is always the maximum possible rank, which is $m$ for all square $\mathbf Q$ matrices and $n$ for the economy $\mathbf Q$. The rank of $\mathbf R$ is the same as the rank of $\mathbf A$.
  * the difference of $\operatorname{rank}(\mathbf A) and \operatorname{rank}(\mathbf Q)$ means $\mathbf Q$ spans all of $\mathbb R^m$ even if the $\operatorname{col}(\mathbf A)$ is only a lower-dimensional subspace of $\mathbb R^m$.
* **non-uniqueness**: QR decomposition is not unique for all matrix sizes and ranks. ($\mathbf A = \mathbf Q_1 \mathbf R_1$ and $\mathbf A = \mathbf Q_2\mathbf R_2$ where $\mathbf Q_1 \ne \mathbf Q_2$).
* **uniqueness with constraints**: QR decomposition can be made unique given additional constraints (e.g., positive values on the diagonals of $\mathbf R$)

```{python}
A = np.array([ [1,-1] ]).T
Q,R = np.linalg.qr(A,'complete')
Q*np.sqrt(2) # scaled by sqrt(2) to get integers
print("A =\n", A)
print("Q =\n", Q)
print("R =\n", R)
print("Q*np.sqrt(2)=\n", Q*np.sqrt(2))
```

### Upper-triangle of $\mathbf R$

* $\mathbf R$ comes from the formula $\mathbf{Q}^T\mathbf{A = R}$.
* The lower triangle of a product matrix comprises dot products between later rows of the left matrix and earlier columns of the right matrix.
* The rows of $\mathbf{Q}^T$ are the columns of $\mathbf{Q}$.

```{python}
# Define matrix A
A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

# Compute QR decomposition of A
Q, R = np.linalg.qr(A)

# Compute Q^T * A
QtA = np.matmul(Q.T, A)

# Extract upper triangle of R
upper_R = np.triu(R)

# Compute dot products between later rows of Q^T and earlier columns of A
dot_products = np.zeros((3, 3))
for i in range(3):
    for j in range(i):
        dot_products[i, j] = np.dot(Q.T[i], A[:, j])

# Print results
print("A = ")
print(A)
print("Q = ")
print(Q)
print("R = ")
print(R)
print(r"$Q^T * A = $")
print(QtA)
print("Upper-triangle of R = ")
print(upper_R)
print("Dot products between later rows of Q^T and earlier columns of A = ")
print(dot_products)
```

The lower triangle of $\mathbf{R}$ comprises dot products of $\mathbf Q^T\mathbf A$ (between later rows of $\mathbf{Q}^T$ and earlier columns of $\mathbf{A}$)
:::{.callout-note}
Note that the lower triangle of $\mathbf{R}$ is zero, because the first column of $\mathbf{A}$ is orthogonal to the remaining columns of $\mathbf{A}$. Thus, the pairs of vectors used to form the lower triangle of $\mathbf{R}$ are orthogonal. On the other hand, the upper triangle of $\mathbf{R}$ comes from the dot product of later rows of $\mathbf{Q}$ and earlier columns of $\mathbf{A}$. Specifically, the $(2,1)$ entry of $\mathbf{R}$ is the dot product of the second row of $\mathbf{Q}$ with the first column of $\mathbf{A}$

If columns $i$ and $j$ of $\mathbf A$ were already orthogonal, then the corresponding $(i,j)$ th element in $\mathbf R$ would be zero. In fact, if you compute the QR decomposition of an orthogonal matrix, then $\mathbf R$ will be a diagonal matrix in which the diagonal elements are the norms of each column in $\mathbf A$. That means that if $\mathbf{A = Q}$, then $\mathbf{R = I}$, which is obvious from the equation solved for $\mathbf{R}$ 
:::

### QR and Inverses

QR decomposition provides a more numerically stable way to compute the matrix inverse, $\mathbf A^{-1}$ of $\mathbf A$ because $\mathbf Q$ is numerically stable due to the Householder reflection algorithm, and $\mathbf R$ is numerically stable because it simply results from matrix multiplication.

$$
\begin{align*}
  \mathbf{A}&=\mathbf{QR}\\
  \mathbf{A}^{-1}&=(\mathbf{QR})^{-1}\\
  \mathbf{A}^{-1}&=\mathbf{R}^{-1}\mathbf{Q}^{-1}\\
  \mathbf{A}^{-1}&=\mathbf{R}^{-1}\mathbf{Q}^{T}
\end{align*}
$$


## Projections

## Least Squares Approximations

## Orthogonal Bases and Gram-Schmidt
