---
title: "Comparative Study of Dimension Reduction Methods"
subtitle: "Statistics and Machine Learning"
description: "Due to security concerns, it is difficult to display the data used in this project. Therefore, we will generate fake data to reproduce and demonstrate a rough analysis pipeline."
author: Kwangmin Kim
date: 2022-12-10
image: images/high_dimensions.jpg
image-alt: "a high dimensional data"
---

## Introduction

### Rationale

### Goal

## Data Preparation

### Package Loading and Option Settings

```{r, echo=FALSE,message=FALSE,warning=FALSE}

library(tidyverse)
library(tidymodels)
library(glmnet)
library(MASS)
#library(mixOmics)
set.seed(20221213) 

```

```{r}

# set the size of a sample and dimension
sample_size <- 500
variable_size <- 10000 # the number of predictors
significant_variables <- variable_size*sample((50:200)/1000,1) # the number of predictors truly associated with a response variable

positively_correlated_variables<-floor(significant_variables*0.4) # the number of variables positively correlated 
negatively_correlated_variables<-significant_variable # the number of variables negatively correlated

# prepare metabolite data with multivariate normal distribution
metabolites <- matrix(rnorm(sample_size*variable_size), 
            nrow=sample_size, ncol=variable_size)%>%
    as.data.frame()
names(metabolites)<-paste("meta",1:10000)

sample()

Sigma <- matrix(
    c(sample((1:100)/10,positive_significant_variables,replace=TRUE),
      sample(-(1:100)/10,negative_significant_variables,replace=TRUE),),
    sample_size,variable_size)
Sigma
var(mvrnorm(n = 1000, rep(0, 2), Sigma))
var(mvrnorm(n = 1000, rep(0, 2), Sigma, empirical = TRUE))

var(mvrnorm(n = 1000, rep(0, 2), Sigma))

mvrnorm(n = 1000, rep(0, 2), Sigma)


Sigma <- matrix(c(10,3,3,2),sample_size,variable_size)
Sigma
var(mvrnorm(n = 1000, rep(0, 2), Sigma))
var(mvrnorm(n = 1000, rep(0, 2), Sigma, empirical = TRUE))

var(mvrnorm(n = 1000, rep(0, 2), Sigma))

mvrnorm(n = 1000, rep(0, 2), Sigma)

# prepare phenotype data

age_distribution=rchisq(500,df=10)
sex_distribution=sample(c(0,1),500,replace=TRUE,prob = c(0.45,0.55))
country_distribution=sample(c(0:3),500,replace=TRUE,prob = c(0.3,0.2,0.2,0.3))
treatment_distribution=sample(c(0:2),500,replace=TRUE,prob = c(0.7,0.1,0.2))
genotype_distribution=sample(c(0:5),500,replace=TRUE,
                             prob = c(0.05,0.15,0.05,0.40,0.25,0.1))


pheno_type<-
    data.frame(
        age=sapply(age_distribution,
                   function(x)(x-min(age_distribution))/(max(age_distribution)-
                    min(age_distribution))*(105-65)+65)%>%round(0),
        sex=sex_distribution,
        country=country_distribution,
        treatment=treatment_distribution,
        genotype=genotype_distribution)%>%
    mutate(age=ifelse(genotype==0,age+3,
                      ifelse(genotype==1,age+2,
                             ifelse(genotype==4,age-1,
                                    ifelse(genotype==5,age-2,age)))),
           age=age+rnorm(sample_size,mean=0,sd=2),
           age=round(age,0))%>%
    mutate(age=ifelse(sex==0,age-3,age))

phenotype_variable="age"
standarization_function=function(phenotype_variable){
    temp=pheno_type[,phenotype_variable]
    return(sapply(temp,function(x)(x-mean(temp))/sd(temp)))
}
    
?mvnorm

y <- apply(x[,1:significant_variables], 1, function(x)sum(x)) + 
    rnorm(sample_size) + 
    standarization_function("age")+
    standarization_function("sex")


pheno_type%>%group_by(genotype)%>%summarise(mean=mean(age),sd=sd(age))
pheno_type%>%names()



```

## Data Description

## Architecture of Analysis Pipeline 

## Methods

```{r, echo=FALSE,message=FALSE,warning=FALSE}
n <- 1000    # sample size
p <- 10000     # the number of predictors
significant_p <- p*0.25  # the number of predictors associated with a response variable

x <- matrix(rnorm(n*p), nrow=n, ncol=p)
y <- apply(x[,1:significant_p], 1, sum) + rnorm(n)

# Split data into train (2/3) and test (1/3) sets
train_rows <- sample(1:n, .66*n)
x.train <- x[train_rows, ]
x.test <- x[-train_rows, ]

y.train <- y[train_rows]
y.test <- y[-train_rows]

list.of.fits <- list()
for (i in 0:10) {
  fit.name <- paste0("alpha", i/10)
  
  list.of.fits[[fit.name]] <-
    cv.glmnet(x.train, y.train, type.measure="mse", alpha=i/10, 
      family="gaussian")
}

results <- data.frame()
for (i in 0:10) {
  fit.name <- paste0("alpha", i/10)
  
  predicted <- 
    predict(list.of.fits[[fit.name]], 
      s=list.of.fits[[fit.name]]$lambda.1se, newx=x.test)
  
  mse <- mean((y.test - predicted)^2)
  
  temp <- data.frame(alpha=i/10, mse=mse, fit.name=fit.name)
  results <- rbind(results, temp)
}

results
```

## Results

## Conclusion

## Bibliography
