<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kwangmin Kim">
<meta name="dcterms.date" content="2023-03-24">
<meta name="description" content="template">

<title>Kwangmin Kim - LDA (2) - Concept</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../.././images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Kwangmin Kim</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/CV/index.html">
 <span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/projects/index.html">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/blog/index.html">
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../about.html">
 <span class="menu-text">Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kmink3225"><i class="bi bi-github" role="img" aria-label="Github">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kwangmin-kim-a5241b200/"><i class="bi bi-linkedin" role="img" aria-label="Linkedin">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/blog/index.html"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#go-to-project-content-list" id="toc-go-to-project-content-list" class="nav-link active" data-scroll-target="#go-to-project-content-list"><span class="toc-section-number">1</span>  Go to Project Content List</a></li>
  <li><a href="#go-to-blog-content-list" id="toc-go-to-blog-content-list" class="nav-link" data-scroll-target="#go-to-blog-content-list"><span class="toc-section-number">2</span>  Go to Blog Content List</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">LDA (2) - Concept</h1>
<p class="subtitle lead">Overview</p>
  <div class="quarto-categories">
    <div class="quarto-category">Statistics</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>template</p>
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kwangmin Kim </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 24, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<ul class="nav nav-pills" id="language-tab" role="tablist">
<li class="nav-item" role="presentation">
<button class="nav-link active" id="Korean-tab" data-bs-toggle="tab" data-bs-target="#Korean" type="button" role="tab" aria-controls="Korean" aria-selected="true">
Korean
</button>
</li>
<li class="nav-item" role="presentation">
<button class="nav-link" id="English-tab" data-bs-toggle="tab" data-bs-target="#English" type="button" role="tab" aria-controls="knitr" aria-selected="false">
English
</button>
</li>
<div class="tab-content" id="language-tabcontent">

<div id="Korean" class="tab-pane fade show active" role="tabpanel" aria-labelledby="Korean-tab">
<div id="Korean" class="tab-pane fade show active" role="tabpanel" aria-labelledby="Korean-tab">
<section id="notations" class="level2" data-number="0.1">
<h2 data-number="0.1" class="anchored" data-anchor-id="notations"><span class="header-section-number">0.1</span> Notations</h2>
<ul>
<li><span class="math inline">\(y_{ij}\)</span> : the univariate response (i.e.&nbsp;scalar) for the <span class="math inline">\(i\)</span> th subject at the <span class="math inline">\(j\)</span> th occasion or measurement
<ul>
<li>later when I use the vector case, I will re-define this notation, but focus on the scalar case for now.</li>
</ul></li>
<li><span class="math inline">\(x_{ij}\)</span> : the predictor at time <span class="math inline">\(t_{ij}\)</span>, which is either a scalr or vector.
<ul>
<li>a scalar case: <span class="math inline">\(x_{ij}\)</span> where <span class="math inline">\(i\)</span> is the <span class="math inline">\(i\)</span> th subject, and <span class="math inline">\(j\)</span> is the <span class="math inline">\(j\)</span> th measurement.</li>
<li>a vector case: <span class="math inline">\(x_{ijk}\)</span> where <span class="math inline">\(i\)</span> is the <span class="math inline">\(i\)</span> th subject, <span class="math inline">\(j\)</span> is the <span class="math inline">\(j\)</span> th measurement, and <span class="math inline">\(k \in [1,p]\)</span> is the <span class="math inline">\(k\)</span> th predictor.</li>
<li>sometimes, covariate for different measurements could be the same. In this case, the notation could be written in <span class="math inline">\(x_{i}\)</span>
<ul>
<li>ex) a gender does not change over time in the most cases.</li>
</ul></li>
</ul></li>
<li><span class="math inline">\(i=1, \dots, m\)</span> : i is the index for the <span class="math inline">\(i\)</span> th subject</li>
<li><span class="math inline">\(j=1, \dots, n_i\)</span> : j is the index for the <span class="math inline">\(j\)</span> th measurement of the <span class="math inline">\(i\)</span> th subject
<ul>
<li><span class="math inline">\({n_i}\)</span> is the number of measurements of the <span class="math inline">\(i\)</span> th subject, each <span class="math inline">\({n_i}\)</span> does not have to the same.</li>
<li>balanced desgin: <span class="math inline">\({n_i}\)</span> is the same.</li>
<li>unbalanced desgin: <span class="math inline">\({n_i}\)</span> is different.</li>
</ul></li>
<li><span class="math inline">\(\mathbf y_i\)</span> : a vector (not a matrix), <span class="math inline">\((y_{i1},y_{i2},\dots ,y_{in_i})\)</span> of the <span class="math inline">\(i\)</span> th subject</li>
<li><span class="math inline">\(\mathbf Y\)</span> : the reponse matrix</li>
<li><span class="math inline">\(\mathbf X\)</span> : the predictor matrix</li>
<li><span class="math inline">\(\text{E}(y_{ij})\)</span> : <span class="math inline">\(\mu_{ij}\)</span></li>
<li><span class="math inline">\(\text{E}(\mathbf y_i)\)</span> : <span class="math inline">\(\mathbf \mu_{i}\)</span></li>
<li><span class="math inline">\(\text{Var}(\mathbf y_i)\)</span> : <span class="math inline">\(\text{Var}(\mathbf y_i)\)</span> is a variance-covariance matrix of the different measurement for the <span class="math inline">\(i\)</span> th subject
<ul>
<li>for now, we do not care of the variance covariance of the different subjects because we assume that the measurements of different subjects are indpendent. <span class="math display">\[
\begin{bmatrix}
\text{Var}(y_{i1}) &amp; \text{Cov}( y_{i1}, y_{i2}) &amp; \dots &amp; \text{Cov}( y_{i1}, y_{in_i}) \\
                           &amp; \text{Var}( y_{i2}) &amp; \dots &amp; \text{Cov}( y_{i2}, y_{in_i}) \\
                             &amp;                           &amp; \ddots &amp; \vdots \\
                             &amp;&amp;                            \dots &amp; \text{Var}( y_{in_i})
\end{bmatrix}
\]</span></li>
</ul></li>
</ul>
</section>
<section id="assumptions" class="level2" data-number="0.2">
<h2 data-number="0.2" class="anchored" data-anchor-id="assumptions"><span class="header-section-number">0.2</span> Assumptions</h2>
<ul>
<li>the measurements for the same subject are not independent.</li>
<li>the measurements for the different subject are independent.</li>
<li>some correlation structures of the different measurements.</li>
</ul>
</section>
<section id="for-continuous-responses" class="level2" data-number="0.3">
<h2 data-number="0.3" class="anchored" data-anchor-id="for-continuous-responses"><span class="header-section-number">0.3</span> For Continuous Responses</h2>
<ul>
<li>Marginal Models
<ul>
<li>$(y_{ij}) = x_{ij} $</li>
<li><span class="math inline">\(\text{Var}(\mathbf y_i)= \mathbf V_i\)</span></li>
<li>to build a marginal model, we just need info on the 3 things
<ul>
<li>the distribution : a multivariate normal distribution</li>
<li>mean and variance-covariance</li>
</ul></li>
<li><span class="math inline">\(\beta\)</span> is fixed. That’s why we call this marginal models ‘fixed effect’</li>
</ul></li>
</ul>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Recall
</div>
</div>
<div class="callout-body-container callout-body">
<p>We find MLE for the linear regression with the 3 things: the normal distribution (iid), <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span></p>
</div>
</div>
<ul>
<li>Mixed Effects Models
<ul>
<li>$(y_{ij}|<em>i) = x</em>{ij} _i $</li>
<li><span class="math inline">\(\mathbf \beta_i = \mathbf \beta (\text{fixed effect}) + \mathbf u_i (\text{subject-specific random effect})\)</span></li>
<li><span class="math inline">\(\mathbf \beta_i\)</span> is a random coefficient specific for the <span class="math inline">\(i\)</span> th subject, That’s why we call this mixed effect models ‘random effect’</li>
<li>subject-specific random effect: differenct subjects have different <span class="math inline">\(\mathbf \beta_i\)</span> s</li>
</ul></li>
<li>Transition Models
<ul>
<li><span class="math inline">\(\text{E}(y_{ij}|y_{i,j-1},\dots,y_{i,1},\mathbf x_{ij})\)</span></li>
<li>Markov Process: the response variable in the previous time point will affect the measurement in the current time point.</li>
</ul></li>
</ul>
<section id="marginal-models" class="level3" data-number="0.3.1">
<h3 data-number="0.3.1" class="anchored" data-anchor-id="marginal-models"><span class="header-section-number">0.3.1</span> Marginal Models</h3>
<p>Consider an example of a simple linear model (i.e., a univaiable linear model) <span class="math display">\[
y_{ij}=\beta_0+\beta_1t_{ij} + \epsilon_{ij}
\]</span></p>
<ul>
<li>mean part: <span class="math inline">\(\text{E}(y_{ij})\)</span></li>
<li>variance part: <span class="math inline">\(\text{Var}(\mathbf y_{i})=\text{Var}(\mathbf \epsilon_{i})\)</span>
<ul>
<li>more often, a correlation matrix is used in LDA because correlation is more interpretable. $$ (y_i) = \begin{bmatrix} 1 &amp; <em>{12}&amp; &amp; </em>{1n_i} \ <em>{21} &amp; 1 &amp; &amp; </em>{2n_i} \ &amp; &amp; &amp; \ <em>{n_i1} &amp; </em>{n_i2}&amp; &amp; 1 \</li>
</ul></li>
</ul>
<p>\end{bmatrix} $$ * in this correlation matrix, there are <span class="math inline">\(\frac{n(n-1)}{2}\)</span> parameters to estimate * in the mean part, there are 2 parameters, <span class="math inline">\(\mathbf \beta\)</span> to estimate Likewise, the number of the estimators depends on the number of the measurements and the covriates.</p>
<p>In LDA, since the responses are multiple, we need to look into the correlation characteristics. In empirical observations about the nature of the correlation among repeated measures,</p>
<ul>
<li>correlations are usually positive</li>
<li>decrease with increasing time separation</li>
<li>rarely approach zero</li>
<li>approach one if a pair of repeated measures are taken very closely in time</li>
</ul>
</section>
<section id="modeling-covariance-structure" class="level3" data-number="0.3.2">
<h3 data-number="0.3.2" class="anchored" data-anchor-id="modeling-covariance-structure"><span class="header-section-number">0.3.2</span> Modeling Covariance Structure</h3>
<p>There are 2 types of covariance structure: unbalanced design and balanced design. For now, let’s focus on the balanced design.</p>
<section id="balanced-design" class="level4" data-number="0.3.2.1">
<h4 data-number="0.3.2.1" class="anchored" data-anchor-id="balanced-design"><span class="header-section-number">0.3.2.1</span> balanced design</h4>
<ul>
<li>number and timing of the repeated measurements are the same for all individuals.</li>
<li>Then, <span class="math inline">\(t_{ij}\)</span> can be denoted as <span class="math inline">\(t_j\)</span> where <span class="math inline">\(j \in 1, \dots, n\)</span> because the size of the measurements is the same (<span class="math inline">\(n_i\)</span> is the same)</li>
<li>The covariance of the response variable <span class="math inline">\(\mathbf Y_{m\times n}\)</span> : $$ \begin{aligned} (Y) &amp;=(y_1,,y_m) \ &amp;=
<span class="math display">\[\begin{bmatrix}
  \text{Var}(\mathbf y_1) &amp; 0 &amp; \dots &amp; 0 \\
  0 &amp; \text{Var}(\mathbf y_2) &amp; \dots &amp; 0 \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
  0 &amp; 0 &amp; \dots &amp; \text{Var}(\mathbf y_m) \\

\end{bmatrix}\]</span>
\ &amp;=
<span class="math display">\[\begin{bmatrix}
  \begin{bmatrix}
  \text{Var}(y_{11}) &amp; \text{Cov}( y_{11}, y_{12}) &amp; \dots &amp; \text{Cov}( y_{11}, y_{1n_1}) \\
                             &amp; \text{Var}( y_{12}) &amp; \dots &amp; \text{Cov}( y_{12}, y_{1n_1}) \\
                               &amp;                           &amp; \ddots &amp; \vdots \\
                               &amp;&amp;                            \dots &amp; \text{Var}( y_{1n_1})
\end{bmatrix} &amp; 0 &amp; \dots &amp; 0 \\
  0 &amp; \begin{bmatrix}
  \text{Var}(y_{21}) &amp; \text{Cov}( y_{21}, y_{22}) &amp; \dots &amp; \text{Cov}( y_{21}, y_{in_2}) \\
                             &amp; \text{Var}( y_{22}) &amp; \dots &amp; \text{Cov}( y_{22}, y_{in_2}) \\
                               &amp;                           &amp; \ddots &amp; \vdots \\
                               &amp;&amp;                            \dots &amp; \text{Var}( y_{2n_2})
\end{bmatrix} &amp; \dots &amp; 0 \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
  0 &amp; 0 &amp; \dots &amp; \begin{bmatrix}
  \text{Var}(y_{m1}) &amp; \text{Cov}( y_{m1}, y_{m2}) &amp; \dots &amp; \text{Cov}( y_{m1}, y_{mn_m}) \\
                             &amp; \text{Var}( y_{m2}) &amp; \dots &amp; \text{Cov}( y_{m2}, y_{mn_m}) \\
                               &amp;                           &amp; \ddots &amp; \vdots \\
                               &amp;&amp;                            \dots &amp; \text{Var}( y_{mn_m})
\end{bmatrix} \\

\end{bmatrix}\]</span>
\ &amp;=
<span class="math display">\[\begin{bmatrix}
  \Sigma_1 &amp; 0 &amp; \dots &amp; 0 \\
  0 &amp; \Sigma_1 &amp; \dots &amp; 0 \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
  0 &amp; 0 &amp; \dots &amp; \Sigma_m \\

\end{bmatrix}\]</span></li>
</ul>
<p>\end{aligned} $$</p>
<p>If we assume the covariance matrices for different subjects are the same, we can denote <span class="math inline">\(\text{Cov}(\mathbf Y)=\Sigma\)</span></p>
</section>
</section>
<section id="covariance-structure-pattern-models" class="level3" data-number="0.3.3">
<h3 data-number="0.3.3" class="anchored" data-anchor-id="covariance-structure-pattern-models"><span class="header-section-number">0.3.3</span> Covariance Structure Pattern Models</h3>
<section id="compound-symmetry-structure" class="level4" data-number="0.3.3.1">
<h4 data-number="0.3.3.1" class="anchored" data-anchor-id="compound-symmetry-structure"><span class="header-section-number">0.3.3.1</span> Compound symmetry Structure</h4>
$$ (y_i)=^2
<span class="math display">\[\begin{bmatrix}
    1 &amp; \rho &amp; \rho &amp; \dots &amp; \rho \\
    \rho &amp; 1 &amp; \rho &amp; \dots &amp; \rho \\
    \rho &amp; \rho &amp; 1 &amp; \dots &amp; \rho \\
    \vdots &amp; \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
    \rho &amp; \rho &amp; \rho &amp; \dots &amp; 1 \\

  \end{bmatrix}\]</span>
<p>$$</p>
<ul>
<li>compound symmetry is a.k.a <strong>Exchangeable</strong></li>
<li>Assume variance is constant across visits (say <span class="math inline">\(\sigma^2\)</span>)</li>
<li>Assume correlation between any two visits are constant (say <span class="math inline">\(\rho\)</span>).</li>
<li>Parsimonious: there are two parameters in the covariance, <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\rho\)</span> (computational benefit)</li>
<li>Without any contraint on <span class="math inline">\(\sigma^2\)</span>, you will get closed form estimate.</li>
<li>Covariance variance matrix is plugged into likelihood function to estimate 3 kinds of parameters <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(\rho\)</span>, and <span class="math inline">\(\beta\)</span></li>
<li>This structure is so parsimonuous that it could be unrealistic: not commonly used</li>
</ul>
</section>
<section id="toeplitz-structure" class="level4" data-number="0.3.3.2">
<h4 data-number="0.3.3.2" class="anchored" data-anchor-id="toeplitz-structure"><span class="header-section-number">0.3.3.2</span> Toeplitz Structure</h4>
$$ (y_i)=^2
<span class="math display">\[\begin{bmatrix}
    1 &amp; \rho_1 &amp; \rho_2 &amp; \dots &amp; \rho_{n-1} \\
    \rho_1 &amp; 1 &amp; \rho_1 &amp; \dots &amp; \rho_{n-2} \\
    \rho_2 &amp; \rho_1 &amp; 1 &amp; \dots &amp; \rho_{n-3} \\
    \vdots &amp; \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
    \rho_{n-1} &amp; \rho_{n-2} &amp; \rho_{n-3} &amp; \dots &amp; 1 \\

  \end{bmatrix}\]</span>
<p>$$</p>
<ul>
<li>Toeplitz Structure is more flexible than compound symmetry</li>
<li>Assume variance is constant across visits and $(y_{ij}, y_{i,j+k}) = _k $.</li>
<li>Assume correlation among responses at adjacent measurements is constant.</li>
<li>Only suitable for measurements made at equal intervals of time between different measurement.</li>
<li>Without any contraint on <span class="math inline">\(\sigma^2\)</span>, you will get closed form estimate.</li>
<li>Toeplitz covariance has free <span class="math inline">\(n\)</span> parameters to estimate (<span class="math inline">\(1\)</span> for variance and <span class="math inline">\(n-1\)</span> correlation parameters)</li>
<li>The larger time differences, the smaller its correlations</li>
</ul>
</section>
<section id="autoregressive-structure" class="level4" data-number="0.3.3.3">
<h4 data-number="0.3.3.3" class="anchored" data-anchor-id="autoregressive-structure"><span class="header-section-number">0.3.3.3</span> Autoregressive Structure</h4>
<p>a special case of toeplitz with corr(y_ij,y_ij+k)=rho^k simpler than toeplitz -&gt; only 2 parameters</p>
</section>
<section id="banded-structure" class="level4" data-number="0.3.3.4">
<h4 data-number="0.3.3.4" class="anchored" data-anchor-id="banded-structure"><span class="header-section-number">0.3.3.4</span> Banded Structure</h4>
</section>
<section id="exponential-structure" class="level4" data-number="0.3.3.5">
<h4 data-number="0.3.3.5" class="anchored" data-anchor-id="exponential-structure"><span class="header-section-number">0.3.3.5</span> Exponential Structure</h4>
<p>the most general and reasonable structure take acture time points (time difference), the larger time difference the smaller correlation</p>
<p>assuption that the variance of different measurements over time is the same, which can be easily generalized. You can put different variance on the diagonal</p>
</section>
</section>
<section id="general-linear-model" class="level3" data-number="0.3.4">
<h3 data-number="0.3.4" class="anchored" data-anchor-id="general-linear-model"><span class="header-section-number">0.3.4</span> General Linear Model</h3>
<p>assume that we picked one, exponential structure. How do we estimate parameters with the corr structures?</p>
<ol type="1">
<li>use OLS to estimate beta-&gt;min||y-xbeta||^2 -&gt; tilde beta-&gt;(X<sup>TX)</sup>-1X^TY-&gt; tilde betea is unbiased, why?-&gt; E(tilde beta)=E((XTX)<sup>-1X</sup>TY)=(XTX)<sup>-1X</sup>TE(Y)</li>
</ol>
<p>To show that this formula is unbiased, we need to show that the expected value of beta equals the true value of the coefficients, that is E(beta) = (b0, b1).</p>
<p>Let’s start by calculating the expected value of beta:</p>
<p>E(beta) = E((X^T X)^{-1} X^T Y)</p>
<p>Since X and Y are assumed to be random variables, we can use the properties of matrix expectations to simplify this expression:</p>
<p>E(beta) = (X^T X)^{-1} X^T E(Y)</p>
<p>Since Y = b0 + b1*X + e (where e is the error term), we have:</p>
<p>E(Y) = E(b0 + b1X + e) = b0 + b1E(X) + E(e) = b0 + b1*X</p>
<p>where we have used the fact that E(e) = 0 and E(X) is a constant.</p>
<p>Substituting this expression for E(Y) into the previous equation, we get:</p>
<p>E(beta) = (X^T X)^{-1} X^T (b0 + b1*X)</p>
<p>Expanding this expression, we get:</p>
<p>E(beta) = (X^T X)^{-1} X^T b0 + (X^T X)^{-1} X^T b1*X</p>
<p>Using the distributive property of matrix multiplication, we can simplify this expression further:</p>
<p>E(beta) = (X^T X)^{-1} X^T b0 + b1 (X^T X)^{-1} (X^T X)</p>
<p>Since (X^T X)^{-1} (X^T X) = I (the identity matrix), we have:</p>
<p>E(beta) = (X^T X)^{-1} X^T b0 + b1 I</p>
<p>Finally, we can compare this expression with the true value of the coefficients (b0, b1):</p>
<p>(b0, b1) = (X^T X)^{-1} X^T Y</p>
<p>Substituting Y = b0 + b1*X + e into this expression, we get:</p>
<p>(b0, b1) = (X^T X)^{-1} X^T (b0 + b1*X + e)</p>
<p>Using the distributive property of matrix multiplication, we can simplify this expression to:</p>
<p>(b0, b1) = (X^T X)^{-1} X^T b0 + b1 (X^T X)^{-1} X^T X + (X^T X)^{-1} X^T e</p>
<p>Since E(e) = 0, the third term in this expression is zero.</p>
<ol start="2" type="1">
<li>WLS</li>
</ol>
<p>W dimension: the same diemnsion of the total measurement m*n</p>
<p>In weighted least squares (WLS) regression, the goal is to find the values of the coefficients b0 and b1 that minimize the weighted sum of the squared residuals. The weights are used to adjust the influence of each data point on the fitting line.</p>
<p>The formula for the coefficients b0 and b1 in WLS is very similar to the one in OLS, with the addition of a weight matrix W.</p>
<p>The equation for WLS is Y = b0 + b1*X + e, where e is the error term and W is a diagonal matrix of weights.</p>
<p>To derive the formula for the coefficients b0 and b1 in WLS, we need to define the following matrices:</p>
<p>X: the matrix of independent variables (also called the design matrix) Y: the matrix of dependent variables W: the weight matrix (a diagonal matrix of weights) beta: the vector of coefficients to be estimated The formula for beta in WLS is beta = (X^T W X)^{-1} X^T W Y, where (^T) denotes the transpose of a matrix and (^{-1}) denotes the inverse of a matrix.</p>
<p>To show that this formula is unbiased, we need to show that the expected value of beta equals the true value of the coefficients, that is E(beta) = (b0, b1).</p>
<p>Let’s start by calculating the expected value of beta:</p>
<p>E(beta) = E((X^T W X)^{-1} X^T W Y)</p>
<p>Using the properties of matrix expectations, we can simplify this expression:</p>
<p>E(beta) = (X^T W X)^{-1} X^T W E(Y)</p>
<p>Since Y = b0 + b1*X + e, we have:</p>
<p>E(Y) = E(b0 + b1X + e) = b0 + b1E(X) + E(e) = b0 + b1*X</p>
<p>where we have used the fact that E(e) = 0 and E(X) is a constant.</p>
<p>Substituting this expression for E(Y) into the previous equation, we get:</p>
<p>E(beta) = (X^T W X)^{-1} X^T W (b0 + b1*X)</p>
<p>Expanding this expression, we get:</p>
<p>E(beta) = (X^T W X)^{-1} X^T W b0 + (X^T W X)^{-1} X^T W b1*X</p>
<p>Using the distributive property of matrix multiplication, we can simplify this expression further:</p>
<p>E(beta) = (X^T W X)^{-1} X^T W b0 + b1 (X^T W X)^{-1} (X^T W X)</p>
<p>Since (X^T W X)^{-1} (X^T W X) = I (the identity matrix), we have:</p>
<p>E(beta) = (X^T W X)^{-1} X^T W b0 + b1 I</p>
<p>Finally, we can compare this expression with the true value of the coefficients (b0, b1):</p>
<p>(b0, b1) = (X^T W X)^{-1} X^T W Y</p>
<p>Substituting Y = b0 + b1*X + e into this expression, we get:</p>
<p>(b0, b1) = (X^T W X)^{-1} X^T W (b0 + b1*X + e)</p>
<p>Using the distributive property of matrix multiplication, we can simplify this expression to:</p>
<p>(b0, b1) = (X^T W X)^{-1} X^T W b0 + b1 (X^T W X)^{-1} X^T W X</p>
<p>uniasedness of WLS</p>
<p>To show that (b0, b1) = (X^T W X)^{-1} X^T W b0 + b1 (X^T W X)^{-1} X^T W X is an unbiased estimator of the true coefficients (b0, b1), we need to show that E((b0, b1)) = (b0, b1).</p>
<p>Taking the expected value of both sides of the equation (b0, b1) = (X^T W X)^{-1} X^T W b0 + b1 (X^T W X)^{-1} X^T W X, we have:</p>
<p>E((b0, b1)) = E((X^T W X)^{-1} X^T W b0 + b1 (X^T W X)^{-1} X^T W X)</p>
<p>Using the linearity of the expectation operator, we can distribute it inside the sum, obtaining:</p>
<p>E((b0, b1)) = E((X^T W X)^{-1} X^T W b0) + E(b1 (X^T W X)^{-1} X^T W X)</p>
<p>We can calculate each term separately. First, let’s consider the term E((X^T W X)^{-1} X^T W b0):</p>
<p>E((X^T W X)^{-1} X^T W b0) = (X^T W X)^{-1} X^T W E(b0)</p>
<p>Since b0 is a constant, we have E(b0) = b0. Substituting this in the expression above, we get:</p>
<p>E((X^T W X)^{-1} X^T W b0) = (X^T W X)^{-1} X^T W b0</p>
<p>Now let’s consider the term E(b1 (X^T W X)^{-1} X^T W X):</p>
<p>E(b1 (X^T W X)^{-1} X^T W X) = b1 E((X^T W X)^{-1} X^T W X)</p>
<p>We need to show that E((X^T W X)^{-1} X^T W X) = I, where I is the identity matrix. This is true because:</p>
<p>E((X^T W X)^{-1} X^T W X) = (X^T W X)^{-1} X^T W E(X) = (X^T W X)^{-1} X^T W X</p>
<p>where we have used the fact that E(X) is a constant and W is a diagonal matrix with non-zero entries. Therefore, we have:</p>
<p>E(b1 (X^T W X)^{-1} X^T W X) = b1 I</p>
<p>Substituting these expressions for the two terms back into the original equation, we obtain:</p>
<p>E((b0, b1)) = (X^T W X)^{-1} X^T W b0 + b1 I</p>
<p>Comparing this with the true coefficients (b0, b1), we see that they are equal. Therefore, we have shown that (b0, b1) = (X^T W X)^{-1} X^T W b0 + b1 (X^T W X)^{-1} X^T W X is an unbiased estimator of the true coefficients in weighted least squares regression.</p>
<p>The expression “blkdiag()^{-1}” means the inverse of a block diagonal matrix where each block is a covariance matrix denoted by Sigma (). Specifically, if we have n covariance matrices _1, _2, …, _n, then the block diagonal matrix is given by:</p>
blkdiag(_1, _2, …, _n) =
<span class="math display">\[\begin{bmatrix} \Sigma_1 &amp; 0 &amp; \cdots &amp; 0 \ 0 &amp; \Sigma_2 &amp; \cdots &amp; 0 \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ 0 &amp; 0 &amp; \cdots &amp; \Sigma_n \end{bmatrix}\]</span>
<p>The inverse of this block diagonal matrix can be computed by taking the inverse of each block matrix and placing them on the diagonal, resulting in the following expression:</p>
[blkdiag(_1, _2, …, _n)]^{-1} =
<span class="math display">\[\begin{bmatrix} \Sigma_1^{-1} &amp; 0 &amp; \cdots &amp; 0 \ 0 &amp; \Sigma_2^{-1} &amp; \cdots &amp; 0 \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ 0 &amp; 0 &amp; \cdots &amp; \Sigma_n^{-1} \end{bmatrix}\]</span>
<p>Therefore, blkdiag()^{-1} is a block diagonal matrix where each block is the inverse of the corresponding covariance matrix, and it can be computed by taking the inverse of each block and placing them on the diagonal.</p>
<p>REML for estimating variance covrainace matrix divides the data into twp parts: irrelevant to sigma and irrelevant to beta (REML estimate)</p>
</section>
</section>
<section id="if-wblkdiag-1-in-longitudinal-data-analysis-why-more-efficient-than-ols" class="level2" data-number="0.4">
<h2 data-number="0.4" class="anchored" data-anchor-id="if-wblkdiag-1-in-longitudinal-data-analysis-why-more-efficient-than-ols"><span class="header-section-number">0.4</span> If W=blkdiag()^{-1} in longitudinal data analysis, why more efficient than OLS?</h2>
<p>In longitudinal data analysis, it is common to use a generalized linear mixed model (GLMM) to account for the correlated nature of the data. The GLMM framework includes random effects to capture the individual-level variation and allows for the specification of a covariance structure to model the correlation between the repeated measurements within each individual. When fitting a GLMM, the covariance matrix of the random effects, denoted by , needs to be estimated.</p>
<p>One way to estimate is to use maximum likelihood estimation (MLE), which involves maximizing the likelihood function of the GLMM with respect to the unknown parameters, including . However, the MLE of can be inefficient when the number of repeated measurements per individual is small or the correlation between the repeated measurements is weak.</p>
<p>To improve the efficiency of the MLE of , a weighted likelihood method can be used, where the likelihood function is multiplied by a weight matrix that depends on the estimated covariance matrix. Specifically, the weight matrix is given by W = blkdiag()^{-1}, where blkdiag() is the block diagonal matrix of the estimated covariance matrix . The inverse of blkdiag() is taken because it is a positive definite matrix and its inverse exists.</p>
<p>By weighting the likelihood function with W, the resulting weighted likelihood estimator (WLE) of is more efficient than the MLE because it incorporates additional information about the covariance structure of the data. The WLE of is then used in the GLMM to estimate the other parameters, such as the fixed effects.</p>
<p>In summary, using W = blkdiag()^{-1} as a weight matrix in the GLMM framework can improve the efficiency of the MLE of the covariance matrix and result in more accurate estimates of the other parameters in the model, making it more efficient than the OLS estimator, which assumes independence between the observations.</p>
<section id="family-and-longitudinal-data" class="level3" data-number="0.4.1">
<h3 data-number="0.4.1" class="anchored" data-anchor-id="family-and-longitudinal-data"><span class="header-section-number">0.4.1</span> Family and Longitudinal Data</h3>
<p>In longitudinal data analysis with family data, one common approach to estimating the regression coefficients is to use a linear mixed-effects model, also known as a multilevel model or a hierarchical model. The linear mixed-effects model can handle the within-subject correlation due to repeated measurements over time and the within-family correlation due to shared genetic or environmental factors among family members.</p>
<p>The linear mixed-effects model assumes that the outcome variable Y is a function of the fixed effects X and the random effects b, which can account for the within-subject and within-family correlation. The model can be written as:</p>
<p>Y = Xβ + Zb + ε</p>
<p>where β is the vector of fixed effects coefficients, b is the vector of random effects coefficients, Z is the design matrix for the random effects, and ε is the error term.</p>
<p>To estimate the fixed effects coefficients β, one can use maximum likelihood estimation (MLE) or restricted maximum likelihood estimation (REML) methods. The MLE estimates the variance components for both the random effects and the error term, while the REML estimates the variance components for only the random effects and adjusts for the bias in the likelihood function.</p>
<p>To fit the linear mixed-effects model, one can use software packages such as R, SAS, or Stata, which have functions for fitting linear mixed-effects models with repeated measurements and random effects. In R, for example, one can use the lme4 package to fit the linear mixed-effects model using the lmer() function. The output of the function includes the estimated fixed effects coefficients β and the estimated variance components for the random effects and the error term.</p>
<p>Overall, estimating the fixed effects coefficients β of family data with repeated measurements in longitudinal data analysis requires fitting a linear mixed-effects model that accounts for the within-subject and within-family correlation, and the choice of model depends on the research question and assumptions of the data.</p>
<p>In longitudinal data analysis with repeated measurements and correlated subjects, the variance-covariance matrix of the responses can be modeled using a mixed-effects model or a generalized estimating equation (GEE) model.</p>
<p>In the mixed-effects model, the variance-covariance matrix of the responses is decomposed into two components: the within-subject covariance and the between-subject covariance. The within-subject covariance accounts for the correlation between the repeated measurements within the same subject, while the between-subject covariance accounts for the correlation between subjects. The mixed-effects model allows for the inclusion of both fixed effects and random effects, which can model the mean and the variance structure of the responses.</p>
<p>In the GEE model, the variance-covariance matrix of the responses is modeled using a working correlation matrix, which specifies the correlation structure between the observations. The GEE model allows for the inclusion of fixed effects, but not random effects, and estimates the population-averaged mean and the variance structure of the responses.</p>
<p>To design the variance-covariance matrix of the responses in a mixed-effects model or a GEE model, one needs to specify the correlation structure between the repeated measurements within the same subject and between subjects. Common correlation structures for the within-subject covariance include the autoregressive (AR), the exchangeable, and the unstructured covariance structures, while common correlation structures for the between-subject covariance include the independent and the compound symmetry covariance structures.</p>
<p>The choice of correlation structure depends on the research question and assumptions of the data. For example, the AR(1) correlation structure assumes that the correlation between two measurements decreases as the time lag between them increases, while the exchangeable correlation structure assumes that all measurements within the same subject are equally correlated. The choice of correlation structure can be evaluated using statistical criteria such as the Akaike information criterion (AIC) or the Bayesian information criterion (BIC).</p>
<p>Overall, designing the variance-covariance matrix of the responses in longitudinal data analysis with repeated measurements and correlated subjects requires specifying the correlation structure between the repeated measurements within the same subject and between subjects, and the choice of model depends on the research question and assumptions of the data.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this file contains code for linear marginal models for longitudinal data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># We test different covariance patterns and show how to fit model with WLS and REML</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># By Gen Li, 1/1/2018</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># also check lme4</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(nlme)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>opposites <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">"https://stats.idre.ucla.edu/stat/r/examples/alda/data/opposites_pp.txt"</span>,<span class="at">header=</span><span class="cn">TRUE</span>,<span class="at">sep=</span><span class="st">","</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(opposites)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># spaghetti plot</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">ggplot</span>(opposites, <span class="fu">aes</span>(time, opp, <span class="at">group=</span>id)) <span class="sc">+</span> <span class="fu">geom_line</span>()</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit different cov model with REML</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="do">###################################################</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># unstructured covariance for marginal model</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>unstruct <span class="ot">&lt;-</span> <span class="fu">gls</span>(opp<span class="sc">~</span>time<span class="sc">*</span>ccog,opposites, <span class="at">correlation=</span><span class="fu">corSymm</span>(<span class="at">form =</span> <span class="sc">~</span> <span class="dv">1</span> <span class="sc">|</span>id),  <span class="at">weights=</span><span class="fu">varIdent</span>(<span class="at">form =</span> <span class="sc">~</span> <span class="dv">1</span><span class="sc">|</span> wave),<span class="at">method=</span><span class="st">"REML"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># corSymm(form = ~ 1 |id) : the same covariance across different measurement, same correlation matrix for different subjects</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># check ?gls ?corClasses ?corSymm</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(unstruct) <span class="co"># focus on corr, var, (weight)</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>unstruct<span class="sc">$</span>modelStruct<span class="sc">$</span>corStruct <span class="co"># corr</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>unstruct<span class="sc">$</span>modelStruct<span class="sc">$</span>varStruct <span class="co"># variance:weight</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>unstruct<span class="sc">$</span>sigma <span class="co"># standard deviation</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="fu">cov2cor</span>(unstruct<span class="sc">$</span>varBeta)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># compound symmetry</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>comsym <span class="ot">&lt;-</span> <span class="fu">gls</span>(opp<span class="sc">~</span>time,opposites, <span class="at">correlation=</span><span class="fu">corCompSymm</span>(<span class="at">form =</span> <span class="sc">~</span> <span class="dv">1</span><span class="sc">|</span>id),   <span class="at">weights=</span><span class="fu">varIdent</span>(<span class="at">form =</span> <span class="sc">~</span> <span class="dv">1</span><span class="sc">|</span> wave), <span class="at">method=</span><span class="st">"REML"</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(comsym)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="fu">corMatrix</span>(comsym<span class="sc">$</span>modelStruct<span class="sc">$</span>corStruct)[[<span class="dv">1</span>]]</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co"># AR(1)</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>auto1 <span class="ot">&lt;-</span> <span class="fu">gls</span>(opp<span class="sc">~</span>time ,opposites, <span class="at">correlation=</span><span class="fu">corAR1</span>(<span class="at">form =</span> <span class="sc">~</span> <span class="dv">1</span> <span class="sc">|</span>id), <span class="at">method=</span><span class="st">"REML"</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(auto1)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="fu">corMatrix</span>(auto1<span class="sc">$</span>modelStruct<span class="sc">$</span>corStruct)[[<span class="dv">1</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Longitudinal Study vs Cross-Sectional Study Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>A cross-sectional study found that older people smoke more.</p>
<p>Possible explanations:</p>
<ul>
<li>People tend to smoke more when they get older.</li>
<li>Older people grew up in an environment where the harm of smoking was less widely accepted. In other words, when older people were younger, smoking was more socially acceptable and its harmful effects were not well-known or well-publicized. As a result, they may have started smoking earlier in life and developed a stronger habit or addiction. This explanation implies that younger people today may be less likely to smoke because they are growing up in an environment where smoking is less socially acceptable and the risks are more widely known.</li>
</ul>
<p>LDA can distinguish the effect due to aging (i.e., changes over time within subject) from cohort effects (i.e., difference between subjects at baseline). Cross-sectional study cannot.</p>
</div>
</div>
</section>
</section>
<section id="advantages-of-longitudinal-data-analysis" class="level2" data-number="0.5">
<h2 data-number="0.5" class="anchored" data-anchor-id="advantages-of-longitudinal-data-analysis"><span class="header-section-number">0.5</span> Advantages of Longitudinal Data Analysis</h2>
<ul>
<li>Each subject can serve as his/her own control. Influence of genetic make-up, environmental exposures, and maybe unmeasured characteristics tend to persist over time.
<ul>
<li>in certain types of studies or experiments, individuals can be used as their own comparison group. This means that the same person is tested or measured at different points in time, and the differences observed can be attributed to changes over time rather than to differences between individuals.</li>
<li>For example, in a study looking at the effect of a new medication on blood pressure, each participant’s blood pressure before and after taking the medication would be compared to determine if there was a change. By using the same participant as their own control, the effects of genetic factors, environmental exposures, and other individual differences that might influence blood pressure would be minimized.</li>
<li>However, even when using this approach, some individual differences that are not directly measured or controlled for may persist over time and influence the results. These could include factors such as diet, stress levels, or other lifestyle habits that could impact the outcome being measured.</li>
</ul></li>
<li>Distinguish the degree of variation in <span class="math inline">\(Y\)</span> across time within a subject from the variation in <span class="math inline">\(Y\)</span> between subjects. With repeated values, one can borrow strength across time for the person of interest as well as across people.
<ul>
<li>when you have repeated measurements of a variable (Y) for the same person over time, you can use the information from those repeated measurements to better estimate the true value of Y for that person at any given time point. This is known as borrowing strength across time.</li>
<li>Additionally, you can also use the information from the repeated measurements of Y across different people to better estimate the true value of Y for a particular time point across the population. This is known as borrowing strength across people.</li>
<li>By doing both, you can distinguish the degree of variation in Y across time within a subject (i.e., how much Y varies for a particular person over time) from the variation in Y between subjects (i.e., how much Y varies between different people at a particular time point).</li>
</ul></li>
<li>Increased power, by repeated measurements. The repeated measurements from the same subject are rarely perfectly correlated. Hence, longitudinal studies are more powerful than cross-sectional studies.
<ul>
<li>Longitudinal studies are more powerful than cross-sectional studies because they allow researchers to directly model and account for the within-subject correlation among repeated measurements. In other words, longitudinal studies take advantage of the fact that individuals are their own controls by measuring outcomes at multiple time points, which allows for a more accurate estimation of the true effect of an exposure or intervention.</li>
<li>In contrast, cross-sectional studies only measure outcomes at a single time point, which makes it difficult to distinguish between within-subject variability and between-subject variability. In a cross-sectional study, any observed differences between groups may be due to differences in the underlying populations, or due to differences in the timing of the outcome measurement, or both.</li>
<li>Furthermore, longitudinal studies can also provide information on the rate of change in the outcome over time, which can be important in understanding disease progression, treatment effects, or the impact of other time-dependent factors.</li>
</ul></li>
<li>Therefore, because longitudinal studies allow for a more accurate estimation of the true effect of an exposure or intervention and provide more information about disease progression, they are generally considered more powerful than cross-sectional studies.</li>
</ul>
<section id="specialty-of-lda" class="level3" data-number="0.5.1">
<h3 data-number="0.5.1" class="anchored" data-anchor-id="specialty-of-lda"><span class="header-section-number">0.5.1</span> Specialty of LDA</h3>
<p>LDA requires special statistical methods because the set of observations on one subject tends to be inter-correlated.</p>
<p><a href="./childhood readbility.PNG">Copied from Diggle et al.&nbsp;(2002, page 2).</a></p>
<ul>
<li>Consider the example, variation of readability of child as they get aged.
<ul>
<li>Assume this is a longitudinal study with two measurements per child at two age or time points.</li>
<li>The two measurements per subject may be highly correlated.</li>
<li>If we use cross-sectional methods to analyze the data, we may not be able to distinguish changes over time within individual and difference among people in their baseline levels.
<ul>
<li>Only plot (a) is from cross sectional study. Not using connected lines might mislead the time trend within subjects.</li>
</ul></li>
</ul></li>
<li>In general, repeated observations <span class="math inline">\(y_{i1}, \dots , y_{in_i}\)</span> for subject <span class="math inline">\(i\)</span> are likely to be correlated, so the independence assumption is violated.</li>
<li>The standard regression methods (ignoring correlation) may lead to
<ul>
<li>Incorrect inference
<ul>
<li>the violation of the independence assumption means that the errors in the model are correlated across observations, and this correlation can bias the estimated coefficients.</li>
<li>The errors in the model are correlated across observations when there is some form of dependence or clustering in the data. This means that the error term in one observation is related to the error terms in other observations, either through some underlying factor or because of the way the data is collected.</li>
<li>When the errors are correlated across observations, the estimated coefficients from standard regression methods may be biased because the regression model assumes that the errors are independent.</li>
<li>The bias in the estimated coefficients can arise in several ways:
<ul>
<li>The standard errors of the estimated coefficients will be too small, which can lead to overconfidence in the results.</li>
<li>The estimated coefficients may not reflect the true relationships between the dependent variable and the independent variables, as the correlation between the errors can distort the estimates.</li>
<li>The estimates of the standard errors will be biased, leading to incorrect inference in hypothesis testing and confidence interval construction.</li>
</ul></li>
<li>To sum up, failing to account for the correlation between errors can lead to incorrect and imprecise estimates of the coefficients and standard errors in a regression model.</li>
</ul></li>
<li>Inefficient estimates of <span class="math inline">\(\beta\)</span>
<ul>
<li>the estimates are less precise than they could be if the correlation between observations were taken into account.</li>
<li>The standard errors of the estimates will be too large, which means that confidence intervals will be wider and hypothesis tests will have less power.</li>
</ul></li>
<li>Oversight of important correlation structure
<ul>
<li>the standard regression methods may miss important patterns of correlation in the data that could be used to improve the accuracy and precision of the estimates.</li>
<li>For example, if there is a time trend in the data that is not accounted for, the standard errors of the estimates may be too large, and the estimates may not capture the true effect of the independent variables.</li>
<li>Accounting for the correlation structure in the data can lead to more accurate and precise estimates, and can also help identify interesting patterns and relationships that might otherwise be missed.</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
</div>
</div>
<div id="English" class="tab-pane fade" role="tabpanel" aria-labelledby="English-tab">
<div id="English" class="tab-pane fade" role="tabpanel" aria-labelledby="English-tab">

</div>
</div>
<section id="go-to-project-content-list" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Go to Project Content List</h1>
<p><a href="./docs/projects/index.qmd">Project Content List</a></p>
</section>
<section id="go-to-blog-content-list" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Go to Blog Content List</h1>
<p><a href="./docs/blog/posts/content_list.qmd">Blog Content List</a></p>


</section>

</div></ul></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>