---
title: LDA (3) - Weight Least Square & REML
subtitle: Overview
description: |
  template
categories:
  - Statistics
author: Kwangmin Kim
date: 03/25/2023
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: true
---

<ul class="nav nav-pills" id="language-tab" role="tablist">
  <li class="nav-item" role="presentation">
    <button class="nav-link active" id="Korean-tab" data-bs-toggle="tab" data-bs-target="#Korean" type="button" role="tab" aria-controls="Korean" aria-selected="true">Korean</button>
  </li>
  <li class="nav-item" role="presentation">
    <button class="nav-link" id="English-tab" data-bs-toggle="tab" data-bs-target="#English" type="button" role="tab" aria-controls="knitr" aria-selected="false">English</button>
  </li>

<div class="tab-content" id="language-tabcontent">

<div class="tab-pane fade  show active" id="Korean" role="tabpanel" aria-labelledby="Korean-tab">

::: {#Korean .tab-pane .fade .show .active role="tabpanel" aria-labelledby="Korean-tab"}

## Notations

* $y_{ij}$ : the univariate response (i.e. scalar) for the $i$ th subject at the $j$ th occasion or measurement
  * later when I use the vector case, I will re-define this notation, but focus on the scalar case for now.
* $x_{ij}$ : the predictor at time $t_{ij}$, which is either a scalr or vector.
  * a scalar case: $x_{ij}$ where $i$ is the $i$ th subject, and $j$ is the $j$ th measurement.
  * a vector case: $x_{ijk}$ where $i$ is the $i$ th subject, $j$ is the $j$ th measurement, and $k \in [1,p]$ is the $k$ th predictor.
  * sometimes, covariate for different measurements could be the same. In this case, the notation could be written in $x_{i}$ 
    * ex) a gender does not change over time in the most cases.
* $i=1, \dots, m$ : i is the index for the $i$ th subject
* $j=1, \dots, n_i$ : j is the index for the $j$ th measurement of the $i$ th subject
  * ${n_i}$ is the number of measurements of the $i$ th subject, each ${n_i}$ does not have to the same.
  * balanced desgin: ${n_i}$ is the same.
  * unbalanced desgin: ${n_i}$ is different.
* $\mathbf y_i$ : a vector (not a matrix), $(y_{i1},y_{i2},\dots ,y_{in_i})$ of the $i$ th subject
* $\mathbf Y$ : the reponse matrix 
* $\mathbf X$ : the predictor matrix 
* $\text{E}(y_{ij})$ : $\mu_{ij}$
* $\text{E}(\mathbf y_i)$ : $\mathbf \mu_{i}$
* $\text{Var}(\mathbf y_i)$ : $\text{Var}(\mathbf y_i)$ is a variance-covariance matrix of the different measurement for the $i$ th subject
  * for now, we do not care of the variance covariance of the different subjects because we assume that the measurements of different subjects are indpendent.
$$
\begin{bmatrix}
    \text{Var}(y_{i1}) & \text{Cov}( y_{i1}, y_{i2}) & \dots & \text{Cov}( y_{i1}, y_{in_i}) \\
                               & \text{Var}( y_{i2}) & \dots & \text{Cov}( y_{i2}, y_{in_i}) \\
                                 &                           & \ddots & \vdots \\
                                 &&                            \dots & \text{Var}( y_{in_i}) 
\end{bmatrix}
$$

## OLS vs GLS

OLS (Ordinary Least Squares) and GLS (Generalized Least Squares) are both methods of regression analysis used to model the relationship between a dependent variable and one or more independent variables. The main difference between OLS and GLS is in the assumptions about the errors in the model. 

### OLS 

It assumes that the errors are homoscedastic and independent of each other. OLS is a simpler and more commonly used method, but it may not be appropriate for datasets with non-constant variances or autocorrelation in the errors.

#### OLS Solutions

$$
\begin{aligned}
  \hat{\beta}&=\min_\beta||Y-X\beta||^2 \\ 
  &=(X^TX)^{-1}X^TY
\end{aligned}
$$

$\hat \beta$ is unbiased because if we assume that the errors $\epsilon \sim N(0,\simga^2I)$, 
$E(\hat \beta)=\text{E}((X^TX)^{-1}X^TY)=(X^TX)^{-1}X^T\text{E}(Y)=(X^TX)^{-1}X^TX\beta=\beta$

### GLS 

It relaxes the assumptions of OLS and allows for heteroscedasticity and autocorrelation in the data. GLS is a more flexible method that can handle heteroscedasticity and autocorrelation in the data, but it requires more complex computations and may be computationally expensive for large datasets.

#### Generalized Least Squares

The estimator $\hat{\beta}$, which is also known as the generalized least squares estimator, is given by $\hat{\beta}=(X^TWX)^{-1}X^TWY$, where $W$ is a positive definite weighting matrix.

$$
\begin{aligned}
  \hat{\beta}&=\min_\beta||Y-X\beta||^2 \\ 
  &=(X^TWX)^{-1}X^TWY
\end{aligned}
$$

Under the assumptions of the GLS model, the GLS estimator is unbiased. This means that on average, the GLS estimator will estimate the true population parameters correctly.

To be specific, the GLS estimator is based on the assumption that the errors or residuals follow a multivariate normal distribution with a mean vector of zeros and a covariance matrix that is known up to a scalar factor. If this assumption holds, then the GLS estimator is the Best Linear Unbiased Estimator (BLUE) of the model parameters.

Being a BLUE estimator implies that the GLS estimator has the smallest possible variance among all linear unbiased estimators. Therefore, under the GLS assumptions, the GLS estimator is not only unbiased but also efficient, meaning that it achieves the lowest possible variance of all unbiased estimators.

However, it is important to note that the GLS estimator is only unbiased and efficient if the underlying assumptions are correct. If the assumptions are violated (e.g., the errors are not normally distributed or the covariance structure is misspecified), then the GLS estimator may not be unbiased or efficient.

$\hat{\beta}_{GLS} = (X^T V^{-1} X)^{-1} X^T V^{-1} Y$

where $V$ is the known covariance matrix of the errors or residuals.

To show that $\hat{\beta}_{GLS}$ is unbiased, we need to show that:

$E(\hat{\beta}_{GLS}) = \beta$

where $\beta$ is the true population parameter.

Using the linearity of expectation, we have:

$$
\begin{align}
E(\hat{\beta}_{GLS}) 
&= E((X^T W^{-1} X)^{-1} X^T W^{-1} Y)\\
&= (X^T W^{-1} X)^{-1} X^T W^{-1} E(Y)\\
&= (X^T W^{-1} X)^{-1} X^T W^{-1} X \beta\\
&= (X^T W^{-1} X)^{-1} X^T W^{-1} X (X^T X)^{-1} X^T X \beta\\
&= (X^T X)^{-1} X^T W^{-1} X (X^T X)^{-1} X^T X \beta\\
&= (X^T X)^{-1} X^T W^{-1} Y\\
&= \beta
\end{align}
$$

where the second-to-last equality follows from the fact that $X^T W^{-1} X$ is a symmetric positive definite matrix, and hence its inverse can be written as $(X^T X)^{-1}$. Therefore, we have shown that the GLS estimator is unbiased, i.e., its expected value is equal to the true population parameter.


2. WLS

W dimension: the same diemnsion of the total measurement m*n

In weighted least squares (WLS) regression, the goal is to find the values of the coefficients b0 and b1 that minimize the weighted sum of the squared residuals. The weights are used to adjust the influence of each data point on the fitting line.

The formula for the coefficients b0 and b1 in WLS is very similar to the one in OLS, with the addition of a weight matrix W.

The equation for WLS is Y = b0 + b1*X + e, where e is the error term and W is a diagonal matrix of weights.

To derive the formula for the coefficients b0 and b1 in WLS, we need to define the following matrices:

X: the matrix of independent variables (also called the design matrix)
Y: the matrix of dependent variables
W: the weight matrix (a diagonal matrix of weights)
beta: the vector of coefficients to be estimated
The formula for beta in WLS is beta = (X^T W X)^{-1} X^T W Y, where (^T) denotes the transpose of a matrix and (^{-1}) denotes the inverse of a matrix.

To show that this formula is unbiased, we need to show that the expected value of beta equals the true value of the coefficients, that is E(beta) = (b0, b1).

Let's start by calculating the expected value of beta:

E(beta) = E((X^T W X)^{-1} X^T W Y)

Using the properties of matrix expectations, we can simplify this expression:

E(beta) = (X^T W X)^{-1} X^T W E(Y)

Since Y = b0 + b1*X + e, we have:

E(Y) = E(b0 + b1X + e) = b0 + b1E(X) + E(e) = b0 + b1*X

where we have used the fact that E(e) = 0 and E(X) is a constant.

Substituting this expression for E(Y) into the previous equation, we get:

E(beta) = (X^T W X)^{-1} X^T W (b0 + b1*X)

Expanding this expression, we get:

E(beta) = (X^T W X)^{-1} X^T W b0 + (X^T W X)^{-1} X^T W b1*X

Using the distributive property of matrix multiplication, we can simplify this expression further:

E(beta) = (X^T W X)^{-1} X^T W b0 + b1 (X^T W X)^{-1} (X^T W X)

Since (X^T W X)^{-1} (X^T W X) = I (the identity matrix), we have:

E(beta) = (X^T W X)^{-1} X^T W b0 + b1 I

Finally, we can compare this expression with the true value of the coefficients (b0, b1):

(b0, b1) = (X^T W X)^{-1} X^T W Y

Substituting Y = b0 + b1*X + e into this expression, we get:

(b0, b1) = (X^T W X)^{-1} X^T W (b0 + b1*X + e)

Using the distributive property of matrix multiplication, we can simplify this expression to:

(b0, b1) = (X^T W X)^{-1} X^T W b0 + b1 (X^T W X)^{-1} X^T W X

uniasedness of WLS

To show that (b0, b1) = (X^T W X)^{-1} X^T W b0 + b1 (X^T W X)^{-1} X^T W X is an unbiased estimator of the true coefficients (b0, b1), we need to show that E((b0, b1)) = (b0, b1).

Taking the expected value of both sides of the equation (b0, b1) = (X^T W X)^{-1} X^T W b0 + b1 (X^T W X)^{-1} X^T W X, we have:

E((b0, b1)) = E((X^T W X)^{-1} X^T W b0 + b1 (X^T W X)^{-1} X^T W X)

Using the linearity of the expectation operator, we can distribute it inside the sum, obtaining:

E((b0, b1)) = E((X^T W X)^{-1} X^T W b0) + E(b1 (X^T W X)^{-1} X^T W X)

We can calculate each term separately. First, let's consider the term E((X^T W X)^{-1} X^T W b0):

E((X^T W X)^{-1} X^T W b0) = (X^T W X)^{-1} X^T W E(b0)

Since b0 is a constant, we have E(b0) = b0. Substituting this in the expression above, we get:

E((X^T W X)^{-1} X^T W b0) = (X^T W X)^{-1} X^T W b0

Now let's consider the term E(b1 (X^T W X)^{-1} X^T W X):

E(b1 (X^T W X)^{-1} X^T W X) = b1 E((X^T W X)^{-1} X^T W X)

We need to show that E((X^T W X)^{-1} X^T W X) = I, where I is the identity matrix. This is true because:

E((X^T W X)^{-1} X^T W X) = (X^T W X)^{-1} X^T W E(X) = (X^T W X)^{-1} X^T W X

where we have used the fact that E(X) is a constant and W is a diagonal matrix with non-zero entries. Therefore, we have:

E(b1 (X^T W X)^{-1} X^T W X) = b1 I

Substituting these expressions for the two terms back into the original equation, we obtain:

E((b0, b1)) = (X^T W X)^{-1} X^T W b0 + b1 I

Comparing this with the true coefficients (b0, b1), we see that they are equal. Therefore, we have shown that (b0, b1) = (X^T W X)^{-1} X^T W b0 + b1 (X^T W X)^{-1} X^T W X is an unbiased estimator of the true coefficients in weighted least squares regression.


The expression "blkdiag(\Sigma)^{-1}" means the inverse of a block diagonal matrix where each block is a covariance matrix denoted by Sigma (\Sigma). Specifically, if we have n covariance matrices \Sigma_1, \Sigma_2, ..., \Sigma_n, then the block diagonal matrix is given by:

blkdiag(\Sigma_1, \Sigma_2, ..., \Sigma_n) = \begin{bmatrix} \Sigma_1 & 0 & \cdots & 0 \ 0 & \Sigma_2 & \cdots & 0 \ \vdots & \vdots & \ddots & \vdots \ 0 & 0 & \cdots & \Sigma_n \end{bmatrix}

The inverse of this block diagonal matrix can be computed by taking the inverse of each block matrix and placing them on the diagonal, resulting in the following expression:

[blkdiag(\Sigma_1, \Sigma_2, ..., \Sigma_n)]^{-1} = \begin{bmatrix} \Sigma_1^{-1} & 0 & \cdots & 0 \ 0 & \Sigma_2^{-1} & \cdots & 0 \ \vdots & \vdots & \ddots & \vdots \ 0 & 0 & \cdots & \Sigma_n^{-1} \end{bmatrix}

Therefore, blkdiag(\Sigma)^{-1} is a block diagonal matrix where each block is the inverse of the corresponding covariance matrix, and it can be computed by taking the inverse of each block and placing them on the diagonal.

REML for estimating \Sigma variance covrainace matrix divides the data into twp parts: irrelevant to sigma and irrelevant to beta (REML estimate)


#### GLS Solution

The Generalized Least Squares (GLS) estimator is obtained by minimizing the weighted sum of squared residuals, where the weights are based on the estimated variance-covariance matrix of the errors.

To derive the GLS estimator, we start with the linear regression model:

$Y = X\beta + \epsilon$

where $Y$ is the response variable, $X$ is the design matrix of predictor variables, $\beta$ is a vector of unknown coefficients, and $\epsilon$ is a vector of errors or residuals.

The covariance matrix of the errors is denoted by $V = \text{Cov}(\epsilon)$, which is assumed to be known up to a scalar factor. Specifically, we assume that $V = \sigma^2 W$, where $\sigma^2$ is an unknown scalar factor and $W$ is a known positive definite matrix.

The GLS estimator of $\beta$ is obtained by minimizing the weighted sum of squared residuals:

$Q(\beta) = (\textbf{Y} - \textbf{X}\beta)^T V^{-1} (\textbf{Y} - \textbf{X}\beta)$

where $\textbf{Y}$ and $\textbf{X}$ are the vectors of observed responses and design matrix of predictors, respectively.

Taking the derivative of $Q(\beta)$ with respect to $\beta$, and setting it to zero, we get:

$\frac{\partial Q(\beta)}{\partial \beta} = -2 \textbf{X}^T V^{-1} (\textbf{Y} - \textbf{X}\beta) = 0$

Solving for $\beta$, we obtain the GLS estimator:

$\hat{\beta}_{GLS} = (\textbf{X}^T V^{-1} \textbf{X})^{-1} \textbf{X}^T V^{-1} \textbf{Y}$

where $V^{-1} = \frac{1}{\sigma^2} W^{-1}$.

Note that the GLS estimator reduces to the OLS estimator when $V$ is a scalar multiple of the identity matrix, i.e., when the errors have constant variance and are uncorrelated. In this case, $W = I$ and $V = \sigma^2 I$, and the GLS estimator simplifies to:

$\hat{\beta}_{OLS} = (\textbf{X}^T \textbf{X})^{-1} \textbf{X}^T \textbf{Y}$

which is the usual OLS estimator.





## If  W=blkdiag(\Sigma)^{-1} in longitudinal data analysis, why more efficient than OLS?

In longitudinal data analysis, it is common to use a generalized linear mixed model (GLMM) to account for the correlated nature of the data. The GLMM framework includes random effects to capture the individual-level variation and allows for the specification of a covariance structure to model the correlation between the repeated measurements within each individual. When fitting a GLMM, the covariance matrix of the random effects, denoted by \Sigma, needs to be estimated.

One way to estimate \Sigma is to use maximum likelihood estimation (MLE), which involves maximizing the likelihood function of the GLMM with respect to the unknown parameters, including \Sigma. However, the MLE of \Sigma can be inefficient when the number of repeated measurements per individual is small or the correlation between the repeated measurements is weak.

To improve the efficiency of the MLE of \Sigma, a weighted likelihood method can be used, where the likelihood function is multiplied by a weight matrix that depends on the estimated covariance matrix. Specifically, the weight matrix is given by W = blkdiag(\Sigma)^{-1}, where blkdiag(\Sigma) is the block diagonal matrix of the estimated covariance matrix \Sigma. The inverse of blkdiag(\Sigma) is taken because it is a positive definite matrix and its inverse exists.

By weighting the likelihood function with W, the resulting weighted likelihood estimator (WLE) of \Sigma is more efficient than the MLE because it incorporates additional information about the covariance structure of the data. The WLE of \Sigma is then used in the GLMM to estimate the other parameters, such as the fixed effects.

In summary, using W = blkdiag(\Sigma)^{-1} as a weight matrix in the GLMM framework can improve the efficiency of the MLE of the covariance matrix \Sigma and result in more accurate estimates of the other parameters in the model, making it more efficient than the OLS estimator, which assumes independence between the observations.

### Family and Longitudinal Data


In longitudinal data analysis with family data, one common approach to estimating the regression coefficients is to use a linear mixed-effects model, also known as a multilevel model or a hierarchical model. The linear mixed-effects model can handle the within-subject correlation due to repeated measurements over time and the within-family correlation due to shared genetic or environmental factors among family members.

The linear mixed-effects model assumes that the outcome variable Y is a function of the fixed effects X and the random effects b, which can account for the within-subject and within-family correlation. The model can be written as:

Y = Xβ + Zb + ε

where β is the vector of fixed effects coefficients, b is the vector of random effects coefficients, Z is the design matrix for the random effects, and ε is the error term.

To estimate the fixed effects coefficients β, one can use maximum likelihood estimation (MLE) or restricted maximum likelihood estimation (REML) methods. The MLE estimates the variance components for both the random effects and the error term, while the REML estimates the variance components for only the random effects and adjusts for the bias in the likelihood function.

To fit the linear mixed-effects model, one can use software packages such as R, SAS, or Stata, which have functions for fitting linear mixed-effects models with repeated measurements and random effects. In R, for example, one can use the lme4 package to fit the linear mixed-effects model using the lmer() function. The output of the function includes the estimated fixed effects coefficients β and the estimated variance components for the random effects and the error term.

Overall, estimating the fixed effects coefficients β of family data with repeated measurements in longitudinal data analysis requires fitting a linear mixed-effects model that accounts for the within-subject and within-family correlation, and the choice of model depends on the research question and assumptions of the data.

In longitudinal data analysis with repeated measurements and correlated subjects, the variance-covariance matrix of the responses can be modeled using a mixed-effects model or a generalized estimating equation (GEE) model.

In the mixed-effects model, the variance-covariance matrix of the responses is decomposed into two components: the within-subject covariance and the between-subject covariance. The within-subject covariance accounts for the correlation between the repeated measurements within the same subject, while the between-subject covariance accounts for the correlation between subjects. The mixed-effects model allows for the inclusion of both fixed effects and random effects, which can model the mean and the variance structure of the responses.

In the GEE model, the variance-covariance matrix of the responses is modeled using a working correlation matrix, which specifies the correlation structure between the observations. The GEE model allows for the inclusion of fixed effects, but not random effects, and estimates the population-averaged mean and the variance structure of the responses.

To design the variance-covariance matrix of the responses in a mixed-effects model or a GEE model, one needs to specify the correlation structure between the repeated measurements within the same subject and between subjects. Common correlation structures for the within-subject covariance include the autoregressive (AR), the exchangeable, and the unstructured covariance structures, while common correlation structures for the between-subject covariance include the independent and the compound symmetry covariance structures.

The choice of correlation structure depends on the research question and assumptions of the data. For example, the AR(1) correlation structure assumes that the correlation between two measurements decreases as the time lag between them increases, while the exchangeable correlation structure assumes that all measurements within the same subject are equally correlated. The choice of correlation structure can be evaluated using statistical criteria such as the Akaike information criterion (AIC) or the Bayesian information criterion (BIC).

Overall, designing the variance-covariance matrix of the responses in longitudinal data analysis with repeated measurements and correlated subjects requires specifying the correlation structure between the repeated measurements within the same subject and between subjects, and the choice of model depends on the research question and assumptions of the data.



```{r}
#| eval: false

# this file contains code for linear marginal models for longitudinal data
# We test different covariance patterns and show how to fit model with WLS and REML
# By Gen Li, 1/1/2018
# also check lme4


library(nlme)
library(tidyverse)
opposites <- read.table("https://stats.idre.ucla.edu/stat/r/examples/alda/data/opposites_pp.txt",header=TRUE,sep=",")
head(opposites)

# spaghetti plot
p = ggplot(opposites, aes(time, opp, group=id)) + geom_line()
print(p)


# Fit different cov model with REML
###################################################
# unstructured covariance for marginal model
unstruct <- gls(opp~time*ccog,opposites, correlation=corSymm(form = ~ 1 |id),  weights=varIdent(form = ~ 1| wave),method="REML")

# corSymm(form = ~ 1 |id) : the same covariance across different measurement, same correlation matrix for different subjects
# check ?gls ?corClasses ?corSymm
summary(unstruct) # focus on corr, var, (weight)
unstruct$modelStruct$corStruct # corr
unstruct$modelStruct$varStruct # variance:weight
unstruct$sigma # standard deviation
cov2cor(unstruct$varBeta)


# compound symmetry
comsym <- gls(opp~time,opposites, correlation=corCompSymm(form = ~ 1|id),   weights=varIdent(form = ~ 1| wave), method="REML")
summary(comsym)
corMatrix(comsym$modelStruct$corStruct)[[1]]



# AR(1)
auto1 <- gls(opp~time ,opposites, correlation=corAR1(form = ~ 1 |id), method="REML")
summary(auto1)
corMatrix(auto1$modelStruct$corStruct)[[1]]



```


::: {.callout-tip}

### Longitudinal Study vs Cross-Sectional Study Example

A cross-sectional study found that older people smoke more.

Possible explanations:

* People tend to smoke more when they get older.
* Older people grew up in an environment where the harm of smoking was less widely accepted. In other words, when older people were younger, smoking was more socially acceptable and its harmful effects were not well-known or well-publicized. As a result, they may have started smoking earlier in life and developed a stronger habit or addiction. This explanation implies that younger people today may be less likely to smoke because they are growing up in an environment where smoking is less socially acceptable and the risks are more widely known.

LDA can distinguish the effect due to aging (i.e., changes over time within subject) from cohort effects (i.e., difference between subjects at baseline). Cross-sectional study cannot.
:::

## Advantages of Longitudinal Data Analysis

* Each subject can serve as his/her own control. Influence of genetic make-up, environmental exposures, and maybe unmeasured characteristics tend to persist over time.
    * in certain types of studies or experiments, individuals can be used as their own comparison group. This means that the same person is tested or measured at different points in time, and the differences observed can be attributed to changes over time rather than to differences between individuals. 
    * For example, in a study looking at the effect of a new medication on blood pressure, each participant's blood pressure before and after taking the medication would be compared to determine if there was a change. By using the same participant as their own control, the effects of genetic factors, environmental exposures, and other individual differences that might influence blood pressure would be minimized.
    * However, even when using this approach, some individual differences that are not directly measured or controlled for may persist over time and influence the results. These could include factors such as diet, stress levels, or other lifestyle habits that could impact the outcome being measured.
* Distinguish the degree of variation in $Y$ across time within a subject from the variation in $Y$ between subjects. With repeated values, one can borrow strength across time for the person of interest as well as across people.
    * when you have repeated measurements of a variable (Y) for the same person over time, you can use the information from those repeated measurements to better estimate the true value of Y for that person at any given time point. This is known as borrowing strength across time.
    * Additionally, you can also use the information from the repeated measurements of Y across different people to better estimate the true value of Y for a particular time point across the population. This is known as borrowing strength across people.
    * By doing both, you can distinguish the degree of variation in Y across time within a subject (i.e., how much Y varies for a particular person over time) from the variation in Y between subjects (i.e., how much Y varies between different people at a particular time point).
* Increased power, by repeated measurements. The repeated measurements from the same subject are rarely perfectly correlated. Hence, longitudinal studies are more powerful than cross-sectional studies.
    * Longitudinal studies are more powerful than cross-sectional studies because they allow researchers to directly model and account for the within-subject correlation among repeated measurements. In other words, longitudinal studies take advantage of the fact that individuals are their own controls by measuring outcomes at multiple time points, which allows for a more accurate estimation of the true effect of an exposure or intervention.
    * In contrast, cross-sectional studies only measure outcomes at a single time point, which makes it difficult to distinguish between within-subject variability and between-subject variability. In a cross-sectional study, any observed differences between groups may be due to differences in the underlying populations, or due to differences in the timing of the outcome measurement, or both. 
    * Furthermore, longitudinal studies can also provide information on the rate of change in the outcome over time, which can be important in understanding disease progression, treatment effects, or the impact of other time-dependent factors.
* Therefore, because longitudinal studies allow for a more accurate estimation of the true effect of an exposure or intervention and provide more information about disease progression, they are generally considered more powerful than cross-sectional studies.

### Specialty of LDA

LDA requires special statistical methods because the set of observations on one subject tends to be inter-correlated.

[Copied from Diggle et al. (2002, page 2).](./childhood%20readbility.PNG)

* Consider the example, variation of readability of child as they get aged.
    * Assume this is a longitudinal study with two measurements per child at two age or time points.
    * The two measurements per subject may be highly correlated.
    * If we use cross-sectional methods to analyze the data, we may not be able to distinguish changes over time within individual and difference among people in their baseline levels.
        * Only plot (a) is from cross sectional study. Not using connected lines might mislead the time trend within subjects.
* In general, repeated observations $y_{i1}, \dots , y_{in_i}$ for subject $i$ are likely to be correlated, so the independence assumption is violated.
* The standard regression methods (ignoring correlation) may lead to
    * Incorrect inference
        * the violation of the independence assumption means that the errors in the model are correlated across observations, and this correlation can bias the estimated coefficients.
        * The errors in the model are correlated across observations when there is some form of dependence or clustering in the data. This means that the error term in one observation is related to the error terms in other observations, either through some underlying factor or because of the way the data is collected.
        * When the errors are correlated across observations, the estimated coefficients from standard regression methods may be biased because the regression model assumes that the errors are independent. 
        * The bias in the estimated coefficients can arise in several ways:
            * The standard errors of the estimated coefficients will be too small, which can lead to overconfidence in the results.
            * The estimated coefficients may not reflect the true relationships between the dependent variable and the independent variables, as the correlation between the errors can distort the estimates.
            * The estimates of the standard errors will be biased, leading to incorrect inference in hypothesis testing and confidence interval construction.
        * To sum up, failing to account for the correlation between errors can lead to incorrect and imprecise estimates of the coefficients and standard errors in a regression model.
    * Inefficient estimates of $\beta$
        * the estimates are less precise than they could be if the correlation between observations were taken into account. 
        * The standard errors of the estimates will be too large, which means that confidence intervals will be wider and hypothesis tests will have less power.
    * Oversight of important correlation structure
        * the standard regression methods may miss important patterns of correlation in the data that could be used to improve the accuracy and precision of the estimates. 
        * For example, if there is a time trend in the data that is not accounted for, the standard errors of the estimates may be too large, and the estimates may not capture the true effect of the independent variables. 
        * Accounting for the correlation structure in the data can lead to more accurate and precise estimates, and can also help identify interesting patterns and relationships that might otherwise be missed.

:::
</div>

<div class="tab-pane fade" id="English" role="tabpanel" aria-labelledby="English-tab">

::: {#English .tab-pane .fade role="tabpanel" aria-labelledby="English-tab"}

:::

</div>


# Go to Project Content List

[Project Content List](./docs/projects/index.qmd)

# Go to Blog Content List

[Blog Content List](./docs/blog/posts/content_list.qmd)