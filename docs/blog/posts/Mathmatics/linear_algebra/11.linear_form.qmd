---
title: "Matrix Transformation (3) - Linear Form"
subtitle: Linear Regression, Fully Connected layers, Neural Networks, Linear Classifiers
description: |
  template
categories:
  - Mathematics
author: Kwangmin Kim
date: 04/02/2023
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

## Linear Form

A linear form is a linear function that maps a vector space to its underlying field. Let $V$ be a vector space over a field $\mathbb{F}$, and let $\mathcal{L}(V,\mathbb{F})$ denote the set of all linear functions from $V$ to $\mathbb{F}$. A linear form on $V$ is an element of $\mathcal{L}(V,\mathbb{F})$.

A linear form $\varphi$ can be represented by a row vector of dimension $1\times n$, where $n$ is the dimension of $V$. Let ${\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_n}$ be a basis for $V$, and let ${\alpha_1, \alpha_2, \dots, \alpha_n}$ be the corresponding dual basis for $\mathcal{L}(V,\mathbb{F})$, such that $\alpha_i(\mathbf{e}j) = \delta{ij}$ (the Kronecker delta). Then, any linear form $\varphi\in\mathcal{L}(V,\mathbb{F})$ can be written as:
$$
\varphi(x)=\sum_{i=1}^{n}a_ix_i=\mathbf a \mathbf x^T=\mathbf x \mathbf a
$$

where $\mathbf{x}\in V$ is a column vector of dimension $n\times 1$, $[\mathbf{a}]$ is the row vector representing $\varphi$, and $[\mathbf{x}]$ is the column vector representing $\mathbf{x}$.

For example, let $V = \mathbb{R}^2$ be the vector space of 2-dimensional column vectors, and let $\varphi\in\mathcal{L}(V,\mathbb{R})$ be the linear form defined by $\varphi(\begin{bmatrix}x\y\end{bmatrix}) = 3x - 2y$. Then, we can represent $\varphi$ as:

$$
[\mathbf a]=\begin{bmatrix} 3 & -2\end{bmatrix} [\mathbf x]=\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \varphi(x)=\sum_{i=1}^{n}\mathbf a\mathbf x^T=3x_1-2x_2
$$

which shows that $\varphi$ is a linear form on $V$.

consider a linear regression model that predicts the price of a house based on its size and location. The model can be represented by the linear form:

$$
\varphi(x)=\mathbf w\mathbf x^T=\sum_{i=1}^{n}w_ix_i=w_0+w_1x_1+w_2x_2
$$

where $\varphi(\mathbf{x})$ is the predicted price, $x_1$ is the size of the house, $x_2$ is a measure of the location (such as the distance from the city center), and $w_0$, $w_1$, and $w_2$ are the model parameters that control the intercept and the weights of the features. This linear form can be written in matrix form as:

$$
\varphi(x)=\mathbf x\mathbf w=\mathbf w \mathbf x^T
$$

where $[\mathbf{w}]$ is a row vector of the model parameters and $[\mathbf{x}]$ is a row vector of the features.

Linear forms can also be used in deep learning and machine learning models that involve linear transformations, such as fully connected layers in neural networks or linear classifiers. For example, consider a simple linear classifier that classifies images of digits into one of 10 classes. The classifier can be represented by the linear form:

$$
\varphi(x)=\mathbf x\mathbf w + b =\mathbf w \mathbf x^T +b
$$

where $\varphi(\mathbf{x})$ is the predicted class score, $[\mathbf{x}]$ is a row vector of the pixel values of the image, $[\mathbf{w}]$ is a row vector of the weights of the classifier, and $b$ is the bias term. This linear form can be used to classify the image by selecting the class with the highest score.

In both of these examples, linear forms are used to represent linear relationships between variables or features, and the model parameters are learned through training on a set of labeled examples.