---
title: MLE
subtitle: Overview
description: |
  template
categories:
  - Statistics
author: Kwangmin Kim
date: 03/24/2023
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: True
---


Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution based on observed data. The basic idea is to find the values of the parameters that maximize the likelihood function, which is a function of the observed data and the unknown parameters.

In other words, the MLE is the set of parameter values that make the observed data most probable, given the assumed probability distribution. The likelihood function is typically the product or the sum (depending on whether the observations are assumed to be independent or not) of the probabilities or probability densities of the observations, evaluated at the values of the parameters.

The MLE has many desirable properties, such as efficiency and consistency under certain conditions. It is widely used in statistical inference and hypothesis testing, as well as in various fields of science and engineering where data analysis is required. However, it is important to note that the MLE is not always the best estimator for a given problem, and other estimation methods may be more appropriate depending on the specific characteristics of the data and the model.



To find the maximum likelihood estimate (MLE) of some data, we need to specify a parametric model that describes the distribution of the data, and then estimate the parameters that maximize the likelihood function for the observed data.

In other words, the MLE is the set of parameter values that maximize the probability of observing the data that we have. The likelihood function is the joint probability density (or mass) function of the data, viewed as a function of the parameters, and we find the maximum of this function by differentiating it with respect to the parameters and setting the derivative to zero.

The MLE is a commonly used method for estimating the parameters of a statistical model, and it has desirable properties such as efficiency and consistency under certain conditions. However, it is important to note that the MLE may not always be the best estimator for a given problem, and other estimation methods may be more appropriate depending on the specific characteristics of the data and the model.


### MLE of OLS

In linear regression with normally distributed errors, the maximum likelihood estimate (MLE) of the ordinary least squares (OLS) coefficients is equivalent to the least squares estimate.

To derive this, we start by assuming that the error terms follow a normal distribution with mean 0 and variance σ^2, and that the observations are independent. Then, the likelihood function for the data Y = (Y1, Y2, ..., Yn) is:

L(Y|β,σ^2) = (2πσ^2)^(-n/2) exp{-1/(2σ^2) Σi=1^n (Yi - Xiβ)^2}

where Xi is the ith row of the design matrix X and β is the vector of regression coefficients.

To find the maximum likelihood estimates of β and σ^2, we maximize the likelihood function with respect to these parameters. Taking the log of the likelihood function and simplifying, we obtain:

log L(Y|β,σ^2) = -n/2 log(2π) - n/2 log(σ^2) - 1/(2σ^2) Σi=1^n (Yi - Xiβ)^2

To maximize this function with respect to β, we differentiate with respect to β and set the derivative to zero:

∂ log L(Y|β,σ^2) / ∂β = -1/(2σ^2) Σi=1^n 2Xi(Yi - Xiβ) = 0

Solving for β, we obtain:

β = (X^T X)^(-1) X^T Y

which is the OLS estimate of β.

To maximize the likelihood function with respect to σ^2, we differentiate with respect to σ^2 and set the derivative to zero:

∂ log L(Y|β,σ^2) / ∂σ^2 = -n/(2σ^2) + 1/(2σ^4) Σi=1^n (Yi - Xiβ)^2 = 0

Solving for σ^2, we obtain:

σ^2 = Σi=1^n (Yi - Xiβ)^2 / n

which is the OLS estimate of σ^2.

Therefore, we see that the maximum likelihood estimates of β and σ^2 in linear regression with normally distributed errors are equivalent to the OLS estimates of these parameters.
