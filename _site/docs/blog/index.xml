<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Kwangmin Kim</title>
<link>kmink3225.netlify.app/docs/blog/index.html</link>
<atom:link href="kmink3225.netlify.app/docs/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>blog</description>
<generator>quarto-1.2.335</generator>
<lastBuildDate>Tue, 31 Dec 2999 15:00:00 GMT</lastBuildDate>
<item>
  <title>Blog Content List</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/content_list.html</link>
  <description><![CDATA[ 



<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Scalars are denoted with a lower-case letter (ex a ) or a non-bolded lower-case Greek letter (ex <img src="https://latex.codecogs.com/png.latex?%5Calpha"> ).</li>
<li>Vectors are denoted using a bold-faced lower-case letter (ex <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20a">).</li>
<li>Matrices are denoted using a bold-faced upper-case letter (ex <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20%5Cphi">) or a bold-faced upper-case Greek letter (ex <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20%5CPhi">).</li>
<li>Tensors are denoted using a bold-faced upper-case letter with multiple subscripts or superscripts, indicating the number of indices and the dimensions of the tensor along each axis.
<ul>
<li>A second-order tensor (also known as a matrix) <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> with dimensions <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m"> can be represented as: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A_%7Bij%7D"> where <img src="https://latex.codecogs.com/png.latex?i%20=%201,%5Cdots,m"> and <img src="https://latex.codecogs.com/png.latex?j%20=%201,%5Cdots,n">, which are the indices that run over the rows and columns of the matrix, respectively.</li>
<li>A third-order tensor <img src="https://latex.codecogs.com/png.latex?T"> with dimensions <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m%20%5Ctimes%20p"> can be represented as: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A_%7Bijk%7D"> where <img src="https://latex.codecogs.com/png.latex?i%20=%201,%5Cdots,m">, <img src="https://latex.codecogs.com/png.latex?j%20=%201,%5Cdots,n">, which are <img src="https://latex.codecogs.com/png.latex?i">, and <img src="https://latex.codecogs.com/png.latex?k%20=%201,%5Cdots,p"> <img src="https://latex.codecogs.com/png.latex?j">, and <img src="https://latex.codecogs.com/png.latex?k">, which are the indices that run over the three dimensions of the tensor.</li>
</ul></li>
</ul>
</div>
</div>
<section id="contents" class="level1">
<h1>Contents</h1>
<ul>
<li><a href="../../../docs/blog/posts/DL/guide_map/index.html">Deep Learning</a></li>
<li><a href="../../../docs/blog/posts/ML/guide_map/index.html">Machine Learning</a></li>
<li><a href="../../../docs/blog/posts/Mathmatics/guide_map/index.html">Mathematics</a></li>
<li><a href="../../../docs/blog/posts/statistics/guide_map/index.html">Statistics</a></li>
<li><a href="../../../docs/blog/posts/Engineering/guide_map/index.html">Engineering</a></li>
<li><a href="../../../docs/blog/posts/Patent/guide_map/index.html">Patent</a></li>
<li><a href="../../../docs/blog/posts/Language/guide_map/index.html">Language</a></li>
<li><a href="../../../docs/blog/posts/Surveilance/guide_map/index.html">Surveilance</a></li>
</ul>
</section>
<section id="reference" class="level1">
<h1>Reference</h1>
<ul>
<li>Statistics
<ul>
<li>George Casella &amp; Rogeer L. Berger - Statistcal Inference, 2nd Edition</li>
<li>Dobson and Barnett (2008) An Introduction to Generalized Linear Model. 3rd Ed. Chapman &amp; Hall.</li>
<li>Fitzmaurice, Laird and Ware (2011) Applied Longitudinal Analysis. 2nd Ed. Wiley.</li>
<li>Hosmer, Lemeshow and May (2008) Applied Survival Analysis. 2nd Ed. Wiley.</li>
<li>슬기로운 통계생활 - https://www.youtube.com/<span class="citation" data-cites="statisticsplaybook">@statisticsplaybook</span></li>
<li>슬기로운 통계생활 - https://github.com/statisticsplaybook</li>
<li>Fast Campus, Coursera, Inflearn</li>
<li>그 외 다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Mathematics
<ul>
<li>James Stewart - Calculus Early Transcedentals, 7th Eidition &amp; any James Stewart series</li>
<li>GILBERT STRANG - Introduction to Linear Algebra, 4th Edition.</li>
<li>임장환 - 머신러닝, 인공지능, 컴퓨터 비전 전공자를 위한 최적화 이론</li>
<li>Fast Campus, Coursera, Inflearn</li>
<li><a href="https://www.youtube.com/playlist?list=PLaqQvlCBe8vIkIEb4GX2ZZ1A4tFYeXR5W">8일간의 선형대수학 기초(이상준 경희대 교수)</a></li>
<li><a href="https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/">Linear Algebra(Prof.&nbsp;Gilbert Strang, MIT Open Courseware)</a></li>
<li><a href="https://github.com/fastai/numerical-linear-algebra/blob/master/README.md">Computational Linear Algebra for Coders</a></li>
<li><a href="http://immersivemath.com/ila/">Immersive linear Algebra</a></li>
<li><a href="https://www.3blue1brown.com/topics/linear-algebra">3blue1brown</a></li>
<li>그 외 다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Machine Learning
<ul>
<li>Gareth M. James, Daniela Witten, Trevor Hastie, Robert Tibshirani - An Introduction to Statistical Learning: With Applications in R 2nd Edition</li>
<li>Trevor Hastie, Robert Tibshirani, Jerome H. Friedman - The Elements of Statistical Learning 2nd Edition</li>
<li>Fast Campus, Coursera, Inflearn</li>
<li>그 외 다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Deep Learning
<ul>
<li>Saito Koki - Deep Learning from Scratch 1,2,3 (밑바닥부터 시작하는 딥러닝 1,2,3)</li>
<li>조준우 - 머신러닝·딥러닝에 필요한 기초 수학 with 파이썬</li>
<li>조준우 - https://github.com/metamath1/noviceml</li>
<li>동빈나 - https://www.youtube.com/c/dongbinna</li>
<li>혁펜하임 - https://www.youtube.com/channel/UCcbPAIfCa4q0x7x8yFXmBag</li>
<li>Fast Campus, Coursera, Inflearn</li>
<li>다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Engineering
<ul>
<li>Fast Campus, Coursera, Inflearn</li>
<li>그 외 다수의 Youtube, and Documents from Googling</li>
</ul></li>
</ul>


</section>

 ]]></description>
  <category>All List</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/content_list.html</guid>
  <pubDate>Tue, 31 Dec 2999 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Statistics</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/statistics/guide_map/index.html</link>
  <description><![CDATA[ 



<p><strong>(Draft)</strong></p>
<section id="contents-list" class="level1">
<h1>Contents List</h1>
<section id="basic" class="level2">
<h2 class="anchored" data-anchor-id="basic">Basic</h2>
<section id="probability-theory" class="level3">
<h3 class="anchored" data-anchor-id="probability-theory">Probability Theory</h3>
<ul>
<li>2023-02-05, Set Theory</li>
<li>2023-02-05, [Basics of Probability Theory - Axiomatic Foundations]</li>
<li>2023-02-05, [Basics of Probability Theory - Calculus of Probabilities]</li>
<li>2023-02-05, <a href="../../../../../docs/blog/posts/statistics/2023-02-05_probability/index.html">Basics of Probability Theory - Probability</a></li>
<li>2023-02-05, <a href="../../../../../docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html">Conditional Probability</a></li>
<li>2023-02-05, [Independence]</li>
<li>2023-02-05, <a href="../../../../../docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html">Bayes’ Rule</a></li>
<li>2023-02-05, Random Variable</li>
<li>1111-11-11, Probability Distribution</li>
</ul>
</section>
<section id="transformations-and-expectations" class="level3">
<h3 class="anchored" data-anchor-id="transformations-and-expectations">Transformations and Expectations</h3>
<ul>
<li>2023-02-21, <a href="../../../../../docs/blog/posts/statistics/2023-02-21_transformation/index.html">Transformation of Random Variables</a></li>
<li>1111-11-11, Expected Value vs Realizaed Value</li>
<li>1111-11-11, Variance</li>
<li>1111-11-11, Covariance and Correlation</li>
<li>2023-02-28, <a href="../../../../../docs/blog/posts/statistics/2023-02-28_mgf/index.html">Moment Generating Function, MGF</a></li>
</ul>
</section>
<section id="exponential-family-distributions" class="level3">
<h3 class="anchored" data-anchor-id="exponential-family-distributions">Exponential Family Distributions</h3>
<ul>
<li>Discrete Random Variable
<ul>
<li>2023-02-27,<a href="../../../../../docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html">Bernoulli Distribution</a></li>
<li>2023-02-28,<a href="../../../../../docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html">Binomial Distribution</a></li>
<li>2023-03-01,<a href="../../../../../docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html">Poisson Distribution</a></li>
<li>2023-03-01,<a href="../../../../../docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html">Geometric Distribution</a></li>
<li>1111-11-11, Hypergeometric Distribution</li>
</ul></li>
<li>Continuous Random Variable
<ul>
<li>1111-11-11, Normal Distribution</li>
<li>1111-11-11, Exponential Distribution</li>
<li>1111-11-11, Beta Distribution</li>
<li>1111-11-11, Chi-squared Distribution</li>
</ul></li>
<li>1111-11-11,</li>
</ul>
</section>
<section id="multiple-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="multiple-random-variables">Multiple Random Variables</h3>
<ul>
<li>1111-11-11, Joint Distribution and Marginal Distribution</li>
</ul>
</section>
<section id="point-estimation" class="level3">
<h3 class="anchored" data-anchor-id="point-estimation">Point Estimation</h3>
<ul>
<li>1111-11-11, Estimation Methods - Method of Moments</li>
<li>2023-03-29, Estimation Methods - <a href="../../../../../docs/blog/posts/statistics/2023-03-25_MLE/index.html">Maximum Likelihood Estimation &amp; Statistical Bias</a></li>
<li>1111-11-11, Estimation Methods - Bayesian Estimation</li>
<li>1111-11-11, Estimation Methods - The EM Algorithm</li>
<li>1111-11-11, Evaluation Methods of Estimators - Mean Squared Error</li>
<li>1111-11-11, Evaluation Methods of Estimators - Best Unbiased Estimators</li>
<li>1111-11-11, Evaluation Methods of Estimators - Sufficiency and Unbiasedness</li>
<li>1111-11-11, Evaluation Methods of Estimators - Loss Function Optimality</li>
</ul>
</section>
</section>
<section id="hypothesis-testing" class="level2">
<h2 class="anchored" data-anchor-id="hypothesis-testing">Hypothesis Testing</h2>
<ul>
<li>1111-11-11, Hypothesis Testing</li>
<li>1111-11-11, Permutation Test</li>
</ul>
<section id="methods-of-finding-tests" class="level3">
<h3 class="anchored" data-anchor-id="methods-of-finding-tests">Methods of Finding Tests</h3>
<ul>
<li>1111-11-11, Likelihood Ratio Tests</li>
<li>1111-11-11, Bayesian Tests</li>
<li>1111-11-11, Union-Intersection and Intersection-Union Tets</li>
</ul>
</section>
<section id="methods-of-evaluating-tests" class="level3">
<h3 class="anchored" data-anchor-id="methods-of-evaluating-tests">Methods of Evaluating Tests</h3>
<ul>
<li>1111-11-11, Power</li>
<li>1111-11-11, Error Proabilities and the Power Function</li>
<li>1111-11-11, Most Powerful Tests</li>
<li>2022-12-28, <a href="../../../../../docs/blog/posts/statistics/2022-12-08-P-value/index.html">p-values</a></li>
<li>1111-11-11, Loss Function Optimality</li>
<li>1111-11-11, Multiple Testing</li>
<li>1111-11-11, Sample Size Calculation</li>
<li>1111-11-11, A/B Testing</li>
<li>2023-01-07, <a href="../../../../../docs/blog/posts/statistics/2023-01-07-anova/index.html">ANOVA</a>
<ul>
<li>2023-01-27, <a href="../2023-01-27_ANCOVA/">ANCOVA</a></li>
<li>2023-01-27, <a href="../2023-01-27_rmANOVA/">repeated measures ANOVA</a></li>
<li>2023-01-28, <a href="../2023-01-28_MANOVA/">MANOVA</a></li>
</ul></li>
</ul>
</section>
</section>
<section id="categorical-data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="categorical-data-analysis">Categorical Data Analysis</h2>
<ul>
<li>1111-11-11, Introduction</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>2022-12-28,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>2023-01-07,</li>
<li>2023-01-27,</li>
<li>2023-01-27,</li>
<li>2023-01-28,</li>
</ul>
</section>
<section id="regression" class="level2">
<h2 class="anchored" data-anchor-id="regression">Regression</h2>
<ul>
<li>1111-11-11, Least Square and Simple Linear Regression</li>
<li>1111-11-11, Multiple Linear Regression</li>
</ul>
<section id="generalized-linear-models" class="level3">
<h3 class="anchored" data-anchor-id="generalized-linear-models">Generalized Linear Models</h3>
<ul>
<li>1111-11-11, Logistic Regression</li>
<li>1111-11-11, Multinomial Regression</li>
<li>1111-11-11, Poisson Regression</li>
<li>1111-11-11, Poisson Regression</li>
<li>1111-11-11, Poisson Regression</li>
</ul>
</section>
</section>
<section id="longitudinal-data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="longitudinal-data-analysis">Longitudinal Data Analysis</h2>
<ul>
<li>2023-03-23, <a href="../../../../../docs/blog/posts/statistics/LDA/1_intro.html">LDA (1) - Intro</a></li>
<li>2023-03-23, <a href="../../../../../docs/blog/posts/statistics/LDA/2_covariance_model.html">LDA (2) - Concepts &amp; Covariance Models</a></li>
<li>2023-03-25, <a href="../LDA/intro.qmd">LDA (3) - WLS &amp; REML</a></li>
<li>2023-03-25, <a href="../LDA/intro.qmd">LDA (4) - Respiratory Infection Data Example</a></li>
<li>2023-03-28, <a href="../LDA/intro.qmd">LDA (5) - Epileptic Seizures Data Example</a></li>
</ul>
<section id="mixed-models" class="level3">
<h3 class="anchored" data-anchor-id="mixed-models">Mixed Models</h3>
<ul>
<li>1111-11-11, Linear Mixed Models</li>
</ul>
</section>
</section>
<section id="generalized-additive-models" class="level2">
<h2 class="anchored" data-anchor-id="generalized-additive-models">Generalized Additive Models</h2>
</section>
<section id="survival-analysis" class="level2">
<h2 class="anchored" data-anchor-id="survival-analysis">Survival Analysis</h2>
<ul>
<li>1111-11-11, Cox-Hazard Model</li>
</ul>


</section>
</section>

 ]]></description>
  <category>Statistics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/statistics/guide_map/index.html</guid>
  <pubDate>Fri, 30 Apr 2100 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Machine Learning</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/ML/guide_map/index.html</link>
  <description><![CDATA[ 



<p><strong>(Draft)</strong></p>
<section id="contents-list" class="level1">
<h1>Contents List</h1>
<section id="basic" class="level2">
<h2 class="anchored" data-anchor-id="basic">Basic</h2>
<section id="r" class="level3">
<h3 class="anchored" data-anchor-id="r">R</h3>
</section>
<section id="python" class="level3">
<h3 class="anchored" data-anchor-id="python">Python</h3>
<section id="tensor-flow-framework" class="level4">
<h4 class="anchored" data-anchor-id="tensor-flow-framework">Tensor Flow Framework</h4>
<ul>
<li>2023-02-03, <a href="../../../../../docs/blog/posts/ML/2023-02-03_tf_introduction/index.html">Tensor Flow Introduction</a></li>
</ul>
</section>
<section id="pytorch-framework" class="level4">
<h4 class="anchored" data-anchor-id="pytorch-framework">Pytorch Framework</h4>
<ul>
<li>2023-02-03, <a href="../../../../../docs/blog/posts/ML/2023-02-03_pytorch_introduction/index.html">Pytorch Introduction</a></li>
</ul>
</section>
</section>
</section>
<section id="machine-learning-methods" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning-methods">Machine Learning Methods</h2>
<section id="supervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="supervised-learning">Supervised Learning</h3>
<ul>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Logistic Regression]</li>
<li>0000-00=00, [Generative Models]
<ul>
<li>0000-00=00, [Linear Discriminant Analysis]</li>
<li>0000-00=00, [Quadratic Discriminant Analysis]</li>
<li>0000-00=00, [Naive Bayes]</li>
</ul></li>
<li>0000-00=00, [Resampling Methods]</li>
<li>0000-00=00, [Regularization]</li>
<li>0000-00=00, [Smoothing]</li>
<li>0000-00=00, [Tree Based Methods]</li>
<li>0000-00=00, [Support Vector Machine]</li>
<li>0000-00=00, [PCR]</li>
<li>0000-00=00, [PLS]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
</ul>
</section>
<section id="unupervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="unupervised-learning">Unupervised Learning</h3>
<ul>
<li>0000-00=00, [PCA]</li>
<li>0000-00=00, [K means clustering]</li>
<li>0000-00=00, [Hierarchical Clustering]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
</ul>


</section>
</section>
</section>

 ]]></description>
  <category>ML</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/ML/guide_map/index.html</guid>
  <pubDate>Wed, 31 Mar 2100 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Mathematics</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/guide_map/index.html</link>
  <description><![CDATA[ 



<section id="contents" class="level1">
<h1>Contents</h1>
<section id="basic" class="level2">
<h2 class="anchored" data-anchor-id="basic">Basic</h2>
<ul>
<li>2023-03-24, <a href="../../../../../docs/blog/posts/Mathmatics/variables/index.html">Variable types</a></li>
<li>1111-11-11, <a href="../../../../../docs/blog/posts/Mathmatics/function/index.html">Function</a>
<ul>
<li>2023-01-31, <a href="../../../../../docs/blog/posts/Mathmatics/function/univariable_scalar_function.html">Function (1) - Univariable Scalar Function (One to One)</a></li>
<li>2023-01-31, <a href="../../../../../docs/blog/posts/Mathmatics/function/multivariable_scalar_function.html">Function (2) - Multi-variable Scalar Function (Many to One)</a></li>
<li>2023-01-31, <a href="../../../../../docs/blog/posts/Mathmatics/function/univariable_vector_function.html">Function (3) - Univariable Vector Function (One to Many)</a></li>
<li>2023-01-31, <a href="../../../../../docs/blog/posts/Mathmatics/function/mutivariable_vector_function.html">Function (4) - Multi-variable Vector Function (Many to Many)</a></li>
<li>2023-02-18, <a href="../../../../../docs/blog/posts/Mathmatics/function/composite_function.html">Function (5) - Composite Function</a></li>
</ul></li>
<li>2023-02-18, <a href="../../../../../docs/blog/posts/Mathmatics/transformation/index.html">Transformations of Functions</a></li>
<li>1111-11-11, Vector &amp; Matrix</li>
<li>2023-03-15, <a href="../epsilon_delta/">Limit, <img src="https://latex.codecogs.com/png.latex?%5Cepsilon-%5Cdelta"> Method</a></li>
<li>Differentiation
<ul>
<li>2023-02-04, <a href="../../../../../docs/blog/posts/Mathmatics/differentiation/2023-02-04_uni_derivative.html">Derivative (1) - Univariable Scalar Funtion</a></li>
<li>1111-11-11, <a href="../../../../../docs/blog/posts/Mathmatics/differentiation/2023-02-10_composite_partial_derivative.html">Derivative (2) - Chain Rule &amp; Partial Derivative</a></li>
<li>1111-11-11, Derivative (3) - Higher Order Derivative</li>
<li>1111-11-11, Derivative (4) - Mean Value Theorem</li>
<li>1111-11-11, Derivative (5) - Gradient</li>
</ul></li>
<li>2023-03-15, <a href="../../../../../docs/blog/posts/Mathmatics/taylor_series/index.html">Talyer’s Series</a></li>
<li>1111-11-11, Gradient Direction</li>
<li>1111-11-11, Random Variable</li>
<li>1111-11-11, Probability Distribution</li>
<li>1111-11-11, Information Theory - Entropy</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
</ul>
</section>
<section id="linear-algebra" class="level2">
<h2 class="anchored" data-anchor-id="linear-algebra">Linear Algebra</h2>
<ul>
<li>2023-03-30, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/01.basic_vector.html">Basics (1) - Vector Operations</a></li>
<li>2023-03-30, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/02.basic_matrix.html">Basics (2) - Matrix Operations</a></li>
<li>2023-03-30, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/03.basic_special_matrix.html">Basics (3) - Special Matrix</a></li>
<li>1111-11-11, Inner Product</li>
<li>1111-11-11, Linear Combination</li>
<li>1111-11-11,</li>
<li>1111-11-11, Linear Independence</li>
<li>1111-11-11, Basis, Dimension, &amp; Rank</li>
<li>1111-11-11, Outer Product</li>
<li>1111-11-11, Eigen Value &amp; Eigen Vector</li>
<li>1111-11-11, Eigen Decomposition</li>
<li>1111-11-11, Singular Value Decomposition (SVD)</li>
<li>1111-11-11, Gram-Schmidt</li>
<li>1111-11-11, Group</li>
<li>1111-11-11, Orthogonal Matrix</li>
<li>1111-11-11, Rotation &amp; Group</li>
<li>2023-04-02, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/quadratic_form.html">Matrix Transformation (5) - Quadratic Form</a></li>
<li>2023-04-02, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/derivative_matrix_vector.html">Matrix Calculus (1) - Quadratic Form</a></li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
</ul>
</section>
<section id="optimization" class="level2">
<h2 class="anchored" data-anchor-id="optimization">Optimization</h2>
<ul>
<li>2023-03-23, <a href="../../../../../docs/blog/posts/Mathmatics/optimization/minimizer.html">Minimizer &amp; Minimum</a></li>
<li>1111-11-11, Convex Set</li>
<li>1111-11-11, Convex Function</li>
<li>1111-11-11, Unconstrained Optimization</li>
<li>1111-11-11, Non-linear Least Square</li>
<li>1111-11-11, Largrange Multiplier Method
<ul>
<li>1111-11-11, Largrange Primal Function</li>
<li>1111-11-11, Largrange Dual Function</li>
<li>1111-11-11, KKT conditions</li>
</ul></li>
<li>1111-11-11, Gradient Descent Optimizers</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
</ul>
</section>
<section id="reference" class="level2">
<h2 class="anchored" data-anchor-id="reference">Reference</h2>
<ul>
<li>Statistics
<ul>
<li>George Casella &amp; Rogeer L. Berger - Statistcal Inference, 2nd Edition</li>
<li>슬기로운 통계생활 - https://www.youtube.com/<span class="citation" data-cites="statisticsplaybook">@statisticsplaybook</span></li>
<li>슬기로운 통계생활 - https://github.com/statisticsplaybook</li>
<li>다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Mathematics
<ul>
<li>James Stewart - Calculus Early Transcedentals, 7th Eidition</li>
<li>any James Stewart series</li>
<li>임장환 - 머신러닝, 인공지능, 컴퓨터 비전 전공자를 위한 최적화 이론</li>
<li>다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Deep Learning
<ul>
<li>조준우 - 머신러닝·딥러닝에 필요한 기초 수학 with 파이썬</li>
<li>조준우 - https://github.com/metamath1/noviceml</li>
<li>동빈나 - https://www.youtube.com/c/dongbinna</li>
<li>혁펜하임 - https://www.youtube.com/channel/UCcbPAIfCa4q0x7x8yFXmBag</li>
<li>다수의 Youtube, and Documents from Googling</li>
</ul></li>
</ul>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/guide_map/index.html</guid>
  <pubDate>Sun, 28 Feb 2100 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Deep Learning</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/DL/guide_map/index.html</link>
  <description><![CDATA[ 



<p><strong>(Draft)</strong></p>
<section id="contents-list" class="level1">
<h1>Contents List</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<ul>
<li>1111-11-11, Artificial Intelligence</li>
<li>1111-11-11, Perceptron</li>
<li>1111-11-11, Artificial Neural Netwroks (ANN)
<ul>
<li>1111-11-11, activation functions</li>
<li>1111-11-11, output layer design</li>
</ul></li>
<li>1111-11-11, loss function</li>
<li>1111-11-11, numerical differentiation</li>
<li>1111-11-11, gradient descent</li>
<li>1111-11-11, backpropagation</li>
<li>1111-11-11, optimizer
<ul>
<li>1111-11-11, stochastic gradient descent</li>
<li>1111-11-11, momentum</li>
<li>1111-11-11, adaGrad</li>
<li>1111-11-11, adam</li>
<li>1111-11-11, weight initalization</li>
</ul></li>
<li>1111-11-11, batch normalization</li>
<li>1111-11-11, dropout</li>
<li>1111-11-11, tuning parameter</li>
<li>1111-11-11, auto-encoder</li>
<li>1111-11-11, stacked auto-encoder</li>
<li>1111-11-11, denoising auto-encoder(DAE)</li>
</ul>
<section id="convolutional-neural-network-cnn" class="level3">
<h3 class="anchored" data-anchor-id="convolutional-neural-network-cnn">Convolutional Neural Network (CNN)</h3>
<ul>
<li>2023-03-10, <a href="../../../../../docs/blog/posts/DL/2023-03-10_cnn/index.html">CNN (1) - Concept</a></li>
<li>2023-03-10, <a href="">CNN (2) - Practice</a></li>
</ul>
</section>
<section id="natural-language-process-nlp" class="level3">
<h3 class="anchored" data-anchor-id="natural-language-process-nlp">Natural Language Process (NLP)</h3>
<ul>
<li>1111-11-11, word2vec</li>
<li>1111-11-11, improved word2vec</li>
</ul>
</section>
<section id="recurrent-neural-network-rnn" class="level3">
<h3 class="anchored" data-anchor-id="recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</h3>
</section>
<section id="gate-recurrent-unit-gru" class="level3">
<h3 class="anchored" data-anchor-id="gate-recurrent-unit-gru">Gate Recurrent Unit (GRU)</h3>
</section>
<section id="long-short-term-memory-lstm" class="level3">
<h3 class="anchored" data-anchor-id="long-short-term-memory-lstm">Long Short-Term Memory (LSTM)</h3>
</section>
<section id="attention-transformer" class="level3">
<h3 class="anchored" data-anchor-id="attention-transformer">Attention (Transformer)</h3>
</section>
<section id="bidirectional-encoder-representations-from-transformers-bert" class="level3">
<h3 class="anchored" data-anchor-id="bidirectional-encoder-representations-from-transformers-bert">Bidirectional Encoder Representations from Transformers (BERT)</h3>
</section>
<section id="generative-pre-training-transformer-gpt" class="level3">
<h3 class="anchored" data-anchor-id="generative-pre-training-transformer-gpt">Generative Pre-training Transformer (GPT)</h3>


</section>
</section>
</section>

 ]]></description>
  <category>DL</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/DL/guide_map/index.html</guid>
  <pubDate>Thu, 31 Dec 2099 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Engineering</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Engineering/guide_map/index.html</link>
  <description><![CDATA[ 



<section id="it-terminology" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="it-terminology"><span class="header-section-number">1</span> IT Terminology</h2>
<ul>
<li>0000-00-00, Terminology</li>
</ul>
</section>
<section id="data-structure" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="data-structure"><span class="header-section-number">2</span> Data Structure</h2>
<ul>
<li>2023-01-17, <a href="../2023-01-17_data_structure_overview/">Overview</a></li>
<li>2023-01-18, <a href="../2023-01-18_array/">Array</a></li>
<li>2023-01-18, <a href="../2023-01-18_linked_list/">Linked List</a></li>
<li>2023-01-18, <a href="../2023-01-18_python_list/">Python List</a></li>
<li>2023-01-19, <a href="../2023-01-19_stack/">Stack</a></li>
<li>2023-01-19, <a href="../2023-01-19_queue/">Queue</a></li>
<li>2023-01-26, <a href="../2023-01-19_deque/">Deque</a></li>
<li>2023-01-26, <a href="../2023-01-20_binary_search_tree/">Binary Search Tree</a></li>
<li>2023-01-20, <a href="../2023-01-20_priority_queue/">Priority Queue</a></li>
<li>2023-01-20, <a href="../2023-01-20_graph/">Graph</a></li>
</ul>
</section>
<section id="conda" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="conda"><span class="header-section-number">3</span> Conda</h2>
</section>
<section id="docker" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="docker"><span class="header-section-number">4</span> Docker</h2>
<ul>
<li>2023-01-30, Docker Install</li>
<li>2023-01-31, Docker Compose</li>
<li>2023-02-01, Docker Container</li>
</ul>
</section>
<section id="dynamic-documentation" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="dynamic-documentation"><span class="header-section-number">5</span> Dynamic Documentation</h2>
<ul>
<li>2023-01-19, <a href="https://quarto.org/docs/get-started/">Quarto</a></li>
<li>2023-01-19, <a href="https://github.com/yihui/xaringan">xaringan[R]</a></li>
<li>2023-01-19, <a href="https://bookdown.org/yihui/bookdown/get-started.html">Bookdown[R]</a></li>
<li>2023-01-19, <a href="https://decile-team-distil.readthedocs.io/en/latest/index.html">DISTL</a></li>
<li>2023-01-26, <a href="https://www.sphinx-doc.org/en/master/">Sphinx[Python]</a></li>
</ul>
</section>
<section id="aws-cloud" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="aws-cloud"><span class="header-section-number">6</span> AWS Cloud</h2>
<p>Coursera Course: AWS Fundamentals</p>
<ul>
<li>2023-03-09, <a href="../../../../../docs/blog/posts/Engineering/2023-03-02_aws/computing_networking.html">Computing and Networking</a></li>
<li>2023-03-12, <a href="../../../../../docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html">Storage and Database</a></li>
<li>2023-03-26, <a href="../../../../../docs/blog/posts/Engineering/2023-03-02_aws/monitoring_sharedresponsibility.html">Monitoring and SharedResponsibility</a></li>
<li>2023-04-05, <a href="../../../../../docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html">Infrastructure Security</a></li>
</ul>
</section>
<section id="azure-cloud" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="azure-cloud"><span class="header-section-number">7</span> Azure Cloud</h2>
</section>
<section id="data-modeling" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="data-modeling"><span class="header-section-number">8</span> Data Modeling</h2>
</section>
<section id="apache-airflow" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="apache-airflow"><span class="header-section-number">9</span> Apache Airflow</h2>
</section>
<section id="apache-spark" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="apache-spark"><span class="header-section-number">10</span> Apache Spark</h2>
</section>
<section id="front-end" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="front-end"><span class="header-section-number">11</span> Front End</h2>
</section>
<section id="back-end" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="back-end"><span class="header-section-number">12</span> Back End</h2>


</section>

 ]]></description>
  <category>Engineering</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Engineering/guide_map/index.html</guid>
  <pubDate>Thu, 31 Dec 2099 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Language</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Language/guide_map/index.html</link>
  <description><![CDATA[ 



<p><strong>(Draft)</strong></p>
<section id="contents-list" class="level1">
<h1>Contents List</h1>
<section id="r" class="level2">
<h2 class="anchored" data-anchor-id="r">R</h2>
<ul>
<li>1111-11-11, tidyverse
<ul>
<li>1111-11-11, dplyr</li>
<li>1111-11-11, ggplot2</li>
<li>1111-11-11, tidyr</li>
<li>1111-11-11, readr</li>
<li>1111-11-11, purrr</li>
<li>1111-11-11, tibble</li>
<li>1111-11-11, stringr</li>
<li>1111-11-11, forcats</li>
</ul></li>
<li>1111-11-11, tidymodels</li>
<li>1111-11-11, R shiny</li>
</ul>
</section>
<section id="python" class="level2">
<h2 class="anchored" data-anchor-id="python">Python</h2>
<ul>
<li>1111-11-11, numpy</li>
<li>1111-11-11, pandas</li>
<li>1111-11-11, matplotlib</li>
<li>1111-11-11, seaborn</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
</ul>
</section>
<section id="sql" class="level2">
<h2 class="anchored" data-anchor-id="sql">SQL</h2>
<section id="sqlite" class="level3">
<h3 class="anchored" data-anchor-id="sqlite">SQLite</h3>
</section>
<section id="oracle-sql" class="level3">
<h3 class="anchored" data-anchor-id="oracle-sql">Oracle SQL</h3>
</section>
<section id="ms-sql" class="level3">
<h3 class="anchored" data-anchor-id="ms-sql">MS-SQL</h3>
</section>
<section id="postgre-sql" class="level3">
<h3 class="anchored" data-anchor-id="postgre-sql">Postgre SQL</h3>
</section>
</section>
<section id="linux" class="level2">
<h2 class="anchored" data-anchor-id="linux">Linux</h2>
</section>
<section id="powershell" class="level2">
<h2 class="anchored" data-anchor-id="powershell">Powershell</h2>
</section>
<section id="c" class="level2">
<h2 class="anchored" data-anchor-id="c">C++</h2>
</section>
<section id="javascript" class="level2">
<h2 class="anchored" data-anchor-id="javascript">Javascript</h2>


</section>
</section>

 ]]></description>
  <category>Language</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Language/guide_map/index.html</guid>
  <pubDate>Thu, 31 Dec 2099 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Patent</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Patent/guide_map/index.html</link>
  <description><![CDATA[ 



<p><strong>(Draft)</strong></p>
<section id="contents-list" class="level1">
<h1>Contents List</h1>
<section id="basic" class="level2">
<h2 class="anchored" data-anchor-id="basic">Basic</h2>


</section>
</section>

 ]]></description>
  <category>Patent</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Patent/guide_map/index.html</guid>
  <pubDate>Thu, 31 Dec 2099 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Validation</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Surveilance/guide_map/index.html</link>
  <description><![CDATA[ 



<section id="sgs" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="sgs"><span class="header-section-number">1</span> SGS</h2>
<ul>
<li>0000-00-00, EN62304</li>
</ul>
</section>
<section id="fda" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="fda"><span class="header-section-number">2</span> FDA</h2>
<ul>
<li>2023-01-27, <a href="../../../../../docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html">General Principles of SW Validation</a></li>
<li>2023-01-27, <a href="../../../../../docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html">General Principles of SW Validation - Diagram Summary</a></li>
<li>1111-11-11, Guidance for the Content of Premarket Submissions for Software Contained in Medical Devices</li>
</ul>
</section>
<section id="dhf" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="dhf"><span class="header-section-number">3</span> DHF</h2>
</section>
<section id="public-health" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="public-health"><span class="header-section-number">4</span> Public Health</h2>
</section>
<section id="wet-lab" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="wet-lab"><span class="header-section-number">5</span> Wet Lab</h2>
<ul>
<li>0000-00-00, PCR (Polymerase Chain Reaction) Experiment</li>
</ul>


</section>

 ]]></description>
  <category>Surveilance</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Surveilance/guide_map/index.html</guid>
  <pubDate>Sat, 31 Dec 2089 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Vector Space</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html</link>
  <description><![CDATA[ 



<section id="vector-space" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="vector-space"><span class="header-section-number">1</span> Vector Space</h2>
<section id="definition" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="definition"><span class="header-section-number">1.1</span> Definition</h3>
<p>A vector space is a set <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D"> equipped with two operations:</p>
<ul>
<li>vector addition and</li>
<li>scalar multiplication,</li>
</ul>
<p>which satisfy the following properties for all vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D,%20%5Cmathbf%7Bv%7D,%20%5Cmathbf%7Bw%7D"> in <img src="https://latex.codecogs.com/png.latex?V"> and all scalars <img src="https://latex.codecogs.com/png.latex?c,%20d">:</p>
<ol type="1">
<li>Closure under vector addition: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D%20%5Cin%20V"></li>
<li>Associativity of vector addition: <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D)%20+%20%5Cmathbf%7Bw%7D%20=%20%5Cmathbf%7Bu%7D%20+%20(%5Cmathbf%7Bv%7D%20+%20%5Cmathbf%7Bw%7D)"></li>
<li>Identity element of vector addition: There exists a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D"> in <img src="https://latex.codecogs.com/png.latex?V"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7B0%7D%20=%20%5Cmathbf%7Bu%7D"> for all <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> in <img src="https://latex.codecogs.com/png.latex?V"></li>
<li>Existence of additive inverse: For each <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> in <img src="https://latex.codecogs.com/png.latex?V">, there exists a vector <img src="https://latex.codecogs.com/png.latex?-%5Cmathbf%7Bu%7D"> in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20(-%5Cmathbf%7Bu%7D)%20=%20%5Cmathbf%7B0%7D"></li>
<li>Closure under scalar multiplication: <img src="https://latex.codecogs.com/png.latex?c%5Cmathbf%7Bu%7D%20%5Cin%20V"></li>
<li>Distributive law of scalar multiplication with respect to scalar addition: <img src="https://latex.codecogs.com/png.latex?(c%20+%20d)%5Cmathbf%7Bu%7D%20=%20c%5Cmathbf%7Bu%7D%20+%20d%5Cmathbf%7Bu%7D"></li>
<li>Distributive law of scalar multiplication with respect to vector addition: <img src="https://latex.codecogs.com/png.latex?c(%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D)%20=%20c%5Cmathbf%7Bu%7D%20+%20c%5Cmathbf%7Bv%7D"></li>
<li>Associativity of scalar multiplication: <img src="https://latex.codecogs.com/png.latex?(cd)%5Cmathbf%7Bu%7D%20=%20c(d%5Cmathbf%7Bu%7D)"></li>
<li>Identity element of scalar multiplication: <img src="https://latex.codecogs.com/png.latex?1%5Cmathbf%7Bu%7D%20=%20%5Cmathbf%7Bu%7D"></li>
</ol>
</section>
<section id="example" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="example"><span class="header-section-number">1.2</span> Example</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?V%20=%20%5Cmathbb%7BR%7D%5E3">, the set of all 3-dimensional real vectors. The vector space <img src="https://latex.codecogs.com/png.latex?V"> is equipped with vector addition and scalar multiplication defined as usual component-wise. Vector addition and scalar multiplication satisfy all the properties listed in the definition of a vector space.</p>
</section>
</section>
<section id="basis-vector" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="basis-vector"><span class="header-section-number">2</span> Basis Vector</h2>
<p>A basis for a vector space <img src="https://latex.codecogs.com/png.latex?V"> is a set of vectors <img src="https://latex.codecogs.com/png.latex?%7Bv_1,%20v_2,%20%5Cldots,%20v_n%7D"> that spans <img src="https://latex.codecogs.com/png.latex?V"> and is linearly independent. In other words, every vector in <img src="https://latex.codecogs.com/png.latex?V"> can be expressed as a linear combination of the basis vectors, and the basis vectors are linearly independent, meaning that no basis vector can be written as a linear combination of the other basis vectors.</p>
<p>Mathematically, a set of vectors <img src="https://latex.codecogs.com/png.latex?%7Bv_1,%20v_2,%20%5Cldots,%20v_n%7D"> is a basis for a vector space <img src="https://latex.codecogs.com/png.latex?V"> if it satisfies the following conditions:</p>
<p>The vectors <img src="https://latex.codecogs.com/png.latex?%7Bv_1,%20v_2,%20%5Cldots,%20v_n%7D"> span <img src="https://latex.codecogs.com/png.latex?V">, which means that for any vector <img src="https://latex.codecogs.com/png.latex?v"> in <img src="https://latex.codecogs.com/png.latex?V">, there exist scalars <img src="https://latex.codecogs.com/png.latex?c_1,%20c_2,%20%5Cldots,%20c_n"> such that <img src="https://latex.codecogs.com/png.latex?v%20=%20c_1%20v_1%20+%20c_2%20v_2%20+%20%5Cldots%20+%20c_n%20v_n">.</p>
<p>The vectors <img src="https://latex.codecogs.com/png.latex?%7Bv_1,%20v_2,%20%5Cldots,%20v_n%7D"> are linearly independent, which means that the only solution to the equation <img src="https://latex.codecogs.com/png.latex?c_1%20v_1%20+%20c_2%20v_2%20+%20%5Cldots%20+%20c_n%20v_n%20=%200"> is <img src="https://latex.codecogs.com/png.latex?c_1%20=%20c_2%20=%20%5Cldots%20=%20c_n%20=%200">.</p>
<section id="example-1" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="example-1"><span class="header-section-number">2.1</span> Example</h3>
<p>Consider the vector space <img src="https://latex.codecogs.com/png.latex?V%20=%20%5Cmathbb%7BR%7D%5E3">, the set of all real-valued vectors with three components. A basis for <img src="https://latex.codecogs.com/png.latex?V"> can be given by the following set of vectors:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Av_1%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20%5C%5C%200%20%5C%5C%200%0A%5Cend%7Bbmatrix%7D,%20%5Cquad%0Av_2%20=%20%5Cbegin%7Bbmatrix%7D%0A0%20%5C%5C%201%20%5C%5C%200%0A%5Cend%7Bbmatrix%7D,%20%5Cquad%0Av_3%20=%20%5Cbegin%7Bbmatrix%7D%0A0%20%5C%5C%200%20%5C%5C%201%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>These three vectors form a basis for <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3">, as they span <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3"> (any vector in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3"> can be expressed as a linear combination of these vectors) and they are linearly independent (the only solution to the equation <img src="https://latex.codecogs.com/png.latex?c_1%20v_1%20+%20c_2%20v_2%20+%20c_3%20v_3%20=%200"> is <img src="https://latex.codecogs.com/png.latex?c_1%20=%20c_2%20=%20c_3%20=%200">).</p>
<p>Basis vectors are fundamental in defining and understanding vector spaces because they provide a way to express any vector in the vector space as a linear combination of basis vectors. The properties of basis vectors, such as linear independence, spanning set, minimal set, and unique representation, are essential in establishing the foundational concepts of vector spaces and their properties. By using basis vectors, we can represent vectors in a vector space in a concise and systematic way, and they provide a basis for studying and analyzing vector spaces in various mathematical and practical applications.</p>
</section>
<section id="properties" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="properties"><span class="header-section-number">2.2</span> Properties</h3>
<ul>
<li>Linear independence: A set of basis vectors is linearly independent, meaning that no vector in the set can be expressed as a linear combination of the others. In other words, the coefficients of the basis vectors in any linear combination that equals the zero vector are all zero.</li>
<li>Spanning set: The set of basis vectors spans the entire vector space, meaning that any vector in the vector space can be expressed as a linear combination of the basis vectors. In other words, the basis vectors “span” the vector space by forming a basis for it.</li>
<li>Minimal set: The set of basis vectors is minimal, meaning that no vector can be removed from the set without changing the span of the vector space. In other words, the basis vectors form the smallest possible set that can generate the entire vector space.</li>
<li>Unique representation: Any vector in the vector space can be uniquely represented as a linear combination of the basis vectors. This means that there is only one way to express a vector as a linear combination of the basis vectors, ensuring that the representation is unique.</li>
</ul>
</section>
</section>
<section id="subspace" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="subspace"><span class="header-section-number">3</span> Subspace</h2>
<section id="definition-of-a-subspace" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="definition-of-a-subspace"><span class="header-section-number">3.1</span> Definition of a subspace</h3>
<p>A subset <img src="https://latex.codecogs.com/png.latex?W"> of <img src="https://latex.codecogs.com/png.latex?V"> is called a subspace of <img src="https://latex.codecogs.com/png.latex?V"> if it satisfies the following three conditions: 1. <img src="https://latex.codecogs.com/png.latex?W"> contains the zero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D"> of <img src="https://latex.codecogs.com/png.latex?V">. 2. <img src="https://latex.codecogs.com/png.latex?W"> is closed under vector addition, i.e., for any vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D,%20%5Cmathbf%7Bv%7D%20%5Cin%20W">, their sum <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D"> is also in <img src="https://latex.codecogs.com/png.latex?W">. 3. <img src="https://latex.codecogs.com/png.latex?W"> is closed under scalar multiplication, i.e., for any vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20%5Cin%20W"> and any scalar <img src="https://latex.codecogs.com/png.latex?c">, their product <img src="https://latex.codecogs.com/png.latex?c%5Cmathbf%7Bu%7D"> is also in <img src="https://latex.codecogs.com/png.latex?W">.</p>
</section>
<section id="example-2" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="example-2"><span class="header-section-number">3.2</span> Example</h3>
<p>Consider the vector space <img src="https://latex.codecogs.com/png.latex?V%20=%20%5Cmathbb%7BR%7D%5E3"> with standard vector addition and scalar multiplication. Let <img src="https://latex.codecogs.com/png.latex?W"> be the subset of <img src="https://latex.codecogs.com/png.latex?V"> consisting of all vectors of the form <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%20x%20%5C%5C%20y%20%5C%5C%200%20%5Cend%7Bbmatrix%7D">, where <img src="https://latex.codecogs.com/png.latex?x,%20y"> are real numbers. We can express <img src="https://latex.codecogs.com/png.latex?W"> as: <img src="https://latex.codecogs.com/png.latex?W%20=%20%5Cleft%5C%7B%20%5Cbegin%7Bbmatrix%7D%20x%20%5C%5C%20y%20%5C%5C%200%20%5Cend%7Bbmatrix%7D%20%5C,%20%5Cmiddle%7C%20%5C,%20x,%20y%20%5Cin%20%5Cmathbb%7BR%7D%20%5Cright%5C%7D"> Then <img src="https://latex.codecogs.com/png.latex?W"> is a subspace of <img src="https://latex.codecogs.com/png.latex?V"> because it satisfies the three conditions mentioned above.</p>
</section>
<section id="properties-1" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="properties-1"><span class="header-section-number">3.3</span> Properties</h3>
<p>A subspace is a subset of a vector space that retains the structure of a vector space. Here are some properties of a subspace in relation to a vector space:</p>
<ol type="1">
<li>Contains the zero vector: A subspace must contain the zero vector (denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D">) of the vector space it is a subset of. This is because the zero vector is required for closure under vector addition and scalar multiplication.</li>
<li>Closed under vector addition: If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D,%20%5Cmathbf%7Bv%7D%20%5Cin%20W">, then <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D%20%5Cin%20W">.</li>
<li>Closed under scalar multiplication: If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20%5Cin%20W"> and <img src="https://latex.codecogs.com/png.latex?c"> is a scalar, then <img src="https://latex.codecogs.com/png.latex?c%5Cmathbf%7Bu%7D%20%5Cin%20W">.</li>
</ol>
<p>These properties provide a way to describe subsets of vector spaces that inherit certain properties from the parent vector space. Subspaces are useful for understanding the structure, properties, and behavior of vector spaces in a more focused and simplified manner. Some reasons why we need subspaces are:</p>
<ul>
<li>Simplification and abstraction: Subspaces allow us to simplify the study of vector spaces by focusing on smaller, more manageable subsets that share similar properties. This abstraction can help us understand the fundamental structure and behavior of vector spaces without getting bogged down by the complexity of the entire vector space.</li>
<li>Study of special cases: Subspaces can represent special cases or special structures within a vector space that are of particular interest.
<ul>
<li>For example, subspaces can represent sub-spaces of solutions to linear systems of equations, eigenspaces associated with eigenvalues of matrices, or orthogonal subspaces related to orthogonality and projections.</li>
</ul></li>
<li>Applications in various fields: Subspaces have numerous applications in various fields, such as physics, computer graphics, data analysis, signal processing, and optimization, among others. Subspaces provide a mathematical framework for modeling, analyzing, and solving problems in these fields.</li>
<li>Computational efficiency: Subspaces can be used in algorithms and techniques for solving problems involving vector spaces in a computationally efficient manner. Techniques such as subspace methods, subspace approximation, and subspace projection can be employed to reduce the computational complexity of certain problems by working within lower-dimensional subspaces.</li>
</ul>
</section>
<section id="span" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="span"><span class="header-section-number">3.4</span> Span</h3>
</section>
<section id="definition-1" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="definition-1"><span class="header-section-number">3.5</span> Definition</h3>
<p>The span of a set of vectors <img src="https://latex.codecogs.com/png.latex?%7Bv_1,%20v_2,%20%5Cldots,%20v_n%7D"> in a vector space <img src="https://latex.codecogs.com/png.latex?V">, denoted by <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bspan%7D%7Bv_1,%20v_2,%20%5Cldots,%20v_n%7D">, is the set of all possible linear combinations of these vectors. Mathematically, it is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bspan%7D%5C%7B%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cldots,%20%5Cmathbf%7Bv%7D_n%5C%7D%20=%20%5Cleft%5C%7B%20c_1%5Cmathbf%7Bv%7D_1%20+%20c_2%5Cmathbf%7Bv%7D_2%20+%20%5Cldots%20+%20c_n%5Cmathbf%7Bv%7D_n%20%7C%20c_1,%20c_2,%20%5Cldots,%20c_n%20%5Cin%20%5Cmathbb%7BR%7D%20%5Cright%5C%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?c_1,%20c_2,%20%5Cldots,%20c_n"> are scalar coefficients and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> represents the set of real numbers.</p>
<p>Example: Let’s consider the set of vectors <img src="https://latex.codecogs.com/png.latex?v_1">, <img src="https://latex.codecogs.com/png.latex?v_2">, and <img src="https://latex.codecogs.com/png.latex?v_3"> defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%200%20%5C%200%20%5Cend%7Bbmatrix%7D,%20%5Cquad%20%5Cmathbf%7Bv%7D_2%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%201%20%5C%200%20%5Cend%7Bbmatrix%7D,%20%5Cquad%20%5Cmathbf%7Bv%7D_3%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%200%20%5C%201%20%5Cend%7Bbmatrix%7D"></p>
<p>The span of these vectors, denoted by <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bspan%7D%7B%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cmathbf%7Bv%7D_3%7D">, is the set of all possible linear combinations of these vectors, which in this case is the entire three-dimensional vector space <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3">, since any vector in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3"> can be expressed as a linear combination of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_2">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_3">.</p>
</section>
<section id="proporties" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="proporties"><span class="header-section-number">3.6</span> Proporties</h3>
<ul>
<li>Span is a subspace: The span of a set of vectors is always a subspace of the vector space in which the vectors belong. This means that it satisfies all the properties of a vector space, including closure under vector addition and scalar multiplication.</li>
<li>Span is the smallest subspace: The span of a set of vectors is the smallest subspace that contains all the vectors in the set. It is the intersection of all subspaces that contain the vectors, and thus it forms the smallest subspace that spans the set of vectors.</li>
<li>Span is closed under linear combinations: The span of a set of vectors is closed under linear combinations of the vectors. This means that any linear combination of the vectors in the set will also be in the span.</li>
<li>Span generates the entire vector space: The span of a set of vectors is capable of generating the entire vector space to which the vectors belong. This means that any vector in the vector space can be expressed as a linear combination of the vectors in the span.</li>
<li>Span is unique: The span of a set of vectors is unique, meaning that it is uniquely determined by the set of vectors being spanned. However, the span may be different for different sets of vectors.</li>
</ul>
<p>These properties make the concept of span important in linear algebra, as it allows us to understand the space spanned by a set of vectors and the relationships between vectors within a vector space.</p>
<p>The concept of span is important in linear algebra because of:</p>
<ul>
<li>Understanding vector space: The span of a set of vectors helps us understand the space that can be generated by those vectors within a vector space. It provides insight into the range of possible values and combinations that can be obtained using the given set of vectors.</li>
<li>Solving systems of linear equations: Span is closely related to solving systems of linear equations. The solutions to a system of linear equations can be expressed as linear combinations of the vectors in the span of the coefficient matrix. By finding the span of a set of vectors, we can determine the possible solutions to a system of linear equations and understand the relationship between different solutions.</li>
<li>Basis for vector spaces: The span of a set of vectors can form a basis for a vector space. A basis is a set of linearly independent vectors that span the entire vector space. By finding the span of a set of vectors, we can determine if they form a basis for a vector space, and if so, use them as a foundation for understanding and manipulating vectors within that space.</li>
<li>Vector space operations: Span is closed under vector space operations, such as vector addition and scalar multiplication. This property allows us to perform operations on vectors within the span and obtain new vectors that are still within the span. It also allows us to express any vector in the vector space as a linear combination of vectors in the span.</li>
<li>Dimensionality and rank: The span of a set of vectors is closely related to the dimensionality and rank of a vector space. The dimension of a vector space is the number of linearly independent vectors in a basis for that space, and the rank of a matrix is the dimension of the span of its column vectors. Understanding the span of vectors can help us determine the dimensionality and rank of a vector space, which has applications in areas such as data analysis, machine learning, and signal processing.</li>
</ul>
<p>Span provides insights into the space spanned by a set of vectors, the relationships between vectors within a vector space, and the possible solutions to systems of linear equations. It also serves as a basis for vector spaces and facilitates vector space operations, and is closely related to the dimensionality and rank of a vector space.</p>
</section>
</section>
<section id="dimensionality-and-rank" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="dimensionality-and-rank"><span class="header-section-number">4</span> Dimensionality and Rank</h2>
<section id="definition-2" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="definition-2"><span class="header-section-number">4.1</span> Definition</h3>
<ul>
<li>The dimensionality of a vector space is defined as the number of linearly independent vectors in its basis.</li>
<li>The rank of a matrix is defined as the maximum number of linearly independent rows or columns in the matrix.</li>
</ul>
</section>
<section id="example-3" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="example-3"><span class="header-section-number">4.2</span> Example</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?V"> be a vector space with a basis <img src="https://latex.codecogs.com/png.latex?%7B%20%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cldots,%20%5Cmathbf%7Bv%7D_n%20%7D">. The dimensionality of <img src="https://latex.codecogs.com/png.latex?V">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdim%7D(V)">, is the number of linearly independent vectors in its basis, which is equal to <img src="https://latex.codecogs.com/png.latex?n">. Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix. The rank of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Brank%7D(%5Cmathbf%20A)">, is the maximum number of linearly independent rows or columns in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">. It can also be defined as the dimensionality of the column space or row space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">.</p>
</section>
<section id="properties-2" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="properties-2"><span class="header-section-number">4.3</span> Properties</h3>
<section id="dimensionality" class="level4" data-number="4.3.1">
<h4 data-number="4.3.1" class="anchored" data-anchor-id="dimensionality"><span class="header-section-number">4.3.1</span> Dimensionality</h4>
<ul>
<li>Dimensionality refers to the number of elements or components in a vector or the size of a vector space.</li>
<li>The dimensionality of a vector space is always a positive integer.</li>
<li>Dimensionality is additive, meaning that the dimensionality of the direct sum of vector spaces is equal to the sum of their individual dimensionality.</li>
<li>A set of vectors is linearly independent if and only if the dimensionality of the vector space they span is equal to the number of vectors in the set.</li>
</ul>
</section>
<section id="rank" class="level4" data-number="4.3.2">
<h4 data-number="4.3.2" class="anchored" data-anchor-id="rank"><span class="header-section-number">4.3.2</span> Rank</h4>
<ul>
<li>The rank of a matrix is the maximum number of linearly independent rows or columns in the matrix.</li>
<li>The rank of a matrix is always a non-negative integer.</li>
<li>The rank of a matrix is equal to the dimensionality of its column space and row space.</li>
<li>The rank of a matrix is invariant under elementary row and column operations.</li>
<li>The rank of a matrix is less than or equal to the minimum of the number of rows and columns in the matrix.</li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In some contexts, the term “dimensionality” may also refer to the dimensionality of the column space or row space of a matrix, which is equivalent to the rank of the matrix.</p>
</div>
</div>
</section>
</section>
</section>
<section id="column-space" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="column-space"><span class="header-section-number">5</span> Column Space</h2>
<section id="definition-3" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="definition-3"><span class="header-section-number">5.1</span> Definition</h3>
<p>The column space of a matrix is the subspace spanned by its column vectors. It is denoted by <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BCol%7D(%5Cmathbf%20A)"> or <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bspan%7D%7B%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cldots,%20%5Cmathbf%7Bv%7D_n%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cldots,%20%5Cmathbf%7Bv%7D_n"> are the column vectors of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">.</p>
</section>
<section id="example-4" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="example-4"><span class="header-section-number">5.2</span> Example</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%20%5C%5C%0A4%20&amp;%205%20&amp;%206%20%5C%5C%0A7%20&amp;%208%20&amp;%209%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>The column space of matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, denoted by <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BCol%7D(%5Cmathbf%20A)"> or <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bspan%7D%5C%7B%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cmathbf%7Bv%7D_3%5C%7D">, is the subspace spanned by its column vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1%20=%20%5B1,%204,%207%5D%5ET">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_2%20=%20%5B2,%205,%208%5D%5ET">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_3%20=%20%5B3,%206,%209%5D%5ET">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BCol%7D(%5Cmathbf%7BA%7D)%20=%20%5Ctext%7Bspan%7D%5Cleft%5C%7B%0A%5Cbegin%7Bbmatrix%7D%0A1%20%5C%5C%204%20%5C%5C%207%20%5C%5C%0A%5Cend%7Bbmatrix%7D,%0A%5Cbegin%7Bbmatrix%7D%0A2%20%5C%5C%205%20%5C%5C%208%20%5C%5C%0A%5Cend%7Bbmatrix%7D,%0A%5Cbegin%7Bbmatrix%7D%0A3%20%5C%5C%206%20%5C%5C%209%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cright%5C%7D%0A"></p>
</section>
<section id="properties-3" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="properties-3"><span class="header-section-number">5.3</span> Properties</h3>
<ul>
<li>It is a subspace: The column space of a matrix is a subspace of the vector space in which the matrix’s columns reside. This means it is closed under vector addition and scalar multiplication, and contains the zero vector.</li>
<li>It is spanned by the columns of the matrix: The column space of a matrix is the span of its column vectors. In other words, it is the smallest subspace that contains all the column vectors of the matrix.</li>
<li>It is the range of the corresponding linear transformation: The column space of a matrix is the range of the linear transformation associated with that matrix. This means it contains all possible output vectors that can be obtained by applying the linear transformation to input vectors.</li>
<li>It has the same dimension as the rank of the matrix: The dimension of the column space of a matrix is equal to the rank of the matrix. This is known as the column space’s dimensionality property.</li>
<li>It provides a basis for the row space and null space: The column space of a matrix provides a basis for both the row space and the null space of the matrix. The row space is the orthogonal complement of the null space, and the column space is the orthogonal complement of the left null space.</li>
<li>It determines the solvability of linear systems: The column space of a coefficient matrix in a system of linear equations determines whether the system has a unique solution, infinitely many solutions, or no solution at all. If the column space spans the entire vector space, the system has a unique solution. If the column space does not span the entire vector space, the system has either infinitely many solutions (if the rank of the matrix is less than the number of variables) or no solution (if the rank of the matrix is less than the number of equations).</li>
</ul>
</section>
</section>
<section id="row-space" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="row-space"><span class="header-section-number">6</span> Row Space</h2>
<section id="definition-4" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="definition-4"><span class="header-section-number">6.1</span> Definition</h3>
<p>The row space of a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRow%7D(%5Cmathbf%7BA%7D)">, is the subspace spanned by the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRow%7D(%5Cmathbf%7BA%7D)%20=%20%5Ctext%7Bspan%7D%5C%7B%5Cmathbf%7Br%7D_1,%20%5Cmathbf%7Br%7D_2,%20%5Cldots,%20%5Cmathbf%7Br%7D_m%5C%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Br%7D_1,%20%5Cmathbf%7Br%7D_2,%20%5Cldots,%20%5Cmathbf%7Br%7D_m"> are the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
</section>
<section id="example-5" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="example-5"><span class="header-section-number">6.2</span> Example</h3>
<p>The row space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is the subspace spanned by the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, which can be expressed as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRow%7D(%5Cmathbf%7BA%7D)%20=%20%5Ctext%7Bspan%7D%5Cleft%5C%7B%0A%5Cbegin%7Bbmatrix%7D%0A1%20%5C%5C%202%20%5C%5C%203%20%5C%5C%0A%5Cend%7Bbmatrix%7D,%0A%5Cbegin%7Bbmatrix%7D%0A4%20%5C%5C%205%20%5C%5C%206%20%5C%5C%0A%5Cend%7Bbmatrix%7D,%0A%5Cbegin%7Bbmatrix%7D%0A7%20%5C%5C%208%20%5C%5C%209%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cright%5C%7D%0A"></p>
</section>
<section id="properties-4" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="properties-4"><span class="header-section-number">6.3</span> Properties</h3>
<ul>
<li>It is a subspace: The row space of a matrix is a subspace of the vector space in which the matrix’s rows reside. This means it is closed under vector addition and scalar multiplication, and contains the zero vector.</li>
<li>It is spanned by the rows of the matrix: The row space of a matrix is the span of its row vectors. In other words, it is the smallest subspace that contains all the row vectors of the matrix.</li>
<li>It is the range of the corresponding linear transformation: The row space of a matrix is the range of the linear transformation associated with the transpose of that matrix. This means it contains all possible output vectors that can be obtained by applying the transpose of the linear transformation to input vectors.</li>
<li>It has the same dimension as the rank of the matrix: The dimension of the row space of a matrix is equal to the rank of the matrix. This is known as the row space’s dimensionality property.</li>
<li>It provides a basis for the null space and left null space: The row space of a matrix provides a basis for both the null space (kernel) and the left null space (cokernel) of the matrix. The null space is the orthogonal complement of the row space, and the left null space is the orthogonal complement of the column space.</li>
<li>It determines the row-rank and column-rank equality: The row space of a matrix determines the row-rank and column-rank equality property, which states that the number of linearly independent rows (the row-rank) is equal to the number of linearly independent columns (the column-rank) of the matrix.</li>
<li>It determines the solvability of linear systems: The row space of a coefficient matrix in a system of linear equations determines whether the system has a unique solution, infinitely many solutions, or no solution at all. If the row space spans the entire vector space, the system has a unique solution. If the row space does not span the entire vector space, the system has either infinitely many solutions (if the rank of the matrix is less than the number of variables) or no solution (if the rank of the matrix is less than the number of equations).</li>
</ul>
</section>
</section>
<section id="null-space" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="null-space"><span class="header-section-number">7</span> Null Space</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BNull%7D(%5Cmathbf%7BA%7D)%20=%20%5Cleft%5C%7B%20%5Cmathbf%7Bx%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En%20%5C,%20%5Cmiddle%7C%20%5C,%20%5Cmathbf%7BA%7D%5Cmathbf%7Bx%7D%20=%20%5Cmathbf%7B0%7D%20%5Cright%5C%7D%0A"></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BNull%7D"> represents the null space of a matrix,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> represents the given matrix,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> represents a vector in the null space of ,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En"> represents the n-dimensional real vector space, and</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D"> represents the zero vector.</li>
</ul>
<section id="example-6" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="example-6"><span class="header-section-number">7.1</span> Example</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%20%5C%5C%0A4%20&amp;%205%20&amp;%206%20%5C%5C%0A7%20&amp;%208%20&amp;%209%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>The null space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is the set of all vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E3"> that satisfy <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5Cmathbf%7Bx%7D%20=%20%5Cmathbf%7B0%7D">.</p>
</section>
<section id="properties-5" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="properties-5"><span class="header-section-number">7.2</span> Properties</h3>
<p>The null space of a matrix, also known as the kernel space, is the set of all vectors that are mapped to the zero vector by the linear transformation associated with the matrix.</p>
<ul>
<li>Contains the zero vector: The null space always contains the zero vector, as any matrix multiplied by the zero vector results in the zero vector.</li>
<li>Subspace property: The null space is a subspace of the vector space from which the vectors are drawn. This means that it is closed under vector addition and scalar multiplication. In other words, if two vectors are in the null space, their sum and any scalar multiple of them will also be in the null space.</li>
<li>Dimensionality: The dimension of the null space is equal to the number of linearly independent solutions to the homogeneous linear system associated with the matrix. This is known as the nullity of the matrix.</li>
<li>Basis: The null space has a basis consisting of linearly independent vectors that span the entire null space. This basis is used to represent all vectors in the null space as linear combinations of the basis vectors.</li>
<li>Relationship to solvability: The null space is directly related to the solvability of a system of linear equations. If the null space contains only the zero vector, the system has a unique solution. If the null space contains non-zero vectors, the system has infinitely many solutions, and the null space vectors represent the general solutions.</li>
<li>Orthogonal complement of row space: The null space is orthogonal to the row space of the matrix. This means that any vector in the null space is orthogonal to every vector in the row space, and vice versa.</li>
<li>Relationship to matrix rank: The dimension of the null space, also known as the nullity, is related to the rank of the matrix through the rank-nullity theorem. The rank of the matrix plus the nullity of the matrix is equal to the number of columns in the matrix.</li>
<li>Computation: The null space can be computed by finding the solutions to the homogeneous linear system <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7B0%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is the coefficient matrix of the system of linear equations.</li>
</ul>
</section>
</section>
<section id="column-space-vs-row-space-vs-null-space" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="column-space-vs-row-space-vs-null-space"><span class="header-section-number">8</span> Column Space vs Row Space vs Null Space</h2>
<p>While both column space and row space are subspaces associated with a matrix, they serve different roles in linear algebra.</p>
<section id="column-space-vs-row-space" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="column-space-vs-row-space"><span class="header-section-number">8.1</span> Column Space vs Row Space</h3>
<ul>
<li>Dimensionality: The dimensionality of the column space and row space can differ. The dimension of the column space is determined by the number of linearly independent columns, which is also known as the rank of the matrix. On the other hand, the dimension of the row space is determined by the number of linearly independent rows of the matrix, which may not necessarily be the same as the rank of the matrix.</li>
<li>Basis and Span: The column space is typically used to find a basis for the range (output space) of a linear transformation associated with the matrix, while the row space is used to find a basis for the null space (kernel) and left null space (cokernel) of the matrix. The column space spans the range of the linear transformation, while the row space spans the orthogonal complement of the null space and left null space.</li>
<li>Solvability of linear systems: The row space of the coefficient matrix in a system of linear equations determines the solvability of the system, while the column space is not directly related to the solvability. Specifically, if the row space spans the entire vector space, the system has a unique solution. If the row space does not span the entire vector space, the system may have infinitely many solutions or no solution. The column space, on the other hand, does not directly determine the solvability of the system.</li>
<li>Transpose relationship: The row space of a matrix is related to the range of the linear transformation associated with the transpose of the matrix, while the column space is directly related to the range of the original matrix. This means that the row space and column space are related through the transpose operation, but they are not identical.</li>
</ul>
</section>
<section id="null-space-1" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="null-space-1"><span class="header-section-number">8.2</span> Null Space</h3>
<p>Null space: The null space of a matrix is the set of all vectors that are mapped to the zero vector by the linear transformation associated with the matrix. It represents the subspace of the vector space that consists of solutions to the homogeneous linear system <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7B0%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is the coefficient matrix of the system of linear equations.</p>
</section>
<section id="the-relationships-between-these-spaces" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="the-relationships-between-these-spaces"><span class="header-section-number">8.3</span> The relationships between these spaces</h3>
<ul>
<li>The column space and row space are related, as they are orthogonal complements of each other. This means that any vector in the row space is orthogonal to any vector in the null space, and vice versa. This relationship is known as the row-space-null-space decomposition.</li>
<li>The dimension of the column space is equal to the rank of the matrix, which is the maximum number of linearly independent columns or rows. Similarly, the dimension of the row space is also equal to the rank of the matrix.</li>
<li>The null space is orthogonal to both the column space and the row space. This means that any vector in the null space is orthogonal to any vector in the column space and the row space.</li>
<li>The null space is useful in determining the solvability of a system of linear equations. If the null space contains only the zero vector, the system has a unique solution. If the null space contains non-zero vectors, the system has infinitely many solutions.</li>
</ul>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html</guid>
  <pubDate>Sun, 09 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Infrastructure Security</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html</link>
  <description><![CDATA[ 



<ul class="nav nav-pills" id="language-tab">
<li class="nav-item">
<button class="nav-link active" id="Korean-tab" data-bs-toggle="tab" data-bs-target="#Korean" type="button" aria-controls="Korean" aria-selected="true">
Korean
</button>
</li>
<li class="nav-item">
<button class="nav-link" id="English-tab" data-bs-toggle="tab" data-bs-target="#English" type="button" aria-controls="knitr" aria-selected="false">
English
</button>
</li>
<div class="tab-content" id="language-tabcontent">

<div id="Korean" class="tab-pane fade show active" aria-labelledby="Korean-tab">
<div id="Korean" class="tab-pane fade show active" aria-labelledby="Korean-tab">
<section id="network-isolation" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="network-isolation"><span class="header-section-number">1</span> Network Isolation</h2>
<p>AWS has implemented network isolation through a <strong>limited number of access points</strong> to the cloud, allowing for comprehensive monitoring of inbound and outbound communications and network traffic.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>inbound and outbound communications involve observing and analyzing the data and traffic that is entering or leaving the AWS network to ensure security and compliance.</p>
</div>
</div>
<ul>
<li>Endpoints are URLs that serve as entry points for web services.</li>
<li>Some services do not support regions (like IAM), so their endpoints do not include a region. But, some services (like US-West-2) do support regions.</li>
<li>AWS offers Amazon Virtual Private Cloud (VPC) as a private network within the AWS Cloud that provides isolation from other customers and from the internet
<ul>
<li>VPC allows you to allocate IP address spaces and build a private infrastructure with networks isolated from the internet.</li>
<li>You can also connect your on-premises environment or other VPN infrastructures to VPC using IPSec tunnels and AWS Direct Connect.</li>
<li>VPC allows resources to communicate with the internet if desired.</li>
</ul></li>
<li>Building a fort on a barren planet to protect themselves and the bees, and further isolating hives inside the fort itself is similar to the concept of isolating resources within a secure environment, such as Amazon VPC, to protect them from potential external threats.</li>
<li>Network Isolation VPC
<ul>
<li>the concept of Virtual Private Cloud (VPC) in AWS is a way to logically separate your AWS infrastructure from other customers.</li>
<li>VPC is like creating a fort around your AWS account and isolating resources into hives, using subnets or logical subdivision of IPs.
<ul>
<li>ex) EC2 instances are able to access the internet and be accessed from by putting them in a public subnet via Network Access Control Lists (NACLs), which are used to control inbound and outbound traffic at the subnet level.</li>
</ul></li>
<li>security groups
<ul>
<li>act as firewalls for EC2 instances by controlling both inbound and outbound traffic at the instance level.</li>
<li>This fine-grained access is defined by allow rules and looks</li>
</ul></li>
</ul></li>
<li>how to secure traffic between VPCs in AWS using VPC endpoints and route tables?
<ul>
<li>further secure communication between Virtual Private Clouds (VPCs) in AWS
<ul>
<li>It compares the traditional method of sending traffic between VPCs through the internet with the use of private links, which allow for direct communication between VPCs within the AWS infrastructure, resulting in a safer path of travel for data.</li>
<li>the concept of route tables in VPCs
<ul>
<li>route tables contain <strong>rules or routes</strong> used to determine where network traffic is directed, and the option to create custom route tables for routing traffic according to specific requirements.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="detective-controls" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="detective-controls"><span class="header-section-number">2</span> Detective Controls</h2>
<ul>
<li>Detective controls: 감사(auditing), 자동화된 분석, 경보를 가능하게 하는 사건 모니터링 및 블록 처리의 지속적인 개선
<ul>
<li>잠재적인 보안 위협 또는 사건을 식별할 수 있는 능력 향상</li>
<li>governance frameworks에 필수적</li>
<li>법이나 compliance 준수 의무, 보안 작업 등의 개선을 지원</li>
</ul></li>
<li>Different types of detective controls
<ul>
<li>자산 인벤토리의 작성(conducting an inventory of AWS resources)</li>
<li>내부 감사(internal auditing)</li>
<li>정보 시스템과 관련된 제어가 정책 및 요구사항 충족하는지 검사</li>
</ul></li>
<li>이상 활동 범위를 식별하고 이해하는데 도움이 됨</li>
</ul>
</section>
<section id="auditing" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="auditing"><span class="header-section-number">3</span> Auditing</h2>
<ul>
<li>AWS infrastructure의 보안 및 compliance를 향상시키는 AWS services (일부는 무료, 일부는 유료)
<ul>
<li>AWS CloudTrail: AWS infrastructure와 interact 하는 사람 추적 가능 (잘못된 변경/데이터 유출 추적에 도움)</li>
<li>AWS Config: configuration 관리 및 변경 기록, 모든 실제 config 세부 사항의 inventory 제공</li>
<li>AWS Inspector: 자동 보안 평가 실행
<ul>
<li>deploy된 applications의 보안 및 compliance를 향상시키기 위해, best practies와의 차이, EC2 instances의 노출, 취약점 등을 체크</li>
</ul></li>
<li>Trusted Advisor
<ul>
<li>AWS resources의 프로비저닝 보조 - best practices를 사용해서 리포트 제공
<ul>
<li>리포트 항목: 비용 최적화, 성능, 보안, 장애 허용 정도, 서비스 제한</li>
<li>조사 또는 실행을 위해, 심각한 수준(녹색,주황,적색)에 따라 권장 사항 제공</li>
</ul></li>
<li>Security section: S3 bucket의 권한, 보안 그룹, IAM 사용, root 계정의 MFA, 노출된 access keys, IAM 비밀번호 정책 등을 스캔</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="monitoring-cloudwatch-and-cloudwatch-log" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="monitoring-cloudwatch-and-cloudwatch-log"><span class="header-section-number">4</span> Monitoring CloudWatch and CloudWatch Log</h2>
<ul>
<li>Monitoring: Infrastructure와 관련된 데이터와 통계를 수집, 추적, 표시하는 과정</li>
<li>AWS의 CloudWatch: metrics repository역할 - repository에 넣은 metrics 기반으로 통계 검색
<ul>
<li>사용자가 정한 threshold를 넘었을때 경보 생성 가능</li>
<li>특정 기간 동안 하나의 metric을 감시 → threshold와 비교한 metric의 상대적인 값에 따라 하나 이상의 특정 action 수행 가능</li>
</ul></li>
<li>CloudWatch Logs: 여러 resources의 log files을 모니터링, 저장, 접근 가능한 tool
<ul>
<li>application, 서버 OS의 로그 수집 및 저장</li>
<li>CloudTrail 사용해서 API activity 수집</li>
<li>Amazon Route 53(Amazon의 DNS 웹 서비스)의 DNS queries를 기록</li>
<li>S3의 로그 데이터 저장</li>
</ul></li>
<li>CloudWatch Logs Insights: 로그 데이터를 interactive하게 검색하고 분석</li>
</ul>
</section>
<section id="monitoring-guard-duty-and-security-hub" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="monitoring-guard-duty-and-security-hub"><span class="header-section-number">5</span> Monitoring Guard Duty and Security Hub</h2>
<ul>
<li>Amazon GuardDuty: 위협 감지 서비스
<ul>
<li>AWS 계정 및 리소스에 대한 허가되지 않거나 악성인 행동들을 계속 모니터링</li>
<li>머신 러닝, 이상 감지, integrated threat intelligence를 사용해서 잠재적인 위협을 식별하고 우선 순위를 정함</li>
<li>여러 AWS data resources에서 수백억건의 사건 분석</li>
<li>잠재적인 위협을 세 단계(low, medium, high) 심각 수준으로 나눠서 대응 우선순위 결정</li>
<li>HTTPs API, CLI tools, Amazon CloudWatch events를 제공해서 보안 관련 발견에 대한 자동화된 보안 제공 지원</li>
</ul></li>
<li>Security Hub: 여러 AWS service의 보안 경고나 발견을 모으고, 정리하고, 우선 순위를 정함 → 통합 dashboards에서 시각화 요약 제공
<ul>
<li>AWS best practies 및 업계 표준을 기반으로, compliance check 자동화를 통해 환경을 지속적으로 모니터링할 수 있도록 함</li>
</ul></li>
</ul>
</section>
</div>
</div>
<div id="English" class="tab-pane fade" aria-labelledby="English-tab">
<div id="English" class="tab-pane fade" aria-labelledby="English-tab">

</div>
</div>
<section id="back-to-content-list" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="back-to-content-list"><span class="header-section-number">6</span> Back to Content List</h2>
<ul>
<li><a href="../../../../../docs/blog/posts/content_list.html">Global Blog Content List</a></li>
</ul>


</section>

</div></ul> ]]></description>
  <category>Engineering</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html</guid>
  <pubDate>Tue, 04 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Matrix Transformation (4) - Biinear Form</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.bilinear_form.html</link>
  <description><![CDATA[ 



<section id="binear-form" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="binear-form"><span class="header-section-number">1</span> Binear Form</h2>
<p>A bilinear form of a matrix is a function that extends the linear form and takes two vectors as inputs and produces a scalar as output. It is linear in both of its arguments, meaning that it satisfies the following properties:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AB(%5Cmathbf%20u+%5Cmathbf%20v)&amp;=B(%5Cmathbf%20u+%5Cmathbf%20w)+B(%5Cmathbf%20v+%5Cmathbf%20w)%5C%5C%0AB(%5Cmathbf%20u,%5Calpha%20%5Cmathbf%20v)&amp;=%5Calpha%20B(%5Cmathbf%20u,%5Cmathbf%20v)%5C%5C%0AB(%5Calpha%5Cmathbf%20u,%5Cmathbf%20v)&amp;=%5Calpha%20B(%5Cmathbf%20u,%5Cmathbf%20v)%0A%5Cend%7Baligned%7D%0A"></p>
<p>for all vectors <img src="https://latex.codecogs.com/png.latex?u">, <img src="https://latex.codecogs.com/png.latex?v">, <img src="https://latex.codecogs.com/png.latex?w"> and scalars <img src="https://latex.codecogs.com/png.latex?%5Calpha">.</p>
<p>A bilinear form can be represented by a matrix <img src="https://latex.codecogs.com/png.latex?B"> such that <img src="https://latex.codecogs.com/png.latex?B_%7Bi,j%7D"> is the coefficient of the product <img src="https://latex.codecogs.com/png.latex?u_i%20v_j"> in the expansion of <img src="https://latex.codecogs.com/png.latex?B(u,v)">. The bilinear form can then be written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AB(%5Cmathbf%20u,%5Cmathbf%20v)=%5Cmathbf%20u%5ET%20B%20%5Cmathbf%20v%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20u"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v"> are column vectors and <img src="https://latex.codecogs.com/png.latex?B"> is a matrix.</p>
<p>For example, consider the bilinear form <img src="https://latex.codecogs.com/png.latex?B(%5Cmathbf%20u,%5Cmathbf%20v)%20=%20u_1%20v_1%20+%20u_2%20v_2">. This bilinear form can be represented by the matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AB=%5Cbegin%7Bbmatrix%7D1&amp;0%5C%5C0&amp;1%5Cend%7Bbmatrix%7D%0A"></p>
<p>and written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AB(%5Cmathbf%20u,%5Cmathbf%20v)=%5Cbegin%7Bbmatrix%7Du_1&amp;%20u_2%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D1&amp;0%5C%5C0&amp;1%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dv_1%5C%5Cv_2%5Cend%7Bbmatrix%7D=u_1v_1+u_2v_2%0A"></p>
<p>This bilinear form computes the dot product of <img src="https://latex.codecogs.com/png.latex?u"> and <img src="https://latex.codecogs.com/png.latex?v">, which measures the similarity between the two vectors. Bilinear forms are commonly used in applications such as optimization, geometry, and physics, where they capture the interaction between two quantities or variables.</p>
<p>The covariance matrix can be represented as a bilinear form using matrix multiplication. Let’s say we have a random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Bx_1,%20x_2,%20%5Cldots,%20x_n%5D%5ET"> with mean vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cmu%7D%20=%20%5B%5Cmu_1,%20%5Cmu_2,%20%5Cldots,%20%5Cmu_n%5D%5ET"> and covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CSigma%7D">. Then, we can represent the covariance matrix as a bilinear form in the following way:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5CSigma&amp;=%5Coperatorname%7BE%7D%5B(%5Cmathbf%20x-%5Cmathbf%20%5Cmu)(%5Cmathbf%20x-%5Cmathbf%20%5Cmu)%5ET%5D%5C%5C%0A&amp;=%5Cfrac%7B1%7D%7Bn-1%7D%5Csum_%7Bi=1%7D%5E%7Bn%7D(x_i-%5Cbar%7Bx%7D)(x_i-%5Cbar%7Bx%7D)%5ET%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BE%7D"> denotes the expectation operator. We can expand this expression as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5CSigma&amp;=%5Coperatorname%7BE%7D%5B%5Cmathbf%20x%5Cmathbf%20x%5ET%5D-%5Cmathbf%20%5Cmu%5Cmathbf%20%5Cmu%5ET%0A%5Cend%7Baligned%7D%0A"></p>
<p>Now, we can represent the covariance matrix as a bilinear form using matrix multiplication as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5CSigma&amp;=%5Coperatorname%7BE%7D%5B(%5Cmathbf%20x-%5Cmathbf%20%5Cmu)(%5Cmathbf%20x-%5Cmathbf%20%5Cmu)%5ET%5D%5C%5C%0A&amp;=%5Csum_%7Bi=1%7D%5E%7Bn%7D%5Csum_%7Bj=1%7D%5E%7Bn%7D(%5Cmathbf%20x_i-%5Cmathbf%5Cmu_i)(%5Cmathbf%20x_i-%5Cmathbf%20%5Cmu_j)%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bx%7D-%5Cmathbf%7B%5Cmu%7D%5D"> is the deviation of the random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> from its mean vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cmu%7D">.</p>
<p>covariance matrix, and correlation matrix</p>
<p>One of the most famous examples is the use of bilinear forms in convolutional neural networks (CNNs), which are a type of deep learning model used for image and video recognition tasks.</p>
<p>In a CNN, a bilinear form is used to compute the similarity between a filter and a local region of an input image. This similarity measure is used to determine how much the filter “matches” the local region of the image, and is used to produce an output feature map.</p>
<p>More specifically, the bilinear form used in a CNN takes the form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Az_%7Bi,j%7D%20=%20%5Csum_%7Bm=1%7D%5E%7BM%7D%5Csum_%7Bn=1%7D%5E%7BN%7D%20w_%7Bm,n%7Dx_%7Bi+m-1,j+n-1%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?z_%7Bi,j%7D"> is the output feature map at location <img src="https://latex.codecogs.com/png.latex?(i,j)">, <img src="https://latex.codecogs.com/png.latex?x_%7Bi+m-1,j+n-1%7D"> is the input image pixel at location <img src="https://latex.codecogs.com/png.latex?(i+m-1,j+n-1)">, and <img src="https://latex.codecogs.com/png.latex?w_%7Bm,n%7D"> is the weight of the filter at position <img src="https://latex.codecogs.com/png.latex?(m,n)">. This computation is performed for each location <img src="https://latex.codecogs.com/png.latex?(i,j)"> in the output feature map.</p>
<p>The bilinear form used in CNNs is a type of convolution operation, and is used to learn features such as edges, corners, and other patterns in the input image. CNNs with bilinear forms have achieved state-of-the-art performance on many image recognition tasks, including object detection, face recognition, and scene classification.</p>
<p>Bilinear forms also have applications in other areas of machine learning, such as natural language processing (NLP). In NLP, bilinear forms can be used to compute the similarity between two word embeddings, which are vector representations of words. This similarity measure can be used for tasks such as sentiment analysis, text classification, and machine translation.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AB(%5Cmathbf%7Bu%7D,%5Cmathbf%7Bv%7D)=%5Cmathbf%7Bu%7D%5ET%20%5Cmathbf%7BW%7D%5Cmathbf%7Bv%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> are word embeddings, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BW%7D"> is a weight matrix, and <img src="https://latex.codecogs.com/png.latex?B(%5Cmathbf%7Bu%7D,%5Cmathbf%7Bv%7D)"> represents the bilinear form used to compute the similarity between the two embeddings.</p>
<p>Overall, bilinear forms are a powerful tool for learning features from complex data such as images and text, and have many applications in deep learning and machine learning.</p>


</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.bilinear_form.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Matrix Transformation (3) - Linear Form</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.linear_form.html</link>
  <description><![CDATA[ 



<section id="linear-form" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="linear-form"><span class="header-section-number">1</span> Linear Form</h2>
<p>A linear form is a linear function that maps a vector space to its underlying field. Let <img src="https://latex.codecogs.com/png.latex?V"> be a vector space over a field <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BF%7D">, and let <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(V,%5Cmathbb%7BF%7D)"> denote the set of all linear functions from <img src="https://latex.codecogs.com/png.latex?V"> to <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BF%7D">. A linear form on <img src="https://latex.codecogs.com/png.latex?V"> is an element of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(V,%5Cmathbb%7BF%7D)">.</p>
<p>A linear form <img src="https://latex.codecogs.com/png.latex?%5Cvarphi"> can be represented by a row vector of dimension <img src="https://latex.codecogs.com/png.latex?1%5Ctimes%20n">, where <img src="https://latex.codecogs.com/png.latex?n"> is the dimension of <img src="https://latex.codecogs.com/png.latex?V">. Let <img src="https://latex.codecogs.com/png.latex?%7B%5Cmathbf%7Be%7D_1,%20%5Cmathbf%7Be%7D_2,%20%5Cdots,%20%5Cmathbf%7Be%7D_n%7D"> be a basis for <img src="https://latex.codecogs.com/png.latex?V">, and let <img src="https://latex.codecogs.com/png.latex?%7B%5Calpha_1,%20%5Calpha_2,%20%5Cdots,%20%5Calpha_n%7D"> be the corresponding dual basis for <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(V,%5Cmathbb%7BF%7D)">, such that <img src="https://latex.codecogs.com/png.latex?%5Calpha_i(%5Cmathbf%7Be%7Dj)%20=%20%5Cdelta%7Bij%7D"> (the Kronecker delta). Then, any linear form <img src="https://latex.codecogs.com/png.latex?%5Cvarphi%5Cin%5Cmathcal%7BL%7D(V,%5Cmathbb%7BF%7D)"> can be written as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cvarphi(x)=%5Csum_%7Bi=1%7D%5E%7Bn%7Da_ix_i=%5Cmathbf%20a%20%5Cmathbf%20x%5ET=%5Cmathbf%20x%20%5Cmathbf%20a%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5Cin%20V"> is a column vector of dimension <img src="https://latex.codecogs.com/png.latex?n%5Ctimes%201">, <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Ba%7D%5D"> is the row vector representing <img src="https://latex.codecogs.com/png.latex?%5Cvarphi">, and <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bx%7D%5D"> is the column vector representing <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?V%20=%20%5Cmathbb%7BR%7D%5E2"> be the vector space of 2-dimensional column vectors, and let <img src="https://latex.codecogs.com/png.latex?%5Cvarphi%5Cin%5Cmathcal%7BL%7D(V,%5Cmathbb%7BR%7D)"> be the linear form defined by <img src="https://latex.codecogs.com/png.latex?%5Cvarphi(%5Cbegin%7Bbmatrix%7Dx%5Cy%5Cend%7Bbmatrix%7D)%20=%203x%20-%202y">. Then, we can represent <img src="https://latex.codecogs.com/png.latex?%5Cvarphi"> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5B%5Cmathbf%20a%5D=%5Cbegin%7Bbmatrix%7D%203%20&amp;%20-2%5Cend%7Bbmatrix%7D%20%5B%5Cmathbf%20x%5D=%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D%20%5Cvarphi(x)=%5Csum_%7Bi=1%7D%5E%7Bn%7D%5Cmathbf%20a%5Cmathbf%20x%5ET=3x_1-2x_2%0A"></p>
<p>which shows that <img src="https://latex.codecogs.com/png.latex?%5Cvarphi"> is a linear form on <img src="https://latex.codecogs.com/png.latex?V">.</p>
<p>consider a linear regression model that predicts the price of a house based on its size and location. The model can be represented by the linear form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cvarphi(x)=%5Cmathbf%20w%5Cmathbf%20x%5ET=%5Csum_%7Bi=1%7D%5E%7Bn%7Dw_ix_i=w_0+w_1x_1+w_2x_2%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cvarphi(%5Cmathbf%7Bx%7D)"> is the predicted price, <img src="https://latex.codecogs.com/png.latex?x_1"> is the size of the house, <img src="https://latex.codecogs.com/png.latex?x_2"> is a measure of the location (such as the distance from the city center), and <img src="https://latex.codecogs.com/png.latex?w_0">, <img src="https://latex.codecogs.com/png.latex?w_1">, and <img src="https://latex.codecogs.com/png.latex?w_2"> are the model parameters that control the intercept and the weights of the features. This linear form can be written in matrix form as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cvarphi(x)=%5Cmathbf%20x%5Cmathbf%20w=%5Cmathbf%20w%20%5Cmathbf%20x%5ET%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bw%7D%5D"> is a row vector of the model parameters and <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bx%7D%5D"> is a row vector of the features.</p>
<p>Linear forms can also be used in deep learning and machine learning models that involve linear transformations, such as fully connected layers in neural networks or linear classifiers. For example, consider a simple linear classifier that classifies images of digits into one of 10 classes. The classifier can be represented by the linear form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cvarphi(x)=%5Cmathbf%20x%5Cmathbf%20w%20+%20b%20=%5Cmathbf%20w%20%5Cmathbf%20x%5ET%20+b%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cvarphi(%5Cmathbf%7Bx%7D)"> is the predicted class score, <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bx%7D%5D"> is a row vector of the pixel values of the image, <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bw%7D%5D"> is a row vector of the weights of the classifier, and <img src="https://latex.codecogs.com/png.latex?b"> is the bias term. This linear form can be used to classify the image by selecting the class with the highest score.</p>
<p>In both of these examples, linear forms are used to represent linear relationships between variables or features, and the model parameters are learned through training on a set of labeled examples.</p>


</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.linear_form.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Matrix Transformation (5) - Quadratic Form</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.matrix_transformation.html</link>
  <description><![CDATA[ 



<section id="matrix" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="matrix"><span class="header-section-number">1</span> Matrix</h2>
<p>A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix, it can be represented as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%20%20a_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7B1n%7D%20%5C%5C%0A%20%20a_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7B2n%7D%20%5C%5C%0A%20%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A%20%20a_%7Bm1%7D%20&amp;%20a_%7Bm2%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7Bmn%7D%0A%5Cend%7Bbmatrix%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> is the element in the <img src="https://latex.codecogs.com/png.latex?i">-th row and <img src="https://latex.codecogs.com/png.latex?j">-th column of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
</section>
<section id="basic-matrix-operations" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="basic-matrix-operations"><span class="header-section-number">2</span> Basic Matrix Operations</h2>
<section id="matrix-addition" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="matrix-addition"><span class="header-section-number">2.1</span> Matrix addition</h3>
<p>The sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.</p>
<p>Given two <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D">, their sum <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D%20=%20%5Cmathbf%7BA%7D%20+%20%5Cmathbf%7BB%7D"> is defined by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ac_%7Bi,j%7D=a_%7Bi,j%7D+b_%7Bi,j%7D%E2%80%8B%0A"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%20m"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20j%20%5Cleq%20n">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%20%201%20&amp;%202%20%5C%5C%0A%20%203%20&amp;%204%20%5C%5C%0A%20%205%20&amp;%206%0A%5Cend%7Bbmatrix%7D%20+%0A%5Cbegin%7Bbmatrix%7D%0A%20%20-1%20&amp;%200%20%5C%5C%0A%20%202%20&amp;%20-3%20%5C%5C%0A%20%20-5%20&amp;%204%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A%20%200%20&amp;%202%20%5C%5C%0A%20%205%20&amp;%201%20%5C%5C%0A%20%200%20&amp;%2010%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="scalar-multiplication" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="scalar-multiplication"><span class="header-section-number">2.2</span> Scalar multiplication</h3>
<p>The product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.</p>
<p>Given a scalar <img src="https://latex.codecogs.com/png.latex?k"> and an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, their product <img src="https://latex.codecogs.com/png.latex?k%5Cmathbf%7BA%7D"> is defined by: <img src="https://latex.codecogs.com/png.latex?%0A(k%5Cmathbf%7BA%7D)_%7Bi,j%7D%20=%20k(a_%7Bi,j%7D)%0A"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%20m"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20j%20%5Cleq%20n">.</p>
<p>Example: <img src="https://latex.codecogs.com/png.latex?%0A2%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%20%5C%5C%0A5%20&amp;%206%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A2%20&amp;%204%20%5C%5C%0A6%20&amp;%208%20%5C%5C%0A10%20&amp;%2012%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="matrix-multiplication" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="matrix-multiplication"><span class="header-section-number">2.3</span> Matrix multiplication</h3>
<p>The product of two matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D"> is a matrix obtained by multiplying the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> by the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D">.</p>
<p>Given two matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D"> with dimensions <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> and <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20p">, respectively, their product <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D%20=%20%5Cmathbf%7BAB%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20p"> matrix defined by: <img src="https://latex.codecogs.com/png.latex?%0Ac_%7Bi,j%7D%20=%20%5Csum_%7Bk=1%7D%5En%20a_%7Bi,k%7Db_%7Bk,j%7D%0A"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%20m"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20j%20%5Cleq%20p">.</p>
<p>Example: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%20%5C%5C%0A5%20&amp;%206%0A%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%0A-1%20&amp;%200%20&amp;%202%20%5C%5C%0A2%20&amp;%20-3%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A3%20&amp;%20-6%20&amp;%204%20%5C%5C%0A5%20&amp;%20-12%20&amp;%2010%20%5C%5C%0A7%20&amp;%20-18%20&amp;%2016%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="transpose" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="transpose"><span class="header-section-number">2.4</span> Transpose</h3>
<p>The transpose of an <img src="https://latex.codecogs.com/png.latex?m%20x%20n"> matrix A, denoted by <img src="https://latex.codecogs.com/png.latex?A%5ET">, is the <img src="https://latex.codecogs.com/png.latex?n%20x%20m"> matrix obtained by interchanging the rows and columns of A. Formally, if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Ba_%7Bij%7D%5D"> is an m x n matrix, then its transpose <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET%20=%20%5Bb_%7Bij%7D%5D"> is an <img src="https://latex.codecogs.com/png.latex?n%20x%20m"> matrix where <img src="https://latex.codecogs.com/png.latex?b_%7Bij%7D"> = <img src="https://latex.codecogs.com/png.latex?a_%7Bji%7D"> for all <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j">. In other words, the element in the <img src="https://latex.codecogs.com/png.latex?i"> th row and <img src="https://latex.codecogs.com/png.latex?j"> th column of <img src="https://latex.codecogs.com/png.latex?A%5ET"> is equal to the element in the <img src="https://latex.codecogs.com/png.latex?j"> th row and ith column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>Given an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, its transpose <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m"> matrix defined by: $$ <em>{i,j}^T = </em>{j,i}</p>
<p>$$</p>
<ul>
<li>When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is transposed, diagnoal entries(<img src="https://latex.codecogs.com/png.latex?a_%7Bii%7D">) do not change but off-diagnoal elements(<img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D%20%5C;%20i%20%5Cneq%20j">) change.</li>
<li>A column vector is tranposed into a row vector, and vice versa.</li>
<li>symmetric matrix: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BA%7D%5ET"></li>
</ul>
<p>Example:</p>
<p>Let A be the matrix <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%5C%5C%0A4%20&amp;%205%20&amp;%206%0A%5Cend%7Bbmatrix%7D%0A"> The transpose of A, denoted by A^T, is the matrix <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5ET%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%204%5C%5C%0A2%20&amp;%205%5C%5C%0A3%20&amp;%206%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="determinant" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="determinant"><span class="header-section-number">2.5</span> Determinant</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> square matrix. The determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted by <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7BA%7D%7C"> or <img src="https://latex.codecogs.com/png.latex?%5Cdet(%5Cmathbf%7BA%7D)">, is a scalar value calculated as the sum of the products of the elements in any row or column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> with their corresponding cofactors, that is,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%5Cmathbf%7BA%7D%7C=%5Csum_%7Bi=1%7D%5E%7Bn%7Da_%7Bij%7DC_%7Bij%7D=%5Csum_%7Bj=1%7D%5E%7Bn%7Da_%7Bij%7DC_%7Bij%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> is the element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> in the <img src="https://latex.codecogs.com/png.latex?i">-th row and <img src="https://latex.codecogs.com/png.latex?j">-th column, and <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D"> is the cofactor of <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D">. The cofactor of <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D">, denoted by <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D">, is given by <img src="https://latex.codecogs.com/png.latex?(-1)%5E%7Bi+j%7D"> times the determinant of the <img src="https://latex.codecogs.com/png.latex?(n-1)%20%5Ctimes%20(n-1)"> matrix obtained by deleting the <img src="https://latex.codecogs.com/png.latex?i">-th row and <img src="https://latex.codecogs.com/png.latex?j">-th column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>The determinant of an <img src="https://latex.codecogs.com/png.latex?n%20x%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a scalar value denoted as <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7BA%7D%7C">. It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a <img src="https://latex.codecogs.com/png.latex?3%20x%203"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20="> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20a_%7B13%7D%20%5C%5C%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B32%7D%20&amp;%20a_%7B33%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%5Cmathbf%7BA%7D%7C%20=%20a_%7B11%7D%0A%5Cbegin%7Bvmatrix%7D%0Aa_%7B22%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B32%7D%20&amp;%20a_%7B33%7D%0A%5Cend%7Bvmatrix%7D%0A-%20a_%7B12%7D%0A%5Cbegin%7Bvmatrix%7D%0Aa_%7B21%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B33%7D%0A%5Cend%7Bvmatrix%7D%0A+%20a_%7B13%7D%0A%5Cbegin%7Bvmatrix%7D%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B32%7D%0A%5Cend%7Bvmatrix%7D%0A"></p>
<p>For example, consider the <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20&amp;%203%20%5C%5C%204%20&amp;%205%20&amp;%206%20%5C%5C%207%20&amp;%208%20&amp;%209%20%5Cend%7Bbmatrix%7D">. We can calculate the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> using any row or column. Let’s use the first column:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%5Cmathbf%7BA%7D%7C%20=%201%0A%5Cbegin%7Bvmatrix%7D%0A5%20&amp;%206%20%5C%5C%0A8%20&amp;%209%0A%5Cend%7Bvmatrix%7D%0A-%204%0A%5Cbegin%7Bvmatrix%7D%0A2%20&amp;%203%5C%5C%0A8%20&amp;%209%0A%5Cend%7Bvmatrix%7D%0A+%207%0A%5Cbegin%7Bvmatrix%7D%0A2%20&amp;%205%20%5C%5C%0A3%20&amp;%206%0A%5Cend%7Bvmatrix%7D%20=%200%0A"></p>
<p>Therefore, the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is zero.</p>
</section>
<section id="inverse" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="inverse"><span class="header-section-number">2.6</span> Inverse</h3>
<p>The inverse of a square matrix <img src="https://latex.codecogs.com/png.latex?A"> of size <img src="https://latex.codecogs.com/png.latex?n"> is a matrix <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> such that the product of <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> is the identity matrix <img src="https://latex.codecogs.com/png.latex?I_n">, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?A%20%5Ctimes%20A%5E%7B-1%7D%20=%20I_n">. If such a matrix exists, then <img src="https://latex.codecogs.com/png.latex?A"> is said to be invertible or non-singular.</p>
<p>The inverse of a square matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E%7B-1%7D"> and is defined as the unique matrix that satisfies the following equation: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BI%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D"> is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.</p>
<p>For example, consider the <img src="https://latex.codecogs.com/png.latex?2%5Ctimes%202"> matrix <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%0A%5Cend%7Bbmatrix%7D.%0A"> The inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%0A%5Cfrac%7B1%7D%7B-2%7D%0A%5Cbegin%7Bbmatrix%7D%0A4%20&amp;%20-2%20%5C%5C%0A-3%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A-2%20&amp;%201%20%5C%5C%0A%5Cfrac%7B3%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>We can verify that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BI%7D"> by computing:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A-2%20&amp;%201%20%5C%5C%0A%5Cfrac%7B3%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20%5C%5C%0A0%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7BI%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%0A%5Cbegin%7Bbmatrix%7D%20-2%20&amp;%201%20%5C%5C%0A%5Cfrac%7B3%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20%5C%5C%0A0%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7BI%7D%0A"></p>
<p>Let me give another example and A be a <img src="https://latex.codecogs.com/png.latex?3x3"> square matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20a_%7B13%7D%20%5C%5C%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B32%7D%20&amp;%20a_%7B33%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"> Then, the inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E%7B-1%7D">, is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cfrac%7B1%7D%7B%5Ctext%7Bdet%7D(%5Cmathbf%7BA%7D)%7D%5Cbegin%7Bbmatrix%7D%0Aa_%7B22%7Da_%7B33%7D-a_%7B23%7Da_%7B32%7D%20&amp;%20a_%7B13%7Da_%7B32%7D-a_%7B12%7Da_%7B33%7D%20&amp;%20a_%7B12%7Da_%7B23%7D-a_%7B13%7Da_%7B22%7D%20%5C%5C%0Aa_%7B23%7Da_%7B31%7D-a_%7B21%7Da_%7B33%7D%20&amp;%20a_%7B11%7Da_%7B33%7D-a_%7B13%7Da_%7B31%7D%20&amp;%20a_%7B13%7Da_%7B21%7D-a_%7B11%7Da_%7B23%7D%20%5C%5C%0Aa_%7B21%7Da_%7B32%7D-a_%7B22%7Da_%7B31%7D%20&amp;%20a_%7B12%7Da_%7B31%7D-a_%7B11%7Da_%7B32%7D%20&amp;%20a_%7B11%7Da_%7B22%7D-a_%7B12%7Da_%7B21%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?det(%5Cmathbf%7BA%7D)"> is the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>For example, let:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%20%5C%5C%0A0%20&amp;%201%20&amp;%204%20%5C%5C%0A5%20&amp;%206%20&amp;%200%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"> Then, <img src="https://latex.codecogs.com/png.latex?det(A)"> = -57, and the inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cfrac%7B1%7D%7B-57%7D%5Cbegin%7Bbmatrix%7D%0A-24%20&amp;%2018%20&amp;%205%20%5C%5C%0A20%20&amp;%20-15%20&amp;%20-4%20%5C%5C%0A-3%20&amp;%202%20&amp;%201%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>There are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.</p>
</section>
<section id="rank" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="rank"><span class="header-section-number">2.7</span> Rank</h3>
<p>The rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Brank%7D(%5Cmathbf%7BA%7D)">.</p>
<p>For example, consider the following matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%20%20%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%201%20&amp;%202%20&amp;%203%5C%5C%0A%20%20%20%204%20&amp;%205%20&amp;%206%20%5C%5C%0A%20%20%20%207%20&amp;%208%20&amp;%209%5C%5C%0A%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>The columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is 2.</p>
</section>
<section id="trace" class="level3" data-number="2.8">
<h3 data-number="2.8" class="anchored" data-anchor-id="trace"><span class="header-section-number">2.8</span> Trace</h3>
<p>The trace of a square matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Btr%7D(%5Cmathbf%7BA%7D)">, is defined as the sum of the diagonal elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. In other words, if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> matrix, then its trace is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathrm%7Btr%7D(%5Cmathbf%7BA%7D)=%5Csum_%7Bi=1%7D%5E%7Bn%7Da_%7Bij%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?a_%7Bii%7D"> denotes the <img src="https://latex.codecogs.com/png.latex?i"> th diagonal element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%20%202%20&amp;%203%20&amp;%201%20%5C%5C%0A%20%200%20&amp;%205%20&amp;%202%20%5C%5C%0A%20%201%20&amp;%201%20&amp;%204%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>Then, the trace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Btr%7D(%5Cmathbf%7BA%7D)%20=%202%20+%205%20+%204%20=%2011"></p>
</section>
<section id="eigenvalues-and-eigenvectors" class="level3" data-number="2.9">
<h3 data-number="2.9" class="anchored" data-anchor-id="eigenvalues-and-eigenvectors"><span class="header-section-number">2.9</span> Eigenvalues and Eigenvectors</h3>
<p>Let A be an <img src="https://latex.codecogs.com/png.latex?n%20%C3%97%20n"> square matrix. A scalar <img src="https://latex.codecogs.com/png.latex?%5Clambda"> is called an eigenvalue of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> if there exists a non-zero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> such that <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BAv%7D=%5Clambda%5Cmathbf%7Bv%7D%0A"></p>
<p>Such a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> is called an eigenvector corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda">.</p>
<p>Example:</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be the matrix</p>
<p>To find the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, we solve the characteristic equation <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdet%7D(%5Cmathbf%20A%20-%20%5Clambda%20%5Cmathbf%20I%20)%20=%200">, where I is the n × n identity matrix.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%5Ctext%7Bdet%7D(%5Cmathbf%20A%20-%20%5Clambda%20%5Cmathbf%20I%20)%0A%20%20&amp;=%0A%20%20%20%20%5Cbegin%7Bvmatrix%7D%0A%20%20%20%203%20-%20%5Clambda%20&amp;%201%20%5C%5C%0A%20%20%20%201%20&amp;%203%20-%20%5Clambda%0A%20%20%20%20%5Cend%7Bvmatrix%7D%20%5C%5C%0A%20%20&amp;=%0A%20%20(3%20-%20%5Clambda)(3%20-%20%5Clambda)%20-%201%20%5C%5C%0A%20%20&amp;=%20%5Clambda%5E2%20-%206%5Clambda%20+%208%20=%200%0A%5Cend%7Balign*%7D%0A"></p>
<p>Solving this quadratic equation gives us the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">: <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202"> and <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">.</p>
<p>To find the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202">, we solve the equation <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%20A%20-%202%20%5Cmathbf%20I)%5Cmathbf%7Bv%7D%20=%20%5Cmathbf%7B0%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20I"> is the <img src="https://latex.codecogs.com/png.latex?2%20%5Ctimes%202"> identity matrix.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20(%5Cmathbf%20A%20-%202%20%5Cmathbf%20I)%5Cmathbf%7Bv%7D%20=%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%201%20&amp;%201%20%5C%5C%0A%20%20%20%20%20%201%20&amp;%201%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20x%20%5C%5C%0A%20%20%20%20y%0A%20%20%5Cend%7Bbmatrix%7D%20=%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%200%20%5C%5C%0A%20%20%20%200%0A%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p>Solving this system of equations gives us the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202">: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_1%7D%20=%20%5Cbegin%7Bbmatrix%7D%20-1%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"></p>
<p>Similarly, for <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">, we solve the equation <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%20A%20-%204%5Cmathbf%20I)%5Cmathbf%7Bv%7D"> = <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D"> to get the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_2%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"></p>
</section>
<section id="singular-value-and-singluar-vectors" class="level3" data-number="2.10">
<h3 data-number="2.10" class="anchored" data-anchor-id="singular-value-and-singluar-vectors"><span class="header-section-number">2.10</span> Singular value and Singluar Vectors</h3>
<p>The singular value decomposition (SVD) of a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is a factorization of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> into the product of three matrices as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BU%7D%20%5Cmathbf%7B%5CSigma%7D%20%5Cmathbf%7BV%7D%5ET%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BU%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20m"> orthogonal matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CSigma%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> rectangular diagonal matrix with non-negative real numbers on the diagonal, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> orthogonal matrix.</p>
<p>The diagonal entries of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CSigma%7D"> are called the singular values of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Csigma_1,%20%5Csigma_2,%20%5Cldots,%20%5Csigma_r"> (where <img src="https://latex.codecogs.com/png.latex?r"> is the rank of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">), and are arranged in descending order. The columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BU%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D"> are called the left and right singular vectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, respectively, and are orthonormal vectors.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be a 3 by 2 matrix given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%201%20&amp;%202%5C%5C%0A%20%20%20%203%20&amp;%204%5C%5C%0A%20%20%20%205%20&amp;%206%0A%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>The SVD of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BU%7D%20%5Cmathbf%7B%5CSigma%7D%20%5Cmathbf%7BV%7D%5ET%20=%0A%5Cbegin%7Bbmatrix%7D%0A%20%20-0.23%20&amp;%20-0.53%20&amp;%20-0.81%5C%5C%0A%20%20-0.53%20&amp;%20-0.72%20&amp;%200.45%5C%5C%0A%20%20-0.81%20&amp;%200.45%20&amp;%20-0.38%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%209.53%20&amp;%200%5C%5C%0A%20%200%20&amp;%200.90%5C%5C%0A%20%200%20&amp;%200%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20-0.62%20&amp;%20-0.78%5C%5C%0A%20%20-0.78%20&amp;%200.62%0A%5Cend%7Bbmatrix%7D%5ET%0A%5Cend%7Bequation*%7D%0A"></p>
<p>where the left singular vectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BU%7D">, the right singular vectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D">, and the singular values of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are the diagonal entries of <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%7B%5CSigma%7D">.</p>
<ul>
<li>연립 방정식을 행렬의 곱으로 나타내보기 <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bmatrix%7Dx_1+2y_1=4%5C%5C2x_1+5y_1=9%5Cend%7Bmatrix%7D%20%5Cquad%20%5Cquad%20%5Cquad%20%5Cbegin%7Bmatrix%7Dx_2+2y_2=3%5C%5C2x_2+5y_2=7%5Cend%7Bmatrix%7D"> <img src="https://latex.codecogs.com/png.latex?%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%202%20&amp;%205%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5C%5C%20y_1%20&amp;%20y_2%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%204%20&amp;%209%20%5C%5C%203%20&amp;%207%20%5Cend%7Bbmatrix%7D"></li>
<li>중요한 사실(….당연한 사실?)
<ul>
<li>곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bm%20%5Ctimes%20n%7D%20%5Ctimes%20B_%7Bo%20%5Ctimes%20p%7D"> 에서 <img src="https://latex.codecogs.com/png.latex?n%20=%20o"> 여야 곱셈 성립</li>
</ul></li>
<li>곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 <img src="https://latex.codecogs.com/png.latex?%5Ctimes"> 곱셈의 오른쪽 행렬의 열 수
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bm%20%5Ctimes%20n%7D%20%5Ctimes%20B_%7Bo%20%5Ctimes%20p%7D%20=%20C_%7Bm%20%5Ctimes%20p%7D"></li>
</ul></li>
<li>교환법칙(Commutative property)이 성립하지 않음
<ul>
<li><img src="https://latex.codecogs.com/png.latex?AB%20%5Cneq%20BA"></li>
</ul></li>
</ul></li>
<li>행렬 곱셈의 여러가지 관점
<ul>
<li>내적으로 바라보기 <img src="https://latex.codecogs.com/png.latex?%20A%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%5ET%7D%20%5C%5C%20%5Cmathbf%7Ba_2%5ET%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cmathbf%7Ba_m%5ET%7D%20%5Cend%7Bbmatrix%7D%20"> <img src="https://latex.codecogs.com/png.latex?%20AB%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%5ET%7D%20%5C%5C%20%5Cmathbf%7Ba_2%5ET%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cmathbf%7Ba_m%5ET%7D%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Bb_1%7D%20&amp;%20%5Cmathbf%7Bb_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Bb_m%7D%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%5ET%20b_1%7D%20&amp;%20%5Cmathbf%7Ba_1%5ET%20b_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Ba_1%5ET%20b_m%7D%20%5C%5C%20%5Cmathbf%7Ba_2%5ET%20b_1%7D%20&amp;%20%5Cmathbf%7Ba_2%5ET%20b_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Ba_2%5ET%20b_m%7D%20%5C%5C%20%5Cvdots%20%20&amp;%20%5Cvdots%20%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%20%5Cmathbf%7Ba_m%5ET%20b_1%7D%20&amp;%20%5Cmathbf%7Ba_m%5ET%20b_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Ba_m%5ET%20b_m%7D%20%5Cend%7Bbmatrix%7D"></li>
<li>rank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) <img src="https://latex.codecogs.com/png.latex?AB%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%7D%20&amp;%20%5Cmathbf%7Ba_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Ba_m%7D%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Bb_1%5ET%7D%20%5C%5C%20%5Cmathbf%7Bb_2%5ET%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cmathbf%7Bb_m%5ET%7D%20%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7Ba_1%20b_1%5ET%7D%20+%20%5Cmathbf%7Ba_2%20b_2%5ET%7D%20+%20%5Ccdots%20+%20%5Cmathbf%7Ba_m%20b_m%5ET%7D"></li>
<li>column space로 바라보기 <img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%7D%20&amp;%20%5Cmathbf%7Ba_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Ba_m%7D%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5C%5C%20%5Cvdots%20%5C%5C%20x_m%20%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7Ba_1%7D%20x_1%20+%20%5Cmathbf%7Ba_2%7D%20x_2%20+%20%5Ccdots%20+%20%5Cmathbf%7Ba_m%7D%20x_m%20"> (스칼라배의 합)
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%200%20%5C%5C%200%20&amp;%201%20%5Cend%7Bbmatrix%7D"> 는 2차원 좌표평면의 모든 점을, <img src="https://latex.codecogs.com/png.latex?A=%5Cbegin%7Bbmatrix%7D%201%20&amp;%200%20&amp;%200%20%5C%5C%200%20&amp;%201%20&amp;%200%20%5C%5C%200%20&amp;%200%20&amp;%201%20%5Cend%7Bbmatrix%7D">은 3차원 좌표평면의 모든 점 표현 가능</li>
<li><img src="https://latex.codecogs.com/png.latex?AB%20=%20A%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Bb_1%7D%20&amp;%20%5Cmathbf%7Bb_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Bb_m%7D%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20A%20%5Cmathbf%7Bb_1%7D%20&amp;%20A%20%5Cmathbf%7Bb_2%7D%20&amp;%20%5Ccdots%20&amp;%20A%20%5Cmathbf%7Bb_m%7D%20%5Cend%7Bbmatrix%7D"></li>
<li>column space: A의 column vector로 만들 수 있는 부분 공간</li>
</ul></li>
<li>row space로 바라보기 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%5ET%7DA%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20&amp;%20%5Ccdots%20&amp;%20x_m%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%5ET%7D%20%5C%5C%20%5Cmathbf%7Ba_2%5ET%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cmathbf%7Ba_m%5ET%7D%20%5Cend%7Bbmatrix%7D%20=%20x_1%20%5Cmathbf%7Ba_1%5ET%7D%20+%20x_2%20%5Cmathbf%7Ba_2%5ET%7D%20+%20%5Ccdots%20+%20x_m%20%5Cmathbf%7Ba_m%5ET%7D%20"></li>
</ul></li>
</ul>
</section>
</section>
<section id="열공간column-space" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="열공간column-space"><span class="header-section-number">3</span> 열공간(Column Space)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/g0eaDeVRdZk" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li>column space: column vector 들이 span 하는 space
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A">의 column space = <img src="https://latex.codecogs.com/png.latex?C(A)"> 또는 <img src="https://latex.codecogs.com/png.latex?range(A)"></li>
</ul></li>
<li>span: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합
<ul>
<li>vector에 따라, 점일수도 선일수도 평면일 수도 있음</li>
<li>vector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space</li>
</ul></li>
<li>vector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_1%7D"> 과 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_2%7D">의 linear combination으로 2차원 좌표평면 나타내기 <img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/images/chap02_09.PNG" class="img-fluid"></li>
</ul></li>
</ul>
</section>
<section id="선형-독립linear-independent" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="선형-독립linear-independent"><span class="header-section-number">4</span> 선형 독립(Linear Independent)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/mOOI4-BfjGQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>…and also see</p>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/9F4PZ_1orF0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li>선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌</li>
<li>orthogonal 하면 independent
<ul>
<li>but independent해도 항상 orthogonal하지는 않음 (Independent &gt; Orthogonal)</li>
</ul></li>
<li>definition: <img src="https://latex.codecogs.com/png.latex?a_1%20%5Cmathbf%7Bv_1%7D%20+%20a_2%20%5Cmathbf%7Bv_2%7D%20+%20a_3%20%5Cmathbf%7Bv_3%7D%20%5Ccdots%20a_n%20%5Cmathbf%7Bv_n%7D%20=%20%5Cmathbf%7B0%7D"> 를 만족하는 <img src="https://latex.codecogs.com/png.latex?a_1,%20a_2,%20a_3,%20%5Ccdots%20a_n"> 이 <img src="https://latex.codecogs.com/png.latex?a_1%20=%20a_2%20=%20a_3%20=%20%5Ccdots%20=%20a_n%20=%200"> 밖에 없을때
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D">는 모든 elements가 <img src="https://latex.codecogs.com/png.latex?0">인 벡터</li>
<li>예: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%202%20%5C%5C%202%20%5Cend%7Bbmatrix%7D"> 는 <img src="https://latex.codecogs.com/png.latex?-2%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%20+%201%20%5Cbegin%7Bbmatrix%7D%202%20%5C%5C%202%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%200%20%5Cend%7Bbmatrix%7D"> 이 되므로, linearly independent 하지 않음</li>
<li>independent한 vector 들의 수 = 표현할 수 있는 차원의 dimension</li>
</ul></li>
</ul>
</section>
<section id="기저basis" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="기저basis"><span class="header-section-number">5</span> 기저(basis)</h2>
<ul>
<li>주어진 vector space를 span하는 linearly independent한 vectors</li>
<li>어떤 공간을 이루는 필수적인 구성요소</li>
<li>orthogonal 하면 orthogonal basis</li>
<li>예: 2차원 좌표평면에 대해
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5Cend%7Bbmatrix%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"> : orthogonal basis</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5Cend%7Bbmatrix%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"> : orthogonal 하지 않은 basis</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5Cend%7Bbmatrix%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%202%20%5C%5C%200%20%5Cend%7Bbmatrix%7D"> : linearly independent 하지 않으므로 basis 아님</li>
</ul></li>
</ul>
</section>
<section id="항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix"><span class="header-section-number">6</span> 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/XqOvyfMUAwA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="identity-matrix항등행렬" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="identity-matrix항등행렬"><span class="header-section-number">6.1</span> Identity matrix(항등행렬)</h3>
<ul>
<li>항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)
<ul>
<li>실수에서 곱셈의 항등원은 1</li>
</ul></li>
<li>행렬의 항등원: 항등행렬(<img src="https://latex.codecogs.com/png.latex?I">) <img src="https://latex.codecogs.com/png.latex?I%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%200%20&amp;%201%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%20%5Cvdots%20%20&amp;%20%5Cvdots%20%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%200%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%201%20%5Cend%7Bbmatrix%7D">
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bm%20%5Ctimes%20n%7D%20%5Ctimes%20I_%7Bn%20%5Ctimes%20n%20=%20n%7D%20=%20A_%7Bm%20%5Ctimes%20n%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?I_%7Bm%20%5Ctimes%20m%20=%20m%7D%20%5Ctimes%20A_%7Bm%20%5Ctimes%20n%7D%20=%20A_%7Bm%20%5Ctimes%20n%7D"></li>
</ul></li>
</ul>
</section>
<section id="sec-inv" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="sec-inv"><span class="header-section-number">6.2</span> Inverse matrix(역행렬)</h3>
<ul>
<li>역원: 연산 결과 항등원이 나오게 하는 연소
<ul>
<li>실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): <img src="https://latex.codecogs.com/png.latex?a%20%5Ctimes%20a%5E%7B-1%7D%20=%201"></li>
</ul></li>
<li>행렬의 역원: 역행렬(<img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D">) <img src="https://latex.codecogs.com/png.latex?A%20%5Ctimes%20A%5E%7B-1%7D%20=%20I%20,%20A%5E%7B-1%7D%20%5Ctimes%20A%20=%20I">
<ul>
<li>존재하지 않는 경우도 있음</li>
<li>존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림
<ul>
<li>존재하지 않으면 singular, degenerate라고 불림</li>
</ul></li>
<li>square matrix(정사각행렬, <img src="https://latex.codecogs.com/png.latex?m%20=%20n">)은 특수한 경우를 제외하면 역행렬이 항상 존재
<ul>
<li>역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우</li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?m%20%5Cneq%20n">인 행렬의 경우에는 역행렬이 존재하지 않음
<ul>
<li>다만, 경우에 따라 <img src="https://latex.codecogs.com/png.latex?A%20%5Ctimes%20A%5E%7B-1%7D%20=%20I"> 를 만족하거나(right inverse), <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D%20%5Ctimes%20A%20=%20I">를 만족하는(left inverse)는 <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D">이 존재함</li>
</ul></li>
<li>연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 <img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D%20=%20%5Cmathbf%7Bb%7D%20%5CRightarrow%20A%5E%7B-1%7DA%5Cmathbf%7Bx%7D%20=%20A%5E%7B-1%7D%5Cmathbf%7Bb%7D%20%5CRightarrow%20I%5Cmathbf%7Bx%7D%20=%20A%5E%7B-1%7D%5Cmathbf%7Bb%7D%20%5CRightarrow%20%5Cmathbf%7Bx%7D%20=%20A%5E%7B-1%7D%5Cmathbf%7Bb%7D"></li>
</ul></li>
</ul>
</section>
<section id="diagonal-matrix대각행렬" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="diagonal-matrix대각행렬"><span class="header-section-number">6.3</span> Diagonal Matrix(대각행렬)</h3>
<ul>
<li>diagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix <img src="https://latex.codecogs.com/png.latex?%20D%20=%20Diag(%5Cmathbf%7Ba%7D)%20=%20%5Cbegin%7Bbmatrix%7D%20a_%7B1,1%7D%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%200%20&amp;%20a_%7B2,2%7D%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%20%5Cvdots%20%20&amp;%20%5Cvdots%20%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%200%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%20a_%7Bn,n%7D%20%5Cend%7Bbmatrix%7D">
<ul>
<li>identity matrix는 diagonal matrix</li>
<li>diagnomal matrix는 symmetric matrix 이기도 함</li>
<li>보통은 square matrix에서 주로 사용됨
<ul>
<li>square matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="orthogonal-matrix직교행렬-orthonomal-matrix" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="orthogonal-matrix직교행렬-orthonomal-matrix"><span class="header-section-number">6.4</span> Orthogonal matrix(직교행렬, orthonomal matrix)</h3>
<ul>
<li>행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) <img src="https://latex.codecogs.com/png.latex?A%20A%5ET%20=%20A%5ET%20A%20=%20I"></li>
<li>identity matrix는 orthogonal matrix</li>
<li>square matrix에서만 정의됨</li>
<li>Orthogonal matrix인 <img src="https://latex.codecogs.com/png.latex?A">이면 <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D%20=%20A%5E%7BT%7D">
<ul>
<li>각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임</li>
</ul></li>
<li>complex matrix(복소수 행렬)에서는 unitary matrix라고 부름</li>
</ul>
</section>
</section>
<section id="계수rank" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="계수rank"><span class="header-section-number">7</span> 계수(Rank)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/HMST0Yc7EXE" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li>rank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension</li>
<li>independent한 column의 수 = independent한 행의 수: <img src="https://latex.codecogs.com/png.latex?rank(A)%20=%20rank(A%5ET)">
<ul>
<li>proof: <a href="https://en.wikipedia.org/wiki/Rank_%28linear_algebra%29#Proofs_that_column_rank_=_row_rank">Wikipedia</a></li>
</ul></li>
<li>예: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20&amp;%203%20%5C%5C%200%20&amp;%200%20&amp;%200%20%5Cend%7Bbmatrix%7D%20%5CRightarrow%20rank=1"> <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20&amp;%200%20&amp;%201%20%5C%5C%200%20&amp;%201%20&amp;%201%20%5Cend%7Bbmatrix%7D%20%5CRightarrow%20rank=2"></li>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bm%20%5Ctimes%20n%7D"> 의 최대 랭크는 <img src="https://latex.codecogs.com/png.latex?min%5C%7Bm,n%5C%7D">
<ul>
<li><img src="https://latex.codecogs.com/png.latex?rank(A)%20%3C%20min%5C%7Bm,n%5C%7D"> 면 rank-deficient, <img src="https://latex.codecogs.com/png.latex?rank(A)%20=%20min%5C%7Bm,n%5C%7D">면 full (row/column) rank</li>
</ul></li>
</ul>
</section>
<section id="영공간null-space" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="영공간null-space"><span class="header-section-number">8</span> 영공간(Null space)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/Eizc9TSRYMQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D=%20%5Cmathbf%7B0%7D"> 을 만족하는 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">의 집합
<ul>
<li>column space 관점에서 보기: <img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D%20=%20x_1%20%5Cmathbf%7Ba_1%7D%20+%20x_2%20%5Cmathbf%7Ba_2%7D%20+%20%5Ccdots%20+%20x_n%20%5Cmathbf%7Ba_n%7D%20=%20%5Cmathbf%7B0%7D"></li>
<li>null space에 항상 들어가는 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%200%20%5C%5C%20%5Cvdots%20%5C%5C%200%20%5Cend%7Bbmatrix%7D"> : trivial solution
<ul>
<li>모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D=%5Cmathbf%7B0%7D">하나 밖에 없음</li>
</ul></li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D=%5Cmathbf%7B0%7D"> 가 아닌 vector가 null space에 있으면, 스칼라배(constant <img src="https://latex.codecogs.com/png.latex?c">에 대해 <img src="https://latex.codecogs.com/png.latex?c%20%5Cmathbf%7Bx%7D">) 역시 null space에 포함됨</li>
<li>혼동 주의! null space는 column space의 일부가 아님
<ul>
<li>row vector의 차원이 null space가 존재하는 공간</li>
</ul></li>
<li>rank와 null space의 dimension의 합은 항상 matrix의 column의 수
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bm%20%5Ctimes%20n%7D">에 대해, <img src="https://latex.codecogs.com/png.latex?dim(N(A))%20=%20n%20-%20r">
<ul>
<li>모든 columns이 다 lienarly independent 하면 null space는 0차원(점)</li>
</ul></li>
<li>null space는 row space와 수직한 space
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D=%20%5Cmathbf%7B0%7D"> : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0</li>
<li>rank는 row space의 dimension → row space의 dimension(<img src="https://latex.codecogs.com/png.latex?dim(R(A))">)과 null space의 dimension(<img src="https://latex.codecogs.com/png.latex?dim(N(A))">)의 합이 <img src="https://latex.codecogs.com/png.latex?n"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%5En%7D"> 공간에 표현: <img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/images/chap02_10.PNG" class="img-fluid">
<ul>
<li>겹친 점: 영벡터</li>
</ul></li>
</ul></li>
</ul></li>
<li>left null space: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%5ET%7D%20A%20=%20%5Cmathbf%7B0%5ET%7D"> 인 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">
<ul>
<li>위의 성질을 row에 대해 적용
<ul>
<li>m 차원에 놓인 벡터</li>
<li>dimension: <img src="https://latex.codecogs.com/png.latex?dim(N_L(A))%20=%20m%20-%20r"></li>
<li>column space와 수직: <img src="https://latex.codecogs.com/png.latex?dim(N_L(A))%20+dim(C(A))%20=%20m"></li>
</ul></li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?R(A)">에 있는 vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_r%7D"> 와 <img src="https://latex.codecogs.com/png.latex?N(A)">에 있는 vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_n%7D">에 대해:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_r%7D">에 <img src="https://latex.codecogs.com/png.latex?A">를 곱하면 column space로 감</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_n%7D">에 <img src="https://latex.codecogs.com/png.latex?A">를 곱하면 $</li>
<li><img src="https://latex.codecogs.com/png.latex?A(%5Cmathbf%7Bx_r%7D%20+%5Cmathbf%7Bx_n%7D)%20=%20A%5Cmathbf%7Bx_r%7D%20+%20A%5Cmathbf%7Bx_n%7D%20=%20A%5Cmathbf%7Bx_r%7D%20=%20%5Cmathbf%7Bb%7D"></li>
</ul></li>
</ul>
</section>
<section id="ax-b의-해의-수" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="ax-b의-해의-수"><span class="header-section-number">9</span> Ax = b의 해의 수</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/nNI2TlD598c" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li><p>full column rank 일때</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">가 column space(<img src="https://latex.codecogs.com/png.latex?C(A)">)안에 있으면 해가 하나</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">가 column space(<img src="https://latex.codecogs.com/png.latex?C(A)">)안에 없으면 해가 없음 <img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/images/chap02_11.PNG" class="img-fluid"></li>
</ul></li>
<li><p>full row rank 일때</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">는 항상 column space 안에 있음: 무한의 해를 가짐</li>
<li>임의의 특정한 해(particular solution) <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_p%7D">와 null space의 vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_n%7D">에 대해, <img src="https://latex.codecogs.com/png.latex?A(%5Cmathbf%7Bx_p%7D%20+%5Cmathbf%7Bx_n%7D)=%5Cmathbf%7Bb%7D">
<ul>
<li>즉, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_p%7D%20+%5Cmathbf%7Bx_n%7D"> 도 해가 됨: complete solution
<ul>
<li>null space는 무한하므로, 해도 무한함</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>full rank 일때(square matrix): 해가 하나 존재 (<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20A%5E%7B-1%7D">$)</p></li>
<li><p>rank-deficient 일때</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">가 column space(<img src="https://latex.codecogs.com/png.latex?C(A)">)안에 있으면 무한한 해를 가짐</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">가 column space(<img src="https://latex.codecogs.com/png.latex?C(A)">)안에 없으면 해가 없음</li>
</ul></li>
</ul>


</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.matrix_transformation.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
  <media:content url="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/images/chap02_09.PNG" medium="image"/>
</item>
<item>
  <title>Matrix Calculus (1) - Matrix to Vector Derivatives</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/derivative_matrix_vector.html</link>
  <description><![CDATA[ 



<section id="matrix-to-vector-derivatives" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="matrix-to-vector-derivatives"><span class="header-section-number">1</span> Matrix to Vector Derivatives</h2>
<p>Matrix-to-vector derivatives refer to the derivatives of a matrix function with respect to a vector argument. Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)"> be a matrix-valued function of a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En">. The matrix-to-vector derivative is denoted as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20x_1%7D%20&amp;%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20x_2%7D%20&amp;%20%5Ccdots%20&amp;%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20x_n%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>Here, the matrix-to-vector derivative is a matrix whose <img src="https://latex.codecogs.com/png.latex?i">th column is the partial derivative of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bf%7D"> with respect to the <img src="https://latex.codecogs.com/png.latex?i">th component of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> be matrices in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%20n%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%201%7D">, respectively. Consider the function <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%20=%20%5Cmathbf%7BAx%7D">, which is a matrix-vector product. The matrix-to-vector derivative of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is given by: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D%20=%20%5Cmathbf%7BA%7D%0A"> Here, the derivative is a matrix whose rows are the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<section id="differentiation-of-quadratic-form" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="differentiation-of-quadratic-form"><span class="header-section-number">1.1</span> Differentiation of Quadratic Form</h3>
<p>This is because the output of this differentiation is a vector (with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">), rather than a scalar.</p>
<p>The differentiation of a quadratic form is the process of finding the gradient of a quadratic form with respect to its input vector.</p>
<p>Given a quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is an <img src="https://latex.codecogs.com/png.latex?n">-dimensional column vector, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En"> is a vector, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> symmetric matrix, the derivative of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%7B%5Cmathbf%20x%7D%20f(%5Cmathbf%20x)%20=%20(A%20+%20A%5ET)%5Cmathbf%20x%20+%20b%0A"></p>
<p>In this expression, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET"> is the transpose of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A+%5Cmathbf%20A%5ET"> is written instead of <img src="https://latex.codecogs.com/png.latex?2%5Cmathbf%20A"> when calculating the gradient of a quadratic form. It is because in general, the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> might not be symmetric, so <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5Cneq%20%5Cmathbf%20A%5ET">. However, for any matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, we have <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A+%5Cmathbf%20A%5ET%20=%20(%5Cmathbf%20A+%5Cmathbf%20A%5ET)%5ET">, which is a symmetric matrix. Therefore, by writing the gradient as <img src="https://latex.codecogs.com/png.latex?%5Cnabla_x%20f(x)%20=%20(%5Cmathbf%20A+%5Cmathbf%20A%5ET)x">, we ensure that the gradient is always a symmetric matrix, even if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is not symmetric. This is useful in many applications where symmetric matrices are preferred. But if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is constrained to be a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?2%5Cmathbf%20A"> can be written.</p>
</section>
<section id="when-mathbfa-is-symmetric" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="when-mathbfa-is-symmetric"><span class="header-section-number">1.2</span> When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is Symmetric</h3>
<p>As an example, consider the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=x_1%5E2+2x_1x_2+3x_2%5E2">, which can be written in the form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D">, where:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20x=%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%20x_2%20%5Cend%7Bbmatrix%7D,%20%5Cmathbf%20A=%5Cbegin%7Bbmatrix%7D%201%20&amp;%201%20%5C%5C%201%20&amp;%203%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>The derivative of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is then:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D=(%5Cmathbf%7BA%7D+%5Cmathbf%7BA%7D%5ET)%5Cmathbf%7Bx%7D=%5Cbegin%7Bbmatrix%7D2%20&amp;%202%5C%5C2%20&amp;%206%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D=%5Cbegin%7Bbmatrix%7D2x_1+2x_2%5C%5C2x_1+6x_2%5Cend%7Bbmatrix%7D%0A"></p>
<p>This represents the gradient vector of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> at any point <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
</section>
<section id="when-mathbfa-is-not-symmetric" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="when-mathbfa-is-not-symmetric"><span class="header-section-number">1.3</span> When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is Not Symmetric</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%203%20&amp;%204%20%5Cend%7Bbmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D">.</p>
<p>Then, we have <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%203%20&amp;%204%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20+%202x_2%20%5C%5C%203x_1%20+%204x_2%20%5Cend%7Bbmatrix%7D%20=%20x_1%5E2%20+%205x_1x_2%20+%204x_2%5E2">.</p>
<p>To find the gradient of this quadratic form, we can take the partial derivatives of <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2"> with respect to each variable:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20x_1%7D%20(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D)%20=%202x_1%20+%205x_2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20x_2%7D%20(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D)%20=%205x_1%20+%208x_2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D=(%5Cmathbf%7BA%7D+%5Cmathbf%7BA%7D%5ET)%5Cmathbf%7Bx%7D=%5Cbegin%7Bbmatrix%7D1+1%20&amp;%202+3%5C%5C3+2%20&amp;%204+4%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D=%5Cbegin%7Bbmatrix%7D2%20&amp;%205%5C%5C5%20&amp;%208%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%202x_1%20+%205x_2%20%5C%5C%205x_1%20+%208x_2%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>So the gradient of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cnabla_x%20f(x)%20=%20%5Cbegin%7Bbmatrix%7D%202x_1%20+%205x_2%20%5C%5C%205x_1%20+%208x_2%20%5Cend%7Bbmatrix%7D">.</p>
</section>
<section id="ordinary-least-square" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="ordinary-least-square"><span class="header-section-number">1.4</span> Ordinary Least Square</h3>
<p>For the <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%201"> vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D">, the <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20k"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">, the <img src="https://latex.codecogs.com/png.latex?k%20%5Ctimes%201"> vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cbeta%7D">, when <img src="https://latex.codecogs.com/png.latex?L=(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)">, what is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Cmathbf%7B%5Cbeta%7D%7D">?</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AL%20&amp;=%20(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%20%5C%5C%0A&amp;=%20%5Cmathbf%7By%7D%5ET%5Cmathbf%7By%7D%20-%20%5Cmathbf%7By%7D%5ET%5Cmathbf%7BA%7D%5Cmathbf%7B%5Cbeta%7D%20-%20%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%20%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D%20%5C%5C%0A&amp;=%20%5Cmathbf%7By%7D%5ET%5Cmathbf%7By%7D%20-%202%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%20%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>Now, we can take the derivative of <img src="https://latex.codecogs.com/png.latex?L"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cbeta%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Cmathbf%7B%5Cbeta%7D%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Cmathbf%7B%5Cbeta%7D%7D%20(%5Cmathbf%7By%7D%5ET%5Cmathbf%7By%7D%20-%202%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%20%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%20%5C%5C%0A&amp;=%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Cmathbf%7B%5Cbeta%7D%7D%20(-%202%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%20%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%20%5C%5C%0A&amp;=%20-%202%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%202%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D"> is the solution to the optimization problem by taking its derivative with respect to and setting it equal to zero.</p>
<p>Starting with the expression for <img src="https://latex.codecogs.com/png.latex?L">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL=(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%0A"></p>
<p>Expanding the quadratic term gives:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL=%5Cmathbf%7By%7D%5ET%5Cmathbf%7By%7D-%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D-%5Cmathbf%7By%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D+%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D%0A"></p>
<p>Taking the derivative of <img src="https://latex.codecogs.com/png.latex?L"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cbeta%7D"> and setting it to zero gives:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Cmathbf%7B%5Cbeta%7D%7D%20=%20-2%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%202%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D%20=%200%0A"></p>
<p>Solving for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cbeta%7D"> gives:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D=%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7B%5Cbeta%7D=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%0A"> Multiplying both sides of the equation by <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)"> gives:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5Cmathbf%7B%5Cbeta%7D=%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%0A"></p>
<p>Therefore, we have verified that <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Cbeta%7D=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%0A"> is the solution to the optimization problem of OLS (Ordinary Least Square).</p>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/derivative_matrix_vector.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Matrix Calculus (1) - Matrix to Vector Derivatives</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/derivative_vector_matrix.html</link>
  <description><![CDATA[ 



<section id="matrix-to-vector-derivatives" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="matrix-to-vector-derivatives"><span class="header-section-number">1</span> Matrix to Vector Derivatives</h2>
<p>The matrix-to-vector derivative is a derivative where a function <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7BX%7D)"> maps an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> to a <img src="https://latex.codecogs.com/png.latex?p">-dimensional vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D">, and we want to find the derivative of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">. It is denoted by <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20%5Cmathbf%7BX%7D%7D">.</p>
<p>Formally, let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D%20=%20f(%5Cmathbf%7BX%7D)%20%5Cin%20%5Cmathbb%7BR%7D%5Ep"> be a function that maps an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix to a <img src="https://latex.codecogs.com/png.latex?p">-dimensional vector. Then, the matrix-to-vector derivative is defined as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B11%7D%7D%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B12%7D%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B1n%7D%7D%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B21%7D%7D%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B22%7D%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B2n%7D%7D%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7Bm1%7D%7D%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7Bm2%7D%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7Bmn%7D%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>where each element of the matrix is the derivative of the corresponding element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D"> with respect to the corresponding element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7BX%7D)%20=%20%5Cmathbf%7BA%7D%5Cmathbf%7BX%7D+%5Cmathbf%7Bb%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bp%20%5Ctimes%20m%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D%20%5Cin%20%5Cmathbb%7BR%7D%5Ep">. Then, the matrix-to-vector derivative of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D%20=%20f(%5Cmathbf%7BX%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> is: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20%5Cmathbf%7BX%7D%7D%20=%20%5Cmathbf%7BA%7D%0A"></p>
<section id="differentiation-of-quadratic-form" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="differentiation-of-quadratic-form"><span class="header-section-number">1.1</span> Differentiation of Quadratic Form</h3>
<p>This is because the output of this differentiation is a vector (with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">), rather than a scalar.</p>
<p>The differentiation of a quadratic form is the process of finding the gradient of a quadratic form with respect to its input vector.</p>
<p>Given a quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is an <img src="https://latex.codecogs.com/png.latex?n">-dimensional column vector, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En"> is a vector, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> symmetric matrix, the derivative of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%7B%5Cmathbf%20x%7D%20f(%5Cmathbf%20x)%20=%20(A%20+%20A%5ET)%5Cmathbf%20x%20+%20b%0A"></p>
<p>In this expression, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET"> is the transpose of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A+%5Cmathbf%20A%5ET"> is written instead of <img src="https://latex.codecogs.com/png.latex?2%5Cmathbf%20A"> when calculating the gradient of a quadratic form. It is because in general, the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> might not be symmetric, so <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5Cneq%20%5Cmathbf%20A%5ET">. However, for any matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, we have <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A+%5Cmathbf%20A%5ET%20=%20(%5Cmathbf%20A+%5Cmathbf%20A%5ET)%5ET">, which is a symmetric matrix. Therefore, by writing the gradient as <img src="https://latex.codecogs.com/png.latex?%5Cnabla_x%20f(x)%20=%20(%5Cmathbf%20A+%5Cmathbf%20A%5ET)x">, we ensure that the gradient is always a symmetric matrix, even if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is not symmetric. This is useful in many applications where symmetric matrices are preferred. But if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is constrained to be a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?2%5Cmathbf%20A"> can be written.</p>
</section>
<section id="when-mathbfa-is-symmetric" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="when-mathbfa-is-symmetric"><span class="header-section-number">1.2</span> When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is Symmetric</h3>
<p>As an example, consider the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=x_1%5E2+2x_1x_2+3x_2%5E2">, which can be written in the form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D">, where:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20x=%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%20x_2%20%5Cend%7Bbmatrix%7D,%20%5Cmathbf%20A=%5Cbegin%7Bbmatrix%7D%201%20&amp;%201%20%5C%5C%201%20&amp;%203%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>The derivative of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is then:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D=(%5Cmathbf%7BA%7D+%5Cmathbf%7BA%7D%5ET)%5Cmathbf%7Bx%7D=%5Cbegin%7Bbmatrix%7D2%20&amp;%202%5C%5C2%20&amp;%206%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D=%5Cbegin%7Bbmatrix%7D2x_1+2x_2%5C%5C2x_1+6x_2%5Cend%7Bbmatrix%7D%0A"></p>
<p>This represents the gradient vector of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> at any point <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
</section>
<section id="when-mathbfa-is-not-symmetric" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="when-mathbfa-is-not-symmetric"><span class="header-section-number">1.3</span> When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is Not Symmetric</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%203%20&amp;%204%20%5Cend%7Bbmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D">.</p>
<p>Then, we have <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%203%20&amp;%204%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20+%202x_2%20%5C%5C%203x_1%20+%204x_2%20%5Cend%7Bbmatrix%7D%20=%20x_1%5E2%20+%205x_1x_2%20+%204x_2%5E2">.</p>
<p>To find the gradient of this quadratic form, we can take the partial derivatives of <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2"> with respect to each variable:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20x_1%7D%20(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D)%20=%202x_1%20+%205x_2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20x_2%7D%20(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D)%20=%205x_1%20+%208x_2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D=(%5Cmathbf%7BA%7D+%5Cmathbf%7BA%7D%5ET)%5Cmathbf%7Bx%7D=%5Cbegin%7Bbmatrix%7D1+1%20&amp;%202+3%5C%5C3+2%20&amp;%204+4%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D=%5Cbegin%7Bbmatrix%7D2%20&amp;%205%5C%5C5%20&amp;%208%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%202x_1%20+%205x_2%20%5C%5C%205x_1%20+%208x_2%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>So the gradient of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cnabla_x%20f(x)%20=%20%5Cbegin%7Bbmatrix%7D%202x_1%20+%205x_2%20%5C%5C%205x_1%20+%208x_2%20%5Cend%7Bbmatrix%7D">.</p>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/derivative_vector_matrix.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Matrix Transformation (5) - Quadratic Form</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/quadratic_form.html</link>
  <description><![CDATA[ 



<section id="quadratic-form" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="quadratic-form"><span class="header-section-number">1</span> Quadratic Form</h2>
<p>For a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Bx_1,x_2,%5Cldots,x_n%5D%5ET">, the quadratic form is defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)%20=%20%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> symmetric matrix.</p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET"> represents the transpose of the vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"> represents the dot product of the vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> with itself after the transformation by the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Bx_1,x_2%5D%5ET"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be a <img src="https://latex.codecogs.com/png.latex?2%20%5Ctimes%202"> symmetric matrix given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A%20=%20%5Cbegin%7Bbmatrix%7D%202&amp;1%20%5C%5C%201&amp;3%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>Then, the quadratic form <img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)"> can be written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20Q(%5Cmathbf%20x)%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D%202&amp;1%20%5C%5C%201&amp;3%20%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D=2x_1%5E2+4x_1x_2+3x_2%5E2%0A"></p>
<p>Here, we can see that the quadratic form can be represented as a polynomial function of degree 2 in the variables <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2"> with the coefficients given by the entries of the symmetric matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>a quadratic form can be expressed as a bilinear form. In other words, a quadratic form can be written in terms of a bilinear form by defining a new matrix that is the sum of the matrix representing the quadratic form and its transpose.</p>
<p>More formally, suppose we have a quadratic form defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?q(%5Cmathbf%7Bx%7D)%20=%20%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a symmetric matrix. Then, we can define a bilinear form as:</p>
<p><img src="https://latex.codecogs.com/png.latex?b(%5Cmathbf%7Bx%7D,%20%5Cmathbf%7By%7D)%20=%20%5Cfrac%7B1%7D%7B2%7D(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7By%7D%20+%20%5Cmathbf%7By%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D)"></p>
<p>Note that the factor of <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D"> is introduced to avoid double-counting. It can be shown that the two forms are equivalent, in the sense that for any <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, <img src="https://latex.codecogs.com/png.latex?q(%5Cmathbf%7Bx%7D)%20=%20b(%5Cmathbf%7Bx%7D,%20%5Cmathbf%7Bx%7D)">.</p>
<p>In other words, every quadratic form can be expressed as a bilinear form, and every symmetric bilinear form can be expressed as a quadratic form.</p>
</section>
<section id="examples" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="examples"><span class="header-section-number">2</span> Examples</h2>
<section id="sum-of-squares" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="sum-of-squares"><span class="header-section-number">2.1</span> Sum of Squares</h3>
<p>The sum of squares of a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Bx_1,%20x_2,%20%5Cldots,%20x_n%5D%5ET"> can be represented as a quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%5Cmathbf%7Bx%7D">. To see this, consider the sum of squares:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bi=1%7D%5E%7Bn%7D%20x_i%5E2%20=%20x_1%5E2%20+%20x_2%5E2%20+%20%5Cdots%20+x_n%5E2%0A"></p>
<p>Now, we can write this in vector form as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20x%5ET%20%5Cmathbf%20I%20%5Cmathbf%20x%20=%20%20%5Cmathbf%20x%5ET%20%5Cmathbf%20x%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20&amp;%20%5Cdots%20&amp;%20x_n%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5C%5C%20%5Cvdots%20%5C%5C%20x_n%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>Therefore, the sum of squares can be represented as a quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%5Cmathbf%7Bx%7D">.</p>
</section>
<section id="variability-of-vector-x" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="variability-of-vector-x"><span class="header-section-number">2.2</span> Variability of Vector, x</h3>
<p>The covariance matrix of a random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> can be represented as a quadratic form in terms of the vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> and the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20x%5ET%20%5Cmathbf%20C%20%5Cmathbf%20x%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> is the covariance matrix. This expression is a quadratic form because it involves a quadratic polynomial in the elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>In this representation, the diagonal elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> correspond to the variances of the individual components of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, and the off-diagonal elements correspond to the covariances between the components. The expression <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BC%7D%20%5Cmathbf%7Bx%7D"> measures the variability of the random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> in all possible directions, weighted by the covariances between the components.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> measures the covariance between each pair of components of the random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">. It is a matrix that summarizes the <strong>pairwise covariances</strong> between the components of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>On the other hand, the quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BC%7D%20%5Cmathbf%7Bx%7D"> measures the total variability of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, taking into account the covariances between <strong>all possible pairs of components</strong>.</p>
<p>It does this by weighting the contribution of each component to the overall variability by its covariance with every other component. So, while the covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> captures the pairwise covariances between the components of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, the quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BC%7D%20%5Cmathbf%7Bx%7D"> captures the total variability of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> in all directions.</p>
</div>
</div>
<p>Let’s take a simple example with a 2-dimensional random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D=%5Bx_1,%20x_2%5D%5ET">. We can think of this random vector as representing data points in a 2D space. The covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> will capture the covariances between <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2">. Let’s say that the covariance matrix is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20C%20=%5Cbegin%7Bbmatrix%7D%20%5Csigma_%7Bx_1%7D%20&amp;%20%5Coperatorname%7BCov%7D(x_1,x_2)%20%5C%5C%20%5Coperatorname%7BCov%7D(x_2,x_1)%20&amp;%20%5Csigma_%7Bx_2%7D%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Csigma_%7Bx_1%7D%5E2"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma_%7Bx_2%7D%5E2"> are the variances of <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2">, respectively, and <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BCov%7D(x_1,x_2)"> is their covariance.</p>
<p>Now, let’s consider the quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BC%7D%20%5Cmathbf%7Bx%7D">. This expression gives us a scalar value that measures the variability of the random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> in all possible directions, weighted by the covariances between the components. We can see this geometrically by plotting the data points in the 2D space and drawing an ellipse that captures the variability of the data. The shape of the ellipse is determined by the eigenvalues and eigenvectors of the covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D">.</p>
<p>To see this, let’s first rewrite the quadratic form as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20x%5ET%20%5Cmathbf%20C%20%5Cmathbf%20x%20=%20%5Csigma_%7Bx_1%7D%5E2x_1%5E2%20+2%5Coperatorname%7BCov%7D(x_1,x_2)x_1x_2+%5Csigma_%7Bx_2%7D%5E2%0A"></p>
<p>This is a quadratic equation in <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2"> and can be thought of as the equation of an ellipse centered at the origin by the determinant of the conic equation. The shape of the ellipse is determined by the coefficients of the quadratic terms, which are the variances and covariances in the covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D">.</p>
<p>Now, let’s find the eigenvectors and eigenvalues of the covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D">. The eigenvectors are the directions along which the data has the most variance, and the corresponding eigenvalues are the variances of the data along those directions.</p>
<p>Let’s assume that the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> are ordered such that <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20%5Cgeq%20%5Clambda_2">. Then, the eigenvectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_2"> satisfy:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20C%20%5Cmathbf%20v_1%20=%5Clambda_1%5Cmathbf%20v_1%20%5Ctext%7B%20%20%7D%0A%5Cmathbf%20C%20%5Cmathbf%20v_2%20=%5Clambda_2%5Cmathbf%20v_2%0A"></p>
<p>These equations can be rewritten as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%5Csigma_%7Bx_1%7D%5E2%20&amp;%20%5Ctext%7BCov%7D(x_1,x_2)%5C%5C%0A%20%20%5Ctext%7BCov%7D(x_1,x_2)%20&amp;%20%5Csigma_%7Bx_2%7D%5E2%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20v_%7B11%7D%5C%5C%0A%20%20v_%7B21%7D%0A%5Cend%7Bbmatrix%7D%0A=%20%5Clambda_1%0A%5Cbegin%7Bbmatrix%7D%0Av_%7B11%7D%5C%5C%0Av_%7B21%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation%7D%0A"></p>
<p>This equation can be expanded as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Csigma_%7Bx_1%7D%5E2%20v_%7B11%7D%20+%20%5Ctext%7BCov%7D(x_1,x_2)%20v_%7B21%7D%20=%20%5Clambda_1%20v_%7B11%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BCov%7D(x_1,x_2)%20v_%7B11%7D%20+%20%5Csigma_%7Bx_2%7D%5E2%20v_%7B21%7D%20=%20%5Clambda_1%20v_%7B21%7D"></p>
<p>Now, let’s multiply the first equation by <img src="https://latex.codecogs.com/png.latex?v_%7B11%7D"> and the second equation by <img src="https://latex.codecogs.com/png.latex?v_%7B21%7D">, and then subtract the second equation from the first:</p>
<p><img src="https://latex.codecogs.com/png.latex?(%5Csigma_%7Bx_1%7D%5E2%20-%20%5Clambda_1)v_%7B11%7Dv_%7B21%7D%20+%20%5Ctext%7BCov%7D(x_1,x_2)(v_%7B21%7D%5E2%20-%20v_%7B11%7D%5E2)%20=%200"></p>
<p>This can be rewritten as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bv_%7B21%7D%7D%7Bv_%7B11%7D%7D%20=%20%5Cfrac%7B%5Csigma_%7Bx_1%7D%5E2%20-%20%5Clambda_1%7D%7B%5Ctext%7BCov%7D(x_1,x_2)%7D%20-%20%5Cfrac%7Bv_%7B11%7D%7D%7Bv_%7B21%7D%7D"></p>
<p>Let <img src="https://latex.codecogs.com/png.latex?t%20=%20%5Cfrac%7Bv_%7B21%7D%7D%7Bv_%7B11%7D%7D">. Then, we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?t%5E2%20-%20%5Cleft(%5Cfrac%7B%5Csigma_%7Bx_1%7D%5E2%20+%20%5Csigma_%7Bx_2%7D%5E2%7D%7B%5Ctext%7BCov%7D(x_1,x_2)%7D%5Cright)t%20+%20%5Cfrac%7B%5Clambda_1%7D%7B%5Ctext%7BCov%7D(x_1,x_2)%7D%20=%200"></p>
<p>This is a quadratic equation in <img src="https://latex.codecogs.com/png.latex?t">, and its roots can be solved using the quadratic formula. The roots are:</p>
<p><img src="https://latex.codecogs.com/png.latex?t_1%20=%20%5Cfrac%7B%5Csigma_%7Bx_1%7D%5E2%20-%20%5Csigma_%7Bx_2%7D%5E2%20+%20%5Csqrt%7B(%5Csigma_%7Bx_1%7D%5E2%20-%20%5Csigma_%7Bx_2%7D%5E2)%5E2%20+%204%5Ctext%7BCov%7D(x_1,x_2)%5E2%7D%7D%7B2%5Ctext%7BCov%7D(x_1,x_2)%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?t_2%20=%20%5Cfrac%7B%5Csigma_%7Bx_1%7D%5E2%20-%20%5Csigma_%7Bx_2%7D%5E2%20-%20%5Csqrt%7B(%5Csigma_%7Bx_1%7D%5E2%20-%20%5Csigma_%7Bx_2%7D%5E2)%5E2%20+%204%5Ctext%7BCov%7D(x_1,x_2)%5E2%7D%7D%7B2%5Ctext%7BCov%7D(x_1,x_2)%7D"></p>
<p>Finally, the eigenvectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_2"> are given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bv%7D_1%20=%20%5Cbegin%7Bbmatrix%7D1%20%5C%5C%20t_1%20%5Cend%7Bbmatrix%7D%20%5Ctext%7B%20%20%7D%0A%5Cmathbf%7Bv%7D_2%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%20t_2%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>These eigenvectors define the principal components of the data, which are the orthogonal directions in the feature space along which the data varies the most.</p>
<p>Let’s apply this difference between <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20C=%20%5Coperatorname%7BCov(%5Cmathbf%20X)%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x%5ET%20%5Cmathbf%7BC%7D%20%5Cmathbf%20x"> or PCA to Iris dataset:</p>
<div class="cell" data-execution_count="1">
<div class="cell-output cell-output-stdout">
<pre><code>C=Cov(X) =
[[ 1.00671141 -0.11835884  0.87760447  0.82343066]
 [-0.11835884  1.00671141 -0.43131554 -0.36858315]
 [ 0.87760447 -0.43131554  1.00671141  0.96932762]
 [ 0.82343066 -0.36858315  0.96932762  1.00671141]]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/quadratic_form_files/figure-html/cell-2-output-2.png" width="587" height="449"></p>
</div>
</div>
<p>This representation is useful in many statistical and machine learning applications, where the covariance matrix provides information about the variability and dependencies between different features or variables. For example, in principal component analysis (PCA), the covariance matrix is used to identify the directions of maximum variability in a dataset, which can be used to reduce the dimensionality of the data while retaining as much information as possible.</p>
</section>
<section id="pca" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="pca"><span class="header-section-number">2.3</span> PCA</h3>
<p>The principal components of a dataset can be obtained by finding the eigenvectors of the covariance matrix. In other words, we can express the covariance matrix as a quadratic form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20C%20=%20%5Cmathbf%20x%5ET%20%5Cmathbf%20A%20%5Cmathbf%20x%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is a column vector of centered data, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a symmetric positive semi-definite matrix (the covariance matrix). Diagonalizing <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> gives us the eigenvalues and eigenvectors, which are used to transform the original data into a new coordinate system, where the first axis (the first principal component) corresponds to the direction of greatest variance, the second axis (the second principal component) corresponds to the direction of second greatest variance, and so on. This new coordinate system is called the principal component space.</p>
<p>In summary, PCA can be seen as a method for finding the principal components of a dataset by diagonalizing the covariance matrix, which can be expressed as a quadratic form.</p>
</section>
<section id="positive-definit-matrix" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="positive-definit-matrix"><span class="header-section-number">2.4</span> Positive Definit Matrix</h3>
<p>a symmetric matrix <img src="https://latex.codecogs.com/png.latex?A"> is positive definite if and only if the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> is positive for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>To see why this is true, consider the eigenvalue decomposition of <img src="https://latex.codecogs.com/png.latex?A">, which can be written as <img src="https://latex.codecogs.com/png.latex?A%20=%20Q%20%5CLambda%20Q%5ET">, where <img src="https://latex.codecogs.com/png.latex?Q"> is an orthogonal matrix and <img src="https://latex.codecogs.com/png.latex?%5CLambda"> is a diagonal matrix containing the eigenvalues of <img src="https://latex.codecogs.com/png.latex?A">. Then, for any nonzero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">,</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Diagonalization
</div>
</div>
<div class="callout-body-container callout-body">
<p>Diagonalization is a process of finding a diagonal matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20D"> and an invertible matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20P"> such that <img src="https://latex.codecogs.com/png.latex?P%5E%7B-1%7DAP%20=%20D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is a square matrix. In other words, diagonalization is a way of representing a matrix as a diagonal matrix, which is a matrix with non-zero values only on its main diagonal.</p>
</div>
</div>
<p>we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbf%20x%5ET%20%5Cmathbf%20A%20%5Cmathbf%20x&amp;=%5Cmathbf%20x%5ET%20%5Cmathbf%20Q%20%5Cmathbf%20%5CLambda%20%5Cmathbf%20Q%5ET%20%5Cmathbf%20x%5C%5C%0A&amp;=(%5Cmathbf%20x%5ET%20%5Cmathbf%20Q)%5Cmathbf%20%5CLambda%20(%20%5Cmathbf%20Q%5ET%5Cmathbf%20x)%5C%5C%0A&amp;=%5Csum_%7Bi=1%7D%5E%7Bn%7D%20%5Clambda_iy_i%5E2%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?y_i%20=%20(%5Cmathbf%7Bx%7D%5ET%20Q)_i"> is the <img src="https://latex.codecogs.com/png.latex?i">th coordinate (i.e., a scalar value that represents the position of a point or a vector relative to a chosen basis) of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20Q"> and <img src="https://latex.codecogs.com/png.latex?n"> is the dimension of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?A">. Note that since <img src="https://latex.codecogs.com/png.latex?Q"> is orthogonal, we have <img src="https://latex.codecogs.com/png.latex?Q%5ET%20Q%20=%20I">, so <img src="https://latex.codecogs.com/png.latex?y_i%20=%20%5Cmathbf%7Bq%7D_i%5ET%20%5Cmathbf%7Bx%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bq%7D_i"> is the <img src="https://latex.codecogs.com/png.latex?i">th column of <img src="https://latex.codecogs.com/png.latex?Q">. Therefore, the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)%20=%20%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> can be written in terms of the eigenvalues of <img src="https://latex.codecogs.com/png.latex?A"> and the coordinates of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> with respect to the eigenvectors of <img src="https://latex.codecogs.com/png.latex?A">.</p>
<p>Since <img src="https://latex.codecogs.com/png.latex?A"> is positive definite, we have <img src="https://latex.codecogs.com/png.latex?%5Clambda_i%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?i">, and so <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5En%20%5Clambda_i%20y_i%5E2%20%3E%200"> for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">. Therefore, the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> is positive for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, which implies that <img src="https://latex.codecogs.com/png.latex?A"> is positive definite.</p>
<p>In other words, the positive definiteness of a symmetric matrix <img src="https://latex.codecogs.com/png.latex?A"> is equivalent to the positivity of the associated quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>Therefore, a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is said to be positive definite if all of its eigenvalues are positive or equivalently, a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is positive definite if left-multiplying and right-multiplying it by the same vector, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x"> always gives a positive number if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x%5ET%20%5Cmathbf%20A%20%5Cmathbf%20x"></p>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/quadratic_form.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Basics (2) - Matrix Operations</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/02.basic_matrix.html</link>
  <description><![CDATA[ 



<section id="matrix" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="matrix"><span class="header-section-number">1</span> Matrix</h2>
<p>A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix, it can be represented as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%20%20a_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7B1n%7D%20%5C%5C%0A%20%20a_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7B2n%7D%20%5C%5C%0A%20%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A%20%20a_%7Bm1%7D%20&amp;%20a_%7Bm2%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7Bmn%7D%0A%5Cend%7Bbmatrix%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> is the element in the <img src="https://latex.codecogs.com/png.latex?i">-th row and <img src="https://latex.codecogs.com/png.latex?j">-th column of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
</section>
<section id="basic-matrix-operations" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="basic-matrix-operations"><span class="header-section-number">2</span> Basic Matrix Operations</h2>
<section id="matrix-with-combinations-of-vectors" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="matrix-with-combinations-of-vectors"><span class="header-section-number">2.1</span> Matrix with Combinations of Vectors</h3>
<p>A matrix with combinations of vectors is a matrix that can be written as a linear combination of column vectors. Given column vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cdots,%20%5Cmathbf%7Bv%7D_n%20%5Cin%20%5Cmathbb%7BR%7D%5Em"> and scalars <img src="https://latex.codecogs.com/png.latex?a_1,%20a_2,%20%5Cdots,%20a_n%20%5Cin%20%5Cmathbb%7BR%7D">, the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> can be written as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A%20=%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Bv_1%7D&amp;%5Cmathbf%7Bv_2%7D&amp;%20%5Cdots%20&amp;%5Cmathbf%7Bv_n%7D%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20a_1%20%5C%5C%20a_2%20%5C%5C%20%5Cvdots%20%5C%5C%20a_n%20%5Cend%7Bbmatrix%7D%20=a_1%5Cmathbf%20v_1%20+a_2%5Cmathbf%20v_2%20+%5Cdots+a_n%20%5Cmathbf%20v_n%0A"></p>
<p>In other words, the columns of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are linear combinations of the column vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cdots,%20%5Cmathbf%7Bv%7D_n">. This can be written more compactly as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A%20=%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Bv_1%7D&amp;%5Cmathbf%7Bv_2%7D&amp;%20%5Cdots%20&amp;%5Cmathbf%7Bv_n%7D%20%5Cend%7Bbmatrix%7D%20%5Cmathbf%20a%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Ba%7D%20=%20%5Cbegin%7Bbmatrix%7D%20a_1%20&amp;%20a_2%20&amp;%20%5Cdots%20&amp;%20a_n%20%5Cend%7Bbmatrix%7D%5ET"> is a column vector of scalars.</p>
<p>An example of a matrix with combinations of vectors is: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A%20=%0A%5Cbegin%7Bbmatrix%7D%0A1&amp;4&amp;7%5C%5C%0A2&amp;5&amp;8%5C%5C%0A3&amp;6&amp;9%5C%5C%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%5C%5C%0A2%5C%5C%0A3%5C%5C%0A%5Cend%7Bbmatrix%7D%5Cmathbf%7Be%7D_1%5E%5Ctext%7BT%7D+%0A%5Cbegin%7Bbmatrix%7D%0A4%5C%5C%0A5%5C%5C%0A6%5C%5C%0A%5Cend%7Bbmatrix%7D%5Cmathbf%7Be%7D_2%5E%5Ctext%7BT%7D+%0A%5Cbegin%7Bbmatrix%7D%0A7%5C%5C%0A8%5C%5C%0A9%5C%5C%0A%5Cend%7Bbmatrix%7D%5Cmathbf%7Be%7D_3%5E%5Ctext%7BT%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D_1,%20%5Cmathbf%7Be%7D_2,%20%5Cmathbf%7Be%7D_3"> are the standard basis vectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3">.</p>
</section>
<section id="matrix-addition" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="matrix-addition"><span class="header-section-number">2.2</span> Matrix addition</h3>
<p>The sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.</p>
<p>Given two <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D">, their sum <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D%20=%20%5Cmathbf%7BA%7D%20+%20%5Cmathbf%7BB%7D"> is defined by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ac_%7Bi,j%7D=a_%7Bi,j%7D+b_%7Bi,j%7D%E2%80%8B%0A"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%20m"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20j%20%5Cleq%20n">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%20%201%20&amp;%202%20%5C%5C%0A%20%203%20&amp;%204%20%5C%5C%0A%20%205%20&amp;%206%0A%5Cend%7Bbmatrix%7D%20+%0A%5Cbegin%7Bbmatrix%7D%0A%20%20-1%20&amp;%200%20%5C%5C%0A%20%202%20&amp;%20-3%20%5C%5C%0A%20%20-5%20&amp;%204%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A%20%200%20&amp;%202%20%5C%5C%0A%20%205%20&amp;%201%20%5C%5C%0A%20%200%20&amp;%2010%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="scalar-multiplication" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="scalar-multiplication"><span class="header-section-number">2.3</span> Scalar multiplication</h3>
<p>The product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.</p>
<p>Given a scalar <img src="https://latex.codecogs.com/png.latex?k"> and an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, their product <img src="https://latex.codecogs.com/png.latex?k%5Cmathbf%7BA%7D"> is defined by: <img src="https://latex.codecogs.com/png.latex?%0A(k%5Cmathbf%7BA%7D)_%7Bi,j%7D%20=%20k(a_%7Bi,j%7D)%0A"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%20m"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20j%20%5Cleq%20n">.</p>
<p>Example: <img src="https://latex.codecogs.com/png.latex?%0A2%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%20%5C%5C%0A5%20&amp;%206%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A2%20&amp;%204%20%5C%5C%0A6%20&amp;%208%20%5C%5C%0A10%20&amp;%2012%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="matrix-multiplication" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="matrix-multiplication"><span class="header-section-number">2.4</span> Matrix multiplication</h3>
<p>The product of two matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D"> is a matrix obtained by multiplying the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> by the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D">.</p>
<p>Given two matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D"> with dimensions <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> and <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20p">, respectively, their product <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D%20=%20%5Cmathbf%7BAB%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20p"> matrix defined by: <img src="https://latex.codecogs.com/png.latex?%0Ac_%7Bi,j%7D%20=%20%5Csum_%7Bk=1%7D%5En%20a_%7Bi,k%7Db_%7Bk,j%7D%0A"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%20m"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20j%20%5Cleq%20p">.</p>
<p>Example: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%20%5C%5C%0A5%20&amp;%206%0A%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%0A-1%20&amp;%200%20&amp;%202%20%5C%5C%0A2%20&amp;%20-3%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A3%20&amp;%20-6%20&amp;%204%20%5C%5C%0A5%20&amp;%20-12%20&amp;%2010%20%5C%5C%0A7%20&amp;%20-18%20&amp;%2016%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="matrix-multiplication-with-a-vector" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="matrix-multiplication-with-a-vector"><span class="header-section-number">2.5</span> Matrix Multiplication with a Vector</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> be a <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%201"> column vector. The matrix-vector product <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D"> is defined as:</p>
<p>$$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%5Cmathbf%7BAx%7D=%0A%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20%5Cdots%20&amp;%20a_%7B1n%7D%20%5C%5C%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20%5Cdots%20&amp;%20a_%7B2n%7D%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0Aa_%7Bm1%7D%20&amp;%20a_%7Bm2%7D%20&amp;%20%5Cdots%20&amp;%20a_%7Bmn%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%0Ax_1%20%5C%5C%20x_2%20%5C%5C%20%5Cvdots%20%5C%5C%20x_n%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7Dx_1%20&amp;%20a_%7B12%7Dx_2%20&amp;%20%5Cdots%20&amp;%20a_%7B1n%7Dx_n%20%5C%5C%0Aa_%7B21%7Dx_1%20&amp;%20a_%7B22%7Dx_2%20&amp;%20%5Cdots%20&amp;%20a_%7B2n%7Dx_n%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20%5C%5C%0Aa_%7Bm1%7Dx_1%20&amp;%20a_%7Bm2%7Dx_2%20&amp;%20%5Cdots%20&amp;%20a_%7Bmn%7Dx_n%0A%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation%7D"> $$</p>
<p>In other words, each entry of the resulting column vector is the dot product of the corresponding row of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and the column vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>For example, let</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A%20=%0A%5Cbegin%7Bbmatrix%7D%0A2%20&amp;%201%20%5C%5C%0A3%20&amp;%204%20%5C%5C%0A1%20&amp;%202%0A%5Cend%7Bbmatrix%7D%20%5Ctext%7B%20%20%7D%0A%5Cmathbf%20x=%0A%5Cbegin%7Bbmatrix%7D%0A%20%201%5C%5C-1%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>Then we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BAx%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A2%20&amp;%201%20%5C%5C%0A3%20&amp;%204%20%5C%5C%0A1%20&amp;%202%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%201%5C%5C-1%0A%5Cend%7Bbmatrix%7D%0A=%0A%5Cbegin%7Bbmatrix%7D%0A%20%201%5C%5C-5%5C%5C-1%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="transpose" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="transpose"><span class="header-section-number">2.6</span> Transpose</h3>
<p>The transpose of an <img src="https://latex.codecogs.com/png.latex?m%20x%20n"> matrix A, denoted by <img src="https://latex.codecogs.com/png.latex?A%5ET">, is the <img src="https://latex.codecogs.com/png.latex?n%20x%20m"> matrix obtained by interchanging the rows and columns of A. Formally, if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Ba_%7Bij%7D%5D"> is an m x n matrix, then its transpose <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET%20=%20%5Bb_%7Bij%7D%5D"> is an <img src="https://latex.codecogs.com/png.latex?n%20x%20m"> matrix where <img src="https://latex.codecogs.com/png.latex?b_%7Bij%7D"> = <img src="https://latex.codecogs.com/png.latex?a_%7Bji%7D"> for all <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j">. In other words, the element in the <img src="https://latex.codecogs.com/png.latex?i"> th row and <img src="https://latex.codecogs.com/png.latex?j"> th column of <img src="https://latex.codecogs.com/png.latex?A%5ET"> is equal to the element in the <img src="https://latex.codecogs.com/png.latex?j"> th row and ith column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>Given an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, its transpose <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m"> matrix defined by: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D_%7Bi,j%7D%5ET%20=%20%5Cmathbf%7BA%7D_%7Bj,i%7D%0A"></p>
<ul>
<li>When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is transposed, diagnoal entries(<img src="https://latex.codecogs.com/png.latex?a_%7Bii%7D">) do not change but off-diagnoal elements(<img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D%20%5C;%20i%20%5Cneq%20j">) change.</li>
<li>A column vector is tranposed into a row vector, and vice versa.</li>
<li>symmetric matrix: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BA%7D%5ET"></li>
</ul>
<p>Example:</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be the matrix <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%5C%5C%0A4%20&amp;%205%20&amp;%206%0A%5Cend%7Bbmatrix%7D%0A"> The transpose of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5ET">, is the matrix <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5ET%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%204%5C%5C%0A2%20&amp;%205%5C%5C%0A3%20&amp;%206%0A%5Cend%7Bbmatrix%7D%0A"></p>
<section id="properties" class="level4" data-number="2.6.1">
<h4 data-number="2.6.1" class="anchored" data-anchor-id="properties"><span class="header-section-number">2.6.1</span> Properties</h4>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20B"> be an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20p"> matrix, and let <img src="https://latex.codecogs.com/png.latex?c"> be a scalar. Then:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BA%7D%5ET)%5ET%20=%20%5Cmathbf%7BA%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BA%7D%20+%20%5Cmathbf%7BB%7D)%5ET%20=%20%5Cmathbf%7BA%7D%5ET%20+%20%5Cmathbf%7BB%7D%5ET"></li>
<li><img src="https://latex.codecogs.com/png.latex?(c%5Cmathbf%7BA%7D)%5ET%20=%20c%5Cmathbf%7BA%7D%5ET"></li>
<li><img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BAB%7D)%5ET%20=%20%5Cmathbf%7BB%7D%5ET%20%5Cmathbf%7BA%7D%5ET"></li>
<li><img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BABC%7D)%5ET%20=%20%5Cmathbf%7BC%7D%5ET%5Cmathbf%7BB%7D%5ET%20%5Cmathbf%7BA%7D%5ET"> , (cyclic properties)</li>
</ul>
</section>
</section>
<section id="linear-equations-of-a-matrix" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="linear-equations-of-a-matrix"><span class="header-section-number">2.7</span> Linear Equations of a Matrix</h3>
<p>Given an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> and an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%201"> vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, the matrix-vector product <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5Cmathbf%7Bx%7D"> is a linear combination of the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> with coefficients given by the entries of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">. The system of linear equations represented by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5Cmathbf%7Bx%7D=%5Cmathbf%7Bb%7D"> has a unique solution if and only if the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> are linearly independent.</p>
<p>A system of linear equations can be written in matrix form as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7B1n%7D%20%5C%5C%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7B2n%7D%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0Aa_%7Bm1%7D%20&amp;%20a_%7Bm2%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7Bmn%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0Ax_1%20%5C%5C%0Ax_2%20%5C%5C%0A%5Cvdots%20%5C%5C%0Ax_n%0A%5Cend%7Bbmatrix%7D=%0A%5Cbegin%7Bbmatrix%7D%0Ab_1%20%5C%5C%0Ab_2%20%5C%5C%0A%5Cvdots%20%5C%5C%0Ab_m%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> are the coefficients of the system, <img src="https://latex.codecogs.com/png.latex?x_i"> are the variables, and <img src="https://latex.codecogs.com/png.latex?b_j"> are the constants.</p>
<p>Here’s an example of a system of linear equations represented by a matrix: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A2x_1%20+%203x_2%20&amp;=%208%20%5C%5C%0A4x_1%20+%205x_2%20&amp;=%2013%0A%5Cend%7Balign*%7D%0A"></p>
<p>This can be written as the matrix equation <img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D=%5Cmathbf%7Bb%7D">, where $$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%20%20%5Cmathbf%7BA%7D%20=%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%202%20&amp;%203%5C%5C%0A%20%20%20%204%20&amp;%205%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%5Cmathbf%7Bx%7D%20=%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20x_1%5C%5C%0A%20%20%20%20x_2%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%0A%20%20%5Cmathbf%7Bb%7D%20=%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%208%5C%5C%0A%20%20%20%2013%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation%7D"> $$</p>
<p>The solution to this system can be found by computing the inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> (if it exists) and multiplying both sides of the equation by it:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bx%7D%20=%20%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7Bb%7D%0A"></p>
<p>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> does not have an inverse, then the system of equations may have either no solutions or infinitely many solutions.</p>
<p>Consider the following system of equations:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A2x_1%20+%203x_2%20&amp;=%205%20%5C%5C%0A4x_1%20-%20x_2%20&amp;=%202%0A%5Cend%7Baligned%7D%0A"></p>
<p>This can be written in matrix form as: $$</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%5Cbegin%7Bbmatrix%7D%0A2%20&amp;%203%20%5C%5C%0A4%20&amp;%20-1%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0Ax_1%20%5C%5C%0Ax_2%0A%5Cend%7Bbmatrix%7D=%0A%5Cbegin%7Bbmatrix%7D%0A5%20%5C%5C%0A2%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation%7D"> $$ where <img src="https://latex.codecogs.com/png.latex?a_%7B11%7D%20=%202,%20a_%7B12%7D%20=%203,%20a_%7B21%7D%20=%204,%20a_%7B22%7D%20=%20-1,%20x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2"> are the variables, and <img src="https://latex.codecogs.com/png.latex?b_1%20=%205,%20b_2%20=%202"> are the constants.</p>
</section>
<section id="determinant" class="level3" data-number="2.8">
<h3 data-number="2.8" class="anchored" data-anchor-id="determinant"><span class="header-section-number">2.8</span> Determinant</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> square matrix. The determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted by <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7BA%7D%7C"> or <img src="https://latex.codecogs.com/png.latex?%5Cdet(%5Cmathbf%7BA%7D)">, is a scalar value calculated as the sum of the products of the elements in any row or column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> with their corresponding cofactors, that is,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%5Cmathbf%7BA%7D%7C=%5Csum_%7Bi=1%7D%5E%7Bn%7Da_%7Bij%7DC_%7Bij%7D=%5Csum_%7Bj=1%7D%5E%7Bn%7Da_%7Bij%7DC_%7Bij%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> is the element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> in the <img src="https://latex.codecogs.com/png.latex?i">-th row and <img src="https://latex.codecogs.com/png.latex?j">-th column, and <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D"> is the cofactor of <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D">. The cofactor of <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D">, denoted by <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D">, is given by <img src="https://latex.codecogs.com/png.latex?(-1)%5E%7Bi+j%7D"> times the determinant of the <img src="https://latex.codecogs.com/png.latex?(n-1)%20%5Ctimes%20(n-1)"> matrix obtained by deleting the <img src="https://latex.codecogs.com/png.latex?i">-th row and <img src="https://latex.codecogs.com/png.latex?j">-th column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>The determinant of an <img src="https://latex.codecogs.com/png.latex?n%20x%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a scalar value denoted as <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7BA%7D%7C">. It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a <img src="https://latex.codecogs.com/png.latex?3%20x%203"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20="> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20a_%7B13%7D%20%5C%5C%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B32%7D%20&amp;%20a_%7B33%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%5Cmathbf%7BA%7D%7C%20=%20a_%7B11%7D%0A%5Cbegin%7Bvmatrix%7D%0Aa_%7B22%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B32%7D%20&amp;%20a_%7B33%7D%0A%5Cend%7Bvmatrix%7D%0A-%20a_%7B12%7D%0A%5Cbegin%7Bvmatrix%7D%0Aa_%7B21%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B33%7D%0A%5Cend%7Bvmatrix%7D%0A+%20a_%7B13%7D%0A%5Cbegin%7Bvmatrix%7D%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B32%7D%0A%5Cend%7Bvmatrix%7D%0A"></p>
<p>For example, consider the <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20&amp;%203%20%5C%5C%204%20&amp;%205%20&amp;%206%20%5C%5C%207%20&amp;%208%20&amp;%209%20%5Cend%7Bbmatrix%7D">. We can calculate the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> using any row or column. Let’s use the first column:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%5Cmathbf%7BA%7D%7C%20=%201%0A%5Cbegin%7Bvmatrix%7D%0A5%20&amp;%206%20%5C%5C%0A8%20&amp;%209%0A%5Cend%7Bvmatrix%7D%0A-%204%0A%5Cbegin%7Bvmatrix%7D%0A2%20&amp;%203%5C%5C%0A8%20&amp;%209%0A%5Cend%7Bvmatrix%7D%0A+%207%0A%5Cbegin%7Bvmatrix%7D%0A2%20&amp;%205%20%5C%5C%0A3%20&amp;%206%0A%5Cend%7Bvmatrix%7D%20=%200%0A"></p>
<p>Therefore, the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is zero.</p>
</section>
<section id="inverse" class="level3" data-number="2.9">
<h3 data-number="2.9" class="anchored" data-anchor-id="inverse"><span class="header-section-number">2.9</span> Inverse</h3>
<p>The inverse of a square matrix <img src="https://latex.codecogs.com/png.latex?A"> of size <img src="https://latex.codecogs.com/png.latex?n"> is a matrix <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> such that the product of <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> is the identity matrix <img src="https://latex.codecogs.com/png.latex?I_n">, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?A%20%5Ctimes%20A%5E%7B-1%7D%20=%20I_n">. If such a matrix exists, then <img src="https://latex.codecogs.com/png.latex?A"> is said to be invertible or non-singular.</p>
<p>The inverse of a square matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E%7B-1%7D"> and is defined as the unique matrix that satisfies the following equation: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BI%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D"> is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.</p>
<p>For example, consider the <img src="https://latex.codecogs.com/png.latex?2%5Ctimes%202"> matrix <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%0A%5Cend%7Bbmatrix%7D.%0A"> The inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%0A%5Cfrac%7B1%7D%7B-2%7D%0A%5Cbegin%7Bbmatrix%7D%0A4%20&amp;%20-2%20%5C%5C%0A-3%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A-2%20&amp;%201%20%5C%5C%0A%5Cfrac%7B3%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>We can verify that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BI%7D"> by computing:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A-2%20&amp;%201%20%5C%5C%0A%5Cfrac%7B3%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20%5C%5C%0A0%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7BI%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%0A%5Cbegin%7Bbmatrix%7D%20-2%20&amp;%201%20%5C%5C%0A%5Cfrac%7B3%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20%5C%5C%0A0%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7BI%7D%0A"></p>
<p>Let me give another example and A be a <img src="https://latex.codecogs.com/png.latex?3x3"> square matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20a_%7B13%7D%20%5C%5C%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B32%7D%20&amp;%20a_%7B33%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"> Then, the inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E%7B-1%7D">, is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cfrac%7B1%7D%7B%5Ctext%7Bdet%7D(%5Cmathbf%7BA%7D)%7D%5Cbegin%7Bbmatrix%7D%0Aa_%7B22%7Da_%7B33%7D-a_%7B23%7Da_%7B32%7D%20&amp;%20a_%7B13%7Da_%7B32%7D-a_%7B12%7Da_%7B33%7D%20&amp;%20a_%7B12%7Da_%7B23%7D-a_%7B13%7Da_%7B22%7D%20%5C%5C%0Aa_%7B23%7Da_%7B31%7D-a_%7B21%7Da_%7B33%7D%20&amp;%20a_%7B11%7Da_%7B33%7D-a_%7B13%7Da_%7B31%7D%20&amp;%20a_%7B13%7Da_%7B21%7D-a_%7B11%7Da_%7B23%7D%20%5C%5C%0Aa_%7B21%7Da_%7B32%7D-a_%7B22%7Da_%7B31%7D%20&amp;%20a_%7B12%7Da_%7B31%7D-a_%7B11%7Da_%7B32%7D%20&amp;%20a_%7B11%7Da_%7B22%7D-a_%7B12%7Da_%7B21%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?det(%5Cmathbf%7BA%7D)"> is the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>For example, let:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%20%5C%5C%0A0%20&amp;%201%20&amp;%204%20%5C%5C%0A5%20&amp;%206%20&amp;%200%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"> Then, <img src="https://latex.codecogs.com/png.latex?det(A)"> = -57, and the inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cfrac%7B1%7D%7B-57%7D%5Cbegin%7Bbmatrix%7D%0A-24%20&amp;%2018%20&amp;%205%20%5C%5C%0A20%20&amp;%20-15%20&amp;%20-4%20%5C%5C%0A-3%20&amp;%202%20&amp;%201%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>There are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.</p>
</section>
<section id="rank" class="level3" data-number="2.10">
<h3 data-number="2.10" class="anchored" data-anchor-id="rank"><span class="header-section-number">2.10</span> Rank</h3>
<p>The rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Brank%7D(%5Cmathbf%7BA%7D)">.</p>
<p>For example, consider the following matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%20%20%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%201%20&amp;%202%20&amp;%203%5C%5C%0A%20%20%20%204%20&amp;%205%20&amp;%206%20%5C%5C%0A%20%20%20%207%20&amp;%208%20&amp;%209%5C%5C%0A%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>The columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is 2.</p>
</section>
<section id="trace" class="level3" data-number="2.11">
<h3 data-number="2.11" class="anchored" data-anchor-id="trace"><span class="header-section-number">2.11</span> Trace</h3>
<p>The trace of a square matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Btr%7D(%5Cmathbf%7BA%7D)">, is defined as the sum of the diagonal elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. In other words, if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> matrix, then its trace is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathrm%7Btr%7D(%5Cmathbf%7BA%7D)=%5Csum_%7Bi=1%7D%5E%7Bn%7Da_%7Bij%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?a_%7Bii%7D"> denotes the <img src="https://latex.codecogs.com/png.latex?i"> th diagonal element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%20%202%20&amp;%203%20&amp;%201%20%5C%5C%0A%20%200%20&amp;%205%20&amp;%202%20%5C%5C%0A%20%201%20&amp;%201%20&amp;%204%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>Then, the trace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Btr%7D(%5Cmathbf%7BA%7D)%20=%202%20+%205%20+%204%20=%2011"></p>
</section>
<section id="eigenvalues-and-eigenvectors" class="level3" data-number="2.12">
<h3 data-number="2.12" class="anchored" data-anchor-id="eigenvalues-and-eigenvectors"><span class="header-section-number">2.12</span> Eigenvalues and Eigenvectors</h3>
<p>Let A be an <img src="https://latex.codecogs.com/png.latex?n%20%C3%97%20n"> square matrix. A scalar <img src="https://latex.codecogs.com/png.latex?%5Clambda"> is called an eigenvalue of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> if there exists a non-zero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> such that <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BAv%7D=%5Clambda%5Cmathbf%7Bv%7D%0A"></p>
<p>Such a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> is called an eigenvector corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda">.</p>
<p>Example:</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be the matrix</p>
<p>To find the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, we solve the characteristic equation <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdet%7D(%5Cmathbf%20A%20-%20%5Clambda%20%5Cmathbf%20I%20)%20=%200">, where I is the n × n identity matrix.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%5Ctext%7Bdet%7D(%5Cmathbf%20A%20-%20%5Clambda%20%5Cmathbf%20I%20)%0A%20%20&amp;=%0A%20%20%20%20%5Cbegin%7Bvmatrix%7D%0A%20%20%20%203%20-%20%5Clambda%20&amp;%201%20%5C%5C%0A%20%20%20%201%20&amp;%203%20-%20%5Clambda%0A%20%20%20%20%5Cend%7Bvmatrix%7D%20%5C%5C%0A%20%20&amp;=%0A%20%20(3%20-%20%5Clambda)(3%20-%20%5Clambda)%20-%201%20%5C%5C%0A%20%20&amp;=%20%5Clambda%5E2%20-%206%5Clambda%20+%208%20=%200%0A%5Cend%7Balign*%7D%0A"></p>
<p>Solving this quadratic equation gives us the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">: <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202"> and <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">.</p>
<p>To find the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202">, we solve the equation <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%20A%20-%202%20%5Cmathbf%20I)%5Cmathbf%7Bv%7D%20=%20%5Cmathbf%7B0%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20I"> is the <img src="https://latex.codecogs.com/png.latex?2%20%5Ctimes%202"> identity matrix.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20(%5Cmathbf%20A%20-%202%20%5Cmathbf%20I)%5Cmathbf%7Bv%7D%20=%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%201%20&amp;%201%20%5C%5C%0A%20%20%20%20%20%201%20&amp;%201%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20x%20%5C%5C%0A%20%20%20%20y%0A%20%20%5Cend%7Bbmatrix%7D%20=%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%200%20%5C%5C%0A%20%20%20%200%0A%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p>Solving this system of equations gives us the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202">: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_1%7D%20=%20%5Cbegin%7Bbmatrix%7D%20-1%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"></p>
<p>Similarly, for <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">, we solve the equation <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%20A%20-%204%5Cmathbf%20I)%5Cmathbf%7Bv%7D"> = <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D"> to get the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_2%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"></p>
<div class="cell">

</div>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/02.basic_matrix.html</guid>
  <pubDate>Thu, 30 Mar 2023 15:00:00 GMT</pubDate>
  <media:content url="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/images/chap02_09.PNG" medium="image"/>
</item>
<item>
  <title>Basics (3) - Special Matrices</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/03.basic_special_matrix.html</link>
  <description><![CDATA[ 



<section id="special-matrix" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="special-matrix"><span class="header-section-number">1</span> Special Matrix</h2>
<section id="square-matrix" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="square-matrix"><span class="header-section-number">1.1</span> Square Matrix</h3>
<p>A square matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a matrix with the same number of rows and columns, i.e., <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> matrix.</p>
<p>For example, the following is a <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203"> square matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%204%20&amp;%207%5C%5C%0A2%20&amp;%205%20&amp;%208%5C%5C%0A3%20&amp;%206%20&amp;%209%0A%5Cend%7Bbmatrix%7D%0A"></p>
<section id="properties" class="level4" data-number="1.1.1">
<h4 data-number="1.1.1" class="anchored" data-anchor-id="properties"><span class="header-section-number">1.1.1</span> Properties</h4>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be an <img src="https://latex.codecogs.com/png.latex?n%5Ctimes%20n"> square matrix. Then, the following properties hold:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is invertible if and only if <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdet%7D(%5Cmathbf%7BA%7D)%20%5Cneq%200">.</li>
<li>The trace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is defined as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Btr%7D(%5Cmathbf%7BA%7D)%20=%20%5Csum_%7Bi=1%7D%5En%20a_%7Bii%7D">, where <img src="https://latex.codecogs.com/png.latex?a_%7Bii%7D"> is the <img src="https://latex.codecogs.com/png.latex?i">th diagonal element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is symmetric, then it has <img src="https://latex.codecogs.com/png.latex?n"> real eigenvalues and an orthonormal set of eigenvectors.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is diagonalizable, then <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> can be written as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BPDP%7D%5E%7B-1%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BP%7D"> is the matrix whose columns are the eigenvectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BD%7D"> is the diagonal matrix whose diagonal elements are the corresponding eigenvalues.</li>
<li>The transpose of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E%5Ctop">, is obtained by reflecting <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> across its main diagonal. That is, <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BA%7D%5E%5Ctop)_%7Bij%7D%20=%20a_%7Bji%7D">.</li>
</ul>
<p>Here’s an example of a <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203"> matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%204%20&amp;%207%5C%5C%0A2%20&amp;%205%20&amp;%208%5C%5C%0A3%20&amp;%206%20&amp;%209%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>With this matrix, we can see that <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdet%7D(%5Cmathbf%7BA%7D)%20=%200">, so <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is not invertible. The trace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Btr%7D(%5Cmathbf%7BA%7D)%20=%201%20+%205%20+%209%20=%2015">. Since <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is not symmetric, we cannot say that it has real eigenvalues and an orthonormal set of eigenvectors. However, we can check that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is diagonalizable, and we can find that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BPDP%7D%5E%7B-1%7D"> with</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0AP=%5Cbegin%7Bpmatrix%7D%0A-0.8252%20&amp;%20-0.2886%20&amp;%200.4848%5C%5C%0A-0.3779%20&amp;%20-0.7551%20&amp;%20-0.5375%5C%5C%0A0.2185%20&amp;%20-0.5800%20&amp;%200.7830%0A%5Cend%7Bpmatrix%7D,%20%5Ctext%7B%20%7D%20D=%5Cbegin%7Bpmatrix%7D%0A16.1168%20&amp;%200%20&amp;%200%5C%5C%0A0%20&amp;%20-1.1168%20&amp;%200%5C%5C%0A0%20&amp;%200%20&amp;%200%0A%5Cend%7Bpmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
</section>
</section>
<section id="diagonal-matrix" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="diagonal-matrix"><span class="header-section-number">1.2</span> Diagonal Matrix</h3>
<p>A diagonal matrix is a square matrix in which all the off-diagonal elements are zero. The diagonal elements can be any scalar value.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BD%7D%20=%20%5Cbegin%7Bpmatrix%7D%0Ad_%7B1%7D%20&amp;%200%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%0A0%20&amp;%20d_%7B2%7D%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%20d_%7B3%7D%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A0%20&amp;%200%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%20d_%7Bn%7D%0A%5Cend%7Bpmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BD%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> diagonal matrix with diagonal elements <img src="https://latex.codecogs.com/png.latex?d_1,%20d_2,%20%5Cldots,%20d_n">. An example of a <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203"> diagonal matrix is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BD%7D%20=%20%5Cbegin%7Bpmatrix%7D%0A2%20&amp;%200%20&amp;%200%20%5C%5C%0A0%20&amp;%20-1%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%204%0A%5Cend%7Bpmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<section id="properties-1" class="level4" data-number="1.2.1">
<h4 data-number="1.2.1" class="anchored" data-anchor-id="properties-1"><span class="header-section-number">1.2.1</span> Properties</h4>
<p>A diagonal matrix is a square matrix in which all the off-diagonal elements are zero, i.e., <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D%20=%200"> for <img src="https://latex.codecogs.com/png.latex?i%20%5Cneq%20j">. Some properties of a diagonal matrix are:</p>
<ul>
<li><p>For two diagnoal matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20E">, <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BDE%7D=%0A%5Cbegin%7Bpmatrix%7D%0Ad_%7B1%7De_%7B1%7D%20&amp;%200%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%0A0%20&amp;%20d_%7B2%7De_%7B2%7D%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%20d_%7B3%7De_%7B3%7D%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A0%20&amp;%200%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%20d_%7Bn%7De_%7Bn%7D%0A%5Cend%7Bpmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p></li>
<li><p>The determinant of a diagonal matrix is the product of its diagonal entries.</p></li>
<li><p>The trace of a diagonal matrix is the sum of its diagonal entries.</p></li>
<li><p>The inverse of a non-singular diagonal matrix is a diagonal matrix with the reciprocal of its diagonal entries as its diagonal entries.</p></li>
</ul>
</section>
</section>
<section id="identity-matrix" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="identity-matrix"><span class="header-section-number">1.3</span> Identity Matrix</h3>
<p>An identity matrix is a square matrix in which all the diagonal elements are equal to <img src="https://latex.codecogs.com/png.latex?1"> and all the off-diagonal elements are equal to 0. The notation for an identity matrix of size <img src="https://latex.codecogs.com/png.latex?n"> is <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D_n">.</p>
<section id="example" class="level4" data-number="1.3.1">
<h4 data-number="1.3.1" class="anchored" data-anchor-id="example"><span class="header-section-number">1.3.1</span> Example</h4>
<p>Example of a <img src="https://latex.codecogs.com/png.latex?3%5Ctimes%203"> identity matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D_3">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Cmathbf%7BI%7D_3%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20&amp;%200%20%5C%5C%0A0%20&amp;%201%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%201%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation%7D%0A"></p>
</section>
<section id="properties-2" class="level4" data-number="1.3.2">
<h4 data-number="1.3.2" class="anchored" data-anchor-id="properties-2"><span class="header-section-number">1.3.2</span> Properties</h4>
<p>Some properties of an identity matrix include:</p>
<ul>
<li>Multiplying any matrix by an identity matrix results in the same matrix: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20%5Cmathbf%7BI%7D%20=%20%5Cmathbf%7BI%7D%20%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BA%7D">.</li>
<li>The product of any matrix and its corresponding inverse is an identity matrix: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BA%7D%5E%7B-1%7D%20%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BI%7D">.</li>
<li>The determinant of an identity matrix is 1: <img src="https://latex.codecogs.com/png.latex?%5Cdet(%5Cmathbf%7BI%7D)%20=%201">.</li>
<li>An identity matrix is symmetric: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D%20=%20%5Cmathbf%7BI%7D%5ET">.</li>
</ul>
</section>
</section>
<section id="symmetric-matrix" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="symmetric-matrix"><span class="header-section-number">1.4</span> Symmetric Matrix</h3>
<p>A symmetric matrix is a square matrix that is equal to its own transpose, i.e., <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BA%7D%5ET">. Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> matrix, then <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is symmetric if and only if <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D%20=%20a_%7Bji%7D"> for all <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j"> such that <img src="https://latex.codecogs.com/png.latex?1%20%5Cle%20i">, <img src="https://latex.codecogs.com/png.latex?j%20%5Cle%20n">.</p>
<p>Here’s an example of a symmetric matrix: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%20%5C%5C%0A2%20&amp;%204%20&amp;%205%20%5C%5C%0A3%20&amp;%205%20&amp;%206%0A%5Cend%7Bbmatrix%7D%0A"></p>
<section id="properties-3" class="level4" data-number="1.4.1">
<h4 data-number="1.4.1" class="anchored" data-anchor-id="properties-3"><span class="header-section-number">1.4.1</span> Properties</h4>
<p>A symmetric matrix is a square matrix that is equal to its own transpose. Some properties of symmetric matrices include:</p>
<ul>
<li>The diagonal entries are real numbers.</li>
<li>The matrix is diagonalizable, meaning it can be expressed as <strong>a product of diagonal and orthogonal matrices</strong>.</li>
<li>The eigenvalues of a symmetric matrix are real numbers.</li>
<li>The eigenvectors corresponding to different eigenvalues are orthogonal.</li>
<li>The sum and difference of two symmetric matrices is also symmetric.</li>
</ul>
</section>
</section>
<section id="idempotent-matrix" class="level3" data-number="1.5">
<h3 data-number="1.5" class="anchored" data-anchor-id="idempotent-matrix"><span class="header-section-number">1.5</span> Idempotent Matrix</h3>
<p>An idempotent matrix is a square matrix that when multiplied by itself yields itself. In other words, an idempotent matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BP%7D"> satisfies <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BP%7D%5E2%20=%20%5Cmathbf%7BP%7D">.</p>
<p>An example of an idempotent matrix is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Cmathbf%7BP%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%201%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation%7D%0A"></p>
<section id="properties-4" class="level4" data-number="1.5.1">
<h4 data-number="1.5.1" class="anchored" data-anchor-id="properties-4"><span class="header-section-number">1.5.1</span> Properties</h4>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E2%20=%20%5Cmathbf%20A">.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an idempotent matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI_n%7D-%5Cmathbf%7BA%7D"> is also an idempotent matrix. <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A(%5Cmathbf%7BI_n%7D-%5Cmathbf%7BA%7D)%5E2&amp;=(%5Cmathbf%7BI_n%7D-%5Cmathbf%7BA%7D)(%5Cmathbf%7BI_n%7D-%5Cmathbf%7BA%7D)%5C%5C%0A&amp;=%5Cmathbf%7BI_n%7D%5Cmathbf%7BI_n%7D-%5Cmathbf%7BI_n%7D%5Cmathbf%7BA%7D-%5Cmathbf%7BA%7D%5Cmathbf%7BI_n%7D-%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5C%5C%0A&amp;=%5Cmathbf%7BI_n%7D%5Cmathbf%7BI_n%7D-%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5C%5C%0A&amp;=%5Cmathbf%7BI_n%7D-%5Cmathbf%7BA%7D%0A%5Cend%7Baligned%7D%0A"></li>
<li>The determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is either 0 or 1.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is symmetric, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is idempotent if only if the eigenvalue of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is either <img src="https://latex.codecogs.com/png.latex?0"> or <img src="https://latex.codecogs.com/png.latex?1">.</li>
<li>The rank of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is equal to the trace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, which is the sum of the diagonal elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">.</li>
</ul>
</section>
</section>
<section id="ones-matrix" class="level3" data-number="1.6">
<h3 data-number="1.6" class="anchored" data-anchor-id="ones-matrix"><span class="header-section-number">1.6</span> Ones Matrix</h3>
<p>The ones matrix, denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BJ%7D">, is a matrix in which every entry is equal to 1.</p>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>The letter <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BJ%7D"> is not related to the jacobian matrix at all. The letter <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BJ%7D"> of the ones matrix is used for convention.</p>
</div>
</div>
<p>Here is an example of a <img src="https://latex.codecogs.com/png.latex?3%5Ctimes%203"> ones matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BJ%7D%20=%0A%5Cbegin%7Bpmatrix%7D%0A1%20&amp;%201%20&amp;%201%20%5C%5C%0A1%20&amp;%201%20&amp;%201%20%5C%5C%0A1%20&amp;%201%20&amp;%201%0A%5Cend%7Bpmatrix%7D%0A"></p>
<section id="properties-5" class="level4" data-number="1.6.1">
<h4 data-number="1.6.1" class="anchored" data-anchor-id="properties-5"><span class="header-section-number">1.6.1</span> Properties</h4>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J"> is a square matrix</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J"> is symmetric</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J"> has rank 1</li>
<li>Trace: The trace of the <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J"> matrix is equal to the dimension of the matrix. For the <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J"> matrix, it is equal to the number of rows or columns in the matrix.</li>
<li>Matrix multiplication: When multiplied by any matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, the <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BJ%7D"> matrix results in a matrix where each row (or column, depending on the multiplication order) is the sum of the elements of the corresponding row (or column) of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.
<ul>
<li>In other words, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BJ%7D"> multiplied by a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> performs a row (or column) summation operation on <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</li>
<li>If A is any <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> matrix, then <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAJ%7D%20=%20%5Cmathbf%7BJA%7D%20=%20%5Coperatorname%7Btrace%7D(%5Cmathbf%7BA%7D)%5Cmathbf%7BJ%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Btrace%7D(%5Cmathbf%7BA%7D)"> is the sum of the diagonal elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J_%7Bm%5Ctimes%20n%7D"> can be represented as the product of two vectors, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%201_%7Bm%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%201_%7Bn%7D">, i.e., <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J_%7Bm%5Ctimes%20n%7D%20=%20%5Cmathbf%201_%7Bm%7D%20%5Cmathbf%201_%7Bn%7D%5ET"></li>
<li>Eigenvalues and eigenvectors: The <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J"> matrix has one eigenvalue equal to the dimension of the matrix, with the corresponding eigenvector being a vector of all 1’s.</li>
<li>Inverse: The <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J"> matrix is a special case where it does not have an inverse, as all its rows (or columns) are linearly dependent.</li>
</ul>
</section>
<section id="applications" class="level4" data-number="1.6.2">
<h4 data-number="1.6.2" class="anchored" data-anchor-id="applications"><span class="header-section-number">1.6.2</span> Applications</h4>
<ul>
<li><p>calculate a sum using a <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%201_n"> vector for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x_n">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%201%5ET%5Cmathbf%20x%20=%5Csum_%7Bi=1%7D%5E%7Bn%7D1%5Ctimes%20x_i=x_1+x_2+%5Cdots+x_n%0A"></p></li>
<li><p>calculate a mean using a <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%201_n"> vector for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x_n">:</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbar%7Bx%7D=%5Cfrac%7B1%7D%7Bn%7D%5Cmathbf%201%5ET%5Cmathbf%20x%20=%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi=1%7D%5E%7Bn%7D1%5Ctimes%20x_i=%5Cfrac%7B1%7D%7Bn%7D(x_1+x_2+%5Cdots+x_n)%0A"></p>
<ul>
<li>calculate <img src="https://latex.codecogs.com/png.latex?n"> column sums of a dataset using a <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%201_m"> vector for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20X_%7Bm%5Ctimes%20n%7D">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbar%7B%5Cmathbf%20x%7D=%5Cfrac%7B1%7D%7Bm%7D%5Cmathbf%20X%5ET%5Cmathbf%201%0A"> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cbar%7B%5Cmathbf%20x%7D&amp;=%5Cfrac%7B1%7D%7Bm%7D%5Cmathbf%20X%5ET%5Cmathbf%201%20%5C%5C%0A&amp;=%5Cfrac%7B1%7D%7Bm%7D%0A%5Cbegin%7Bbmatrix%7D%0Ax_%7B11%7D%20&amp;%20x_%7B21%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7Bm1%7D%20%5C%5C%0Ax_%7B12%7D%20&amp;%20x_%7B22%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7Bm2%7D%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0Ax_%7B1n%7D%20&amp;%20x_%7B2n%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7Bmn%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A1_1%20%5C%5C%201_2%20%5C%5C%20%5Cvdots%20%5C%5C%201_m%0A%5Cend%7Bbmatrix%7D%5C%5C%0A&amp;=%5Cfrac%7B1%7D%7Bm%7D%0A%5Cbegin%7Bbmatrix%7D%0Ax_%7B11%7D%20+%20x_%7B21%7D%20+%20%5Ccdots%20+%20x_%7Bm1%7D%20%5C%5C%0Ax_%7B12%7D%20+%20x_%7B22%7D%20+%20%5Ccdots%20+%20x_%7Bm2%7D%20%5C%5C%0A%5Cvdots%20%20%5C%5C%0Ax_%7B1n%7D%20+%20x_%7B2n%7D%20+%20%5Ccdots%20+%20x_%7Bmn%7D%0A%5Cend%7Bbmatrix%7D%20%5C%5C%0A&amp;=%5Cfrac%7B1%7D%7Bm%7D%0A%5Cbegin%7Bbmatrix%7D%0A%5Csum_%7Bi=1%7D%5E%7Bm%20%7Dx_%7Bi1%7D%20%5C%5C%0A%5Csum_%7Bi=1%7D%5E%7Bm%20%7Dx_%7Bi2%7D%20%5C%5C%0A%5Cvdots%20%20%5C%5C%0A%5Csum_%7Bi=1%7D%5E%7Bm%20%7Dx_%7Bjn%7D%0A%5Cend%7Bbmatrix%7D%20%5C%5C%0A&amp;=%0A%5Cbegin%7Bbmatrix%7D%0A%5Cbar%7Bx%7D_%7B1%7D%20%5C%5C%0A%5Cbar%7Bx%7D_%7B2%7D%20%5C%5C%0A%5Cvdots%20%20%5C%5C%0A%5Cbar%7Bx%7D_%7Bn%7D%0A%5Cend%7Bbmatrix%7D%20%5C%5C%0A&amp;=%5Cbar%7B%5Cmathbf%20x%7D%0A%5Cend%7Baligned%7D%0A"></li>
</ul>
</section>
</section>
<section id="centering-matrix" class="level3" data-number="1.7">
<h3 data-number="1.7" class="anchored" data-anchor-id="centering-matrix"><span class="header-section-number">1.7</span> Centering Matrix</h3>
<p>A centering matrix is a square matrix that is used in multivariate statistical analysis to center data by subtracting the mean of each variable from each observation. The centering matrix is a <strong>symmetric</strong> and <strong>idempotent</strong> matrix, meaning that it is equal to its own transpose and that multiplying it by itself results in the same matrix. The centering matrix is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%20%20%5Cmathbf%20C%20=%20%5Cmathbf%20I%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Cmathbf%20J%0A%5Cend%7Bequation%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20I"> is the identity matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J"> is a matrix of ones, and <img src="https://latex.codecogs.com/png.latex?m"> is the number of observations.</p>
<section id="properties-6" class="level4" data-number="1.7.1">
<h4 data-number="1.7.1" class="anchored" data-anchor-id="properties-6"><span class="header-section-number">1.7.1</span> Properties</h4>
<p>The centering matrix has the property that when it is multiplied by a data matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20X">, it centers the data by subtracting the mean of each column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20X"> from each element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20X">. The resulting matrix is called the centered data matrix with the mean equal to <img src="https://latex.codecogs.com/png.latex?0">. The centering matrix is often used in multivariate statistical analysis, such as principal component analysis, to transform the data into a new coordinate system where the variance of each variable is equal to its eigenvalue.</p>
<ul>
<li>A centering matrix is a square matrix.</li>
<li>The diagonal elements of a centering matrix are all equal and are given by <img src="https://latex.codecogs.com/png.latex?m%5E%7B-1%7D">, where <img src="https://latex.codecogs.com/png.latex?n"> is the size of the matrix.</li>
<li>The off-diagonal elements of a centering matrix are all equal and are given by <img src="https://latex.codecogs.com/png.latex?-m%5E%7B-1%7D">.</li>
<li>Multiplying a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> on the left by a centering matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20C"> is equivalent to subtracting the mean of the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> from each column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">.</li>
<li>Multiplying a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> on the right by a centering matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20C"> is equivalent to subtracting the mean of the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> from each row of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">.</li>
</ul>
<p>Here is an example of a centering matrix of size <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%20C%20=%20%5Cfrac%7B1%7D%7B3%7D%20%5Cbegin%7Bpmatrix%7D%0A2%20&amp;%20-1%20&amp;%20-1%20%5C%5C%0A-1%20&amp;%202%20&amp;%20-1%20%5C%5C%0A-1%20&amp;%20-1%20&amp;%202%0A%5Cend%7Bpmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
</section>
<section id="applications-1" class="level4" data-number="1.7.2">
<h4 data-number="1.7.2" class="anchored" data-anchor-id="applications-1"><span class="header-section-number">1.7.2</span> Applications</h4>
<ul>
<li>find a centered matrix, <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7B%5Cmathbf%20X%7D"> using a <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%201_m"> vector for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20X_%7Bm%5Ctimes%20n%7D">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20C%20=%20%5Cmathbf%20I%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Cmathbf%20J%0A"> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbf%20X&amp;=%0A%5Cbegin%7Bbmatrix%7D%0Ax_%7B11%7D%20&amp;%20x_%7B12%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7B1n%7D%20%5C%5C%0Ax_%7B21%7D%20&amp;%20x_%7B22%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7B2n%7D%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0Ax_%7Bm1%7D%20&amp;%20x_%7Bm2%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7Bmn%7D%0A%5Cend%7Bbmatrix%7D%20%5Ctext%7B%20%7D%20%5C%5C%0A%5Cmathbf%20%7B1%5Cbar%7Bx%7D%5ET%7D&amp;=%0A%5Cbegin%7Bbmatrix%7D%0A1%20%5C%5C%0A1%20%5C%5C%0A%5Cvdots%20%5C%5C%0A1%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%5Cbar%7Bx%7D_%7B1%7D%20&amp;%20%5Cbar%7Bx%7D_%7B2%7D%20&amp;%20%5Cdots%20&amp;%20%5Cbar%7Bx%7D_%7Bn%7D%0A%5Cend%7Bbmatrix%7D%5C%5C%0A&amp;=%5Cbegin%7Bbmatrix%7D%0A%5Cbar%7Bx%7D_%7B1%7D%20&amp;%20%5Cbar%7Bx%7D_%7B2%7D%20&amp;%20%5Cdots%20&amp;%20%5Cbar%7Bx%7D_%7Bn%7D%5C%5C%0A%5Cbar%7Bx%7D_%7B1%7D%20&amp;%20%5Cbar%7Bx%7D_%7B2%7D%20&amp;%20%5Cdots%20&amp;%20%5Cbar%7Bx%7D_%7Bn%7D%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%5C%5C%0A%5Cbar%7Bx%7D_%7B1%7D%20&amp;%20%5Cbar%7Bx%7D_%7B2%7D%20&amp;%20%5Cdots%20&amp;%20%5Cbar%7Bx%7D_%7Bn%7D%0A%5Cend%7Bbmatrix%7D%5C%5C%0A%5C%5C%0A%5Ctilde%7B%5Cmathbf%20X%7D&amp;=%5Cmathbf%20X%20-%7B%5Cmathbf%7B1%7D%7D_%7Bm%7D%5Cbar%7B%5Cmathbf%20x%7D%5ET%5C%5C%0A&amp;=%5Cmathbf%20X%20-%5Cmathbf%7B1%7D_%7Bm%7D(%5Cfrac%7B1%7D%7Bm%7D%5Cmathbf%20X%5ET%20%5Cmathbf%7B1%7D_m)%5ET%5C%5C%0A&amp;=%5Cmathbf%20X%20-%5Cmathbf%7B1%7D_%7Bm%7D%5Cmathbf%7B1%7D_m%5ET%5Cfrac%7B1%7D%7Bm%7D%5Cmathbf%20X%5C%5C%0A&amp;=(%5Cmathbf%20I%20-%5Cfrac%7B1%7D%7Bm%7D%5Cmathbf%20J)%5Cmathbf%20X%5C%5C%0A&amp;=%5Cmathbf%20C%20%5Cmathbf%20X%5C%5C%0A%5Cend%7Baligned%7D%0A"></li>
</ul>
</section>
</section>
<section id="covariance-matrix" class="level3" data-number="1.8">
<h3 data-number="1.8" class="anchored" data-anchor-id="covariance-matrix"><span class="header-section-number">1.8</span> Covariance Matrix</h3>
<p>The covariance matrix of a dataset matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> with <img src="https://latex.codecogs.com/png.latex?m"> observations and <img src="https://latex.codecogs.com/png.latex?n"> variables is a symmetric <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> matrix given by:</p>
<section id="algebraic-expression" class="level4" data-number="1.8.1">
<h4 data-number="1.8.1" class="anchored" data-anchor-id="algebraic-expression"><span class="header-section-number">1.8.1</span> Algebraic Expression</h4>
<p>For two random variables, <img src="https://latex.codecogs.com/png.latex?X_1,X_2">, if there are <img src="https://latex.codecogs.com/png.latex?m"> of the observed samples or realized values, we can represent the sample data as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(x_%7B11%7D,x_%7B12%7D),(x_%7B21%7D,x_%7B22%7D),%5Cdots,(x_%7Bm1%7D,x_%7Bm2%7D)%0A"></p>
<p>Then, the sample variance between the two random variables are represented as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0As_%7Bx_1x_2%7D%20=%20%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7Bm%7D(x_%7Bi1%7D-%5Cbar%7Bx_1%7D)(x_%7Bi2%7D-%5Cbar%7Bx_2%7D)%7D%7Bm-1%7D%0A"></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Sample Variance
</div>
</div>
<div class="callout-body-container callout-body">
<p>For each variate, <img src="https://latex.codecogs.com/png.latex?X_j">, <img src="https://latex.codecogs.com/png.latex?j=1,%5Cdots,%20n"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0As%5E2_%7Bx%7D=s_%7Bxx%7D%20=%20%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7Bm%7D(x_%7Bij%7D-%5Cbar%7Bx%7D)%5E2%7D%7Bm-1%7D%0A"></p>
<p>Often for the notation of sample variance, the squared power is added for the sample variance with one random variable, but it is not for the covariance between two random variables.</p>
</div>
</div>
<p>For two random variables ,<img src="https://latex.codecogs.com/png.latex?X_1,X_2">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%20%20%5Cmathbf%7BS%7D%0A%20%20&amp;=%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20s_%7Bx_1x_2%7D&amp;s_%7Bx_1x_2%7D%5C%5C%0A%20%20%20%20s_%7Bx_2x_1%7D&amp;s_%7Bx_2x_2%7D%0A%20%20%5Cend%7Bbmatrix%7D%5C%5C%0A%20%20&amp;=%5Cfrac%7B1%7D%7Bn-1%7D%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%5Csum_%7Bi=1%7D%5E%7Bm%7D(x_%7Bi1%7D-%5Cbar%7Bx_1%7D)(x_%7Bi1%7D-%5Cbar%7Bx_1%7D)&amp;%5Csum_%7Bi=1%7D%5E%7Bm%7D(x_%7Bi1%7D-%5Cbar%7Bx_1%7D)(x_%7Bi2%7D-%5Cbar%7Bx_2%7D)%5C%5C%0A%20%20%20%20%5Csum_%7Bi=1%7D%5E%7Bm%7D(x_%7Bi1%7D-%5Cbar%7Bx_1%7D)(x_%7Bi2%7D-%5Cbar%7Bx_2%7D)&amp;%5Csum_%7Bi=1%7D%5E%7Bm%7D(x_%7Bi2%7D-%5Cbar%7Bx_2%7D)(x_%7Bi2%7D-%5Cbar%7Bx_2%7D)%0A%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>For <img src="https://latex.codecogs.com/png.latex?n"> of random variables, <img src="https://latex.codecogs.com/png.latex?X_1,%5Cdots,X_n">, if we observed the sample vectors with <img src="https://latex.codecogs.com/png.latex?m"> observations, the sample covariance matrix can be represented as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BS%7D=%5Cbegin%7Bbmatrix%7D%0As_%7B11%7D&amp;s_%7B12%7D&amp;%5Cdots&amp;s_%7B1n%7D%5C%5C%0As_%7B21%7D&amp;s_%7B22%7D&amp;%5Cdots&amp;s_%7B2n%7D%5C%5C%0A%5Cvdots&amp;%5Cvdots&amp;%5Cddots&amp;%5Cvdots%5C%5C%0As_%7Bn1%7D&amp;s_%7Bn2%7D&amp;%5Cdots&amp;s_%7Bnn%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?s_%7Bjk%7D%20=%20%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7Bm%7D(x_%7Bij%7D-%5Cbar%7Bx_j%7D)(x_%7Bik%7D-%5Cbar%7Bx_k%7D)%7D%7Bm-1%7D">. The diagonal entries are the sample variances of each random variable, and the off-diagonal entires are the sample covariance between two different random variables.</p>
</section>
<section id="vector-form-of-sample-variance-in-linear-algebra" class="level4" data-number="1.8.2">
<h4 data-number="1.8.2" class="anchored" data-anchor-id="vector-form-of-sample-variance-in-linear-algebra"><span class="header-section-number">1.8.2</span> Vector Form of Sample Variance in Linear Algebra</h4>
<p>The complicating algebraic notion can be represented as the simpler form in linear algebra. To do so, let a sample vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x%20=%5Cbegin%7Bbmatrix%7Dx_%7B1%7D%5C%5Cx_%7B2%7D%5C%5C%20%5Cvdots%5C%5Cx_%7Bn%7D%5Cend%7Bbmatrix%7D">. In othe words, a record, a row, or the observations across <img src="https://latex.codecogs.com/png.latex?n"> variables in a dataset:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BX%7D=%0A%5Cbegin%7Bbmatrix%7D%0A%5Cmathbf%7Bx%7D_%7B1%7D%5ET%5C%5C%0A%5Cmathbf%7Bx%7D_%7B2%7D%5ET%5C%5C%0A%5Cvdots%5C%5C%0A%5Cmathbf%7Bx%7D_%7Bm%7D%5ET%0A%5Cend%7Bbmatrix%7D%0A=%5Cbegin%7Bbmatrix%7D%0Ax_%7B11%7D&amp;x_%7B12%7D&amp;%5Cdots&amp;x_%7B1n%7D%5C%5C%0Ax_%7B21%7D&amp;x_%7B22%7D&amp;%5Cdots&amp;x_%7B2n%7D%5C%5C%0A%5Cvdots&amp;%5Cvdots&amp;%5Cddots&amp;%5Cvdots%5C%5C%0Ax_%7Bm1%7D&amp;x_%7Bm2%7D&amp;%5Cdots&amp;x_%7Bmn%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbf%20S%0A&amp;=%20%5Coperatorname%7BCov%7D(%5Cmathbf%7BX%7D)%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7Bm-1%7D%5Csum_%7Bi=1%7D%5E%7Bm%7D(%5Cmathbf%20x_i%20-%20%5Cbar%7B%5Cmathbf%20x%7D_i)%5ET(%5Cmathbf%20x_i%20-%20%5Cbar%7B%5Cmathbf%20x%7D_i)%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7Bm-1%7D%5Ctilde%7B%5Cmathbf%7BX%7D%7D%5ET%5Ctilde%7B%5Cmathbf%7BX%7D%7D%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7Bm-1%7D(%5Cmathbf%7BCX%7D)%5ET(%5Cmathbf%7BCX%7D)%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7Bm-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BC%7D%5ET%5Cmathbf%7BCX%7D%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7Bm-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BC%7D%5Cmathbf%7BC%7D%5Cmathbf%7BX%7D%5Cquad%20(%5Cbecause%20%5Cmathbf%7BC%7D%5Ctext%7B%20is%20symmetric%7D)%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7Bm-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BC%7D%5Cmathbf%7BX%7D%5Cquad%20(%5Cbecause%20%5Cmathbf%7BC%7D%5Ctext%7B%20is%20an%20idempotent%20matrix%7D)%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> is the dataset matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_i%7D=%5Cbegin%7Bbmatrix%7Dx_%7Bi1%7D%5C%5Cx_%7Bi2%7D%5C%5C%20%5Cvdots%5C%5Cx_%7Bin%7D%5Cend%7Bbmatrix%7D"> is the observations of each variable along the columns, <img src="https://latex.codecogs.com/png.latex?%5Cbar%7B%5Cmathbf%7Bx%7D%7D_i=%5Cbegin%7Bbmatrix%7D%5Cbar%7Bx%7D_%7B1%7D%5C%5C%5Cbar%7Bx%7D_%7B2%7D%5C%5C%20%5Cvdots%5C%5C%5Cbar%7Bx%7D_%7Bn%7D%5Cend%7Bbmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7Bx%7D_i%20-%20%5Cbar%7B%5Cmathbf%7Bx%7D%7D_i)%5ET=%5Ctilde%7B%5Cmathbf%7BX%7D%7D"> is the transpose of the centered dataset matrix.</p>
<p>The size of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20S"> is <img src="https://latex.codecogs.com/png.latex?(1%20%5Ctimes%20n)%5ET(1%20%5Ctimes%20n)=(n%20%5Ctimes%201)(1%20%5Ctimes%20n)=(n%20%5Ctimes%20n)"></p>
</section>
<section id="example-1" class="level4" data-number="1.8.3">
<h4 data-number="1.8.3" class="anchored" data-anchor-id="example-1"><span class="header-section-number">1.8.3</span> Example</h4>
<p>Let’s say we have a dataset with three variables, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_1,%20%5Cmathbf%7Bx%7D_2,%20%5Ctext%7B%20and%20%7D%20%5Cmathbf%7Bx%7D_3,"> with <img src="https://latex.codecogs.com/png.latex?m"> observations. The dataset can be represented as a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> with dimensions <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%203">, where each row represents an observation and each column represents a variable. The covariance matrix, <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCov%7D(%5Cmathbf%7BX%7D)">, of the dataset can be computed as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Coperatorname%7BCov%7D(%5Cmathbf%7BX%7D)%20&amp;=%20%5Cfrac%7B1%7D%7Bm-1%7D(%5Cmathbf%7BX%7D%20-%20%5Cbar%7B%5Cmathbf%7BX%7D%7D)%5ET(%5Cmathbf%7BX%7D%20-%20%5Cbar%7B%5Cmathbf%7BX%7D%7D)%5C%5C%0A&amp;=%5Cfrac%7B%5Ctilde%7B%5Cmathbf%20X%7D%5ET%5Ctilde%7B%5Cmathbf%20X%7D%7D%7Bm-1%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> is the dataset matrix, <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Bx%7D"> is the mean of each variable computed along the rows, and <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BX%7D%20-%20%5Cbar%7B%5Cmathbf%7BX%7D%7D)%5ET"> is the transpose of the centered dataset matrix. The covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCov%7D(%5Cmathbf%7BX%7D)"> will be a <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203"> matrix, with the <img src="https://latex.codecogs.com/png.latex?(i,%20j)"> th entry representing the covariance between the <img src="https://latex.codecogs.com/png.latex?i"> th and <img src="https://latex.codecogs.com/png.latex?j"> th variables in the dataset.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">X<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">as.matrix</span>(mtcars)</span>
<span id="cb1-2">x_bar_vec<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">colMeans</span>(mtcars)</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="fu" style="color: #4758AB;">as.numeric</span>(mtcars[<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">5</span>]<span class="sc" style="color: #5E5E5E;">-</span>x_bar_vec)<span class="sc" style="color: #5E5E5E;">%*%</span><span class="fu" style="color: #4758AB;">t</span>(<span class="fu" style="color: #4758AB;">as.numeric</span>(mtcars[<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">5</span>]<span class="sc" style="color: #5E5E5E;">-</span>x_bar_vec)) <span class="co" style="color: #5E5E5E;"># for 1th row</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>            [,1]        [,2]       [,3]        [,4]         [,5]
[1,]   0.8269629 -0.17050781  -64.31271  -33.362695   0.27593848
[2,]  -0.1705078  0.03515625   13.26035    6.878906  -0.05689453
[3,] -64.3127051 13.26035156 5001.58360 2594.608789 -21.45966895
[4,] -33.3626953  6.87890625 2594.60879 1345.972656 -11.13236328
[5,]   0.2759385 -0.05689453  -21.45967  -11.132363   0.09207432</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="fu" style="color: #4758AB;">as.numeric</span>(mtcars[<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">5</span>]<span class="sc" style="color: #5E5E5E;">-</span>x_bar_vec)<span class="sc" style="color: #5E5E5E;">%*%</span><span class="fu" style="color: #4758AB;">t</span>(<span class="fu" style="color: #4758AB;">as.numeric</span>(mtcars[<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">5</span>]<span class="sc" style="color: #5E5E5E;">-</span>x_bar_vec)) <span class="co" style="color: #5E5E5E;"># for 2nd row</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>            [,1]        [,2]       [,3]        [,4]         [,5]
[1,]   0.8269629 -0.17050781  -64.31271  -33.362695   0.27593848
[2,]  -0.1705078  0.03515625   13.26035    6.878906  -0.05689453
[3,] -64.3127051 13.26035156 5001.58360 2594.608789 -21.45966895
[4,] -33.3626953  6.87890625 2594.60879 1345.972656 -11.13236328
[5,]   0.2759385 -0.05689453  -21.45967  -11.132363   0.09207432</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">X_sweeped<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">sweep</span>(X,<span class="at" style="color: #657422;">MARGIN=</span><span class="dv" style="color: #AD0000;">2</span>,<span class="at" style="color: #657422;">STAT=</span>x_bar_vec,<span class="at" style="color: #657422;">FUN=</span><span class="st" style="color: #20794D;">'-'</span>)</span>
<span id="cb5-2">result<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">matrix</span>(<span class="dv" style="color: #AD0000;">0</span>,<span class="at" style="color: #657422;">ncol=</span><span class="fu" style="color: #4758AB;">ncol</span>(mtcars),<span class="at" style="color: #657422;">nrow=</span><span class="fu" style="color: #4758AB;">ncol</span>(mtcars))</span>
<span id="cb5-3"><span class="cf" style="color: #003B4F;">for</span> (i <span class="cf" style="color: #003B4F;">in</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="fu" style="color: #4758AB;">nrow</span>(mtcars)){</span>
<span id="cb5-4">  result<span class="ot" style="color: #003B4F;">&lt;-</span>result<span class="sc" style="color: #5E5E5E;">+</span>X_sweeped[i,]<span class="sc" style="color: #5E5E5E;">%*%</span><span class="fu" style="color: #4758AB;">t</span>(X_sweeped[i,])</span>
<span id="cb5-5">}</span>
<span id="cb5-6">result<span class="sc" style="color: #5E5E5E;">/</span>(<span class="fu" style="color: #4758AB;">nrow</span>(mtcars)<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>              mpg         cyl        disp          hp         drat          wt
 [1,]   36.324103  -9.1723790  -633.09721 -320.732056   2.19506351  -5.1166847
 [2,]   -9.172379   3.1895161   199.66028  101.931452  -0.66836694   1.3673710
 [3,] -633.097208 199.6602823 15360.79983 6721.158669 -47.06401915 107.6842040
 [4,] -320.732056 101.9314516  6721.15867 4700.866935 -16.45110887  44.1926613
 [5,]    2.195064  -0.6683669   -47.06402  -16.451109   0.28588135  -0.3727207
 [6,]   -5.116685   1.3673710   107.68420   44.192661  -0.37272073   0.9573790
 [7,]    4.509149  -1.8868548   -96.05168  -86.770081   0.08714073  -0.3054816
 [8,]    2.017137  -0.7298387   -44.37762  -24.987903   0.11864919  -0.2736613
 [9,]    1.803931  -0.4657258   -36.56401   -8.320565   0.19015121  -0.3381048
[10,]    2.135685  -0.6491935   -50.80262   -6.358871   0.27598790  -0.4210806
[11,]   -5.363105   1.5201613    79.06875   83.036290  -0.07840726   0.6757903
              qsec           vs           am        gear        carb
 [1,]   4.50914919   2.01713710   1.80393145   2.1356855 -5.36310484
 [2,]  -1.88685484  -0.72983871  -0.46572581  -0.6491935  1.52016129
 [3,] -96.05168145 -44.37762097 -36.56401210 -50.8026210 79.06875000
 [4,] -86.77008065 -24.98790323  -8.32056452  -6.3588710 83.03629032
 [5,]   0.08714073   0.11864919   0.19015121   0.2759879 -0.07840726
 [6,]  -0.30548161  -0.27366129  -0.33810484  -0.4210806  0.67579032
 [7,]   3.19316613   0.67056452  -0.20495968  -0.2804032 -1.89411290
 [8,]   0.67056452   0.25403226   0.04233871   0.0766129 -0.46370968
 [9,]  -0.20495968   0.04233871   0.24899194   0.2923387  0.04637097
[10,]  -0.28040323   0.07661290   0.29233871   0.5443548  0.32661290
[11,]  -1.89411290  -0.46370968   0.04637097   0.3266129  2.60887097</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><span class="co" style="color: #5E5E5E;"># J matrix</span></span>
<span id="cb7-2">m<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">nrow</span>(mtcars)</span>
<span id="cb7-3">J<span class="ot" style="color: #003B4F;">&lt;-</span>(<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span>m)<span class="sc" style="color: #5E5E5E;">*</span><span class="fu" style="color: #4758AB;">matrix</span>(<span class="dv" style="color: #AD0000;">1</span>,<span class="at" style="color: #657422;">ncol=</span>m,<span class="at" style="color: #657422;">nrow=</span>m)</span>
<span id="cb7-4"><span class="co" style="color: #5E5E5E;"># Centering matrix</span></span>
<span id="cb7-5">C<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">diag</span>(m)<span class="sc" style="color: #5E5E5E;">-</span>J</span>
<span id="cb7-6"><span class="co" style="color: #5E5E5E;"># Covariance</span></span>
<span id="cb7-7">(<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span>(m<span class="dv" style="color: #AD0000;">-1</span>))<span class="sc" style="color: #5E5E5E;">*</span><span class="fu" style="color: #4758AB;">t</span>(X)<span class="sc" style="color: #5E5E5E;">%*%</span>C<span class="sc" style="color: #5E5E5E;">%*%</span>X</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>             mpg         cyl        disp          hp         drat          wt
mpg    36.324103  -9.1723790  -633.09721 -320.732056   2.19506351  -5.1166847
cyl    -9.172379   3.1895161   199.66028  101.931452  -0.66836694   1.3673710
disp -633.097208 199.6602823 15360.79983 6721.158669 -47.06401915 107.6842040
hp   -320.732056 101.9314516  6721.15867 4700.866935 -16.45110887  44.1926613
drat    2.195064  -0.6683669   -47.06402  -16.451109   0.28588135  -0.3727207
wt     -5.116685   1.3673710   107.68420   44.192661  -0.37272073   0.9573790
qsec    4.509149  -1.8868548   -96.05168  -86.770081   0.08714073  -0.3054816
vs      2.017137  -0.7298387   -44.37762  -24.987903   0.11864919  -0.2736613
am      1.803931  -0.4657258   -36.56401   -8.320565   0.19015121  -0.3381048
gear    2.135685  -0.6491935   -50.80262   -6.358871   0.27598790  -0.4210806
carb   -5.363105   1.5201613    79.06875   83.036290  -0.07840726   0.6757903
             qsec           vs           am        gear        carb
mpg    4.50914919   2.01713710   1.80393145   2.1356855 -5.36310484
cyl   -1.88685484  -0.72983871  -0.46572581  -0.6491935  1.52016129
disp -96.05168145 -44.37762097 -36.56401210 -50.8026210 79.06875000
hp   -86.77008065 -24.98790323  -8.32056452  -6.3588710 83.03629032
drat   0.08714073   0.11864919   0.19015121   0.2759879 -0.07840726
wt    -0.30548161  -0.27366129  -0.33810484  -0.4210806  0.67579032
qsec   3.19316613   0.67056452  -0.20495968  -0.2804032 -1.89411290
vs     0.67056452   0.25403226   0.04233871   0.0766129 -0.46370968
am    -0.20495968   0.04233871   0.24899194   0.2923387  0.04637097
gear  -0.28040323   0.07661290   0.29233871   0.5443548  0.32661290
carb  -1.89411290  -0.46370968   0.04637097   0.3266129  2.60887097</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><span class="fu" style="color: #4758AB;">var</span>(X) <span class="co" style="color: #5E5E5E;">#=cov(X)</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>             mpg         cyl        disp          hp         drat          wt
mpg    36.324103  -9.1723790  -633.09721 -320.732056   2.19506351  -5.1166847
cyl    -9.172379   3.1895161   199.66028  101.931452  -0.66836694   1.3673710
disp -633.097208 199.6602823 15360.79983 6721.158669 -47.06401915 107.6842040
hp   -320.732056 101.9314516  6721.15867 4700.866935 -16.45110887  44.1926613
drat    2.195064  -0.6683669   -47.06402  -16.451109   0.28588135  -0.3727207
wt     -5.116685   1.3673710   107.68420   44.192661  -0.37272073   0.9573790
qsec    4.509149  -1.8868548   -96.05168  -86.770081   0.08714073  -0.3054816
vs      2.017137  -0.7298387   -44.37762  -24.987903   0.11864919  -0.2736613
am      1.803931  -0.4657258   -36.56401   -8.320565   0.19015121  -0.3381048
gear    2.135685  -0.6491935   -50.80262   -6.358871   0.27598790  -0.4210806
carb   -5.363105   1.5201613    79.06875   83.036290  -0.07840726   0.6757903
             qsec           vs           am        gear        carb
mpg    4.50914919   2.01713710   1.80393145   2.1356855 -5.36310484
cyl   -1.88685484  -0.72983871  -0.46572581  -0.6491935  1.52016129
disp -96.05168145 -44.37762097 -36.56401210 -50.8026210 79.06875000
hp   -86.77008065 -24.98790323  -8.32056452  -6.3588710 83.03629032
drat   0.08714073   0.11864919   0.19015121   0.2759879 -0.07840726
wt    -0.30548161  -0.27366129  -0.33810484  -0.4210806  0.67579032
qsec   3.19316613   0.67056452  -0.20495968  -0.2804032 -1.89411290
vs     0.67056452   0.25403226   0.04233871   0.0766129 -0.46370968
am    -0.20495968   0.04233871   0.24899194   0.2923387  0.04637097
gear  -0.28040323   0.07661290   0.29233871   0.5443548  0.32661290
carb  -1.89411290  -0.46370968   0.04637097   0.3266129  2.60887097</code></pre>
</div>
</div>
</section>
<section id="properties-7" class="level4" data-number="1.8.4">
<h4 data-number="1.8.4" class="anchored" data-anchor-id="properties-7"><span class="header-section-number">1.8.4</span> Properties</h4>
<ul>
<li>Symmetry</li>
<li>Diagonal Entries: The diagonal entries of the covariance matrix represent the variances of the individual variables.</li>
<li>Linearity: The covariance matrix exhibits linearity in the sense that the covariance of a linear combination of variables can be expressed as a linear combination of their covariances. Mathematically, if <img src="https://latex.codecogs.com/png.latex?a"> and <img src="https://latex.codecogs.com/png.latex?b"> are constants and <img src="https://latex.codecogs.com/png.latex?X">, <img src="https://latex.codecogs.com/png.latex?Y">, and <img src="https://latex.codecogs.com/png.latex?Z"> are random variables, then <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCov%7D(aX%20+%20bY,%20Z)%20=%20a%5Coperatorname%7BCov%7D(X,%20Z)%20+%20b%5Coperatorname%7BCov%7D(Y,%20Z)">.</li>
<li>Scale Invariance: The covariance between variables is invariant to changes in scale or units of measurement. For example, if variables are measured in different units or are on different scales, the covariance matrix will still be valid and informative for measuring the linear relationship between the variables.</li>
<li>Independence: If two variables <img src="https://latex.codecogs.com/png.latex?X_i"> and <img src="https://latex.codecogs.com/png.latex?X_j"> are independent, their covariance <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCov%7D(X_j,%20X_k)"> is zero.</li>
<li>Positive Semidefiniteness: The covariance matrix is positive semidefinite, which means that all of its eigenvalues are non-negative. This property ensures that the covariance matrix is a valid variance-covariance matrix.</li>
</ul>
</section>
</section>
<section id="positive-definite-matrix" class="level3" data-number="1.9">
<h3 data-number="1.9" class="anchored" data-anchor-id="positive-definite-matrix"><span class="header-section-number">1.9</span> Positive Definite Matrix</h3>
<p>A symmetric matrix <img src="https://latex.codecogs.com/png.latex?A"> is positive definite if and only if the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> is positive for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>To see why this is true, consider the eigenvalue decomposition of <img src="https://latex.codecogs.com/png.latex?A">, which can be written as <img src="https://latex.codecogs.com/png.latex?A%20=%20Q%20%5CLambda%20Q%5ET">, where <img src="https://latex.codecogs.com/png.latex?Q"> is an orthogonal matrix and <img src="https://latex.codecogs.com/png.latex?%5CLambda"> is a diagonal matrix containing the eigenvalues of <img src="https://latex.codecogs.com/png.latex?A">. Then, for any nonzero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbf%20x%5ET%20%5Cmathbf%20A%20%5Cmathbf%20x&amp;=%5Cmathbf%20x%5ET%20%5Cmathbf%20Q%20%5Cmathbf%20%5CLambda%20%5Cmathbf%20Q%5ET%20%5Cmathbf%20x%5C%5C%0A&amp;=(%5Cmathbf%20x%5ET%20%5Cmathbf%20Q)%5Cmathbf%20%5CLambda%20(%20%5Cmathbf%20Q%5ET%5Cmathbf%20x)%5C%5C%0A&amp;=%5Csum_%7Bi=1%7D%5E%7Bn%7D%20%5Clambda_iy_i%5E2%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?y_i%20=%20(%5Cmathbf%7Bx%7D%5ET%20Q)_i"> is the <img src="https://latex.codecogs.com/png.latex?i">th coordinate (i.e., a scalar value that represents the position of a point or a vector relative to a chosen basis) of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20Q"> and <img src="https://latex.codecogs.com/png.latex?n"> is the dimension of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?A">. Note that since <img src="https://latex.codecogs.com/png.latex?Q"> is orthogonal, we have <img src="https://latex.codecogs.com/png.latex?Q%5ET%20Q%20=%20I">, so <img src="https://latex.codecogs.com/png.latex?y_i%20=%20%5Cmathbf%7Bq%7D_i%5ET%20%5Cmathbf%7Bx%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bq%7D_i"> is the <img src="https://latex.codecogs.com/png.latex?i">th column of <img src="https://latex.codecogs.com/png.latex?Q">. Therefore, the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)%20=%20%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> can be written in terms of the eigenvalues of <img src="https://latex.codecogs.com/png.latex?A"> and the coordinates of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> with respect to the eigenvectors of <img src="https://latex.codecogs.com/png.latex?A">.</p>
<p>Since <img src="https://latex.codecogs.com/png.latex?A"> is positive definite, we have <img src="https://latex.codecogs.com/png.latex?%5Clambda_i%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?i">, and so <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5En%20%5Clambda_i%20y_i%5E2%20%3E%200"> for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">. Therefore, the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> is positive for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, which implies that <img src="https://latex.codecogs.com/png.latex?A"> is positive definite.</p>
<p>In other words, the positive definiteness of a symmetric matrix <img src="https://latex.codecogs.com/png.latex?A"> is equivalent to the positivity of the associated quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>Therefore, a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is said to be positive definite if all of its eigenvalues are positive or equivalently, a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is positive definite if left-multiplying and right-multiplying it by the same vector, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x"> always gives a positive number if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x%5ET%20%5Cmathbf%20A%20%5Cmathbf%20x"></p>
<section id="properties-8" class="level4" data-number="1.9.1">
<h4 data-number="1.9.1" class="anchored" data-anchor-id="properties-8"><span class="header-section-number">1.9.1</span> Properties</h4>
<p>Properties of a positive definite matrix:</p>
<ul>
<li>All eigenvalues are positive.</li>
<li>The matrix is symmetric.</li>
<li>All principal submatrices have determinants that are positive.</li>
</ul>
<p>Example of a positive definite matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bpmatrix%7D%0A4%20&amp;%20-1%20&amp;%200%20%5C%5C%0A-1%20&amp;%205%20&amp;%20-1%20%5C%5C%0A0%20&amp;%20-1%20&amp;%202%0A%5Cend%7Bpmatrix%7D%0A%5Cend%7Bequation%7D%0A"></p>


</section>
</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/03.basic_special_matrix.html</guid>
  <pubDate>Thu, 30 Mar 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Basics (4) - Tensor</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/04.basic_tensor.html</link>
  <description><![CDATA[ 



<section id="tensor" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="tensor"><span class="header-section-number">1</span> Tensor</h2>
<p>A tensor is a mathematical object that generalizes vectors and matrices to higher dimensions. A tensor of order <img src="https://latex.codecogs.com/png.latex?n"> is an object that can be represented by a multidimensional array of <img src="https://latex.codecogs.com/png.latex?n"> indices. Each index can take on a range of values, which determine the dimensionality of the tensor along that axis.</p>
<p>For example, a rank-2 tensor (i.e., a matrix) can be represented as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A_%7Bij%7D%20%5Ctext%7B%20,%20%7D%20i=1,%5Cdots%20m%20%5Ctext%7B,%20%7D%20j=1,%5Cdots,n%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is the tensor, <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j"> are the indices, and <img src="https://latex.codecogs.com/png.latex?m"> and <img src="https://latex.codecogs.com/png.latex?n"> are the dimensions of the tensor along each axis. The entries of the tensor are given by <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D">.</p>
<p>A rank-3 tensor can be represented as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A_%7Bijk%7D%20%5Ctext%7B%20,%20%7D%20i=1,%5Cdots%20m%20%5Ctext%7B,%20%7D%20j=1,%5Cdots,n%5Ctext%7B,%20%7D%20k=1,%5Cdots,p%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is the tensor, <img src="https://latex.codecogs.com/png.latex?i">, <img src="https://latex.codecogs.com/png.latex?j">, and <img src="https://latex.codecogs.com/png.latex?k"> are the indices, and <img src="https://latex.codecogs.com/png.latex?m">, <img src="https://latex.codecogs.com/png.latex?n">, and <img src="https://latex.codecogs.com/png.latex?p"> are the dimensions of the tensor along each axis. The entries of the tensor are given by <img src="https://latex.codecogs.com/png.latex?A_%7Bijk%7D">.</p>
</section>
<section id="basic-tensor-operations" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="basic-tensor-operations"><span class="header-section-number">2</span> Basic Tensor Operations</h2>
<section id="section" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="section"><span class="header-section-number">2.1</span> </h3>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/04.basic_tensor.html</guid>
  <pubDate>Thu, 30 Mar 2023 15:00:00 GMT</pubDate>
</item>
</channel>
</rss>
