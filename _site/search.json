[
  {
    "objectID": "about.html#enthusiastic-data-scientist",
    "href": "about.html#enthusiastic-data-scientist",
    "title": "Kwangmin Kim",
    "section": "Enthusiastic Data Scientist",
    "text": "Enthusiastic Data Scientist\n\nInterests\nData Modeling, Statistics, Machine Learning, Deep Learning, Optimization"
  },
  {
    "objectID": "docs/blog/index.html",
    "href": "docs/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Source: General Principles of Software Validation\n\n\n\n\nPublic Health\n\n\n\n\nThe purpose of this article is to help understand the summary of the ‘General Principles of the ’Software Validation; Final Guidance for Industry and FDA Staff’ document issued on 2002-01-11. This article provides short sentences with many diagraams for intuitive understanding.\n\n\n\n\n\n\nDec 28, 2022\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\n\nStatistical Hypothesis Test\n\n\n\n\nStatistics\n\n\n\n\np-value is one of the most commonly used statistcal index to show significance level of a hypothesis testing result of your experiment.\n\n\n\n\n\n\nDec 15, 2022\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\n\nDcoument: General Principles of Software Validation\n\n\n\n\nPublic Health\n\n\n\n\nThe purpose of this blog is to get a rough concept of the FDA approval process by making a summary of the ‘General Principles of the ’Software Validation; Final Guidance for Industry and FDA Staff’ document issued on 2002-01-11. So far, the document seems to be still valid taking into account that its guidance for the FDA approval are broad, general, and comprehensive, and that many recent FDA documents supplement it.\n\n\n\n\n\n\nDec 15, 2022\n\n\nKwangmin Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/blog/posts/2022-12-08-P-value/index.html",
    "href": "docs/blog/posts/2022-12-08-P-value/index.html",
    "title": "p-values",
    "section": "",
    "text": "It is said to be statistically significant if a result of your experiment is more extreme than one that is produced by chance. (Try thinking that your result could have come from a different distribution from the one under the null hypothesis.)"
  },
  {
    "objectID": "docs/blog/posts/2022-12-08-P-value/index.html#p-value-good-vs-bad",
    "href": "docs/blog/posts/2022-12-08-P-value/index.html#p-value-good-vs-bad",
    "title": "p-values",
    "section": "p-value: Good vs Bad?",
    "text": "p-value: Good vs Bad?\n\nGoodness\np-value is an efficient and effective statistical index when to measure the significance of your test result. Let’s make an assumption that you have conducted a regression analysis. Then, you can get beta coefficients and their standard errors as results of your regression model.\n\nNumber of Cases of How You Interpret Regresssion Result\n\n\n\nhigh Standard Error\nlow Standard Error\n\n\n\n\nhigh \\(\\beta\\)\nUnclear Interpretation\nOK\n\n\nlow \\(\\beta\\)\nOK\nUnclear Interpretation\n\n\n\nThe above table shows the number of cases you can interprete the results of your regression model. There are 4 cases for each coefficient \\(\\beta\\).\n\nhigh \\(\\beta\\) and high Standard Error mean that the corresponding variable has a strong effect but its effect may be fluctuated, so the \\(\\beta\\) coefficient resulted from your regression model is likely to be not significant. We are not sure that its effect is statistically significant.\nhigh \\(\\beta\\) and low Standard Error mean that the corresponding variable has a strong effect, and its variation is small, so the \\(\\beta\\) coefficient resulted from your regression model is likely to be significant.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, its effect has a high variation. So, we can clearly interprete the variable with the \\(\\beta\\) as a variable that is not significantly associated with your response variable.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, but its effect has a low variation. So, it is difficult to conclude that the variable is significant.\n\nThe p-value could be used to provide a clearer interpretation of the unclear situation (i.e. (high \\(\\beta\\), high Standard Error), (low \\(\\beta\\), high Standard Error) ) by looking at the ratio of the estimated value of a parameter(= \\(\\beta\\)) to its standard error on the distribution under the null hypothesis. By general convention, the cut-off of p-value indicating statistical signficance is 0.05.\n\n\nBadness\nDespite the goodness of p-value, it is controversial to make a decision based solely on the p-value. As mentioned above, p-value is the probability that the result of your experiment is due to chance. In addition, looking into \\(\\frac{\\beta}{\\frac{s.e}{\\sqrt{n}}}\\), the p-value gets smaller as the sample size becomes larger and larger. It should be avoided that something is proved just because a low p-value is calucated.\nEven if a result is statistically significant, that does not necessarily mean it has real significance. A small difference that has no practical meaning can be statistically significant if the sample size is large enough. It is because large samples ensure that meaningless effects can become big enough to possibly exclude chance due to simple math.\nThe American Statistical Association (ASA) has released a statement of six principles for researchers and journal editors on p-values:\nSource: ASA Statement on Statistical Significance and p-values\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nProper inference requires full reporting and transparency.\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis."
  },
  {
    "objectID": "docs/blog/posts/2022-12-08-P-value/index.html#how-to-use-p-vlaues",
    "href": "docs/blog/posts/2022-12-08-P-value/index.html#how-to-use-p-vlaues",
    "title": "p-values",
    "section": "How to use p-vlaues?",
    "text": "How to use p-vlaues?\nPersonally, I make use of p-values as a tool in data science to just check whether a model result or a set of variables that appears interesting and useful is in the range of normal variability by chance in the exploratory data analysis(EDA) or data mining step.\nIf you want to get a statistical significance level through p-values, other methodologies could help increase the accuracy of real significance such as permuted p-values, q-values, and penalization on multiple comparison tests"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/index.html",
    "href": "docs/blog/posts/2022-12-10-FDA/index.html",
    "title": "FDA Software Validation Guidance Summary",
    "section": "",
    "text": "I am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto.\nThe FDA validation guidance document is a bit difficult to understand because its explanations provides abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\n\n\n2022-12-28, download this article as PDF\n2022-12-28, summary with diagrams\n\n\n\n\nFDA: General Principles of Software Validation\n\n\n\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\n\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/index.html#purpose",
    "href": "docs/blog/posts/2022-12-10-FDA/index.html#purpose",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.1 Purpose",
    "text": "2.1 Purpose\nThe purpose is to make a sketch of general validation principle of the validation of medical device software or software used to design or develop."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/index.html#scope",
    "href": "docs/blog/posts/2022-12-10-FDA/index.html#scope",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.2 Scope",
    "text": "2.2 Scope\nThe scope of this guidance is broad. The important activities for the software validation include at least:\n\nplanning,\nverfication,\ntesting,\ntraceability, and\nconfiguration management.\n\nAll of the activities above should be\n\nintegrated\nbe able to describe software life cycle management and\nbe able to describe software risk management.\n\nThe software validation and verification activities should be focused into the entire software life cycle. (It does not necessarily mean that the activies must follow any technical models.)\nThe guidance is applicable to any software related to a regulated medical device and anyone who is employed in a bio or medical industry.\n\n2.2.1 The Least Burdensome Approach\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\n2.2.2 Regulatory Requirements for Software Validation\n\nSoftware validation: a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997. (See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\nSpecific requirements for validation of device software are found in 21 CFR §820.30(g). Other design controls, such as planning, input, verification, and reviews, are required for medical device software. (See 21 CFR §820.30.)\ncomputer systems used to create, modify, and maintain electronic records and to manage electronic signatures are also subject to the validation requirements. (See 21 CFR §11.10(a).)\n\n\n2.2.2.1 Objective\nThe objective of software validation is to ensure:\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\n2.2.2.2 What to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\n\n2.2.3 Quality System Regulation vs Pre-market Submissions\nThis document does not address any specific requirements but general ones. Specific issues should be addressed to\n\nthe Office of Device Evaluation (ODE),\nCenter for Devices and Radiological Health (CDRH)\nthe Office of Blood Research and Review,\nCenter for Biologics Evaluation and Research (CBER). See the references in Appendix A for applicable FDA guidance documents for pre-market submissions."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/index.html#context-for-software-validation",
    "href": "docs/blog/posts/2022-12-10-FDA/index.html#context-for-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.3 Context for Software Validation",
    "text": "2.3 Context for Software Validation\n\nValidation elements that FDA expects to do for the Quality System regulation, using the principles and tasks are listed in Sections 4 and 5.\nAdditional specific information is available from many of the references listed in Appendix A\n\n\n2.3.1 Definition and Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology between the medical device Quality System regulation and the software industry:\n\nrequirements,\nspecification,\nverification, and\nvalidation.\n\n\n\n2.3.1.1 Requirements and Specifications\nThe Quality System regulation states\n\nthat design input requirements must be documented and\nthat specified requirements must be verified\n\nBut, the regulation does not further clarify the distinction between the terms “requirement” and “specification.”\n\nRequirement\n\ncan be any need or expectation for a system or for its software.\nreflects the stated or implied needs of the customer: requirements may be\n\nmarket-based,\ncontractual,\nstatutory, or\nan organization’s internal requirements.\n\nvarious examples of requirements\n\ndesign, functional, implementation, interface, performance, or physical requirements\n\nSoftware requirements derived from the system requirements for those aspects of system functionality\nSoftware requirements are typically stated in functional terms and are defined, refined, and updated as a development project progresses.\nSuccess in accurately and completely documenting software requirements is a crucial factor in successful validation of the resulting software.\n\nSpecification\n\ndefined as “a document that states requirements.” (See 21 CFR §820.3(y).)\nIt may refer to or include drawings, patterns, or other relevant documents\nIt usually indicates the means and the criteria whereby conformity with the requirement can be checked.\nVarious examples of written specifications\n\nsystem requirements specification,\nsoftware requirements specification,\nsoftware design specification,\nsoftware test specification,\nsoftware integration specification, etc.\n\nAll of these documents are design outputs for which various forms of verification are necessary.\n\n\n\n\n2.3.1.2 Verifiaction and Validation\nThe Quality System regulation is harmonized with ISO 8402:1994, which treats “verification” and “validation” as separate and distinct terms.\n\nSoftware verification\n\nIt provides objective evidence that the design outputs of a particular phase of the software development life cycle meet all of the specified requirements for that phase.\nIt looks for\n\nconsistency,\ncompleteness, and\ncorrectness of the software and its supporting documentation\n\nSoftware testing\n\nverification activities intended to confirm that software development output meets its input requirements.\n\nTypes of verification activities include\n\nvarious static and dynamic analyses,\ncode and document inspections,\nwalkthroughs, and other techniques.\n\n\nSoftware Validation\n\nConfirmation by examination and provision of the following objective evidence:\nEvidence 1: software specifications conform to user needs and intended uses, and\nEvidnece 2: the particular requirements implemented through software can be consistently fulfilled.\nEvidnece 3: all software requirements have been implemented correctly and completely and are traceable to system requirements.\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device.\n\nDifficulty in Software verification and validation\n\na developer cannot test forever, and\nit is difficult to know how much evidence is enough.\nIn large measure, software validation is a matter of developing a “level of confidence” that the device meets all requirements and user expectations for the software automated functions and features of the device.\nConsiderations for an acceptable level of confidence\n\nmeasures such as defects found in specifications documents,\nestimates of defects remaining,\ntesting coverage, and other techniques are all used to develop before shipping the product.\nHowever, a level of confidence varies depending upon the safety risk (hazard) posed by the automated functions of the device. (Info on safety risk is found in Section 4 and in the international standards ISO/IEC 14971-1 and IEC 60601-1-4 referenced in Appendix A).\n\n\n\n\n\n2.3.1.3 IQ/OQ/PQ\nIQ/OQ/PQ are the terminology related to user site software validation\n\nInstallation qualification (IQ)\nOperational qualification (OQ)\nPerformance qualification (PQ).\n\nDefinitions of these terms may be found in FDA’s Guideline on General Principles of Process Validation, dated May 11, 1987, and in FDA’s Glossary of Computerized System and Software Development Terminology, dated August 1995. Both FDA personnel and device manufacturers need to be aware of these differences in terminology as they ask for and provide information regarding software validation.\n\n\n\n2.3.2 Software Development as Part of System Design\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nthe user’s needs\nintended uses from which the product is developed.\n\nA primary goal of software validation is to then demonstrate that all completed software products comply with all documented software and system requirements.\n\n\n2.3.3 Software Is Different from Hardware\nSoftware engineering needs an even greater level of managerial scrutiny and control than does hardware engineering.\n\n\n2.3.4 Benefits of Software Validation\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nSoftware validation can also reduce long term costs by making it easier and less costly to reliably modify software and revalidate software changes.\n\n\n\n2.3.5 Design Review\nDesign reviews are documented, comprehensive, and systematic examinations of a design to evaluate\n\nthe adequacy of the design requirements,\nthe capability of the design to meet these requirements, and\nto identify problems.\n\nDesign review is a primary tool for managing and evaluating development projects.\n\nIt is strongly recommended that it should be formal design because it is more structured than the informal one.\nIt includes participation from others outside the development team.\nIt may review reference or include results from other formal and informal reviews.\nDesign reviews should include\n\nexamination of development plans,\nrequirements specifications,\ndesign specifications,\ntesting plans and procedures,\nall other documents and activities associated with the project,\nverification results from each stage of the defined life cycle, and\nvalidation results for the overall device.\n\nThe Quality System regulation requires that at least one formal design review be conducted during the device design process. However, it is recommended that multiple design reviews be conducted\n\n(e.g., at the end of each software life cycle activity, in preparation for proceeding to the next activity).\n\nFormal design reviews documented should include:\n\nthe appropriate tasks and expected results, outputs, or products been established for each software life cycle activity\ncorrectness, completeness, consistency, and accuracy\nsatisfaction for the standards, practices, and conventions of that activity\nestablishment of a proper basis for initiating tasks for the next software life cycle activity"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/index.html#principles-of-software-validation",
    "href": "docs/blog/posts/2022-12-10-FDA/index.html#principles-of-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.4 Principles of Software Validation",
    "text": "2.4 Principles of Software Validation\n\n2.4.1 Requirements\nA documented software requirements specification provides a baseline for both validation and verification. The software validation process must include an established software requirements specification (Ref: 21 CFR 820.3(z) and (aa) and 820.30(f) and (g)).\n\n\n2.4.2 Defect Prevention\nIn order to establish that confidence, software developers should use a mixture of methods and techniques to prevent software errors and to detect software errors that do occur.\n\n\n2.4.3 Time and Effort\nPreparation for software validation should begin early, i.e., during design and development planning and design input. The final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\n2.4.4 Software Life Cycle\n\nSoftware validation takes place within the environment of an established software life cycle.\nThe software life cycle contains software engineering tasks and documentation necessary to support the software validation effort.\nspecific verification and validation tasks need to be appropriate for the intended use of the software\n\n\n\n2.4.5 Plans\n\nThe software validation process is defined and controlled through the use of a plan.\nThe software validation plan defines “what” is to be accomplished through the software validation effort.\nSoftware validation plans specify areas such as\n\nscope,\napproach,\nresources,\nschedules and the types and extent of activities,\ntasks, and\nwork items.\n\n\n\n\n2.4.6 Procedures\nThe software validation process is executed through the use of procedures. These procedures establish “how” to conduct the software validation effort. The procedures should identify the specific actions or sequence of actions that must be taken to complete individual validation activities, tasks, and work items.\n\n\n2.4.7 Software Validation After a Change\n\nDue to the complexity of software, a small local change may have a significant global system impact.\nIf a change exists in the software, the whole validation status of the software needs to be re-established.\nneed to determine the extent and impact of that change on the entire software system.\nthe software developer should then conduct an appropriate level of software regression testing to show that unchanged but vulnerable portions of the system have not been adversely affected.\n\n\n\n2.4.8 Validation Coverage\n\nValidation coverage should be based on the software’s complexity and safety risk.\nThe selection of validation activities, tasks, and work items should be commensurate with the complexity of the software design and the risk associated with the use of the software for the specified intended use.\n\n\n\n2.4.9 Independence of Review\n\nValidation activities should be based on the basic quality assurance precept of “independence of review.”\nSelf-validation is extremely difficult.\nWhen possible, an independent evaluation is always better (like a contracted third-party independent verification and validation)\nAnother approach is to assign internal staff members that are not involved in a particular design or its implementation, but who have sufficient knowledge to evaluate the project and conduct the verification and validation activities.\n\n\n\n2.4.10 Flexibility and Responsibility\nThe device manufacturer has flexibility in choosing how to apply these validation principles, but retains ultimate responsibility for demonstrating that the software has been validated. FDA regulated medical device applications include software that:\n\nIs a component, part, or accessory of a medical device;\n\ncomponents: e.g., application software, operating systems, compilers, debuggers, configuration management tools, and many more\n\nIs itself a medical device; or\nIs used in manufacturing, design and development, or other parts of the quality system.\nNo matter how complex and disperse the software is, the manufacturer is in charge of responsibility for software validation."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/index.html#activities-and-tasks",
    "href": "docs/blog/posts/2022-12-10-FDA/index.html#activities-and-tasks",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.5 Activities and Tasks",
    "text": "2.5 Activities and Tasks\nSoftware validation is accomplished through a series of activities and tasks that are planned and executed at various stages of the software development life cycle. These tasks may be\n\none time occurrences\niterated many times\n\n\n2.5.1 Software Life Cycle Activities\n\nSoftware developers should establish a software life cycle model that is appropriate for their product and organization.\nThe selected software life cycle model should cover the software from its birth to its retirement.\nActivities in a typical software life cycle model:\n\nQuality Planning\nSystem Requirements Definition\nDetailed Software Requirements Specification\nSoftware Design Specification\nConstruction or Coding\nTesting\nInstallation\nOperation and Support\nMaintenance\nRetirement\n\nVerification, testing, and other tasks that support software validation occur during each of these activities.\nSeveral software life cycle models defined in FDA’s Glossary of Computerized System and Software Development\n\nTerminology dated August 1995:\n\nwaterfall\nspiral\nrapid prototyping\nincremental development, etc.\n\n\n\n2.5.2 Typical Tasks Supporting Validation\nthe software developer should at least consider each of the risk-related tasks and should define and document which tasks are or are not appropriate for their specific application.\n\n2.5.2.1 Quality Planning\nDesign and development planning should culminate in a plan that identifies\n\nnecessary tasks,\nprocedures for anomaly reporting and resolution,\nnecessary resources, and\nmanagement review requirements including formal design reviews.\n\nThe plan should include:\n\nThe specific tasks for each life cycle activity;\nEnumeration of important quality factors (e.g., reliability, maintainability, and usability);\nMethods and procedures for each task;\nTask acceptance criteria;\nCriteria for defining and documenting outputs in terms that will allow evaluation of their conformance to input requirements;\nInputs for each task;\nOutputs from each task;\nRoles, resources, and responsibilities for each task;\nRisks and assumptions; and\nDocumentation of user needs.\n\nThe plan should identify\n\nthe personnel,\nthe facility and equipment resources for each task, and\nthe role that risk (hazard) management will play.\n\nA configuration management plan should be developed that will guide and control multiple parallel development activities and ensure proper communications and documentation.\nControls are necessary to ensure positive and correct correspondence among all approved versions of the specifications documents, source code, object code, and test suites that comprise a software system. The controls also should ensure accurate identification of, and access to, the currently approved versions.\nProcedures should be created for reporting and resolving software anomalies found through validation or other activities.\nManagement should identify the reports and specify the contents, format, and responsible organizational elements for each report. Procedures also are necessary for the review and approval of software development results, including the responsible organizational elements for such reviews and approvals.\nTypical Tasks – Quality Planning\n\nRisk (Hazard) Management Plan\nConfiguration Management Plan\nSoftware Quality Assurance Plan\n\nSoftware Verification and Validation Plan\n\nVerification and Validation Tasks, and Acceptance Criteria\nSchedule and Resource Allocation (for software verification and validation activities)\nReporting Requirements\n\nFormal Design Review Requirements\nOther Technical Review Requirements\n\nProblem Reporting and Resolution Procedures\nOther Support Activities\n\n\n\n2.5.2.2 Requirements\nRequirements development includes the\n\nidentification,\nanalysis, and\ndocumentation of information about the device and its intended use.\n\nAreas of special importance include allocation of system functions to\n\nhardware/software,\noperating conditions,\nuser characteristics,\npotential hazards, and\nanticipated tasks.\n\nIn addition, the requirements should state clearly the intended use of the software. It is not possible to validate software without predetermined and documented software requirements. Typical software requirements specify the following:\n\nAll software system inputs;\nAll software system outputs;\nAll functions that the software system will perform;\nAll performance requirements that the software will meet, (e.g., data throughput, reliability, and timing);\nThe definition of all external and user interfaces, as well as any internal software-to-system interfaces;\nHow users will interact with the system;\nWhat constitutes an error and how errors should be handled;\nRequired response times;\nThe intended operating environment for the software, if this is a design constraint (e.g., hardware platform, operating system);\nAll ranges, limits, defaults, and specific values that the software will accept; and\nAll safety related requirements, specifications, features, or functions that will be implemented in software.\n\nSoftware requirement specifications should identify clearly the potential hazards that can result from a software failure in the system as well as any safety requirements to be implemented in software.\nThe consequences of software failure should be evaluated, along with means of mitigating such failures (e.g., hardware mitigation, defensive programming, etc.).\nThe Quality System regulation requires a mechanism for addressing incomplete, ambiguous, or conflicting requirements. (See 21 CFR 820.30(c).) Each requirement (e.g., hardware, software, user, operator interface, and safety) identified in the software requirements specification should be evaluated for accuracy, completeness, consistency, testability, correctness, and clarity.\nFor example, software requirements should be evaluated to verify that:\n\nThere are no internal inconsistencies among requirements;\nAll of the performance requirements for the system have been spelled out;\nFault tolerance, safety, and security requirements are complete and correct;\nAllocation of software functions is accurate and complete;\nSoftware requirements are appropriate for the system hazards; and\nAll requirements are expressed in terms that are measurable or objectively verifiable.\n\nA software requirements traceability analysis should be conducted to trace software requirements to (and from) system requirements and to risk analysis results. In addition to any other analyses and documentation used to verify software requirements, a formal design review is recommended to confirm that requirements are fully specified and appropriate before extensive software design efforts begin. Requirements can be approved and released incrementally, but care should be taken that interactions and interfaces among software (and hardware) requirements are properly reviewed, analyzed, and controlled.\nTypical Tasks – Requirements\n\nPreliminary Risk Analysis\nTraceability Analysis\n\nSoftware Requirements to System Requirements (and vice versa)\nSoftware Requirements to Risk Analysis\n\nDescription of User Characteristics\nListing of Characteristics and Limitations of Primary and Secondary Memory\nSoftware Requirements Evaluation\nSoftware User Interface Requirements Analysis\nSystem Test Plan Generation\nAcceptance Test Plan Generation\nAmbiguity Review or Analysis\n\n\n\n2.5.2.3 Design\nIn the design process, the software requirements specification is translated into a logical and physical representation of the software to be implemented. The software design specification is a description of what the software should do and how it should do it. The design specification may contain both a high level summary of the design and detailed design information. Human factors engineering should be woven into\n\nthe entire design and development process,\nthe device design requirements,\nanalyses, and\ntests.\n\nDevice safety and usability issues should be considered when developing\n\nflowcharts,\nstate diagrams,\nprototyping tools, and\ntest plans.\n\nAlso, task and function analyses, risk analyses, prototype tests and reviews, and full usability tests should be performed. Participants from the user population should be included when applying these methodologies.\nThe software design specification should include:\n\nSoftware requirements specification, including predetermined criteria for acceptance of the software;\nSoftware risk analysis;\nDevelopment procedures and coding guidelines (or other programming procedures);\nSystems documentation (e.g., a narrative or a context diagram) that describes the systems context in which the program is intended to function, including the relationship of hardware, software, and the physical environment;\nHardware to be used;\nParameters to be measured or recorded;\nLogical structure (including control logic) and logical processing steps (e.g., algorithms);\nData structures and data flow diagrams;\nDefinitions of variables (control and data) and description of where they are used;\nError, alarm, and warning messages;\nSupporting software (e.g., operating systems, drivers, other application software);\nCommunication links (links among internal modules of the software, links with the supporting software, links with the hardware, and links with the user);\nSecurity measures (both physical and logical security); and\nAny additional constraints not identified in the above elements.\n\nThe first four of the elements noted above usually are separate pre-existing documents that are included by reference in the software design specification. Software requirements specification was discussed in the preceding section, as was software risk analysis.\nSoftware design evaluations criteria:\n\ncomplete,\ncorrect,\nconsistent,\nunambiguous,\nfeasible,\nmaintainable,\nanalyses of control flow,\ndata flow,\ncomplexity,\ntiming,\nsizing,\nmemory allocation,\ncriticality analysis, and many other aspects of the design\n\nAppropriate consideration of software architecture (e.g., modular structure) during design can reduce the magnitude of future validation efforts when software changes are needed.\nA traceability analysis should be conducted to verify that the software design implements all of the software requirements. As a technique for identifying where requirements are not sufficient, the traceability analysis should also verify that all aspects of the design are traceable to software requirements.\nAn analysis of communication links should be conducted to evaluate the proposed design with respect to hardware, user, and related software requirements. At the end of the software design activity, a Formal Design Review should be conducted to verify that the design is correct, consistent, complete, accurate, and testable, before moving to implement the design.\nSeveral versions of both the software requirement specification and the software design specification should be maintained. All approved versions should be archived and controlled in accordance with established configuration management procedures.\nTypical Tasks – Design\n\nUpdated Software Risk Analysis\nTraceability Analysis - Design Specification to Software Requirements (and vice versa)\nSoftware Design Evaluation\nDesign Communication Link Analysis\nModule Test Plan Generation\nIntegration Test Plan Generation\nTest Design Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.4 Construction or Coding\nSoftware may be constructed either by coding. Coding is the software activity where the detailed design specification is implemented as source code. It is the last stage in decomposition of the software requirements where module specifications are translated into a programming language.\nCoding usually involves the use of a high-level programming language, but may also entail the use of assembly language (or microcode) for time-critical operations.\nA source code traceability analysis is an important tool to verify that all code is linked to established specifications and established test procedures. A source code traceability analysis should be conducted and documented to verify that:\n\nEach element of the software design specification has been implemented in code;\nModules and functions implemented in code can be traced back to an element in the software design specification and to the risk analysis;\nTests for modules and functions can be traced back to an element in the software design specification and to the risk analysis; and\nTests for modules and functions can be traced to source code for the same modules and functions.\n\nTypical Tasks – Construction or Coding\n\nTraceability Analyses\n\nSource Code to Design Specification (and vice versa)\nTest Cases to Source Code and to Design Specification\n\nSource Code and Source Code Documentation Evaluation\nSource Code Interface Analysis\nTest Procedure and Test Case Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.5 Testing by the Software Developer\nSoftware testing entails running software products under known conditions with defined inputs and documented outcomes that can be compared to their predefined expectations. It is a time consuming, difficult, and imperfect activity.\nAs such, it requires early planning in order to be effective and efficient. Test plans and test cases should be created as early in the software development process as feasible.\nThey should identify\n\nthe schedules,\nenvironments,\nresources (personnel, tools, etc.),\nmethodologies,\ncases (inputs, procedures, outputs, expected results),\ndocumentation, and\nreporting criteria.\n\nDescriptions of categories of software and software testing effort appear in the literature\n\nNIST Special Publication 500-235, Structured Testing: A Testing Methodology Using the Cyclomatic Complexity Metric;\nNUREG/CR-6293, Verification and Validation Guidelines for High Integrity Systems; and\nIEEE Computer Society Press, Handbook of Software Reliability Engineering.\n\nTesting of all program functionality does not mean all of the program has been tested. Testing of all of a program’s code does not mean all necessary functionality is present in the program. Testing of all program functionality and all program code does not mean the program is 100% correct! Software testing that finds no errors should not be interpreted to mean that errors do not exist in the software product; it may mean the testing was superficial.\nAn essential element of a software test case is the expected result. It is the key detail that permits objective evaluation of the actual test result. This necessary testing information is obtained from the corresponding, predefined definition or specification.\nA software testing process should be based on principles that foster effective examinations of a software product. Applicable software testing tenets include:\n\nThe expected test outcome is predefined;\nA good test case has a high probability of exposing an error;\nA successful test is one that finds an error;\nThere is independence from coding;\nBoth application (user) and software (programming) expertise are employed;\nTesters use different tools from coders;\nExamining only the usual case is insufficient;\nTest documentation permits its reuse and an independent confirmation of the pass/fail status of a test outcome during subsequent review.\n\nCode-based testing is also known as structural testing or “white-box” testing. It identifies test cases based on knowledge obtained from the source code, detailed design specification, and other development documents. Structural testing can identify “dead” code that is never executed when the program is run. Structural testing is accomplished primarily with unit (module) level testing, but can be extended to other levels of software testing.\nThe level of structural testing can be evaluated using metrics that are designed to show what percentage of the software structure has been evaluated during structural testing. These metrics are typically referred to as “coverage” and are a measure of completeness with respect to test selection criteria. The amount of structural coverage should be commensurate with the level of risk posed by the software. Use of the term “coverage” usually means 100% coverage. Common structural coverage metrics include:\n\nStatement Coverage – This criteria requires sufficient test cases for each program statement to be executed at least once; however, its achievement is insufficient to provide confidence in a software product’s behavior.\nDecision (Branch) Coverage – This criteria requires sufficient test cases for each program decision or branch to be executed so that each possible outcome occurs at least once. It is considered to be a minimum level of coverage for most software products, but decision coverage alone is insufficient for high-integrity applications.\nCondition Coverage – This criteria requires sufficient test cases for each condition in a program decision to take on all possible outcomes at least once. It differs from branch coverage only when multiple conditions must be evaluated to reach a decision.\nMulti-Condition Coverage – This criteria requires sufficient test cases to exercise all possible combinations of conditions in a program decision.\nLoop Coverage – This criteria requires sufficient test cases for all program loops to be executed for zero, one, two, and many iterations covering initialization, typical running and termination (boundary) conditions.\nPath Coverage – This criteria requires sufficient test cases for each feasible path, basis path, etc., from start to exit of a defined program segment, to be executed at least once. Because of the very large number of possible paths through a software program, path coverage is generally not achievable. The amount of path coverage is normally established based on the risk or criticality of the software under test.\nData Flow Coverage – This criteria requires sufficient test cases for each feasible data flow to be executed at least once. A number of data flow testing strategies are available.\n\nThe following types of functional software testing involve generally increasing levels of effort:\n\nNormal Case – Testing with usual inputs is necessary. However, testing a software product only with expected, valid inputs does not thoroughly test that software product. By itself, normal case testing cannot provide sufficient confidence in the dependability of the software product.\nOutput Forcing – Choosing test inputs to ensure that selected (or all) software outputs are generated by testing.\nRobustness – Software testing should demonstrate that a software product behaves correctly when given unexpected, invalid inputs. Methods for identifying a sufficient set of such test cases include Equivalence Class Partitioning, Boundary Value Analysis, and Special Case Identification (Error Guessing). While important and necessary, these techniques do not ensure that all of the most appropriate challenges to a software product have been identified for testing.\nCombinations of Inputs – The functional testing methods identified above all emphasize individual or single test inputs. Most software products operate with multiple inputs under their conditions of use. Thorough software product testing should consider the combinations of inputs a software unit or system may encounter during operation. Error guessing can be extended to identify combinations of inputs, but it is an ad hoc technique. Cause-effect graphing is one functional software testing technique that systematically identifies combinations of inputs to a software product for inclusion in test cases.\n\nFunctional and structural software test case identification techniques provide specific inputs for testing, rather than random test inputs. One weakness of these techniques is the difficulty in linking structural and functional test completion criteria to a software product’s reliability.\nAdvanced software testing methods, such as statistical testing, can be employed to provide further assurance that a software product is dependable. Statistical testing uses randomly generated test data from defined distributions based on an operational profile (e.g., expected use, hazardous use, or malicious use of the software product). Large amounts of test data are generated and can be targeted to cover particular areas or concerns, providing an increased possibility of identifying individual and multiple rare operating conditions that were not anticipated by either the software product’s designers or its testers. Statistical testing also provides high structural coverage. It does require a stable software product. Thus, structural and functional testing are prerequisites for statistical testing of a software product.\nAnother aspect of software testing is the testing of software changes. Changes occur frequently during software development. These changes are the result of\n\ndebugging that finds an error and it is corrected,\nnew or changed requirements (“requirements creep”), and\nmodified designs as more effective or efficient implementations are found.\n\nOnce a software product has been baselined (approved), any change to that product should have its own “mini life cycle,” including testing. Testing of a changed software product requires additional effort. It should demonstrate\n\nthat the change was implemented correctly, and\nthat the change did not adversely impact other parts of the software product.\n\nRegression analysis is the determination of the impact of a change based on review of the relevant documentation in order to identify the necessary regression tests to be run. Regression testing is the rerunning of test cases that a program has previously executed correctly and comparing the current result to the previous result in order to detect unintended effects of a software change. Regression analysis and regression testing should also be employed when using integration methods to build a software product to ensure that newly integrated modules do not adversely impact the operation of previously integrated modules.\nIn order to provide a thorough and rigorous examination of a software product, development testing is typically organized into levels: unit, integration, and system levels of testing.\n\nUnit (module or component) level testing focuses on the early examination of sub-program functionality and ensures that functionality not visible at the system level is examined by testing. Unit testing ensures that quality software units are furnished for integration into the finished software product.\nIntegration level testing focuses on the transfer of data and control across a program’s internal and external interfaces. External interfaces are those with\n\nother software (including operating system software),\nsystem hardware, and\nthe users and can be described as communications links.\n\nSystem level testing demonstrates that all specified functionality exists and that the software product is trustworthy. This testing verifies the as-built program’s functionality and performance with respect to the requirements for the software product as exhibited on the specified operating platform(s). System level software testing addresses functional concerns and the following elements of a device’s software that are related to the intended use(s):\n\nPerformance issues (e.g., response times, reliability measurements);\nResponses to stress conditions, e.g., behavior under maximum load, continuous use;\nOperation of internal and external security features;\nEffectiveness of recovery procedures, including disaster recovery;\nUsability; (Usability vs Utility??)\nCompatibility with other software products;\nBehavior in each of the defined hardware configurations; and\nAccuracy of documentation.\n\n\nControl measures (e.g., a traceability analysis) should be used to ensure that the intended coverage is achieved.\nSystem level testing also exhibits the software product’s behavior in the intended operating environment. The location of such testing is dependent upon the software developer’s ability to produce the target operating environment(s). Depending upon the circumstances, simulation and/or testing at (potential) customer locations may be utilized.\nTest plans should identify the controls needed to ensure\n\nthat the intended coverage is achieved and\nthat proper documentation is prepared when planned system level testing is conducted at sites not directly controlled by the software developer.\n\nTest procedures, test data, and test results\n\nshould be documented in a manner permitting objective pass/fail decisions to be reached.\nshould also be suitable for review and objective decision making subsequent to running the test,\nshould be suitable for use in any subsequent regression testing.\n\nErrors detected during testing should be\n\nlogged,\nclassified,\nreviewed, and\nresolved prior to release of the software.\n\nSoftware error data that is collected and analyzed during a development life cycle may be used to determine the suitability of the software product for release for commercial distribution. Test reports should comply with the requirements of the corresponding test plans.\nSoftware testing tools are frequently used to ensure consistency, thoroughness, and efficiency in the testing of such software products and to fulfill the requirements of the planned testing activities.\nAppropriate documentation providing evidence of the validation of these software tools for their intended use should be maintained (see section 6 of this guidance).\nTypical Tasks – Testing by the Software Developer\n\nTest Planning\nStructural Test Case Identification\nFunctional Test Case Identification\nTraceability Analysis - Testing\nUnit (Module) Tests to Detailed Design\nIntegration Tests to High Level Design\nSystem Tests to Software Requirements\nUnit (Module) Test Execution\nIntegration Test Execution\nFunctional Test Execution\nSystem Test Execution\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.6 User Site Testing\nTesting at the user site is an essential part of software validation. The Quality System regulation requires\n\ninstallation and\ninspection procedures (including testing where appropriate) as well as\ndocumentation of inspection and\ntesting to demonstrate proper installation. (See 21 CFR §820.170.)\n\nLikewise, manufacturing equipment must meet specified requirements, and automated systems must be validated for their intended use. (See 21 CFR §820.70(g) and 21 CFR §820.70(i) respectively.)\nTerminology regarding user site testing can be confusing. Terms such as\n\nbeta test,\nsite validation,\nuser acceptance test,\ninstallation verification, and\ninstallation testing have all been used to describe user site testing.\n\nFor the purposes of this guidance, the term “user site testing” encompasses all of these and any other testing that takes place outside of the developer’s controlled environment.\nThis testing should take place at a user’s site with the actual hardware and software that will be part of the installed system configuration. The testing is accomplished through either actual or simulated use of the software being tested within the context in which it is intended to function.\nTest planners should check with the FDA Center(s) with the corresponding product jurisdiction to determine whether there are any additional regulatory requirements for user site testing.\nUser site testing should follow a pre-defined written plan with\n\na formal summary of testing and\na record of formal acceptance.\n\nThe following documented evidence should be retained:\n\nall testing procedures,\ntest input data, and\ntest results\n\nThere should be evidence that hardware and software are installed and configured as specified. Measures should ensure that all system components are exercised during the testing and that the versions of these components are those specified. The testing plan should specify testing throughout the full range of operating conditions and should specify continuation for a sufficient time to allow the system to encounter a wide spectrum of conditions and events in an effort to detect any latent faults that are not apparent during more normal activities.\nSome of the evaluations of the system’s ability that have been performed earlier by the software developer at the developer’s site should be repeated at the site of actual use. These may include tests for:\n\na high volume of data,\nheavy loads or stresses,\nsecurity,\nfault testing (avoidance, detection, tolerance, and recovery),\nerror messages, and\nimplementation of safety requirements.\n\nThere should be an evaluation of the ability of the users of the system to understand and correctly interface with it.\nOperators should be able to perform the intended functions and respond in an appropriate and timely manner to all alarms, warnings, and error messages.\nRecords should be maintained of both proper system performance and any system failures that are encountered.\nThe revision of the system to compensate for faults detected during this user site testing should follow the same procedures and controls as for any other software change.\nThe developers of the software may or may not be involved in the user site testing.\n\nIf the developers are involved, they may seamlessly carry over to the user’s site the last portions of design-level systems testing.\nIf the developers are not involved, it is all the more important that the user have persons who understand the importance of careful test planning, the definition of expected test results, and the recording of all test outputs.\n\nTypical Tasks – User Site Testing\n\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.7 Maintenance and Software Changes\n\n2.5.2.7.1 Hardware vs Software\nHardware maintenance typically includes\n\npreventive hardware maintenance actions,\ncomponent replacement, and\ncorrective changes.\n\nSoftware maintenance includes\n\ncorrective,\nperfective, and\nadaptive maintenance\nbut does not include preventive maintenance actions or software component replacement.\n\n\n\n2.5.2.7.2 Maintenance Types\n\nCorrective maintenance: Changes made to correct errors and faults in the software.\nPerfective maintenance: Changes made to the software to improve the performance, maintainability, or other attributes of the software system .\nAdaptive maintenance: Changes to make the software system usable in a changed environment.\n\nSufficient regression analysis and testing should be conducted to demonstrate that portions of the software not involved in the change were not adversely impacted. When changes are made to a software system,\n\neither during initial development or\nduring post release maintenance,\n\nThis is in addition to testing that evaluates the correctness of the implemented change(s). The specific validation effort necessary for each software change is determined by\n\nthe type of change,\nthe development products affected, and the\nimpact of those products on the operation of the software.\n\n\n\n2.5.2.7.3 Factors of Limitting Validation Effort Needed When a Change Is Made\n\ncareful and complete documentation of the design structure and\ncareful and complete documentation of interrelationships of various modules,\ninterfaces, etc.\nFor example,\n\ntest documentation,\ntest cases, and\nresults of previous verification and validation testing All of them need to be archived if they are to be available for performing subsequent regression testing.\n\n\nThe following additional maintenance tasks should be addressed:\n\nSoftware Validation Plan Revision - For software that was previously validated, the existing software validation plan should be revised to support the validation of the revised software. If no previous software validation plan exists, such a plan should be established to support the validation of the revised software.\nAnomaly Evaluation – Software organizations frequently maintain documentation, such as software problem reports that describe software anomalies discovered and the specific corrective action taken to fix each anomaly.\n\nToo often, however, mistakes are repeated because software developers do not take the next step to determine the root causes of problems and make the process and procedural changes needed to avoid recurrence of the problem.\nSoftware anomalies should be evaluated in terms of their severity and their effects on system operation and safety,\nbut they should also be treated as symptoms of process deficiencies in the quality system.\nA root cause analysis of anomalies can identify specific quality system deficiencies.\nWhere trends are identified (e.g., recurrence of similar software anomalies), appropriate corrective and preventive actions must be implemented and documented to avoid further recurrence of similar quality problems. (See 21 CFR 820.100.)\n\nProblem Identification and Resolution Tracking - All problems discovered during maintenance of the software should be documented. The resolution of each problem should be tracked to ensure it is fixed, for historical reference, and for trending.\nProposed Change Assessment - All proposed modifications, enhancements, or additions should be assessed to determine the effect each change would have on the system. This information should determine the extent to which verification and/or validation tasks need to be iterated.\nTask Iteration - For approved software changes, all necessary verification and validation tasks should be performed to ensure that planned changes are implemented correctly, all documentation is complete and up to date, and no unacceptable changes have occurred in software performance.\nDocumentation Updating – Documentation should be carefully reviewed to determine which documents have been impacted by a change. All approved documents (e.g., specifications, test procedures, user manuals, etc.) that have been affected should be updated in accordance with configuration management procedures. Specifications should be updated before any maintenance and software changes are made."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "href": "docs/blog/posts/2022-12-10-FDA/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.6 Validation of Automated Process Equipment and Quality System Software",
    "text": "2.6 Validation of Automated Process Equipment and Quality System Software\nThe Quality System regulation requires that “when computers or automated data processing systems are used as part of production or the quality system, the [device] manufacturer shall validate computer software for its intended use according to an established protocol.” (See 21 CFR §820.70(i)). This has been a regulatory requirement of FDA’s medical device Good Manufacturing Practice (GMP) regulations since 1978.\nComputer systems that implement part of a device manufacturer’s production processes or quality system (or that are used to create and maintain records required by any other FDA regulation) are subject to the Electronic Records; Electronic Signatures regulation. (See 21 CFR Part 11.) This regulation establishes additional security, data integrity, and validation requirements when records are created or maintained electronically. These additional Part 11 requirements should be carefully considered and included in system requirements and software requirements for any automated record keeping systems. System validation and software validation should demonstrate that all Part 11 requirements have been met.\nComputers and automated equipment are used extensively throughout all aspects of\n\nmedical device design,\nlaboratory testing and analysis,\nproduct inspection and acceptance,\nproduction and process control,\nenvironmental controls,\npackaging,\nlabeling,\ntraceability,\ndocument control,\ncomplaint management, and many other aspects of the quality system.\n\nIncreasingly, automated plant floor operations can involve extensive use of embedded systems in:\n\nprogrammable logic controllers;\ndigital function controllers;\nstatistical process control;\nsupervisory control and data acquisition;\nrobotics;\nhuman-machine interfaces;\ninput/output devices; and\ncomputer operating systems.\n\nAll software tools used for software design are subject to the requirement for software validation, but the validation approach used for each application can vary widely.\nValidation is typically supported by:\n\nverifications of the outputs from each stage of that software development life cycle; and\nchecking for proper operation of the finished software in the device manufacturer’s intended use environment.\n\n\n2.6.1 How Much Validation Evidence Is Needed?\nThe level of validation effort should be commensurate with\n\nthe risk posed by the automated operation,\nthe complexity of the process software,\nthe degree to which the device manufacturer is dependent upon that automated process to produce a safe and effective device\n\nDocumented requirements and risk analysis of the automated process help to define the scope of the evidence needed to show that the software is validated for its intended use. Without a plan, extensive testing may be needed for:\n\na plant-wide electronic record and electronic signature system;\nan automated controller for a sterilization cycle; or\nautomated test equipment used for inspection and acceptance of finished circuit boards in a lifesustaining / life-supporting device.\n\nHigh risk applications should not be running in the same operating environment with non-validated software functions, even if those software functions are not used. Risk mitigation techniques such as memory partitioning or other approaches to resource protection may need to be considered when high risk applications and lower risk applications are to be used in the same operating environment.\nWhen software is upgraded or any changes are made to the software, the device manufacturer should consider how those changes may impact the “used portions” of the software and must reconfirm the validation of those portions of the software that are used. (See 21 CFR §820.70(i).)\n\n\n2.6.2 Defined User Equipment\nA very important key to software validation is a documented user requirements specification that defines:\n\nthe “intended use” of the software or automated equipment; and\nthe extent to which the device manufacturer is dependent upon that software or equipment for production of a quality medical device.\n\nThe device manufacturer (user) needs to define the expected operating environment including any required hardware and software configurations, software versions, utilities, etc. The user also needs to:\n\ndocument requirements for system performance, quality, error handling, startup, shutdown, security, etc.;\nidentify any safety related functions or features, such as sensors, alarms, interlocks, logical processing steps, or command sequences; and\ndefine objective criteria for determining acceptable performance.\n\nThe validation must be conducted in accordance with a documented protocol, and the validation results must also be documented. (See 21 CFR §820.70(i).) Test cases should be documented that will exercise the system to challenge its performance against the pre-determined criteria, especially for its most critical parameters.\nTest cases should address\n\nerror and alarm conditions,\nstartup, shutdown,\nall applicable user functions and operator controls,\npotential operator errors,\nmaximum and minimum ranges of allowed values, and\nstress conditions applicable to the intended use of the equipment.\n\nThe test cases should be executed and the results should be recorded and evaluated to determine whether the results support a conclusion that the software is validated for its intended use.\nA device manufacturer may conduct a validation using their own personnel or may depend on a third party such as the equipment/software vendor or a consultant. In any case, the device manufacturer retains the ultimate responsibility for ensuring that the production and quality system software:\n\nis validated according to a written procedure for the particular intended use; and\nwill perform as intended in the chosen application.\n\nThe device manufacturer should have documentation including:\n\ndefined user requirements;\nvalidation protocol used;\nacceptance criteria;\ntest cases and results; and\na validation summary that objectively confirms that the software is validated for its intended use.\n\n\n\n2.6.3 Validation of Off-The-Shelf Software and Automated Equipment\nMost of the automated equipment and systems used by device manufacturers are supplied by thirdparty vendors and are purchased off-the-shelf (OTS). The device manufacturer is responsible for ensuring that the product development methodologies used by the OTS software developer are appropriate and sufficient for the device manufacturer’s intended use of that OTS software.\nWhere possible and depending upon the device risk involved, the device manufacturer should consider auditing the vendor’s design and development methodologies used in the construction of the OTS software and should assess the development and validation documentation generated for the OTS software. Such audits can be conducted by the device manufacturer or by a qualified third party.\nThe audit should demonstrate that the vendor’s procedures for and results of the verification and validation activities performed the OTS software are appropriate and sufficient for the safety and effectiveness requirements of the medical device to be produced using that software."
  },
  {
    "objectID": "docs/CV/index.html",
    "href": "docs/CV/index.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "an application for a position in industry, non-profit, and public sector\nskills, work experience, educuation\nwithin 2 pages"
  },
  {
    "objectID": "docs/CV/index.html#cv",
    "href": "docs/CV/index.html#cv",
    "title": "Curriculum Vitae",
    "section": "CV",
    "text": "CV\n\napplying for positions in academia, fellowships and grants\nacademic accomplishments\nLength depends upon experience and includes a complete list of publications, posters, and presentations\nbegins with education and can include name of advisor and dissertation title or summary (see examples). Also used for merit/tenure review and sabbatical leave"
  },
  {
    "objectID": "docs/projects/high_dimension.html#data-preparation",
    "href": "docs/projects/high_dimension.html#data-preparation",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nPackage Loading and Option Settings"
  },
  {
    "objectID": "docs/projects/high_dimension.html#data-description",
    "href": "docs/projects/high_dimension.html#data-description",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Data Description",
    "text": "Data Description"
  },
  {
    "objectID": "docs/projects/high_dimension.html#architecture-of-analysis-pipeline",
    "href": "docs/projects/high_dimension.html#architecture-of-analysis-pipeline",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Architecture of Analysis Pipeline",
    "text": "Architecture of Analysis Pipeline"
  },
  {
    "objectID": "docs/projects/high_dimension.html#methods",
    "href": "docs/projects/high_dimension.html#methods",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Methods",
    "text": "Methods"
  },
  {
    "objectID": "docs/projects/high_dimension.html#results",
    "href": "docs/projects/high_dimension.html#results",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "docs/projects/high_dimension.html#conclusion",
    "href": "docs/projects/high_dimension.html#conclusion",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "docs/projects/high_dimension.html#bibliography",
    "href": "docs/projects/high_dimension.html#bibliography",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Bibliography",
    "text": "Bibliography"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "EnglishKorean\n\n\n\n\nI am passionate and self-taught in statistics, machine learning, deep learning, and programming using open-source tools such as Python, R, and SQL. It has been 7years since I dealth with data research from data modeling to data visualization through modeling.\n\n\n\nMy educational background is a Bachelor of Science in biochemistry in South Korea, Bachelor of Arts in mathematics in the USA, and Master of Sceince in biostatistics in the USA. I started my career in the medical area of analytics because I have a strong background in biology. My work experience exposed me to an environment where I had to interact with non-experts to make them understand data science.\n\n\n\n(Pursuing Goal in Data Science)\nAfter dealing with data and collaborating with SW developers and non-professional colleagues, I realized that the importance of communicating with them efficiently and effectively. This requires more knowledge in mathematics, statistics, and IT to master the art of explaining easily, clearly, and concisely. So, I started blogging to have an opportunity to have a detailed and systematic understanding of new techaniques of machine learning, as well as fundamental statistics.\n\n\n\n\n\n저는 R과 Python같은 open-source tool을 사용하여 통계, machine learning 및 deep learning을 독학하는 열정 가득한 data scientist입니다. 약 7년여 동안 data modeling, 통계적 모델링, machine learning 모델링 및 시각화를 통하여 data 관련 업무 경험을 쌓았습니다.\n\n\n\n저는 한국에서 학부과정으로 생화학과 미국에서 학부 과정으로 수학과 석사 과정으로 생물통계학을 전공했습니다. 저의 생화학 전공을 살려 Bio와 의료분야에서 커리어를 시작했고 그 과정에서 많은 data science 비전문가들과 협업하면서 통계 및 data science에 대하여 그들과 소통하는 법을 익혔습니다.\n\n\n\nData science에 대하여 비전문가들과 SW 개발자들과 협업을 하면서 그들과 효율적이고 효과적으로 소통하는것이 중요하다는 것을 깨달았습니다. 그 효과적인 소통이 수학, 통계 및 IT에 대한 지식으로부터 온다고 생각해서 새로운 기술에 대하여 세부적이고 체계적인 지식을 쌓기위해 블로깅을 시작했습니다."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/sw_validation.html",
    "href": "docs/blog/posts/2022-12-10-FDA/sw_validation.html",
    "title": "FDA Validation Document Summary",
    "section": "",
    "text": "FDA: General Principles of Software Validation\n\n\n\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\n\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#purpose",
    "href": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#purpose",
    "title": "FDA Validation Document Summary",
    "section": "Purpose",
    "text": "Purpose\nThe purpose is to make a sketch of general validation principle of the validation of medical device software or software used to design or develop."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#scope",
    "href": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#scope",
    "title": "FDA Validation Document Summary",
    "section": "Scope",
    "text": "Scope\nThe scope of this guidance is broad. The important activities for the software validation include at least:\n\nplanning,\nverfication,\ntesting,\ntraceability, and\nconfiguration management.\n\nAll of the activities above should be\n\nintegrated\nbe able to describe software life cycle management and\nbe able to describe software risk management.\n\nThe software validation and verification activities should be focused into the entire software life cycle. (It does not necessarily mean that the activies must follow any technical models.)\nThe guidance is applicable to any software related to a regulated medical device and anyone who is employed in a bio or medical industry.\n\nThe Least Burdensome Approach\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\nRegulatory Requirements for Software Validation\n\nSoftware validation: a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997. (See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\nSpecific requirements for validation of device software are found in 21 CFR §820.30(g). Other design controls, such as planning, input, verification, and reviews, are required for medical device software. (See 21 CFR §820.30.)\ncomputer systems used to create, modify, and maintain electronic records and to manage electronic signatures are also subject to the validation requirements. (See 21 CFR §11.10(a).)\n\n\nObjective\nThe objective of software validation is to ensure:\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\nWhat to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\n\nQuality System Regulation vs Pre-market Submissions\nThis document does not address any specific requirements but general ones. Specific issues should be addressed to\n\nthe Office of Device Evaluation (ODE),\nCenter for Devices and Radiological Health (CDRH)\nthe Office of Blood Research and Review,\nCenter for Biologics Evaluation and Research (CBER). See the references in Appendix A for applicable FDA guidance documents for pre-market submissions."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#context-for-software-validation",
    "href": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#context-for-software-validation",
    "title": "FDA Validation Document Summary",
    "section": "Context for Software Validation",
    "text": "Context for Software Validation\n\nValidation elements that FDA expects to do for the Quality System regulation, using the principles and tasks are listed in Sections 4 and 5.\nAdditional specific information is available from many of the references listed in Appendix A\n\n\nDefinition and Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology between the medical device Quality System regulation and the software industry:\n\nrequirements,\nspecification,\nverification, and\nvalidation.\n\n\n\nRequirements and Specifications\nThe Quality System regulation states\n\nthat design input requirements must be documented and\nthat specified requirements must be verified\n\nBut, the regulation does not further clarify the distinction between the terms “requirement” and “specification.”\n\nRequirement\n\ncan be any need or expectation for a system or for its software.\nreflects the stated or implied needs of the customer: requirements may be\n\nmarket-based,\ncontractual,\nstatutory, or\nan organization’s internal requirements.\n\nvarious examples of requirements\n\ndesign, functional, implementation, interface, performance, or physical requirements\n\nSoftware requirements derived from the system requirements for those aspects of system functionality\nSoftware requirements are typically stated in functional terms and are defined, refined, and updated as a development project progresses.\nSuccess in accurately and completely documenting software requirements is a crucial factor in successful validation of the resulting software.\n\nSpecification\n\ndefined as “a document that states requirements.” (See 21 CFR §820.3(y).)\nIt may refer to or include drawings, patterns, or other relevant documents\nIt usually indicates the means and the criteria whereby conformity with the requirement can be checked.\nVarious examples of written specifications\n\nsystem requirements specification,\nsoftware requirements specification,\nsoftware design specification,\nsoftware test specification,\nsoftware integration specification, etc.\n\nAll of these documents are design outputs for which various forms of verification are necessary.\n\n\n\n\nVerifiaction and Validation\nThe Quality System regulation is harmonized with ISO 8402:1994, which treats “verification” and “validation” as separate and distinct terms.\n\nSoftware verification\n\nIt provides objective evidence that the design outputs of a particular phase of the software development life cycle meet all of the specified requirements for that phase.\nIt looks for\n\nconsistency,\ncompleteness, and\ncorrectness of the software and its supporting documentation\n\nSoftware testing\n\nverification activities intended to confirm that software development output meets its input requirements.\n\nTypes of verification activities include\n\nvarious static and dynamic analyses,\ncode and document inspections,\nwalkthroughs, and other techniques.\n\n\nSoftware Validation\n\nConfirmation by examination and provision of the following objective evidence:\nEvidence 1: software specifications conform to user needs and intended uses, and\nEvidnece 2: the particular requirements implemented through software can be consistently fulfilled.\nEvidnece 3: all software requirements have been implemented correctly and completely and are traceable to system requirements.\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device.\n\nDifficulty in Software verification and validation\n\na developer cannot test forever, and\nit is difficult to know how much evidence is enough.\nIn large measure, software validation is a matter of developing a “level of confidence” that the device meets all requirements and user expectations for the software automated functions and features of the device.\nConsiderations for an acceptable level of confidence\n\nmeasures such as defects found in specifications documents,\nestimates of defects remaining,\ntesting coverage, and other techniques are all used to develop before shipping the product.\nHowever, a level of confidence varies depending upon the safety risk (hazard) posed by the automated functions of the device. (Info on safety risk is found in Section 4 and in the international standards ISO/IEC 14971-1 and IEC 60601-1-4 referenced in Appendix A).\n\n\n\n\n\nIQ/OQ/PQ\nIQ/OQ/PQ are the terminology related to user site software validation\n\nInstallation qualification (IQ)\nOperational qualification (OQ)\nPerformance qualification (PQ).\n\nDefinitions of these terms may be found in FDA’s Guideline on General Principles of Process Validation, dated May 11, 1987, and in FDA’s Glossary of Computerized System and Software Development Terminology, dated August 1995. Both FDA personnel and device manufacturers need to be aware of these differences in terminology as they ask for and provide information regarding software validation.\n\n\n\nSoftware Development as Part of System Design\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nthe user’s needs\nintended uses from which the product is developed.\n\nA primary goal of software validation is to then demonstrate that all completed software products comply with all documented software and system requirements.\n\n\nSoftware Is Different from Hardware\nSoftware engineering needs an even greater level of managerial scrutiny and control than does hardware engineering.\n\n\nBenefits of Software Validation\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nSoftware validation can also reduce long term costs by making it easier and less costly to reliably modify software and revalidate software changes.\n\n\n\nDesign Review\nDesign reviews are documented, comprehensive, and systematic examinations of a design to evaluate\n\nthe adequacy of the design requirements,\nthe capability of the design to meet these requirements, and\nto identify problems.\n\nDesign review is a primary tool for managing and evaluating development projects.\n\nIt is strongly recommended that it should be formal design because it is more structured than the informal one.\nIt includes participation from others outside the development team.\nIt may review reference or include results from other formal and informal reviews.\nDesign reviews should include\n\nexamination of development plans,\nrequirements specifications,\ndesign specifications,\ntesting plans and procedures,\nall other documents and activities associated with the project,\nverification results from each stage of the defined life cycle, and\nvalidation results for the overall device.\n\nThe Quality System regulation requires that at least one formal design review be conducted during the device design process. However, it is recommended that multiple design reviews be conducted\n\n(e.g., at the end of each software life cycle activity, in preparation for proceeding to the next activity).\n\nFormal design reviews documented should include:\n\nthe appropriate tasks and expected results, outputs, or products been established for each software life cycle activity\ncorrectness, completeness, consistency, and accuracy\nsatisfaction for the standards, practices, and conventions of that activity\nestablishment of a proper basis for initiating tasks for the next software life cycle activity"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#principles-of-software-validation",
    "href": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#principles-of-software-validation",
    "title": "FDA Validation Document Summary",
    "section": "Principles of Software Validation",
    "text": "Principles of Software Validation\n\nRequirements\nA documented software requirements specification provides a baseline for both validation and verification. The software validation process must include an established software requirements specification (Ref: 21 CFR 820.3(z) and (aa) and 820.30(f) and (g)).\n\n\nDefect Prevention\nIn order to establish that confidence, software developers should use a mixture of methods and techniques to prevent software errors and to detect software errors that do occur.\n\n\nTime and Effort\nPreparation for software validation should begin early, i.e., during design and development planning and design input. The final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\nSoftware Life Cycle\n\nSoftware validation takes place within the environment of an established software life cycle.\nThe software life cycle contains software engineering tasks and documentation necessary to support the software validation effort.\nspecific verification and validation tasks need to be appropriate for the intended use of the software\n\n\n\nPlans\n\nThe software validation process is defined and controlled through the use of a plan.\nThe software validation plan defines “what” is to be accomplished through the software validation effort.\nSoftware validation plans specify areas such as\n\nscope,\napproach,\nresources,\nschedules and the types and extent of activities,\ntasks, and\nwork items.\n\n\n\n\nProcedures\nThe software validation process is executed through the use of procedures. These procedures establish “how” to conduct the software validation effort. The procedures should identify the specific actions or sequence of actions that must be taken to complete individual validation activities, tasks, and work items.\n\n\nSoftware Validation After a Change\n\nDue to the complexity of software, a small local change may have a significant global system impact.\nIf a change exists in the software, the whole validation status of the software needs to be re-established.\nneed to determine the extent and impact of that change on the entire software system.\nthe software developer should then conduct an appropriate level of software regression testing to show that unchanged but vulnerable portions of the system have not been adversely affected.\n\n\n\nValidation Coverage\n\nValidation coverage should be based on the software’s complexity and safety risk.\nThe selection of validation activities, tasks, and work items should be commensurate with the complexity of the software design and the risk associated with the use of the software for the specified intended use.\n\n\n\nIndependence of Review\n\nValidation activities should be based on the basic quality assurance precept of “independence of review.”\nSelf-validation is extremely difficult.\nWhen possible, an independent evaluation is always better (like a contracted third-party independent verification and validation)\nAnother approach is to assign internal staff members that are not involved in a particular design or its implementation, but who have sufficient knowledge to evaluate the project and conduct the verification and validation activities.\n\n\n\nFlexibility and Responsibility\nThe device manufacturer has flexibility in choosing how to apply these validation principles, but retains ultimate responsibility for demonstrating that the software has been validated. FDA regulated medical device applications include software that:\n\nIs a component, part, or accessory of a medical device;\n\ncomponents: e.g., application software, operating systems, compilers, debuggers, configuration management tools, and many more\n\nIs itself a medical device; or\nIs used in manufacturing, design and development, or other parts of the quality system.\nNo matter how complex and disperse the software is, the manufacturer is in charge of responsibility for software validation."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#activities-and-tasks",
    "href": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#activities-and-tasks",
    "title": "FDA Validation Document Summary",
    "section": "Activities and Tasks",
    "text": "Activities and Tasks\nSoftware validation is accomplished through a series of activities and tasks that are planned and executed at various stages of the software development life cycle. These tasks may be\n\none time occurrences\niterated many times\n\n\nSoftware Life Cycle Activities\n\nSoftware developers should establish a software life cycle model that is appropriate for their product and organization.\nThe selected software life cycle model should cover the software from its birth to its retirement.\nActivities in a typical software life cycle model:\n\nQuality Planning\nSystem Requirements Definition\nDetailed Software Requirements Specification\nSoftware Design Specification\nConstruction or Coding\nTesting\nInstallation\nOperation and Support\nMaintenance\nRetirement\n\nVerification, testing, and other tasks that support software validation occur during each of these activities.\nSeveral software life cycle models defined in FDA’s Glossary of Computerized System and Software Development\n\nTerminology dated August 1995:\n\nwaterfall\nspiral\nrapid prototyping\nincremental development, etc.\n\n\n\nTypical Tasks Supporting Validation\nthe software developer should at least consider each of the risk-related tasks and should define and document which tasks are or are not appropriate for their specific application.\n\nQuality Planning\nDesign and development planning should culminate in a plan that identifies\n\nnecessary tasks,\nprocedures for anomaly reporting and resolution,\nnecessary resources, and\nmanagement review requirements including formal design reviews.\n\nThe plan should include:\n\nThe specific tasks for each life cycle activity;\nEnumeration of important quality factors (e.g., reliability, maintainability, and usability);\nMethods and procedures for each task;\nTask acceptance criteria;\nCriteria for defining and documenting outputs in terms that will allow evaluation of their conformance to input requirements;\nInputs for each task;\nOutputs from each task;\nRoles, resources, and responsibilities for each task;\nRisks and assumptions; and\nDocumentation of user needs.\n\nThe plan should identify\n\nthe personnel,\nthe facility and equipment resources for each task, and\nthe role that risk (hazard) management will play.\n\nA configuration management plan should be developed that will guide and control multiple parallel development activities and ensure proper communications and documentation.\nControls are necessary to ensure positive and correct correspondence among all approved versions of the specifications documents, source code, object code, and test suites that comprise a software system. The controls also should ensure accurate identification of, and access to, the currently approved versions.\nProcedures should be created for reporting and resolving software anomalies found through validation or other activities.\nManagement should identify the reports and specify the contents, format, and responsible organizational elements for each report. Procedures also are necessary for the review and approval of software development results, including the responsible organizational elements for such reviews and approvals.\nTypical Tasks – Quality Planning\n\nRisk (Hazard) Management Plan\nConfiguration Management Plan\nSoftware Quality Assurance Plan\n\nSoftware Verification and Validation Plan\n\nVerification and Validation Tasks, and Acceptance Criteria\nSchedule and Resource Allocation (for software verification and validation activities)\nReporting Requirements\n\nFormal Design Review Requirements\nOther Technical Review Requirements\n\nProblem Reporting and Resolution Procedures\nOther Support Activities\n\n\n\nRequirements\nRequirements development includes the\n\nidentification,\nanalysis, and\ndocumentation of information about the device and its intended use.\n\nAreas of special importance include allocation of system functions to\n\nhardware/software,\noperating conditions,\nuser characteristics,\npotential hazards, and\nanticipated tasks.\n\nIn addition, the requirements should state clearly the intended use of the software. It is not possible to validate software without predetermined and documented software requirements. Typical software requirements specify the following:\n\nAll software system inputs;\nAll software system outputs;\nAll functions that the software system will perform;\nAll performance requirements that the software will meet, (e.g., data throughput, reliability, and timing);\nThe definition of all external and user interfaces, as well as any internal software-to-system interfaces;\nHow users will interact with the system;\nWhat constitutes an error and how errors should be handled;\nRequired response times;\nThe intended operating environment for the software, if this is a design constraint (e.g., hardware platform, operating system);\nAll ranges, limits, defaults, and specific values that the software will accept; and\nAll safety related requirements, specifications, features, or functions that will be implemented in software.\n\nSoftware requirement specifications should identify clearly the potential hazards that can result from a software failure in the system as well as any safety requirements to be implemented in software.\nThe consequences of software failure should be evaluated, along with means of mitigating such failures (e.g., hardware mitigation, defensive programming, etc.).\nThe Quality System regulation requires a mechanism for addressing incomplete, ambiguous, or conflicting requirements. (See 21 CFR 820.30(c).) Each requirement (e.g., hardware, software, user, operator interface, and safety) identified in the software requirements specification should be evaluated for accuracy, completeness, consistency, testability, correctness, and clarity.\nFor example, software requirements should be evaluated to verify that:\n\nThere are no internal inconsistencies among requirements;\nAll of the performance requirements for the system have been spelled out;\nFault tolerance, safety, and security requirements are complete and correct;\nAllocation of software functions is accurate and complete;\nSoftware requirements are appropriate for the system hazards; and\nAll requirements are expressed in terms that are measurable or objectively verifiable.\n\nA software requirements traceability analysis should be conducted to trace software requirements to (and from) system requirements and to risk analysis results. In addition to any other analyses and documentation used to verify software requirements, a formal design review is recommended to confirm that requirements are fully specified and appropriate before extensive software design efforts begin. Requirements can be approved and released incrementally, but care should be taken that interactions and interfaces among software (and hardware) requirements are properly reviewed, analyzed, and controlled.\nTypical Tasks – Requirements\n\nPreliminary Risk Analysis\nTraceability Analysis\n\nSoftware Requirements to System Requirements (and vice versa)\nSoftware Requirements to Risk Analysis\n\nDescription of User Characteristics\nListing of Characteristics and Limitations of Primary and Secondary Memory\nSoftware Requirements Evaluation\nSoftware User Interface Requirements Analysis\nSystem Test Plan Generation\nAcceptance Test Plan Generation\nAmbiguity Review or Analysis\n\n\n\nDesign\nIn the design process, the software requirements specification is translated into a logical and physical representation of the software to be implemented. The software design specification is a description of what the software should do and how it should do it. The design specification may contain both a high level summary of the design and detailed design information. Human factors engineering should be woven into\n\nthe entire design and development process,\nthe device design requirements,\nanalyses, and\ntests.\n\nDevice safety and usability issues should be considered when developing\n\nflowcharts,\nstate diagrams,\nprototyping tools, and\ntest plans.\n\nAlso, task and function analyses, risk analyses, prototype tests and reviews, and full usability tests should be performed. Participants from the user population should be included when applying these methodologies.\nThe software design specification should include:\n\nSoftware requirements specification, including predetermined criteria for acceptance of the software;\nSoftware risk analysis;\nDevelopment procedures and coding guidelines (or other programming procedures);\nSystems documentation (e.g., a narrative or a context diagram) that describes the systems context in which the program is intended to function, including the relationship of hardware, software, and the physical environment;\nHardware to be used;\nParameters to be measured or recorded;\nLogical structure (including control logic) and logical processing steps (e.g., algorithms);\nData structures and data flow diagrams;\nDefinitions of variables (control and data) and description of where they are used;\nError, alarm, and warning messages;\nSupporting software (e.g., operating systems, drivers, other application software);\nCommunication links (links among internal modules of the software, links with the supporting software, links with the hardware, and links with the user);\nSecurity measures (both physical and logical security); and\nAny additional constraints not identified in the above elements.\n\nThe first four of the elements noted above usually are separate pre-existing documents that are included by reference in the software design specification. Software requirements specification was discussed in the preceding section, as was software risk analysis.\nSoftware design evaluations criteria:\n\ncomplete,\ncorrect,\nconsistent,\nunambiguous,\nfeasible,\nmaintainable,\nanalyses of control flow,\ndata flow,\ncomplexity,\ntiming,\nsizing,\nmemory allocation,\ncriticality analysis, and many other aspects of the design\n\nAppropriate consideration of software architecture (e.g., modular structure) during design can reduce the magnitude of future validation efforts when software changes are needed.\nA traceability analysis should be conducted to verify that the software design implements all of the software requirements. As a technique for identifying where requirements are not sufficient, the traceability analysis should also verify that all aspects of the design are traceable to software requirements.\nAn analysis of communication links should be conducted to evaluate the proposed design with respect to hardware, user, and related software requirements. At the end of the software design activity, a Formal Design Review should be conducted to verify that the design is correct, consistent, complete, accurate, and testable, before moving to implement the design.\nSeveral versions of both the software requirement specification and the software design specification should be maintained. All approved versions should be archived and controlled in accordance with established configuration management procedures.\nTypical Tasks – Design\n\nUpdated Software Risk Analysis\nTraceability Analysis - Design Specification to Software Requirements (and vice versa)\nSoftware Design Evaluation\nDesign Communication Link Analysis\nModule Test Plan Generation\nIntegration Test Plan Generation\nTest Design Generation (module, integration, system, and acceptance)\n\n\n\nConstruction or Coding\nSoftware may be constructed either by coding. Coding is the software activity where the detailed design specification is implemented as source code. It is the last stage in decomposition of the software requirements where module specifications are translated into a programming language.\nCoding usually involves the use of a high-level programming language, but may also entail the use of assembly language (or microcode) for time-critical operations.\nA source code traceability analysis is an important tool to verify that all code is linked to established specifications and established test procedures. A source code traceability analysis should be conducted and documented to verify that:\n\nEach element of the software design specification has been implemented in code;\nModules and functions implemented in code can be traced back to an element in the software design specification and to the risk analysis;\nTests for modules and functions can be traced back to an element in the software design specification and to the risk analysis; and\nTests for modules and functions can be traced to source code for the same modules and functions.\n\nTypical Tasks – Construction or Coding\n\nTraceability Analyses\n\nSource Code to Design Specification (and vice versa)\nTest Cases to Source Code and to Design Specification\n\nSource Code and Source Code Documentation Evaluation\nSource Code Interface Analysis\nTest Procedure and Test Case Generation (module, integration, system, and acceptance)\n\n\n\nTesting by the Software Developer\nSoftware testing entails running software products under known conditions with defined inputs and documented outcomes that can be compared to their predefined expectations. It is a time consuming, difficult, and imperfect activity.\nAs such, it requires early planning in order to be effective and efficient. Test plans and test cases should be created as early in the software development process as feasible.\nThey should identify\n\nthe schedules,\nenvironments,\nresources (personnel, tools, etc.),\nmethodologies,\ncases (inputs, procedures, outputs, expected results),\ndocumentation, and\nreporting criteria.\n\nDescriptions of categories of software and software testing effort appear in the literature\n\nNIST Special Publication 500-235, Structured Testing: A Testing Methodology Using the Cyclomatic Complexity Metric;\nNUREG/CR-6293, Verification and Validation Guidelines for High Integrity Systems; and\nIEEE Computer Society Press, Handbook of Software Reliability Engineering.\n\nTesting of all program functionality does not mean all of the program has been tested. Testing of all of a program’s code does not mean all necessary functionality is present in the program. Testing of all program functionality and all program code does not mean the program is 100% correct! Software testing that finds no errors should not be interpreted to mean that errors do not exist in the software product; it may mean the testing was superficial.\nAn essential element of a software test case is the expected result. It is the key detail that permits objective evaluation of the actual test result. This necessary testing information is obtained from the corresponding, predefined definition or specification.\nA software testing process should be based on principles that foster effective examinations of a software product. Applicable software testing tenets include:\n\nThe expected test outcome is predefined;\nA good test case has a high probability of exposing an error;\nA successful test is one that finds an error;\nThere is independence from coding;\nBoth application (user) and software (programming) expertise are employed;\nTesters use different tools from coders;\nExamining only the usual case is insufficient;\nTest documentation permits its reuse and an independent confirmation of the pass/fail status of a test outcome during subsequent review.\n\nCode-based testing is also known as structural testing or “white-box” testing. It identifies test cases based on knowledge obtained from the source code, detailed design specification, and other development documents. Structural testing can identify “dead” code that is never executed when the program is run. Structural testing is accomplished primarily with unit (module) level testing, but can be extended to other levels of software testing.\nThe level of structural testing can be evaluated using metrics that are designed to show what percentage of the software structure has been evaluated during structural testing. These metrics are typically referred to as “coverage” and are a measure of completeness with respect to test selection criteria. The amount of structural coverage should be commensurate with the level of risk posed by the software. Use of the term “coverage” usually means 100% coverage. Common structural coverage metrics include:\n\nStatement Coverage – This criteria requires sufficient test cases for each program statement to be executed at least once; however, its achievement is insufficient to provide confidence in a software product’s behavior.\nDecision (Branch) Coverage – This criteria requires sufficient test cases for each program decision or branch to be executed so that each possible outcome occurs at least once. It is considered to be a minimum level of coverage for most software products, but decision coverage alone is insufficient for high-integrity applications.\nCondition Coverage – This criteria requires sufficient test cases for each condition in a program decision to take on all possible outcomes at least once. It differs from branch coverage only when multiple conditions must be evaluated to reach a decision.\nMulti-Condition Coverage – This criteria requires sufficient test cases to exercise all possible combinations of conditions in a program decision.\nLoop Coverage – This criteria requires sufficient test cases for all program loops to be executed for zero, one, two, and many iterations covering initialization, typical running and termination (boundary) conditions.\nPath Coverage – This criteria requires sufficient test cases for each feasible path, basis path, etc., from start to exit of a defined program segment, to be executed at least once. Because of the very large number of possible paths through a software program, path coverage is generally not achievable. The amount of path coverage is normally established based on the risk or criticality of the software under test.\nData Flow Coverage – This criteria requires sufficient test cases for each feasible data flow to be executed at least once. A number of data flow testing strategies are available.\n\nThe following types of functional software testing involve generally increasing levels of effort:\n\nNormal Case – Testing with usual inputs is necessary. However, testing a software product only with expected, valid inputs does not thoroughly test that software product. By itself, normal case testing cannot provide sufficient confidence in the dependability of the software product.\nOutput Forcing – Choosing test inputs to ensure that selected (or all) software outputs are generated by testing.\nRobustness – Software testing should demonstrate that a software product behaves correctly when given unexpected, invalid inputs. Methods for identifying a sufficient set of such test cases include Equivalence Class Partitioning, Boundary Value Analysis, and Special Case Identification (Error Guessing). While important and necessary, these techniques do not ensure that all of the most appropriate challenges to a software product have been identified for testing.\nCombinations of Inputs – The functional testing methods identified above all emphasize individual or single test inputs. Most software products operate with multiple inputs under their conditions of use. Thorough software product testing should consider the combinations of inputs a software unit or system may encounter during operation. Error guessing can be extended to identify combinations of inputs, but it is an ad hoc technique. Cause-effect graphing is one functional software testing technique that systematically identifies combinations of inputs to a software product for inclusion in test cases.\n\nFunctional and structural software test case identification techniques provide specific inputs for testing, rather than random test inputs. One weakness of these techniques is the difficulty in linking structural and functional test completion criteria to a software product’s reliability.\nAdvanced software testing methods, such as statistical testing, can be employed to provide further assurance that a software product is dependable. Statistical testing uses randomly generated test data from defined distributions based on an operational profile (e.g., expected use, hazardous use, or malicious use of the software product). Large amounts of test data are generated and can be targeted to cover particular areas or concerns, providing an increased possibility of identifying individual and multiple rare operating conditions that were not anticipated by either the software product’s designers or its testers. Statistical testing also provides high structural coverage. It does require a stable software product. Thus, structural and functional testing are prerequisites for statistical testing of a software product.\nAnother aspect of software testing is the testing of software changes. Changes occur frequently during software development. These changes are the result of\n\ndebugging that finds an error and it is corrected,\nnew or changed requirements (“requirements creep”), and\nmodified designs as more effective or efficient implementations are found.\n\nOnce a software product has been baselined (approved), any change to that product should have its own “mini life cycle,” including testing. Testing of a changed software product requires additional effort. It should demonstrate\n\nthat the change was implemented correctly, and\nthat the change did not adversely impact other parts of the software product.\n\nRegression analysis is the determination of the impact of a change based on review of the relevant documentation in order to identify the necessary regression tests to be run. Regression testing is the rerunning of test cases that a program has previously executed correctly and comparing the current result to the previous result in order to detect unintended effects of a software change. Regression analysis and regression testing should also be employed when using integration methods to build a software product to ensure that newly integrated modules do not adversely impact the operation of previously integrated modules.\nIn order to provide a thorough and rigorous examination of a software product, development testing is typically organized into levels: unit, integration, and system levels of testing.\n\nUnit (module or component) level testing focuses on the early examination of sub-program functionality and ensures that functionality not visible at the system level is examined by testing. Unit testing ensures that quality software units are furnished for integration into the finished software product.\nIntegration level testing focuses on the transfer of data and control across a program’s internal and external interfaces. External interfaces are those with\n\nother software (including operating system software),\nsystem hardware, and\nthe users and can be described as communications links.\n\nSystem level testing demonstrates that all specified functionality exists and that the software product is trustworthy. This testing verifies the as-built program’s functionality and performance with respect to the requirements for the software product as exhibited on the specified operating platform(s). System level software testing addresses functional concerns and the following elements of a device’s software that are related to the intended use(s):\n\nPerformance issues (e.g., response times, reliability measurements);\nResponses to stress conditions, e.g., behavior under maximum load, continuous use;\nOperation of internal and external security features;\nEffectiveness of recovery procedures, including disaster recovery;\nUsability; (Usability vs Utility??)\nCompatibility with other software products;\nBehavior in each of the defined hardware configurations; and\nAccuracy of documentation.\n\n\nControl measures (e.g., a traceability analysis) should be used to ensure that the intended coverage is achieved.\nSystem level testing also exhibits the software product’s behavior in the intended operating environment. The location of such testing is dependent upon the software developer’s ability to produce the target operating environment(s). Depending upon the circumstances, simulation and/or testing at (potential) customer locations may be utilized.\nTest plans should identify the controls needed to ensure\n\nthat the intended coverage is achieved and\nthat proper documentation is prepared when planned system level testing is conducted at sites not directly controlled by the software developer.\n\nTest procedures, test data, and test results\n\nshould be documented in a manner permitting objective pass/fail decisions to be reached.\nshould also be suitable for review and objective decision making subsequent to running the test,\nshould be suitable for use in any subsequent regression testing.\n\nErrors detected during testing should be\n\nlogged,\nclassified,\nreviewed, and\nresolved prior to release of the software.\n\nSoftware error data that is collected and analyzed during a development life cycle may be used to determine the suitability of the software product for release for commercial distribution. Test reports should comply with the requirements of the corresponding test plans.\nSoftware testing tools are frequently used to ensure consistency, thoroughness, and efficiency in the testing of such software products and to fulfill the requirements of the planned testing activities.\nAppropriate documentation providing evidence of the validation of these software tools for their intended use should be maintained (see section 6 of this guidance).\nTypical Tasks – Testing by the Software Developer\n\nTest Planning\nStructural Test Case Identification\nFunctional Test Case Identification\nTraceability Analysis - Testing\nUnit (Module) Tests to Detailed Design\nIntegration Tests to High Level Design\nSystem Tests to Software Requirements\nUnit (Module) Test Execution\nIntegration Test Execution\nFunctional Test Execution\nSystem Test Execution\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\nUser Site Testing\nTesting at the user site is an essential part of software validation. The Quality System regulation requires\n\ninstallation and\ninspection procedures (including testing where appropriate) as well as\ndocumentation of inspection and\ntesting to demonstrate proper installation. (See 21 CFR §820.170.)\n\nLikewise, manufacturing equipment must meet specified requirements, and automated systems must be validated for their intended use. (See 21 CFR §820.70(g) and 21 CFR §820.70(i) respectively.)\nTerminology regarding user site testing can be confusing. Terms such as\n\nbeta test,\nsite validation,\nuser acceptance test,\ninstallation verification, and\ninstallation testing have all been used to describe user site testing.\n\nFor the purposes of this guidance, the term “user site testing” encompasses all of these and any other testing that takes place outside of the developer’s controlled environment.\nThis testing should take place at a user’s site with the actual hardware and software that will be part of the installed system configuration. The testing is accomplished through either actual or simulated use of the software being tested within the context in which it is intended to function.\nTest planners should check with the FDA Center(s) with the corresponding product jurisdiction to determine whether there are any additional regulatory requirements for user site testing.\nUser site testing should follow a pre-defined written plan with\n\na formal summary of testing and\na record of formal acceptance.\n\nThe following documented evidence should be retained:\n\nall testing procedures,\ntest input data, and\ntest results\n\nThere should be evidence that hardware and software are installed and configured as specified. Measures should ensure that all system components are exercised during the testing and that the versions of these components are those specified. The testing plan should specify testing throughout the full range of operating conditions and should specify continuation for a sufficient time to allow the system to encounter a wide spectrum of conditions and events in an effort to detect any latent faults that are not apparent during more normal activities.\nSome of the evaluations of the system’s ability that have been performed earlier by the software developer at the developer’s site should be repeated at the site of actual use. These may include tests for:\n\na high volume of data,\nheavy loads or stresses,\nsecurity,\nfault testing (avoidance, detection, tolerance, and recovery),\nerror messages, and\nimplementation of safety requirements.\n\nThere should be an evaluation of the ability of the users of the system to understand and correctly interface with it.\nOperators should be able to perform the intended functions and respond in an appropriate and timely manner to all alarms, warnings, and error messages.\nRecords should be maintained of both proper system performance and any system failures that are encountered.\nThe revision of the system to compensate for faults detected during this user site testing should follow the same procedures and controls as for any other software change.\nThe developers of the software may or may not be involved in the user site testing.\n\nIf the developers are involved, they may seamlessly carry over to the user’s site the last portions of design-level systems testing.\nIf the developers are not involved, it is all the more important that the user have persons who understand the importance of careful test planning, the definition of expected test results, and the recording of all test outputs.\n\nTypical Tasks – User Site Testing\n\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\nMaintenance and Software Changes\n\nHardware vs Software\nHardware maintenance typically includes\n\npreventive hardware maintenance actions,\ncomponent replacement, and\ncorrective changes.\n\nSoftware maintenance includes\n\ncorrective,\nperfective, and\nadaptive maintenance\nbut does not include preventive maintenance actions or software component replacement.\n\n\n\nMaintenance Types\n\nCorrective maintenance: Changes made to correct errors and faults in the software.\nPerfective maintenance: Changes made to the software to improve the performance, maintainability, or other attributes of the software system .\nAdaptive maintenance: Changes to make the software system usable in a changed environment.\n\nSufficient regression analysis and testing should be conducted to demonstrate that portions of the software not involved in the change were not adversely impacted. When changes are made to a software system,\n\neither during initial development or\nduring post release maintenance,\n\nThis is in addition to testing that evaluates the correctness of the implemented change(s). The specific validation effort necessary for each software change is determined by\n\nthe type of change,\nthe development products affected, and the\nimpact of those products on the operation of the software.\n\n\n\nFactors of Limitting Validation Effort Needed When a Change Is Made\n\ncareful and complete documentation of the design structure and\ncareful and complete documentation of interrelationships of various modules,\ninterfaces, etc.\nFor example,\n\ntest documentation,\ntest cases, and\nresults of previous verification and validation testing All of them need to be archived if they are to be available for performing subsequent regression testing.\n\n\nThe following additional maintenance tasks should be addressed:\n\nSoftware Validation Plan Revision - For software that was previously validated, the existing software validation plan should be revised to support the validation of the revised software. If no previous software validation plan exists, such a plan should be established to support the validation of the revised software.\nAnomaly Evaluation – Software organizations frequently maintain documentation, such as software problem reports that describe software anomalies discovered and the specific corrective action taken to fix each anomaly.\n\nToo often, however, mistakes are repeated because software developers do not take the next step to determine the root causes of problems and make the process and procedural changes needed to avoid recurrence of the problem.\nSoftware anomalies should be evaluated in terms of their severity and their effects on system operation and safety,\nbut they should also be treated as symptoms of process deficiencies in the quality system.\nA root cause analysis of anomalies can identify specific quality system deficiencies.\nWhere trends are identified (e.g., recurrence of similar software anomalies), appropriate corrective and preventive actions must be implemented and documented to avoid further recurrence of similar quality problems. (See 21 CFR 820.100.)\n\nProblem Identification and Resolution Tracking - All problems discovered during maintenance of the software should be documented. The resolution of each problem should be tracked to ensure it is fixed, for historical reference, and for trending.\nProposed Change Assessment - All proposed modifications, enhancements, or additions should be assessed to determine the effect each change would have on the system. This information should determine the extent to which verification and/or validation tasks need to be iterated.\nTask Iteration - For approved software changes, all necessary verification and validation tasks should be performed to ensure that planned changes are implemented correctly, all documentation is complete and up to date, and no unacceptable changes have occurred in software performance.\nDocumentation Updating – Documentation should be carefully reviewed to determine which documents have been impacted by a change. All approved documents (e.g., specifications, test procedures, user manuals, etc.) that have been affected should be updated in accordance with configuration management procedures. Specifications should be updated before any maintenance and software changes are made."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#validation-of-automated-process-equipment-and-quality-system-software",
    "href": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#validation-of-automated-process-equipment-and-quality-system-software",
    "title": "FDA Validation Document Summary",
    "section": "Validation of Automated Process Equipment and Quality System Software",
    "text": "Validation of Automated Process Equipment and Quality System Software\nThe Quality System regulation requires that “when computers or automated data processing systems are used as part of production or the quality system, the [device] manufacturer shall validate computer software for its intended use according to an established protocol.” (See 21 CFR §820.70(i)). This has been a regulatory requirement of FDA’s medical device Good Manufacturing Practice (GMP) regulations since 1978.\nComputer systems that implement part of a device manufacturer’s production processes or quality system (or that are used to create and maintain records required by any other FDA regulation) are subject to the Electronic Records; Electronic Signatures regulation. (See 21 CFR Part 11.) This regulation establishes additional security, data integrity, and validation requirements when records are created or maintained electronically. These additional Part 11 requirements should be carefully considered and included in system requirements and software requirements for any automated record keeping systems. System validation and software validation should demonstrate that all Part 11 requirements have been met.\nComputers and automated equipment are used extensively throughout all aspects of\n\nmedical device design,\nlaboratory testing and analysis,\nproduct inspection and acceptance,\nproduction and process control,\nenvironmental controls,\npackaging,\nlabeling,\ntraceability,\ndocument control,\ncomplaint management, and many other aspects of the quality system.\n\nIncreasingly, automated plant floor operations can involve extensive use of embedded systems in:\n\nprogrammable logic controllers;\ndigital function controllers;\nstatistical process control;\nsupervisory control and data acquisition;\nrobotics;\nhuman-machine interfaces;\ninput/output devices; and\ncomputer operating systems.\n\nAll software tools used for software design are subject to the requirement for software validation, but the validation approach used for each application can vary widely.\nValidation is typically supported by:\n\nverifications of the outputs from each stage of that software development life cycle; and\nchecking for proper operation of the finished software in the device manufacturer’s intended use environment.\n\n\nHow Much Validation Evidence Is Needed?\nThe level of validation effort should be commensurate with\n\nthe risk posed by the automated operation,\nthe complexity of the process software,\nthe degree to which the device manufacturer is dependent upon that automated process to produce a safe and effective device\n\nDocumented requirements and risk analysis of the automated process help to define the scope of the evidence needed to show that the software is validated for its intended use. Without a plan, extensive testing may be needed for:\n\na plant-wide electronic record and electronic signature system;\nan automated controller for a sterilization cycle; or\nautomated test equipment used for inspection and acceptance of finished circuit boards in a lifesustaining / life-supporting device.\n\nHigh risk applications should not be running in the same operating environment with non-validated software functions, even if those software functions are not used. Risk mitigation techniques such as memory partitioning or other approaches to resource protection may need to be considered when high risk applications and lower risk applications are to be used in the same operating environment.\nWhen software is upgraded or any changes are made to the software, the device manufacturer should consider how those changes may impact the “used portions” of the software and must reconfirm the validation of those portions of the software that are used. (See 21 CFR §820.70(i).)\n\n\nDefined User Equipment\nA very important key to software validation is a documented user requirements specification that defines:\n\nthe “intended use” of the software or automated equipment; and\nthe extent to which the device manufacturer is dependent upon that software or equipment for production of a quality medical device.\n\nThe device manufacturer (user) needs to define the expected operating environment including any required hardware and software configurations, software versions, utilities, etc. The user also needs to:\n\ndocument requirements for system performance, quality, error handling, startup, shutdown, security, etc.;\nidentify any safety related functions or features, such as sensors, alarms, interlocks, logical processing steps, or command sequences; and\ndefine objective criteria for determining acceptable performance.\n\nThe validation must be conducted in accordance with a documented protocol, and the validation results must also be documented. (See 21 CFR §820.70(i).) Test cases should be documented that will exercise the system to challenge its performance against the pre-determined criteria, especially for its most critical parameters.\nTest cases should address\n\nerror and alarm conditions,\nstartup, shutdown,\nall applicable user functions and operator controls,\npotential operator errors,\nmaximum and minimum ranges of allowed values, and\nstress conditions applicable to the intended use of the equipment.\n\nThe test cases should be executed and the results should be recorded and evaluated to determine whether the results support a conclusion that the software is validated for its intended use.\nA device manufacturer may conduct a validation using their own personnel or may depend on a third party such as the equipment/software vendor or a consultant. In any case, the device manufacturer retains the ultimate responsibility for ensuring that the production and quality system software:\n\nis validated according to a written procedure for the particular intended use; and\nwill perform as intended in the chosen application.\n\nThe device manufacturer should have documentation including:\n\ndefined user requirements;\nvalidation protocol used;\nacceptance criteria;\ntest cases and results; and\na validation summary that objectively confirms that the software is validated for its intended use.\n\n\n\nValidation of Off-The-Shelf Software and Automated Equipment\nMost of the automated equipment and systems used by device manufacturers are supplied by thirdparty vendors and are purchased off-the-shelf (OTS). The device manufacturer is responsible for ensuring that the product development methodologies used by the OTS software developer are appropriate and sufficient for the device manufacturer’s intended use of that OTS software.\nWhere possible and depending upon the device risk involved, the device manufacturer should consider auditing the vendor’s design and development methodologies used in the construction of the OTS software and should assess the development and validation documentation generated for the OTS software. Such audits can be conducted by the device manufacturer or by a qualified third party.\nThe audit should demonstrate that the vendor’s procedures for and results of the verification and validation activities performed the OTS software are appropriate and sufficient for the safety and effectiveness requirements of the medical device to be produced using that software."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "",
    "text": "I am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto.\nThe FDA validation guidance document is a bit difficult to understand because its explanations provide abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\n\n\n2022-12-28, the summary of the document"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#definition-of-software-validation",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#definition-of-software-validation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Definition of Software Validation",
    "text": "Definition of Software Validation\nSoftware Validation is a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997.\n(See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\n\nSome Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology: requirements, specification, verification, and validation.\n\n\n\nObjective of SW validation is to ensure\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#quality-system-regulation",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#quality-system-regulation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Quality System Regulation",
    "text": "Quality System Regulation\n\n\n\n\nflowchart TB\n    subgraph Quality_System_Regulation\n        direction LR\n        subgraph Requirement\n            direction TB\n            user_requirements\n        end\n        subgraph Specification\n           direction TB\n           document_user_requirements \n        end \n        subgraph Verification\n           direction TB\n           verify_spacified_requirements\n        end\n        subgraph Validation\n           direction TB\n           Confirmation_by_Examinations\n           Provision_of_objective_3evidences\n        end\n        Requirement--> Specification --> Verification --> Validation                    \n    end\n    subgraph First_Detail\n        direction TB\n        subgraph User_Requirement\n            direction TB\n            any_need_for_customer---\n            any_need_for_system---\n            any_need_for_software\n        end\n            subgraph Document_User_Requirement\n            direction TB\n            define_means_for_requirements---\n          define_criteria_for_requirements\n        end         \n        subgraph Verify_Spacified_Requirement\n            direction TB\n            Objective_Evidence--->|needs|Software_Testing\n        end\n        subgraph SW_Validation\n            direction TB\n            subgraph Confirmation_by_Examination\n            direction TB\n                subgraph Examination_List_of_SW_LifeCycle\n                    direction TB\n                    comprehensiveness_of_software_testing---\n                    inspection_verification_test---\n                    analysis_verification_test---\n                    other_varification_tests    \n                end \n            end             \n            subgraph Provision_of_Objective_3evidences\n                direction TB\n                Software_specifications_conformity---\n                Consistent_SW_Implementation---\n                Correctness_Completeness_Traceability\n            end\n        end\n        Requirement---User_Requirement\n        Specification---Document_User_Requirement\n        Verification---Verify_Spacified_Requirement\n        Confirmation_by_Examinations---Confirmation_by_Examination\n        Provision_of_objective_3evidences---Provision_of_Objective_3evidences             \n    end"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#verification",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#verification",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Verification",
    "text": "Verification\n\n\n\n\nflowchart LR\n    subgraph Objective_Evidence\n        direction LR\n        subgraph Design_Outputs_of_SW_life_cycle_for_Specified_Requirements\n            direction TB\n            Consistency---\n            Completeness---\n            Correctness---\n            Documentation\n        end       \n        subgraph Software_Testing\n            direction LR\n            subgraph Testing_Environments\n                direction TB\n                satisfaction_for_input_requirements\n                satisfaction_for_input_requirements---Simulated_Use_Environment\n                subgraph User_Site_Testing\n                    direction TB                            \n                    Installation_Qualification---\n                    Operational_Qualification---\n                    Performance_Qualification\n                end\n            end\n            satisfaction_for_input_requirements---User_Site_Testing\n            subgraph Testing_Activities\n                direction TB\n                static_analyses---\n                dynamic_analyses---\n                code_and_document_inspections---\n                walkthroughs\n            end \n        Testing_Environments-->Testing_Activities\n        end\n    Design_Outputs_of_SW_life_cycle_for_Specified_Requirements-->Software_Testing-->Testing_Activities\nend    \n\n\n\n\n\n\n\n\n\n\nInstallation_Qualification (IQ): documentation of correct installations according to requirements, specifications, vendor’s recommendations, and the FDA’s guidance for all hardware, software, equipment and systems.\nOperational_Qualification (OQ): establishment of confidence that the software shows constant performances according to specified requirements.\nPerformance_Qualification (PQ): confirmation of the performance in the intended use according to the specified requirements for functionality and safety throughout the SW life cycle."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation",
    "text": "Validation\n\n\n\n\nflowchart LR\n    subgraph Validation\n    direction LR\n        subgraph Confirmation_by_Examination\n            direction TB\n            subgraph Examination_List_at_each_stage_of_SW_Life_Cycle\n                direction TB\n                comprehensiveness_of_software_testing---\n                inspection_verification_test---\n                analysis_verification_test---\n                other_varification_tests    \n            end \n        end\n        subgraph Provision_of_objective_3evidences\n            direction TB\n            subgraph Software_specifications_conform_to\n                direction TB\n                user_needs \n                intended_uses\n            end\n            subgraph Consistent_SW_Implementation\n                direction TB\n                particular_requirements\n            end\n            subgraph Correctness_Completeness_Traceability\n                direction TB\n                correct_complete_implementation_by_all_SW_requirements---\n                traceable_to_system_requirements\n            end\n            Software_specifications_conform_to---\n            Consistent_SW_Implementation---\n            Correctness_Completeness_Traceability\n        end\n        Confirmation_by_Examination-->\n        Provision_of_objective_3evidences\n    end"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#benefits-and-difficulty-of-sw-vv",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#benefits-and-difficulty-of-sw-vv",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Benefits and Difficulty of SW V&V",
    "text": "Benefits and Difficulty of SW V&V\n\nBenefits of SW V&V\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nReduce long term costs by making V&V easier and less costly to reliably modify software and revalidate software changes.\n\n\n\nDifficulty in SW V&V\n\na developer cannot test forever, and\n\nit is difficult to know how much evidence is enough.\na matter of developing a level of confidence that the device meets all requirements\n\nConsiderations for an acceptable level of confidence\nmeasures and estimates such as defects found in specifications documents\ntesting coverage, and other techniques are all used before shipping the product.\na level of confidence varies depending upon the safety risk (hazard) of a SW or device"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#sw-development-as-part-of-system-design",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#sw-development-as-part-of-system-design",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Development as Part of System Design",
    "text": "SW Development as Part of System Design\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        purpose_design_review---\n        design_review_types---\n        design_review_requirements---\n        design_review_outputs\n    end\n\n\n\n\n\n\n\n\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nuser’s needs\nintended uses from which the product is developed.\n\nA primary goal of SW validation is to demonstrate that all completed SW products comply with all documented requirements."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#design-review",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#design-review",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Design Review",
    "text": "Design Review\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        subgraph Purpose_Design_Review\n            direction TB\n            documented_structured_comprehensive_systematic_examinations---\n            adequacy_of_design_requirements---\n            capability_of_design_for_requirements---\n            identification_of_problem   \n        end\n        subgraph Design_Reivew_Types\n            direction TB\n            subgraph Formal_Design_Review\n                direction TB\n                3rd_parties_outside_development_team\n            end\n            subgraph Informal_Design_Review\n                direction TB\n                within_development_team\n            end\n        Formal_Design_Review---Informal_Design_Review    \n        end\n        subgraph Design_Review_Requirements\n            direction TB\n               necessary_at_least_one_formal_design_review---\n               optinal_informal_design_review---\n               recommended_multiple_design_reviews\n        end\n        subgraph Formal_Design_Review_Outputs\n            direction TB\n            more_than_10_outputs\n        end\n        Purpose_Design_Review--> Design_Reivew_Types--> Design_Review_Requirements\n        Design_Review_Requirements-->Formal_Design_Review_Outputs\n    end"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation-principles",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation-principles",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation Principles",
    "text": "Validation Principles\n\nOverview\n\n\n\n\nflowchart LR\n  subgraph Validation_Principles\n        direction LR\n        subgraph Validation_Starting_Point\n            direction TB\n            during_design_planning---\n            during_development_planning---\n            all_results_should_be_supported_by_evidence_collected_from_planning_SW_lifecylce\n        end\n        subgraph Validation_Conditions\n            direction TB\n            Requirements---Estabilishment_Confidence---SW_Lifecycle\n        end\n\n        subgraph Validation_Planning\n            direction TB\n            Specify_Areas\n            subgraph Validation_Coverage\n                direction TB\n            end\n            subgraph Validation_Process_Establishment\n                direction TB\n            end\n        Specify_Areas---Validation_Coverage---Validation_Process_Establishment\n        end\n\n        subgraph After_Self_Validation\n            direction TB\n            subgraph Validation_After_SW_Change\n        direction TB\n        end\n\n        subgraph Independence_of_Review\n        direction TB\n\n        end\n        Validation_After_SW_Change---Independence_of_Review\n        end\n            Validation_Starting_Point-->Validation_Conditions-->Validation_Planning-->\nAfter_Self_Validation\n    end\n\n\n\n\n\n\n\n\n\n\nConditions\n\n\n\n\nflowchart LR\n\nsubgraph Validation_Conditions\n    direction LR\n    subgraph SW_Requirments\n        direction TB\n        subgraph Documented_SW_Requirments_Specification\n            direction TB\n            Baseline_Provision_for_V&V---\n            establishment_of_software_requirements_specification\n        end\n    end\n    subgraph Estabilishment_Confidence\n        direction TB\n            mixture_of_methods_techinques---\n            preventing_SW_errors---\n            detecting_SW_errors                 \n    end\n    subgraph SW_Lifecycle\n        direction TB\n        validation_must_be_conducted_within_established_environment_across_lifecycle---\n        lifecycle_contains_SW_engineering_tasks_and_documentation---\n        V&V_tasks_must_reflect_intended_use\n    end\nend\nSW_Requirments---Estabilishment_Confidence---SW_Lifecycle\n\n\n\n\n\n\n\n\n\n\nPlanning\n\n\n\n\nflowchart LR\n    subgraph Validation_Planning\n        direction LR\n        define_what_to_accomplish\n        subgraph Specify_Areas\n            direction TB\n            scope---\n            approach---\n            resources---\n            schedules_activities---\n            types_activitieis---\n            extent_of_activities---\n            tasks---\n            work_items\n        end\n            define_what_to_accomplish-->Specify_Areas\n        subgraph Validation_Coverage\n               direction TB\n            depending_on_SW_complexity_of_SW_design---\n            depending_on_safety_risk_for_specified_intended_use---\n            select_activities_tasks_work_items_for_complexity_safety_risk\n        end\n        subgraph Validation_Process_Establishment\n            direction TB\n            establish_how_to_conduct-->\n            identify_sequence_of_specific_actions-->\n            identify_specific_activitieis-->\n            identify_specific_tasks-->\n            identify_specific_work_items\n        end\n    Specify_Areas-->Validation_Coverage-->Validation_Process_Establishment\n    end\n\n\n\n\n\n\n\n\n\n\nAfter SW Change\n\n\n\n\nflowchart LR\n\nsubgraph After_Self_Validation\n    direction LR\n    subgraph Validation_After_SW_Change\n        direction TB\n        determine_extent_of_change_on_entire_SW_system-->\n        determine_impact_of_change_on_entire_SW_system-->\n        conduct_SW_regression_testing_on_unchanged_but_vulnerable_modules\n    end\n    subgraph Independence_of_Review\n        direction TB\n        follow_basic_quality_assurance_precept_of_independence_of_review---\n        avoid_self_validation---\n        should_conduct_contracted_3rd_party_independent_V&V---\n        or_conduct_blind_test_with_internal_staff\n    end\n    Validation_After_SW_Change---Independence_of_Review\nend\n    \n\n\n\n\n\n\n\n\n\n\nSW Lifecycle\n\n\n\n\nflowchart LR\nsubgraph SW_Lifecycle\n    direction TB\n    validation_must_be_conducted_within_the_established_environment_across_lifecycle---\n    lifecycle_contains_SW_engineering_tasks_and_documentation---\n    V&V_tasks_must_reflect_intended_use\nend\n\nsubgraph SW_Lifecycle_Activities\n    direction TB\n    subgraph should_establish_lifecycle_model\n        direction TB\n        subgraph SW_Lifecycle_Model_List_Defined_in_FDA\n            direction TB\n            waterfall---\n            spiral---\n            rapid_prototyping---\n            incremental_development---\n            etc\n        end     \n    end\n    subgraph should_cover_SW_birth_to_retirement\n        direction TB\n        subgraph Lifecycle_Activities\n            direction TB\n            Quality_Plan-->\n            System_Requirements_Definition-->\n            Detailed_Software_Requirements_Specification-->\n            Software_Design_Specification-->\n            Construction_or_Coding-->\n            Testing-->\n            Installation-->\n            Operation_and_Support-->\n            Maintenance-->\n            Retirement\n        end\n    end\n    should_establish_lifecycle_model-->should_cover_SW_birth_to_retirement\n    should_cover_SW_birth_to_retirement-->Lifecycle_Activities\nend\nsubgraph SW_Lifecycle_Tasks\n    direction TB\n    should_define_and_document_risk_related_tasks---\n    should_define_and_document_which_tasks_are_appropriate_in_vice_versa---\n    Quality_Planning---\n    Quality_Planning_Tasks---\n    Inclusion_Task_List_for_Plan---\n    Identification_Task_List_for_Plan---\n    Configuration_Management---\n    Control---\n    Management---\n    Procedures---\n    ensure_proper_communications_and_documentation---\n    Task_Requirements\nend\nSW_Lifecycle-->SW_Lifecycle_Activities-->SW_Lifecycle_Tasks"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#sw-lifecycle-tasks",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#sw-lifecycle-tasks",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Lifecycle Tasks",
    "text": "SW Lifecycle Tasks\n\nOverview\n\n\n\n\n \nflowchart TB\n\nsubgraph SW_Lifecycle_Tasks\n    direction LR\n    subgraph Define_and_Document_List\n        direction TB\n        risk_related_tasks---\n        whether_or_not_tasks_are_appropriate\n    end\n    \n    subgraph Quality_Planning\n        direction TB\n        subgraph Quality_Planning_Tasks\n            direction TB\n        \n        end\n        subgraph Inclusion_List_for_Plan\n            direction TB\n            \n        end\n        subgraph Identification_List_for_Plan\n            direction TB\n            \n        end\n    Quality_Planning_Tasks-->Inclusion_List_for_Plan-->Identification_List_for_Plan\n    end\n    \n    subgraph Configuration_Management\n        direction TB\n        subgraph Control\n            direction TB\n            \n        end\n        subgraph Management\n            direction TB\n        end\n        subgraph Procedures\n            direction TB\n        end\n        ensure_proper_communications_and_documentation\n        Control-->Management-->Procedures-->ensure_proper_communications_and_documentation \n    end\n    subgraph Task_Requirements\n        direction TB\n        identification---\n        analysis---\n        predetermined_documentation_about_device_its_intended_use---\n        Requirements_Specification_List---\n        Verfification_List_by_Evaluation---\n        Requirements_Tasks    \n    end\nDefine_and_Document_List-->Quality_Planning-->Configuration_Management-->Task_Requirements\nend     \n\n\n\n\n\n\n\n\n\n\nQuality Planning\n\n\n\n\nflowchart TB\nsubgraph Quality_Planning\n    direction LR\n    subgraph Quality_Planning_Tasks\n        direction TB\n        Risk_Hazard_Management_Plan---\n        Configuration_Management_Plan---\n        Software_Quality_Assurance_Plan---\n        Software_Verification_and_Validation_Plan---\n        Verification_and_Validation_Tasks---\n        Acceptance_Criteria---\n        Schedule_and_Resource_Allocation_for_V&V_activities---\n        Reporting_Requirements---\n        Formal_Design_Review_Requirements---\n        Other_Technical_Review_Requirements---\n        Problem_Reporting_and_Resolution_Procedures---\n        Other_Support_Activities\n    end\n    subgraph Inclusion_List_for_Plan\n        direction TB\n        specific_tasks_for_each_life_cycle_activity---\n        Enumeration_of_important_quality_factors--- \n        like_reliability_maintainability_usability---\n        Methods_and_procedures_for_each_task---\n        Task_acceptance_criteria---\n        Criteria_for_defining_and_documenting_outputs_for_input_requirements---\n        Inputs_for_each_task---\n        Outputs_from_each_task---\n        Roles_resources_and_responsibilities_for_each_task---\n        Risks_and_assumptions---\n        Documentation_of_user_needs    \n    end\n    subgraph Identification_List_for_Plan\n        direction TB\n        personnel---\n        facility_and_equipment_resources_for_each_task---\n        role_that_risk_hazard_management        \n    end\nQuality_Planning_Tasks-->Inclusion_List_for_Plan-->Identification_List_for_Plan\nend\n\n\n\n\n\n\n\n\n\n\nConfiguration Management\n\n\n\n\nflowchart LR\nsubgraph Configuration_Management\n    direction LR\n    subgraph Control\n        direction TB\n        control_multiple_parallel_development_activities---\n        ensure_positive_and_correct_correspondence_of---\n        specifications_documents---\n        source_code---\n        object_code---\n        test_suites---\n        ensure_accurate_identification_of_approved_versions---\n        ensure_access_to_approved_versions---\n        create_procedures_for_reporting---\n        create_procedures_for_resolving_SW_anomalies                            \n    end\n    subgraph Management\n        direction TB\n        identify_reports---\n        specify_contents---\n        specify_format---\n        specify_responsible_organizational_elements_for_each_report\n    end\n    subgraph Procedures\n        direction TB\n        necessary_for_review_of_SW_development_results---\n        necessary_for_approval_of_SW_development_results\n    end\n    ensure_proper_communications_and_documentation\n    Control-->Management-->Procedures-->ensure_proper_communications_and_documentation \nend\n\n\n\n\n\n\n\n\n\n\nTask Requirements\n\n\n\n\n\nflowchart TB\n    subgraph Task_Requirements\n        direction LR\n        subgraph group\n            direction TB\n            identification---\n            analysis---\n            predetermined_documentation_about_device_its_intended_use\n        end\n        \n        subgraph Requirements_Specification_List\n            direction TB\n            All_software_system_inputs---\n            All_software_system_outputs---\n            All_functions_that_software_system_will_perform---\n            All_performance_requirements_that_software_will_meet---\n            requirement_example_data_throughput_reliability_timing---\n            definition_of_all_external_and_user_interfaces---\n            any_internal_software_to_system_interfaces---\n            How_users_will_interact_with_system---\n            What_constitutes_error---\n            how_errors_should_be_handled---\n            Required_response_times---\n            Intended_operating_environment_for_software---\n            All_acceptable_ranges_limits_defaults_specific_values---\n            All_safety_related_requirements_that_will_be_implemented_in_SW---\n            All_safety_related_specifications_that_will_be_implemented_in_SW---\n            All_safety_related_features_that_will_be_implemented_in_SW---\n            All_safety_related_functions_that_will_be_implemented_in_SW---\n            clearly_identify_potential_hazards---\n            risk_evaluation_for_accuracy---\n            risk_evaluation_for_completeness---\n            risk_evaluation_for_consistency---\n            risk_evaluation_for_testability---\n            risk_evaluation_for_correctness---\n            risk_evaluation_for_clarity\n        end\n        subgraph Verfification_List_by_Evaluation\n            direction TB\n            no_internal_inconsistencies_among_requirements---\n            All_of_performance_requirements_for_system---\n            Complete_correct_Fault_tolerance_safety_security_requirements---\n            Accurate_Complete_Allocation_of_software_functions---\n            Appropriate_Software_requirements_for_system_hazards---\n            mesurable_requirements---\n            objectively_verifiable_requirements---\n            traceable_requirements\n        end\n        subgraph Requirements_Tasks\n            direction TB\n            Preliminary_Risk_Analysis---\n            Traceability_Analysis---\n            ex_Software_Requirements_to_System_Requirements_vice_versa---\n            ex_Software_Requirements_to_Risk_Analysis---\n            Description_of_User_Characteristics---\n            Listing_of_Characteristics_and_Limitations_of_Memory---\n            Software_Requirements_Evaluation---\n            Software_User_Interface_Requirements_Analysis---\n            System_Test_Plan_Generation---\n            Acceptance_Test_Plan_Generation---\n            Ambiguity_Review_or_Analysis\n        end\n    group-->Requirements_Specification_List \n    Verfification_List_by_Evaluation-->Requirements_Tasks\n    end\n\n\n\n\n\n\n\n\n\n\nDesign Overview\n\n\n\n\nflowchart TB\n    subgraph Deign_Task\n        direction LR\n    subgraph Design_Consideration_List\n        direction TB\n        subgraph Description\n                    direction TB\n                end\n        subgraph Human_Factors_Engineering\n          direction TB\n    \n        end\n        subgraph Safety_Usability_Issues_Conisderation\n            direction TB\n\n            end\n        Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n    subgraph Design_Specificiation\n        direction TB\n        subgraph Performing_List\n            direction TB\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n        end\n    Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design \n    end\n    subgraph Design_Activity_and_Task_List\n        direction TB\n        subgraph Final_Design_activity\n            direction TB\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n            end\n            subgraph Coding_Tasks\n                direction TB\n            end\n        traceability_analysis-->Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end\n    Design_Consideration_List---Design_Specificiation---Design_Activity_and_Task_List\n\n    end\n\n\n\n\n\n\n\n\n\n\nDesign Consideration\n\n\n\n\nflowchart TB\nsubgraph Design_Consideration_List\n    direction LR\n        subgraph Requirement_Specification\n            direction TB\n            logical_representation---\n            physical_representation\n        end\n    subgraph Description\n            direction TB\n            what_to_do---\n            how_to_do                   \n        end\n    subgraph Human_Factors_Engineering\n      direction TB\n            entire_design_and_development_process---\n            device_design_requirements---\n            analyses---\n            tests\n    end\n    subgraph Safety_Usability_Issues_Conisderation\n        direction TB\n                flowcharts--- \n                state_diagrams--- \n                prototyping_tools---\n                test_plans\n        end\n        Requirement_Specification---Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n\n\n\n\n\n\n\n\n\n\nDesign Specification\n\n\n\n\nflowchart TB\nsubgraph Design_Specificiation\n        direction LR\n        subgraph Conceptual_Specification\n            direction TB\n            requirements_specification---\n            predetermined_criteria---\n            Software_risk_analysis---\n            Development_procedures---\n            coding_guidelines\n        end\n        subgraph Performing_List\n            direction TB\n            task---\n            function_analyses---\n            risk_analyses---\n            prototype_tests_and_reviews---\n            full_usability_tests\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n            SW_requirements_specification---\n            predetermined_criteria_for_SW_acceptance---\n            SW_risk_analysis---\n            Development_procedure_list---\n            coding_guidance---\n            Systems_documentation---\n            Hardware_to_be_used---\n            Parameters_to_be_measured---\n            Logical_structure---\n            Control_logic---\n            logical_processing_steps_aka_algorithms---\n            Data_structures_diagram---\n            data_flow_diagrams---\n            Definitions_of_variables---\n            description_of_where_they_are_used---\n            Error_alarm_and_warning_messages---\n            Supporting_software---\n            internal_modules_Communication_links---\n            supporting_sw_links---\n            link_with_hardware---\n            link_with_user---\n            physical_Security_measures---\n            logical_security_measures\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n            complete--- \n            correct---\n            consistent--- \n            unambiguous--- \n            feasible---\n            maintainable---\n            analyses_of_control_flow---\n            data_flow--- \n            complexity--- \n            timing--- \n            sizing--- \n            memory_allocation---\n            module_architecture---\n            traceability_analysis_of_modules--- \n            criticality_analysis\n        end\n    Conceptual_Specification---Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design  \n    end\n\n\n\n\n\n\n\n\n\n\nDesign Activity and Task\n\n\n\n\n\nflowchart TB\nsubgraph Design_Activity_and_Task_List\n        direction LR\n        subgraph Final_Design_activity\n            direction TB\n            Formal_Design_Review_Before_Design_Implementation---\n            correct_consistent_complete_accurate_testable\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n            Updated_Software_Risk_Analysis---\n            Traceability_Analysis---\n            Software_Design_Evaluation---\n            Design_Communication_Link_Analysis---\n            Module_Test_Plan_Generation---\n            Integration_Test_Plan_Generation---\n            module_Test_Design_Generation---\n            integration_Test_Design_Generation---\n            system_Test_Design_Generation---\n            acceptance_Test_Design_Generation   \n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n                each_element_implementation---\n                each_module_implementation_to_element_and_risk_analysis---\n                each_functions_implemented_to_element_and_risk_analysis---\n                Tests_for_modules_to_element_and_risk_analysis--- \n                Tests_for_functions_to_element_and_risk_analysis---\n                Tests_for_modules_to_source_code---\n                Tests_for_functions_to_source_code\n            end\n            subgraph Coding_Tasks\n                direction TB\n                Traceability_Analyses---\n                Source_Code_to_Design_Specification_and_vice_versa---\n                Test_Cases_to_Source_Code_and_to_Design_Specification---\n                Source_Code_and_Source_Code_Documentation_Evaluation---\n                Source_Code_Interface_Analysis---\n                Test_Procedure_and_Test_Case_Generation \n            end\n        traceability_analysis-->Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#testing_task",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#testing_task",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Testing_Task",
    "text": "Testing_Task\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction TB\n            subgraph Test_Plans\n                direction TB\n            end\n            subgraph Conditions\n                direction TB\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n        subgraph Code_Based_Testing\n            direction TB\n            subgraph white_box_testing\n                direction TB\n            end\n            subgraph Evaluation_of_level_of_white_box_testing\n                direction TB\n            end\n            subgraph Coverage_Metrics_of_White_Box_Testing\n                direction TB\n            end\n        white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n        end\n        subgraph Alternatives_to_White_Box_Testing\n            direction TB\n            subgraph Types_of_Functional_Software_Testing_Increasing_Cost\n                direction TB\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n            end\n            subgraph Change_in_SW\n                direction TB    \n            end\n        Types_of_Functional_Software_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW\n        end\n        \n\n        subgraph Development_Testing\n            direction TB\n            subgraph unit_level_testing\n                direction TB    \n            end\n            subgraph integration_level_testing\n                direction TB\n            end\n            subgraph system_level_testing\n                direction TB\n            end\n            subgraph Error_Detected\n                direction TB        \n            end\n        unit_level_testing-->integration_level_testing-->system_level_testing-->Error_Detected\n        end\n\n        subgraph Testing_Tasks\n            direction TB\n        end\n        subgraph User_Site_Testing\n            direction TB\n            subgraph Quality_System_Rregulation\n                direction TB\n            end\n            subgraph Understand_Terminology\n                direction TB\n            end\n            subgraph Testing\n                direction TB\n            end\n            Quality_System_Rregulation---Understand_Terminology---Testing\n        end\nConsideration_Before_Testing_Tasks---Code_Based_Testing---Alternatives_to_White_Box_Testing\nDevelopment_Testing---Testing_Tasks---User_Site_Testing\n    end\n\n\n\n\n\n\n\n\n\n\nConsideration_Before_Testing_Tasks\n\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction LR\n            subgraph Test_Plans\n                direction TB\n                should_identify_control_measures_like_traceability_analysis---\n                ensure_that_intended_coverage_is_achieved---\n                ensure_that_proper_documentation_is_prepared---\n                conduct_tests_not_by_SW_developers_but_in_other_sites\n            end\n            subgraph Conditions\n                direction TB\n                use_defined_inputs---\n                documented_outcomes---\n                gonnabe_time_consuming_activity---\n                gonnabe_difficult_activity---\n                gonnabe_imperfect_activity---\n                testing_all_program_functionality---\n                does_not_mean_100_prcnt_correction_perfection---\n                make_detailed_objective_evaluation---\n                requires_sophisticated_definition_specificiation---\n                all_test_procedures_data_results_are_documented---\n                all_test_procedures_data_results_are_suitable_for_review---\n                all_test_procedures_data_results_are_suitable_for_objective_decision_making---\n                all_test_procedures_data_results_are_suitable_for_subsequent_regression_testing\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n                make_test_plans---\n                make_test_cases---\n                plan_schedules---\n                plan_environments---\n                plan_resources_of_personnel_tools---\n                plan_methodologies---\n                plan_inputs_procedures_outputs_expected_results---\n                plan_documentation---\n                plan_reporting_criteria\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n                expected_test_outcome_is_predefined---\n                good_test_case_has_high_probability_of_exposing_errors---\n                successful_test_is_one_that_finds_errors---\n                There_is_independence_from_coding---\n                Both_application_for_user_and_SW_for_programming_expertise_are_employed---\n                Testers_use_different_tools_from_coders---\n                Examining_only_the_usual_case_is_insufficient---\n                Test_documentation_permits_its_reuse---\n                Test_documentation_permits_independent_confirmation_---\n                of_pass/fail_test_outcome_during_subsequent_review\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n\nend\n\n\n\n\n\n\n\n\n\n\nCode_Based_Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n            subgraph Code_Based_Testing\n                direction LR\n                subgraph white_box_testing\n                    direction TB\n                    identify_dead_code_never_executed---\n                    conduct_unit_test---\n                    conduct_other_level_tests\n                end\n                subgraph Evaluation_of_level_of_white_box_testing\n                    direction TB\n                    use_coverage_metrics---\n                    metrics_of_completeness_of_test_selection_criteria---\n                    coverage_should_be_commensurate_with_level_of_SW_risk---\n                    coverage_means_100_prcnt_coverage\n                end\n                subgraph Coverage_Metrics_of_White_Box_Testing\n                    direction TB\n                    Statement_Coverage---\n                    Decision_or_Branch_Coverage---\n                    Condition_Coverage---\n                    Multi_Condition_Coverage\n                    Loop_Coverage---\n                    Path_Coverage---\n                    Data_Flow_Coverage\n                end\n            white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n            end\nend"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#testing_task-1",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#testing_task-1",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Testing_Task",
    "text": "Testing_Task\n\nSolution_to_White_Box_Testing\n\n\n\n\n\n \nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Alternatives_to_White_Box_Testing\n            direction LR\n            subgraph Types_of_Testing_Increasing_Cost\n                direction TB\n                    Normal_Case---\n                    Output_Forcing---\n                    Robustness---\n                    Combinations_of_Inputs\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n                difficulty_in_linking_---\n                tests_completion_criteria_to_SW_reliability\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n                statistical_testing---\n                provide_further_assurance_of_reliability---\n                generate_randomly_test_data_from_defined_distributions---\n                distribution_defined_by_expected_use---\n                distribution_defined_by_hazardous_use---\n                distribution_defined_by_malicious_use---\n                large_test_data_cover_particular_areas_or_concerns---\n                statistical_testing_provides_high_structural_coverage---\n                statistical_testing_requires_stable_system---\n                structural_and_functional_testing_are_prerequisites_for_statistical_testing\n            end\n            subgraph Change_in_SW\n                direction TB\n                conduct_regression_analysis_and_testing---\n                should_demonstrate_correct_implementation---\n                should_demonstrate_no_adverse_impact_on_other_modules   \n            end\n            subgraph Testing_Tasks\n                direction TB\n                Test_Planning---\n                Structural_Test_Case_Identification---\n                Functional_Test_Case_Identification---\n                Traceability_Analysis_Testing---\n                Unit_Tests_to_Detailed_Design---\n                Integration_Tests_to_High_Level_Design---\n                System_Tests_to_Software_Requirements---\n                Unit_Test_Execution---\n                Integration_Test_Execution---\n                Functional_Test_Execution---\n                System_Test_Execution---\n                Acceptance_Test_Execution---\n                Test_Results_Evaluation---\n                Error_Evaluation_Resolution---\n                Final_Test_Report\n            end\n        Types_of_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW---Testing_Tasks\n        end\nend\n\n\n\n\n\n\n\n\n\n\nDevelopment_Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Development_Testing\n            direction LR\n            subgraph unit_level_testing\n                direction TB    \n                focus_on_early_examination_of_sub_program_functionality---\n                ensure_functionality_invisible_at_system_level_examined---\n                ensure_quality_software_units_furnished_for_integration\n            end\n            subgraph integration_level_testing\n                direction TB\n                focuses_on_transfer_of_data---\n                focuses_on_control_across_program's_internal_and_external_interfaces\n            end\n            subgraph system_level_testing\n                direction TB\n                demonstrate_all_specified_functionality_exists---\n                demonstrate_SW_is_trustworthy---\n                verifies_as_built_program's_functionality_and_performance_on_requirements---\n                addresses_functional_concerns_and_intended_uses---\n                like_Performance_issues---\n                like_Responses_to_stress_conditions---\n                like_Operation_of_internal_and_external_security_features---\n                like_Effectiveness_of_recovery_procedures---\n                like_disaster_recovery---\n                like_Usability---\n                like_Compatibility_with_other_SW---\n                like_Behavior_in_each_of_the_defined_hardware_configurations---\n                like_Accuracy_of_documentation\n            end\n            subgraph Error_Detected\n                direction TB        \n                should_be_logged---\n                should_be_classified---\n                should_be_reviewed---\n                should_be_resolved_before_SW_release\n            end\n        unit_level_testing-->integration_level_testing-->system_level_testing-->Error_Detected\n        end\nend"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#user_site_testing",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#user_site_testing",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "User_Site_Testing",
    "text": "User_Site_Testing\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph User_Site_Testing\n            direction LR\n            subgraph Quality_System_Rregulation\n                direction TB\n                installation---\n                inspection_procedures---\n                testing_appropriateness---\n                documentation_of_inspection---\n                testing_to_demonstrate_proper_installation\n            end\n            subgraph Understand_Terminology\n                direction TB\n                beta_test---\n                site_validation---\n                user_acceptance_test---\n                installation_verification---\n                installation_testing\n            end\n            subgraph Testing\n                direction TB\n                subgraph Requirements\n                    direction TB\n                    either_actual_or_simulated_use---\n                    verification_of_intended_functionality---\n                    constant_contact_FDA_center\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n    \n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_System_Ability\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_User_Ability\n                        direction TB\n        \n                    end \n                    subgraph Evaluation_of_Operator_Ability\n                        direction TB\n        \n                    end\n                constant_contact_FDA_center-->Follow_Predefiened_Plan-->Documented_Evidence-->\n            Evaluation_of_System_Ability-->Evaluation_of_User_Ability-->Evaluation_of_Operator_Ability    \n                end \n                        \n            \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements-->User_Site_Testing_Task\n        end\n        Quality_System_Rregulation-->    Understand_Terminology-->Testing-->User_Site_Testing_Task\n        end\nend\n\n\n\n\n\n\n\n\n\n\nTesting\n\n\n\n\nflowchart TB\n            subgraph Testing\n                direction LR\n                subgraph Requirements\n                    direction LR\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n                        either_actual_or_simulated_use---\n                        verification_of_intended_functionality---\n                        constant_contact_FDA_center---\n                        formal_summary_of_testing---\n                        record_of_formal_acceptance\n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n                        testing_plan_of_full_range_of_operating_conditions---\n                        testing_plan_to_detect_any_latent_faults---\n                        all_testing_procedures---\n                        test_input_data---\n                        test_results---\n                        hardware_installation_and_configuration---\n                        software_installation_and_configuration---\n                        exercising_measure_of_all_system_components---\n                        versions_of_all_system_components           \n                    end\n                    subgraph Evaluation\n                        direction TB\n                      subgraph Evaluation_of_System_Ability\n                            direction TB\n                            high_volume_of_data---\n                            heavy_loads_or_stresses---\n                            security\n                            subgraph fault_testing\n                                direction TB\n                                avoidance---\n                                detection---\n                                tolerance---\n                                recovery\n                            end\n                        security---fault_testing---\n                        error_message---\n                        implementation_of_safety_requirements\n                        end\n                      subgraph Evaluation_of_User_Ability\n                            direction TB\n                            ability_to_understand_system---\n                            ability_to_interface_with_system\n                        end \n                        subgraph Evaluation_of_Operator_Ability\n                            direction TB\n                            ability_to_perform_intended_functions---\n                            ability_to_respond_in_alarms---\n                            ability_to_respond_in_warnings---\n                            ability_to_respond_in_error_messages\n                        end\n\n                    end\n            Follow_Predefiened_Plan-->Documented_Evidence-->\n            Evaluation_of_System_Ability-->Evaluation_of_User_Ability-->Evaluation_of_Operator_Ability    \n            end     \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements-->User_Site_Testing_Task\n        end"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#maintenance-and-software-changes",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#maintenance-and-software-changes",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Maintenance and Software Changes",
    "text": "Maintenance and Software Changes\n\n\n\n\nflowchart LR\n    subgraph Hardware_VS_Software\n        direction LR\n        subgraph HW_maintenance_Inclusion\n            direction TB\n            preventive_hardware_maintenance_actions--- \n            component_replacement---\n            corrective_changes\n        end\n        subgraph SW_maintenance_Inclusion\n            direction TB\n            corrective---\n            perfective---\n            adaptive_maintenance---\n            not_include_preventive_maintenance_actions---\n            not_include_software_component_replacement\n        end\n    end\n    subgraph Maintenance_Type\n        direction TB\n        Corrective_maintenance---\n        Perfective_maintenance---\n        Adaptive_maintenance---\n        Sufficient_regression_analysis---\n        Sufficient_regression_testing\n    end\n    subgraph Factors_of_Validation_for_SW_change\n        direction TB\n        type_of_change---\n        development_products_affected---\n        impact_of_those_products_on_operation\n    end\n    subgraph Factors_of_Limitting_Validation_Effort\n        direction TB\n        documentation_of_design_structure---\n        documentation_of_interrelationships_of_modules---\n        documentation_of_interrelationships_of_interfaces---\n        test_documentation---\n    test_cases---\n        results_of_previous_verification_and_validation_testing\n    end\n    subgraph Maintenance_tasks\n        direction TB\n        Software_Validation_Plan_Revision---\n        Anomaly_Evaluation---\n        Problem_Identification_and_Resolution_Tracking---\n        Proposed_Change_Assessment---\n        Task_Iteration---\n        Documentation_Updating\n    end\nHardware_VS_Software-->Maintenance_Type-->Factors_of_Validation_for_SW_change-->\nFactors_of_Limitting_Validation_Effort-->Maintenance_tasks"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation-of-quality-system-software",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation-of-quality-system-software",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation of Quality System Software",
    "text": "Validation of Quality System Software\n\nOverview\n\n\n\n\nflowchart LR\n    subgraph Use_of_Computers_and_automated_equipment\n        direction TB\n        medical_device_design---\n        laboratory_testing_and_analysis---\n        product_inspection_and_acceptance---\n        production_and_process_control---\n        environmental_controls---\n        packaging---\n        labeling---\n        traceability---\n        document_control---\n        complaint_management---\n        programmable_logic_controllers---\n        digital_function_controllers---\n        statistical_process_control---\n        supervisory_control_and_data_acquisition---\n        robotics---\n        human_machine_interfaces---\n        input_output_devices---\n        computer_operating_systems\n    end\n    subgraph Factors_in_Validation\n        direction TB\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System-->Validation_Supporting_Factors-->Factors_of_Validation_Evidence_Level-->\nFactors_of_Easing_Validation_Effort\n    end\n    subgraph Documented_User_Requirements\n        direction TB\n        intended_use_of_software_or_automated_equipment---\n      level_of_dependency_on_software_or_equipment\n    end\n    subgraph List_That_Must_Be_Defined_by_User\n        direction TB\n        \n    end\n    subgraph Documentation_List\n        direction TB\n        documented_protocol---\n        documented_validation_results\n        subgraph Documented_Test_Cases\n            direction TB\n        \n        end\n        documented_validation_results---Documented_Test_Cases\n    end\n\n    subgraph Manufaturer's_Responsbility\n        direction TB\n        \n    end\nUse_of_Computers_and_automated_equipment---Factors_in_Validation---Documented_User_Requirements---\nList_That_Must_Be_Defined_by_User---Documentation_List---Manufaturer's_Responsbility\n\n\n\n\n\n\n\n\n\n\nFactors in Validation\n\n\n\n\nflowchart LR\n    subgraph Factors_in_Validation\n        direction LR\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n                electronic_records_regulation---\n                electronic_signatures_regulation---\n                regulations_establishment---\n                security---\n                data_integrity---\n                validation_requirements \n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n            verifications_of_outputs_from_each_stage--- \n            verifications_of_outputs_throught_SW_life_cycle---\n            checking_for_proper_operation_in_intended_use_environment\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n            risk_posed_by_automated_operation---\n            complexity_of_process_software---\n            degree_of_dependence_on_automated_process\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System-->Validation_Supporting_Factors-->Factors_of_Validation_Evidence_Level-->\nFactors_of_Easing_Validation_Effort\n    end"
  }
]