---
title: "Template Variabler"
subtitle: DAG Creation, Bash Operator, Task Performance Subject, 
description: |
  template
categories:
  - Engineering
author: Kwangmin Kim
date: 05/01/2023
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
comments: 
  utterances: 
    repo: ./docs/comments
draft: False
---

<ul class="nav nav-pills" id="language-tab" role="tablist">
  <li class="nav-item" role="presentation">
    <button class="nav-link active" id="Korean-tab" data-bs-toggle="tab" data-bs-target="#Korean" type="button" role="tab" aria-controls="Korean" aria-selected="true">Korean</button>
  </li>
  <li class="nav-item" role="presentation">
    <button class="nav-link" id="English-tab" data-bs-toggle="tab" data-bs-target="#English" type="button" role="tab" aria-controls="knitr" aria-selected="false">English</button>
  </li>

<div class="tab-content" id="language-tabcontent">

<div class="tab-pane fade  show active" id="Korean" role="tabpanel" aria-labelledby="Korean-tab">

::: {#Korean .tab-pane .fade .show .active role="tabpanel" aria-labelledby="Korean-tab"}

# Goal

* posgres DB를 container로 띄우기
* airflow의 connection & hook 설정

# Docker Compose Interpretation

* 목적
    * 1 개 이상 도커 컨테이너 생성시 스크립트 하나로 컨테이너들의 설정을 관리할 수 있도록 해주는 Docker 의 기능 중 하나
    * 각 각의 container 설정 관리 뿐만 아니라 containers간의 연관관계 및 dependency까지 설정할 수 있다.
    * 일부 containers를 같은 network에서 띄워지는지도 설정할 수 있는 network 설정도 할 수 있다.
* 작성방법
    * 모든 설정은 docker compose.yaml 파일에 컨테이너들의 설정 내용을 입력
* 도커컴포즈 서비스 시작
    * yaml 파일이 있는 위치에서 sudo docker compose up 명령 입력하면 docker compose.yaml에 있는 모든 설정이 적용된다.
    * 기본적으로 Docker 서비스가 설치되어 있어야 함
* docker compose.yaml 파일의 구성
    * yaml 파일은 json 이나 xml 과 같이 **key, value 로 구성되며 계층적인 구조**를 가지며 파이썬처럼 들여쓰기 문법을 사용함
    * 다시 말해서, json 이나 xml은 파이썬의 dictionary 같이 nested {key:value} structure로 작성할 수 있다.
* docker compose.yaml 파일의 1 Level 내용

```markdown
version: '3.8' # yaml 파일의버전 정보 옵션
x-airflow-common: # 'x-': Extention Fields(각 서비스 항목에 또는 container에  공통 적용될 항목들 정의)
services: # 컨테이너로 실행할 서비스 정의로 가장 신경써서 적어야할 부분
volumes:  # 컨테이너에 할당할 volume 정의
networks: # 컨테이너에 연결할 network 정의
```
위의 내용에서 key값은  version, x-airflow-common, services, volumes, networks

* x-airflow-common: 공통 지정할 항목을 를 붙여서 지정

```markdown
x-airflow-common
    &airflow-common # 공통 지정 parameters 정의: image, environment, depends_on
    image: ${AIRFLOW_IMAGE:-apache/airflow:2.5.2}
    environment
        &airflow-common-env
        AIRFLOW__CORE__DEFAULT_TIMEZONE: 'Asia/Seoul'
        AIRFLOW__CORE__EXECUTOR: CeleryExecutor
        AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    depends_on # containers 실행 순서를 결정
        &airflow-common-depends-on # 공통 지정 parameters 정의: redis, postgres
        redis:
            condition: service_healthy
        postgres:
            condition: service_healthy
```
* 공통 지정 항목1: &airflow-common
    * 변수 또는 parameter: image, environment, depends_on
* 공통 지정 항목2: &airflow-common-env
    * 변수 또는 parameter: AIRFLOW__CORE__DEFAULT_TIMEZONE, AIRFLOW__CORE__EXECUTOR, AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
* 공통 지정 항목3: &airflow-common-depends-on
    * 변수 또는 parameter: redis, postgres

* services: 컨테이너로 올릴 서비스 지정
    ```markdown
    services:
       postgres:
         image: postgres:13
         environment:
           POSTGRES_USER: airflow
           POSTGRES_PASSWORD: airflow
           POSTGRES_DB: airflow
         volumes:
           - postgres-db-volume:/var/lib/postgresql/data
         healthcheck:
           test: ["CMD", "pg_isready", "-U", "airflow"]
           interval: 10s
           retries: 5
           start_period: 5s
        ports
            - 5432:5432
         restart: always
    ```
    * `image: postgres:13` 은 image는 postgre:13 버전의 image를 쓴다는 것이고 이 image가 local에 있으면 그대로 쓰고 없으면 인터넷에서 download됨. 
    * `environment:` 는 postgres OS에 설정할 환경 변수들
    * `volumes:` container와 연결할 local file system 경로를 의미
        * `postgres-db-volume:/var/lib/postgresql/data` `:`을 기준으로 왼쪽이 local file system의 경로 오른쪽이 연결할 container의 directory. 이 과정을 **mount** 시켰다라고 말함
        * `postgres-db-volume` 문서 제일 하단에 미리 만들어져 있음
        * container가 실행되었다가 (띄어졌다가) 꺼지면 (내려지면) 안에 있는 data들이 모두 사라지기 때문에 mount시키는 것이 필요함. 특히, DB container는 mount가 잘 됐는지 확인해야함
        * `postgres-db-volume:/var/lib/postgresql/data`는 postgresql의 data가 저장되는 directory를 local file system으로 연결시켜 놓은 것  
    * `healthcheck:`  container가 상태 꺼졌는지 켜졌는지 확인
    * `ports`: container에 접속하기 위해 공개할 port를 명시
        * `5432:5432` `:` 을 기준으로 왼쪽이 local에서 web에 접속할 port 번호고 오른쪽이 service가 갖고 있는 port번호. 다시 말해서, wsl 시스템안에 여러 컨테이너들이 있고 그 중 postgres 이미지가 깔려 있다면 postgres는 5432 port를 가지고 있는 상태이다. postgres 이미지에 접근하려면 wsl의 port를 통해서 접근해야하는데 그 wsl의 port가 5432로 지정된 것을 의미한다.
    * `restart: always` container가 죽으면 언제 새로 띄워주겠냐는 것이고 always니까 항상 새로 띄워줌 

    ```markdown
     redis:
       image: redis:latest
       expose:
         - 6379
       healthcheck:
         test: ["CMD", "redis-cli", "ping"]
         interval: 10s
         timeout: 30s
         retries: 50
         start_period: 30s
       restart: always

    ```
    * `expose: 6379` 이것 역시 port번호인데 외부와 연결될 때 사용되는 게 아니라 내부 다른 containers와 연결시 사용되는 port번호로 expose parameter로 공개 설정한다.

    ```markdown
    services
      airflow-webserver:
        <<: *airflow-common # 공통 지정 parameters 호출: image, environment, depends_on
        command: webserver # container를 띄울 때 실행할 명령어
        ports:
          - "8080:8080"
        healthcheck:
          test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
          interval: 30s
          timeout: 10s
          retries: 5
          start_period: 30s
        restart: always
        depends_on: # 2번째 depends_on 선언으로 x-airflow-common의 depends_on 의 내용은 무시된다.
          <<: *airflow-common-depends-on # 공통 지정 parameters 호출: redis, postgres
          airflow-init:
            condition: service_completed_successfully
    ```
    * services: 1-level (x-airflow-common 같은 level)
        * airflow-webserver, airflow-scheduler, redis, postgres 등이 같은 level의 서비스 항목으로 열거 된다.
    * depends_on: containers를 띄우는 순서를 설정하는 부분으로 위의 예시는
        * redis, postgres, airflow-init을 띄우고 나서 airflow-webserver를 띄우겠다는 것.
        * [redis, postgres, airflow-init]>>airflow-webserver

    ```markdown

    airflow-scheduler: # 1-level
        <<: *airflow-common
        command: scheduler
        healthcheck:
          test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
          interval: 30s
          timeout: 10s
          retries: 5
          start_period: 30s
        restart: always
        depends_on:
          <<: *airflow-common-depends-on
          airflow-init:
            condition: service_completed_successfully
    ```
* volumes: 컨테이너와 연결하기 위한 볼륨 (공간) 정보
    ```markdown
    volumns:
        postgree-db-volume: #새로 만들 볼륨 이름

    ```
    * 볼륨에 대한 정보 확인하기
        1.볼륨 리스트 보기 : sudo docker volume ls (현재 만들어진 volumes 리스트와 volumne_id가 보임)
        2.볼륨 상세 보기 : sudo docker volume inspect {volume_id}
* networks: 컨테이너의 network 정보 구성
    ```markdown
    networks:
        network_custom: # 새로 만들 네트워크 이름
            driver: bridge
            ipam:
                driver: default
                config:
                    - subnet : 172.18.0.0/16 # 네트워크 IP의 주소값이 2^16 개, host가 가질 IP의 주소값은 2^16-2 만큼을 할당할 수 있다.
                      gateway: 172.18.0.1
    ```
    * config:
        - subnet : 172.18.0.0/16
        - gateway: 172.18.0.1

    주어진 서브넷은 172.18.0.0/16으로 주어졌는데 이 경우, 16비트의 네트워크 부분이 주소에 할당되어 있다. 네트워크 부분은 호스트를 식별하는데 사용되는 부분이 아닌 네트워크를 식별하는데 사용되는 부분이다. 이러한 설정에서는 네트워크 IP의 주소값이 2^16 (또는 65536) 개로 할당된다. 호스트는 네트워크에서 실제로 사용되는 장치 또는 시스템을 의미한다. 이 설정에서는 게이트웨이 주소를 172.18.0.1로 지정하여 해당 IP 주소를 게이트웨이로 사용한다. 따라서 호스트가 할당받을 수 있는 IP 주소값의 개수는 네트워크 IP 주소값에서 게이트웨이 주소를 제외한 개수이다. 즉, 2^16-2 (또는 65534) 개의 주소를 호스트에 할당할 수 있다.
    * 네트워크에 대한 정보 확인하기
        1. 네트워크 리스트 보기 : sudo docker network ls
        2. 네트워크 상세 보기 : sudo docker network inspect {network_id}

# Postgres 컨테이너 올리기

## Postgress 컨테이너 추가하기

* postgres_custom 이라는 이름의 컨테이너 서비스 추가하기

## 컨테이너 고정 IP 할당하기

* 기본적으로 컨테이너들은 유동 IP를 지니며 (재기동시 IP 변경 가능)
* 고정 IP를 할당하려면 networks를 만들어 할당해야 함.
* networks 를 지정하지 않은 컨테이너들(airflow를 설치하면서 기본적으로 설치되는 containers)은 default network에 묶이게 됨
* 따라서 동일 네트워크에 두고 싶은 컨테이너들은 모두 동일 netwworks 할당 필요
  ```markdown
  services:
    postgres_custom:
      image: postgres:13
      environment:
        POSTGRES_USER: kmkim
        POSTGRES_PASSWORD: kmkim
        POSTGRES_DB: kmkim
        TZ: Asia/Seoul
      volumes:
        - postgres-custom-db-volume:/var/lib/postgresql/data
      ports:
        - 5432:5432
      networks:
        network_custom: # 밑에서 정의한 network_custom 을 쓰겠다는 의미
          ipv4_address: 172.28.0.3 # 할당된 IP
  
  networks:
    network_custom: 
      driver: bridge
      ipam:
          driver: default
          config:
              - subnet: 172.28.0.0/16 # 네트 워크 ID 주소 밑에 16개/host id 주소 밑에 16개를 할당하겠다는 의미
                gateway: 172.28.0.1 # default 네트워크 (172.18.0.0)가 쓰고 있지 않은 서브넷으로 구성
  volumes:
    postgres-db-volume:
    postgres-custom-db-volume:
  ```
* Postgres_custom 컨테이너 뿐만 아니라 다른 컨테이너에도 network_custom 할당하고 IP 부여
  ```markdown
  postgres_custom:  # 172.28.0.3
  postgres:  # 172.28.0.3 +  포트 노출 설정:(5431:5432)
  redis: # 172.28.0.5
  airflow-webserver: # 172.28.0.6
  airflow-scheduler: # 172.28.0.7
  airflow-worker: # 172.28.0.8
  airflow-triggerer: # 172.28.0.9
  airflow-init: # 172.28.0.10
  ```
    * postgres:  # airflow가 기본 메타DB로 쓰고 있는 postgress 컨테이너에 포트 번호 5431로 노출

## DB 접속하기

* Dbeaver: 설치 https://dbeaver.io/

## Airflow에서 사용법

* 오퍼레이터 파라미터 입력시 중괄호 {} 2개를 이용하면 Airflow에서 기본적으로 제공하는 변수들을 치환된 값으로 입력할 수 있음. (ex: 수행 날짜, DAG_ID)
    - https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html
* 모든 오퍼레이터, 모든 파라미터에 Template 변수 적용이 가능한가? No!
* Airflow 문서에서 어떤 파라미터에 Template 변수 적용 가능한지 확인 필요
    - https://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/index.html

# BashOperator with Template

## BashOperator

* Bash 오퍼레이터는 어떤 파라미터에 Template를 쓸 수 있는가?
* 파라미터
    * bash_command (str)
    * env (dict[str, str] | None)
    * append_env (bool)
    * output_encoding (str)
    * skip_exit_code (int)
    * cwd (str | None)
* https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html


```markdown
bash_t1 = BashOperator(
    task_id='bash_t1',
    bash_command='echo "End date is {{ data_interval_end }}"'
)
bash_t2 = BashOperator(
    task_id='bash_t2',
    env={'START_DATE': '{{ data_interval_start | ds}}','END_DATE':'{{ data_interval_end | ds }}'},
    bash_command='echo "Start date is $START_DATE " && ''echo "End date is $END_DATE"'
)

```

# Airflow Date

## 데이터 추출 예시

* Daily ETL 처리를 위한 조회 쿼리(2023/02/25 0시 실행)
* example: 등록 테이블

|REG_DATE|NAME|ADDRESS|
|-----|:-:|:-:|
|2023-02-24 15:34:35|홍길동|Busan|
|2023-02-24 19:14:42|김태희|Seoul|
|2023-02-24 23:52:19|조인성|Daejeon|

```markdown
SELECT NAME, ADDRESS
FROM TBL_REG
WHERE REG_DATE BETWEEN TIMESTAMP('2023-02-24 00:00:00')
AND TIMESTAMP('2023-02-24 23:59:59')
```
데이터 관점의 시작일: 2023-02-24
데이터 관점의 종료일: 2023-02-25


## Airflow 날짜 Template 변수

* 예시: 일 배치
    * ex. 2023-02-24 이전 배치일 (논리적 기준일)
        * = data_interval_start
        * = dag_run.logical_date
        * = ds (yyyy-mm-dd 형식)
        * = ts (타임스탬프)
        * = execution_date (과거버전)
    * ex. 2023-02-25 배치일
        * = data_interval_end
        * = 
        * = 
        * = 
        * = next_execution_date (과거버전)

# Python Operator with Template

## Python 오퍼레이터에서 Template 변수 사용

* Python 오퍼레이터는 어떤 파라미터에 Template을 쓸 수 있는가?
* 파라미터
    * python_callable
    * op_kwargs
    * op_args
    * templates_dict
    * templates_exts
    * show_return_value_in_logs
* https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html

    ```markdown
    def python_function1(start_date, end_date, **kwargs):
        print(start_date)
        print(end_date)

    python_t1 = PythonOperator(
        task_id='python_t1',
        python_callable=python_function,
        op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}
    )
    ```

* 파이썬 오퍼레이터는 **kwargs에 Template 변수들을 자동으로 제공해주고 있음

    ```markdown
    @task(task_id='python_t2')
    def python_function2(**kwargs):
        print(kwargs)
        print('ds:' + kwargs['ds'])
        print('ts:' + str(kwargs['ts']))
        print('data_interval_start:' + str(kwargs['data_interval_start']))
        print('data_interval_end:' + str(kwargs['data_interval_end']))
        print('task_instance': + str(kwargs['ti']))
    python_function2()
    ```

# Bash Operator with Macro

## Macro 변수의 이해

* Macro 변수의 필요성

    ```markdown
    sql = f'''
    SELECT NAME, ADDRESS
    FROM TBL_REG
    WHERE REG_DATE BETWEEN ?? AND ??
    '''
    ```

    DAG 스케줄은 매일 말일에 도는 스케줄인데
    BETWEEN 값을 전월 마지막일부터 어제 날짜까지 주고
    싶은데 어떻게 하지?
    예를 들어
    배치일이 1월 31일이면 12월 31일부터 1월 30일 까지
    배치일이 2월 28일이면 1월 31일부터 2월 27일까지
    BETWEEN 이 설정되었으면 좋겠어.
    DAG 스케줄이 월 단위이니까
    Template 변수에서 data_interval_start 값은 한달 전 말일
    이니까 시작일은 해결될 것 같은데 끝 부분은 어떻게 만들지? 
    data_interval_end 에서 하루 뺀 값이 나와야 하는데…

* Template 변수 기반 다양한 날짜 연산이 가능하도록 연산 모듈을 제공하고 있음

    |Variable|Description|
    |:--:|------|
    |macros.datetime|The standard lib's datetime.datetime|
    |macros.timedelta|The standard lib's datetime.timedelta|
    |macros.dateutil|A reference to the dateutil package|
    |macros.time|The standard lib's time|
    |macros.uuid|The standard lib's uuid|
    |macros.random|The standard lib's random|

    * macros.datetime & macros.dateutil: 날짜 연산에 유용한 파이썬 라이브러리
* Macro를 잘 쓰려면 파이썬 datetime 및 dateutil 라이브러리에 익숙해져야 함.

## 파이썬 datetime + dateutil 라이브러리 이해

```markdown
from datetime import datetime
from dateutil import relativedelta

now = datetime(year=2003, month=3, day=30)
print('current time:'+str(now))
print('-------------month operation-------------')
print(now+relativedelta.relativedelta(month=1)) # 1월로 변경
print(now.replace(month=1)) # 1월로 변경
print(now+relativedelta.relativedelta(months=-1)) # 1개월 빼기
print('-------------day operation-------------')
print(now+relativedelta.relativedelta(day=1)) #1일로 변경
print(now.replace(day=1)) #1일로 변경
print(now+relativedelta.relativedelta(days=-1)) #1일 빼기
print('-------------multiple operations-------------')
print(now+relativedelta.relativedelta(months=-1)+relativedelta.relativedelta(days=-1)) #1개월, 1일 빼기
```

## Bash 오퍼레이터에서 Macro 변수 활용하기

* 예시1. 매월 말일 수행되는 Dag에서 변수 START_DATE: 전월 말일, 변수 END_DATE: 어제로 env 셋팅하기
* 예시2. 매월 둘째주 토요일에 수행되는 Dag에서 변수 START_DATE: 2주 전 월요일 변수 END_DATE: 2주 전 토요일로 env 셋팅하기
* 변수는 YYYY-MM-DD 형식으로 나오도록 할 것

    ```markdown
    t1 = BashOperator(
        task_id='t1',
        env={'START_DATE':''},
    )
    ```

이 부분에 template + macro 활용

# Python 오퍼레이터 with macro

* 어떤 파라미터가 Template 변수를 지원할까?
* 패러미터
    * python_callable (Callable | None)
    * op_kwargs
    * op_args
    * templates_dict
    * templates_exts
    * show_return_value_in_logs
* https://airflow.apache.org/docs/apacheairflow/stable/_api/airflow/operators/python/index.html#airflow.operators.python.PythonOperator

    ```markdown
    @task(task_id='task_using_macros',
        templates_dict={'start_date':'{{ (data_interval_end.in_timezone("Asia/Seoul")
    + macros.dateutil.relativedelta.relativedelta(months=-1, day=1)) | ds }}',
    'end_date': '{{
    (data_interval_end.in_timezone("Asia/Seoul").replace(day=1) +
    macros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'
        }
    )
    
    def get_datetime_macro(**kwargs):
        templates_dict = kwargs.get('templates_dict') or {}
        if templates_dict:
        start_date = templates_dict.get('start_date') or 'start_date없음'
        end_date = templates_dict.get('end_date') or 'end_date없음'
        print(start_date)
        print(end_date)
    ```
* 그러나 Python 오퍼레이터에서 굳이 macro를 사용할 필요가 있을까? 날짜 연산을 DAG안에서 직접 할 수 있다면?
    * macro 사용
    ```markdown
    @task(task_id='task_using_macros',
        templates_dict={'start_date':'{{ (data_interval_end.in_timezone("Asia/Seoul") + macros.dateutil.relativedelta.relativedelta(months=-1,day=1)) | ds }}',
        'end_date': '{{ (data_interval_end.in_timezone("Asia/Seoul").replace(day=1) +
        macros.dateutil.relativedelta.relativedelta(days=-1)) | ds }}'
        }
    )

    def get_datetime_macro(**kwargs):
        templates_dict = kwargs.get('templates_dict') or {}
        if templates_dict:
            start_date = templates_dict.get('start_date') or 'start_date없음'
            end_date = templates_dict.get('end_date') or 'end_date없음'
            print(start_date)
            print(end_date)

    @task(task_id='task_direct_calc')
    def get_datetime_calc(**kwargs):
        from dateutil.relativedelta import relativedelta
        data_interval_end = kwargs['data_interval_end']
    ```

    * 직접 연산

    ```markdown
    prev_month_day_first = data_interval_end.in_timezone('Asia/Seoul') + relativedelta(months=-1, day=1)
    prev_month_day_last = data_interval_end.in_timezone('Asia/Seoul').replace(day=1) + relativedelta(days=-1)
    print(prev_month_day_first.strftime('%Y-%m-%d'))
    print(prev_month_day_last.strftime('%Y-%m-%d'))
    ```
:::
</div>

<div class="tab-pane fade" id="English" role="tabpanel" aria-labelledby="English-tab">

::: {#English .tab-pane .fade role="tabpanel" aria-labelledby="English-tab"}

:::


</div>


# Go to Blog Content List

[Blog Content List](../../content_list.qmd)