---
title: "Basics (3) - Special Matrices"
subtitle: template
description: |
  template
categories:
  - Mathematics
author: Kwangmin Kim
date: 03/31/2023
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: False
---

## Special Matrix

### Square Matrix

A square matrix $\mathbf{A}$ is a matrix with the same number of rows and columns, i.e., $\mathbf{A}$ is an $n \times n$ matrix.

For example, the following is a $3 \times 3$ square matrix:

$$
\mathbf A= \begin{bmatrix}
1 & 4 & 7\\
2 & 5 & 8\\
3 & 6 & 9
\end{bmatrix}
$$

#### Properties

Let $\mathbf{A}$ be an $n\times n$ square matrix. Then, the following properties hold:

* $\mathbf{A}$ is invertible if and only if $\text{det}(\mathbf{A}) \neq 0$.
* The trace of $\mathbf{A}$ is defined as $\text{tr}(\mathbf{A}) = \sum_{i=1}^n a_{ii}$, where $a_{ii}$ is the $i$th diagonal element of $\mathbf{A}$.
* If $\mathbf{A}$ is symmetric, then it has $n$ real eigenvalues and an orthonormal set of eigenvectors.
* If $\mathbf{A}$ is diagonalizable, then $\mathbf{A}$ can be written as $\mathbf{A} = \mathbf{PDP}^{-1}$, where $\mathbf{P}$ is the matrix whose columns are the eigenvectors of $\mathbf{A}$, and $\mathbf{D}$ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues.
* The transpose of $\mathbf{A}$, denoted $\mathbf{A}^\top$, is obtained by reflecting $\mathbf{A}$ across its main diagonal. That is, $(\mathbf{A}^\top){ij} = a{ji}$.

Here's an example of a $3 \times 3$ matrix:

$$
\mathbf A= \begin{bmatrix}
1 & 4 & 7\\
2 & 5 & 8\\
3 & 6 & 9
\end{bmatrix}
$$

With this matrix, we can see that $\text{det}(\mathbf{A}) = 0$, so $\mathbf{A}$ is not invertible. The trace of $\mathbf{A}$ is $\text{tr}(\mathbf{A}) = 1 + 5 + 9 = 15$. Since $\mathbf{A}$ is not symmetric, we cannot say that it has real eigenvalues and an orthonormal set of eigenvectors. However, we can check that $\mathbf{A}$ is diagonalizable, and we can find that $\mathbf{A} = \mathbf{PDP}^{-1}$ with

$$
\begin{equation*}
P=\begin{pmatrix}
-0.8252 & -0.2886 & 0.4848\\
-0.3779 & -0.7551 & -0.5375\\
0.2185 & -0.5800 & 0.7830
\end{pmatrix}, \text{ } D=\begin{pmatrix}
16.1168 & 0 & 0\\
0 & -1.1168 & 0\\
0 & 0 & 0
\end{pmatrix}
\end{equation*}
$$

### Diagonal Matrix

A diagonal matrix is a square matrix in which all the off-diagonal elements are zero. The diagonal elements can be any scalar value. 

$$
\begin{equation*}
\mathbf{D} = \begin{pmatrix}
d_{1} & 0 & 0 & \cdots & 0 \\
0 & d_{2} & 0 & \cdots & 0 \\
0 & 0 & d_{3} & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & d_{n}
\end{pmatrix}
\end{equation*}
$$


Here, $\mathbf{D}$ is an $n \times n$ diagonal matrix with diagonal elements $d_1, d_2, \ldots, d_n$. An example of a $3 \times 3$ diagonal matrix is:

$$
\begin{equation*}
\mathbf{D} = \begin{pmatrix}
2 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & 4
\end{pmatrix}
\end{equation*}
$$

#### Properties

A diagonal matrix is a square matrix in which all the off-diagonal elements are zero, i.e., $a_{ij} = 0$ for $i \neq j$. Some properties of a diagonal matrix are:

* For two diagnoal matrices $\mathbf D$ and $\mathbf E$, 
$$
\begin{equation*}
\mathbf{DE}=
\begin{pmatrix}
d_{1}e_{1} & 0 & 0 & \cdots & 0 \\
0 & d_{2}e_{2} & 0 & \cdots & 0 \\
0 & 0 & d_{3}e_{3} & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & d_{n}e_{n}
\end{pmatrix}
\end{equation*}
$$

* The determinant of a diagonal matrix is the product of its diagonal entries.
* The trace of a diagonal matrix is the sum of its diagonal entries.
* The inverse of a non-singular diagonal matrix is a diagonal matrix with the reciprocal of its diagonal entries as its diagonal entries.

An example of a $3 \times 3$ diagonal matrix is:

$$
\begin{equation*}
\mathbf{D} = \begin{pmatrix}
5 & 0 & 0 \\
0 & -3 & 0 \\
0 & 0 & 7
\end{pmatrix}
\end{equation*}
$$

### Identity Matrix

An identity matrix is a square matrix in which all the diagonal elements are equal to $1$ and all the off-diagonal elements are equal to 0. The notation for an identity matrix of size n is $\mathbf{I}_n$.

Here's an example of a 3x3 identity matrix:
$$
\begin{equation}
\mathbf{I}_3 =
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{bmatrix}
\end{equation}
$$

#### Properties

An identity matrix, denoted by $\mathbf{I}$, is a square matrix with ones on the diagonal and zeros elsewhere.

For example, the $3 \times 3$ identity matrix is:
$$
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix}
$$

Some properties of an identity matrix include:

* Multiplying any matrix by an identity matrix results in the same matrix: $\mathbf{A} \mathbf{I} = \mathbf{I} \mathbf{A} = \mathbf{A}$.
* The product of any matrix and its corresponding inverse is an identity matrix: $\mathbf{A} \mathbf{A}^{-1} = \mathbf{A}^{-1} \mathbf{A} = \mathbf{I}$.
* The determinant of an identity matrix is 1: $\det(\mathbf{I}) = 1$.
* An identity matrix is symmetric: $\mathbf{I} = \mathbf{I}^T$.


### Symmetric Matrix

A symmetric matrix is a square matrix that is equal to its own transpose, i.e., $\mathbf{A} = \mathbf{A}^T$. Let $\mathbf{A}$ be an $n \times n$ matrix, then $\mathbf{A}$ is symmetric if and only if $a_{ij} = a_{ji}$ for all $i$ and $j$ such that $1 \le i$, $j \le n$.

Here's an example of a symmetric matrix:
$$
\mathbf{A} = \begin{pmatrix}
1 & 2 & 3 \\
2 & 4 & 5 \\
3 & 5 & 6
\end{pmatrix}
$$

#### Properties

A symmetric matrix is a square matrix that is equal to its own transpose. Some properties of symmetric matrices include:

* The diagonal entries are real numbers.
* The matrix is diagonalizable, meaning it can be expressed as a product of diagonal and orthogonal matrices.
* The eigenvalues of a symmetric matrix are real numbers.
* The eigenvectors corresponding to different eigenvalues are orthogonal.
* The sum and difference of two symmetric matrices is also symmetric.

$$
\mathbf A = \begin{pmatrix}
1 & 2 & 3 \\
2 & 4 & 5 \\
3 & 5 & 6 
\end{pmatrix}
$$

### Idempotent Matrix

An idempotent matrix is a square matrix that when multiplied by itself yields itself. Formally, an idempotent matrix $\mathbf{P}$ satisfies $\mathbf{P}^2 = \mathbf{P}$.

An example of an idempotent matrix is:

$$
\begin{equation}
\mathbf{P} =
\begin{bmatrix}
1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 1 
\end{bmatrix}
\end{equation}
$$

#### Properties

* $\mathbf{A}^2 = \mathbf A$.
* If $\mathbf{A}$ is an idempotent matrix, $\mathbf{I_n}-\mathbf{A}$ is also an idempotent matrix.
$$
\begin{aligned}
(\mathbf{I_n}-\mathbf{A})^2&=(\mathbf{I_n}-\mathbf{A})(\mathbf{I_n}-\mathbf{A})\\
&=\mathbf{I_n}\mathbf{I_n}-\mathbf{I_n}\mathbf{A}-\mathbf{A}\mathbf{I_n}-\mathbf{A}\mathbf{A}\\
&=\mathbf{I_n}\mathbf{I_n}-\mathbf{A}\mathbf{A}\\
&=\mathbf{I_n}-\mathbf{A}
\end{aligned}
$$
* The determinant of $\mathbf{A}$ is either 0 or 1.
* If $\mathbf{A}$ is symmetric, $\mathbf{A}$ is idempotent if only if the eigenvalue of $\mathbf{A}$ is either $0$ or $1$.
* The rank of $\mathbf A$ is equal to the trace of $\mathbf A$, which is the sum of the diagonal elements of $\mathbf A$.

### One Matrix

The one matrix, denoted as $\mathbf{J}$, is a matrix in which every entry is equal to 1.

Here is an example of a 3x3 one matrix:

$$
\mathbf{J} =
\begin{pmatrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1 
\end{pmatrix}
$$


#### Properties

* $\mathbf J$ is a square matrix
* $\mathbf J$ is symmetric
* $\mathbf J$ is idempotent
* $\mathbf J$ has rank 1
If A is any $n \times n$ matrix, then $\mathbf{AJ} = \mathbf{JA} = trace(\mathbf{A})\mathbf{J}$, where $\operatorname{trace}(\mathbf{A})$ is the sum of the diagonal elements of $\mathbf{A}$.
* $\mathbf J_{m\times n}$ can be represented as the product of two vectors, $\mathbf 1_{m}$, $\mathbf 1_{n}$, i.e., $\mathbf J_{m\times n} = \mathbf 1_{m} \mathbf 1_{n}^T$ 

#### Applications

* calculate a sum using a $\mathbf 1_n$ vector for $\mathbf x_n$:
$$
\mathbf 1^T\mathbf x =\sum_{i=1}^{n}1\times x_i=x_1+x_2+\dots+x_n
$$

* calculate a mean using a $\mathbf 1_n$ vector for $\mathbf x_n$:

$$
\bar{x}=\frac{1}{n}\mathbf 1^T\mathbf x =\frac{1}{n}\sum_{i=1}^{n}1\times x_i=\frac{1}{n}(x_1+x_2+\dots+x_n)
$$

* calculate $n$ column sums of a dataset using a $\mathbf 1_m$ vector for $\mathbf X_{m\times n}$:
$$
\bar{\mathbf x}=\frac{1}{m}\mathbf X^T\mathbf 1
$$
$$
\begin{aligned}
\bar{\mathbf x}&=\frac{1}{m}\mathbf X^T\mathbf 1 \\
&=\frac{1}{m}
\begin{bmatrix}
x_{11} & x_{21} & \cdots & x_{m1} \\
x_{12} & x_{22} & \cdots & x_{m2} \\
\vdots & \vdots & \ddots & \vdots \\
x_{1n} & x_{2n} & \cdots & x_{mn}
\end{bmatrix}
\begin{bmatrix}
1_1 \\ 1_2 \\ \vdots \\ 1_m
\end{bmatrix}\\
&=\frac{1}{m}
\begin{bmatrix}
x_{11} + x_{21} + \cdots + x_{m1} \\
x_{12} + x_{22} + \cdots + x_{m2} \\
\vdots  \\
x_{1n} + x_{2n} + \cdots + x_{mn}
\end{bmatrix} \\
&=\frac{1}{m}
\begin{bmatrix}
\sum_{i=1}^{m }x_{i1} \\
\sum_{i=1}^{m }x_{i2} \\
\vdots  \\
\sum_{i=1}^{m }x_{jn} 
\end{bmatrix} \\
&=
\begin{bmatrix}
\bar{x}_{1} \\
\bar{x}_{2} \\
\vdots  \\
\bar{x}_{n}
\end{bmatrix} \\
&=\bar{\mathbf x}
\end{aligned}
$$


### Centering Matrix

A centering matrix is a square matrix that is used in multivariate statistical analysis to center data by subtracting the mean of each variable from each observation. The centering matrix is a symmetric and idempotent matrix, meaning that it is equal to its own transpose and that multiplying it by itself results in the same matrix. The centering matrix is defined as:

$$
\begin{equation}
  \mathbf C = \mathbf I - \frac{1}{m}\mathbf J
\end{equation}
$$
where $\mathbf I$ is the identity matrix, $\mathbf J$ is a matrix of ones, and $m$ is the number of observations. 

#### Properties

The centering matrix has the property that when it is multiplied by a data matrix $\mathbf X$, it centers the data by subtracting the mean of each column of $\mathbf X$ from each element of $\mathbf X$. The resulting matrix is called the centered data matrix with the mean equal to $0$. The centering matrix is often used in multivariate statistical analysis, such as principal component analysis, to transform the data into a new coordinate system where the variance of each variable is equal to its eigenvalue.

* A centering matrix is a square matrix.
* The diagonal elements of a centering matrix are all equal and are given by $n^{-1}$, where $n$ is the size of the matrix.
* The off-diagonal elements of a centering matrix are all equal and are given by $-n^{-1}$.
* Multiplying a matrix $\mathbf A$ on the left by a centering matrix $\mathbf C$ is equivalent to subtracting the mean of the columns of $\mathbf A$ from each column of $\mathbf A$.
* Multiplying a matrix $\mathbf A$ on the right by a centering matrix $\mathbf C$ is equivalent to subtracting the mean of the rows of $\mathbf A$ from each row of $\mathbf A$.

Here is an example of a centering matrix of size $3 \times 3$:

$$
\begin{equation*}
\mathbf C = \frac{1}{3} \begin{pmatrix}
2 & -1 & -1 \\
-1 & 2 & -1 \\
-1 & -1 & 2
\end{pmatrix}
\end{equation*}
$$

#### Applications

* find a centered matrix, $\tilde{\mathbf X}$ using a $\mathbf 1_m$ vector for $\mathbf X_{m\times n}$:
$$
\mathbf C = \mathbf I - \frac{1}{m}\mathbf J
$$
$$
\begin{aligned}
\mathbf X&= 
\begin{bmatrix}
x_{11} & x_{21} & \cdots & x_{m1} \\
x_{12} & x_{22} & \cdots & x_{m2} \\
\vdots & \vdots & \ddots & \vdots \\
x_{1n} & x_{2n} & \cdots & x_{mn}
\end{bmatrix} \text{ } \\
\mathbf {1\bar{x}^T}&= 
\begin{bmatrix}
1 \\
1 \\
\vdots \\
1
\end{bmatrix}
\begin{bmatrix}
\bar{x}_{1} & \bar{x}_{2} & \dots & \bar{x}_{n}
\end{bmatrix}\\
&=\begin{bmatrix}
\bar{x}_{1} & \bar{x}_{2} & \dots & \bar{x}_{n}\\
\bar{x}_{1} & \bar{x}_{2} & \dots & \bar{x}_{n}\\
\vdots & \vdots & \ddots & \vdots\\
\bar{x}_{1} & \bar{x}_{2} & \dots & \bar{x}_{n}
\end{bmatrix}\\
\\
\tilde{\mathbf X}&=\mathbf X -{1}_{m}\bar{\mathbf x}^T\\
&=\mathbf X -\mathbf{1}_{m}(\frac{1}{m}\mathbf X^T \mathbf{1}_m)^T\\
&=\mathbf X -\mathbf{1}_{m}\mathbf{1}_m^T\frac{1}{m}\mathbf X\\
&=(\mathbf I -\frac{1}{m}\mathbf J)\mathbf X\\
&=\mathbf C \mathbf X\\
\end{aligned}
$$

* represent a covariance matrix as a centered matrix form:
$$
\begin{aligned}
  \operatorname{Cov}(\mathbf X)&=\operatorname{E}(\mathbf X-\operatorname{E}(\mathbf X))^2\\
  &=\operatorname{E}(\mathbf X-\operatorname{E}(\mathbf X))\operatorname{E}(\mathbf X-\operatorname{E}(\mathbf X))\\
  &=\frac{\sum_{}^{}(x_i-\bar{x_i})(x_j-\bar{x_j})}{m-1}\\
  &=\frac{\tilde{\mathbf{X}}^T\tilde{\mathbf{X}}}{m-1}
\end{aligned}
$$


### Positive Definite Matrix

A symmetric matrix $A$ is positive definite if and only if the quadratic form $f(\mathbf{x})=\mathbf{x}^T A \mathbf{x}$ is positive for all nonzero vectors $\mathbf{x}$.

To see why this is true, consider the eigenvalue decomposition of $A$, which can be written as $A = Q \Lambda Q^T$, where $Q$ is an orthogonal matrix and $\Lambda$ is a diagonal matrix containing the eigenvalues of $A$. Then, for any nonzero vector $\mathbf{x}$, 

$$
\begin{aligned}
\mathbf x^T \mathbf A \mathbf x&=\mathbf x^T \mathbf Q \mathbf \Lambda \mathbf Q^T \mathbf x\\
&=(\mathbf x^T \mathbf Q)\mathbf \Lambda ( \mathbf Q^T\mathbf x)\\
&=\sum_{i=1}^{n} \lambda_iy_i^2
\end{aligned}
$$

where $y_i = (\mathbf{x}^T Q)_i$ is the $i$th coordinate (i.e., a scalar value that represents the position of a point or a vector relative to a chosen basis) of $\mathbf{x}^T Q$ and $n$ is the dimension of $\mathbf{x}$ and $A$. Note that since $Q$ is orthogonal, we have $Q^T Q = I$, so $y_i = \mathbf{q}_i^T \mathbf{x}$, where $\mathbf{q}_i$ is the $i$th column of $Q$. Therefore, the quadratic form $f(\mathbf{x}) = \mathbf{x}^T A \mathbf{x}$ can be written in terms of the eigenvalues of $A$ and the coordinates of $\mathbf{x}$ with respect to the eigenvectors of $A$.

Since $A$ is positive definite, we have $\lambda_i > 0$ for all $i$, and so $\sum_{i=1}^n \lambda_i y_i^2 > 0$ for all nonzero vectors $\mathbf{x}$. Therefore, the quadratic form $f(\mathbf{x})=\mathbf{x}^T A \mathbf{x}$ is positive for all nonzero vectors $\mathbf{x}$, which implies that $A$ is positive definite.

In other words, the positive definiteness of a symmetric matrix $A$ is equivalent to the positivity of the associated quadratic form $f(\mathbf{x})=\mathbf{x}^T A \mathbf{x}$ for all nonzero vectors $\mathbf{x}$.

Therefore, a symmetric matrix, $\mathbf A$ is said to be positive definite if all of its eigenvalues are positive or equivalently, a symmetric matrix, $\mathbf A$ is positive definite if left-multiplying and right-multiplying it by the same vector, $\mathbf x$ always gives a positive number if $\mathbf x^T \mathbf A \mathbf x$

#### Properties

Properties of a positive definite matrix:

* All eigenvalues are positive.
* The matrix is symmetric.
* All principal submatrices have determinants that are positive.

Example of a positive definite matrix:

$$
\begin{equation}
A = \begin{pmatrix}
4 & -1 & 0 \\
-1 & 5 & -1 \\
0 & -1 & 2
\end{pmatrix}
\end{equation}
$$