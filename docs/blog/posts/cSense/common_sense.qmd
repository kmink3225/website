---
title: "Common Sense in Data Sense"
subtitle: Statistics, Data Science, Data Analystics, Machine Learning, Deep Learning, MISCELLANEOUS
description: |
  template
categories:
  - Common Sense
author: Kwangmin Kim
date: 04/22/2023
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
draft: false
---

## Statistics

### WHAT IS THE CENTRAL LIMIT THEOREM AND WHY IS IT IMPORTANT?

The central limit theorem is a fundamental concept in probability theory and statistics. It states that if a large enough sample size is taken from a population, the distribution of the sample means will be approximately normal, regardless of the shape of the original population distribution.

In simpler terms, this means that as the sample size increases, the sample mean will become a more accurate representation of the population mean, and the distribution of the sample means will become more bell-shaped and symmetrical, resembling a normal distribution.

The central limit theorem is important because it allows us to make statistical inferences about a population based on a sample, even if we don't know the exact shape of the population distribution. It also helps us to understand the behavior of the sample mean and standard deviation, which are key parameters used in many statistical tests and models.

#### Example of Quality Control

Suppose a factory produces a large number of identical products, and the weight of each product follows a normal distribution with a mean of 500 grams and a standard deviation of 10 grams. The factory wants to ensure that the average weight of a batch of 100 products is within a certain range, say between 498 and 502 grams, with a confidence level of 95%.

Using the central limit theorem, we can take random samples of 100 products from the population, calculate the mean weight of each sample, and create a sampling distribution of the means. Since the sample size is sufficiently large (n > 30), the central limit theorem tells us that the sampling distribution of the means will be approximately normal, with a mean of 500 grams and a standard deviation of 1 gram (calculated as the population standard deviation divided by the square root of the sample size).

We can then use this information to calculate the probability that the average weight of a batch of 100 products will fall within the desired range. Using a standard normal distribution table or software, we can find the z-score for each endpoint of the range (498 and 502), and calculate the probability that a randomly selected sample of 100 products will have a mean weight within this range. If the probability is at least 95%, we can conclude that the factory's quality control standards are met.

### WHAT IS SAMPLING? HOW MANY SAMPLING METHODS ARE THERE?

The purpose of sampling is to obtain a representative subset of the population that can be used to make inferences or draw conclusions about the larger population.

There are several sampling methods, including:

#### Probabilistic Sampling

* Simple Random Sampling: In this method, each individual or unit in the population has an equal chance of being selected for the sample. This is often done using random number generators or by using a table of random numbers.
* Stratified Sampling: This method involves dividing the population into subgroups, or strata, based on some characteristic (such as age, gender, or geographic location), and then selecting a random sample from each stratum. This ensures that the sample is representative of the population with respect to the characteristic of interest.
* Cluster Sampling: This method involves dividing the population into clusters, such as households or schools, and then selecting a random sample of clusters to survey. All individuals or units within each selected cluster are then surveyed.
* Systematic Sampling: In this method, a random starting point is chosen from the population, and then every nth individual or unit is selected for the sample, where n is a predetermined interval.

#### Non-Probabilistic Sampling

* Convenience Sampling: This method involves selecting individuals or units from the population based on convenience or availability, rather than using a formal sampling technique. While convenient, this method can lead to biased results and is generally not recommended for research purposes.
* Snowball Sampling: This method is often used when studying hard-to-reach populations or groups with limited accessibility. It involves starting with a small group of individuals who meet certain criteria, and then having them refer other individuals who meet the same criteria. This process continues until the desired sample size is reached.

### WHAT IS THE DIFFERENCE BETWEEN TYPE I VS TYPE II ERROR?

| Truth Table       | Reject $H_{null}$ | Fail to Reject $H_{null}$ |
|:-----------------:|:---------------------:|:------------------------------:|
| $H_{null}$ = True | Type I Error, $\alpha$, FP| Correct Decision (1 - $\alpha$), TN|
| $H_{null}$ = False| Correct Decision (Power, 1-$\beta$) TP | Type II Error, $\beta$, FN|


In hypothesis testing, a Type I error occurs when we reject a null hypothesis that is actually true. In other words, we conclude that there is a significant effect or difference when there really isn't one. This error is also known as a false positive.

On the other hand, a Type II error occurs when we fail to reject a null hypothesis that is actually false. In other words, we conclude that there is no significant effect or difference when there actually is one. This error is also known as a false negative.

### WHAT IS LINEAR REGRESSION?

Linear regression is a statistical technique used to model the relationship between a dependent variable (also known as the response variable) and one or more independent variables (also known as predictors or explanatory variables). The goal of linear regression is to find the best linear relationship between the dependent variable and the independent variable(s) that can be used to predict the values of the dependent variable.

In linear regression, the p-value, coefficient, and R-squared value are important components that can help to evaluate the statistical significance and predictive power of the model:

P-value: The p-value measures the probability of obtaining a result as extreme as the one observed, assuming that the null hypothesis is true. In the context of linear regression, the null hypothesis is that the coefficient of the independent variable is zero, indicating no linear relationship between the independent variable and the dependent variable. A low p-value (typically less than 0.05) indicates that the relationship between the independent variable and the dependent variable is statistically significant, and that the null hypothesis can be rejected.

Coefficient: The coefficient is the slope of the line that represents the linear relationship between the independent variable and the dependent variable. It measures the amount of change in the dependent variable that can be attributed to a one-unit change in the independent variable, while holding all other variables constant. A positive coefficient indicates a positive relationship between the independent variable and the dependent variable, while a negative coefficient indicates a negative relationship.

R-squared value: The R-squared value measures the proportion of the variation in the dependent variable that is explained by the variation in the independent variable(s). It is a measure of the goodness-of-fit of the model, and it ranges from 0 to 1. A high R-squared value indicates that the model explains a large proportion of the variation in the dependent variable, and that it is a good predictor of the dependent variable.

The significance of each of these components in linear regression is as follows:

P-value: It helps to determine whether the relationship between the independent variable and the dependent variable is statistically significant, and whether the model can be used to make valid predictions.

Coefficient: It helps to determine the direction and strength of the linear relationship between the independent variable and the dependent variable, and to make predictions about the dependent variable based on changes in the independent variable.

R-squared value: It helps to evaluate the overall fit of the model, and to compare different models to determine which one is the best predictor of the dependent variable.

### WHAT IS SELECTION BIAS?

Selection bias is a type of bias that occurs when the process of selecting individuals or groups to participate in a study or analysis is not random or representative of the population of interest. Selection bias can lead to biased estimates of the population parameters and affect the internal and external validity of the study.

Selection bias can occur in different stages of a study or analysis, such as:

Sampling bias: Occurs when the sample is not representative of the population of interest. For example, if a study on the prevalence of a disease only includes participants from a certain demographic or geographic area, the results may not be generalizable to the entire population.

Non-response bias: Occurs when the response rate is low, or participants who decline to participate are different from those who agree to participate. This can lead to an unrepresentative sample and biased estimates.

Survivorship bias: Occurs when the sample only includes individuals or groups that have survived or persisted through a certain selection process, such as a follow-up study. This can lead to an overestimation of the survival rates or other outcomes.

Berkson's bias: Occurs when the sample is selected based on a condition that is related to the exposure or outcome of interest, such as hospital-based samples. This can lead to an underestimation of the association between the exposure and outcome.

Selection bias can be minimized by using random sampling techniques and ensuring that the sample is representative of the population of interest. It is also important to analyze and report any potential biases and limitations of the study to ensure that the results are interpreted correctly.

###
###
###
###
###
###
###
###
###
###
###
###

## Data Science

### WHAT IS DATA SCIENCE?

Data science is an interdisciplinary field that involves the use of statistical, computational, and domain-specific knowledge and techniques to extract insights and knowledge from data. It combines methods and concepts from statistics, computer science, mathematics, and domain-specific knowledge to solve complex data-related problems.

Statistics and computer science are two important components of data science. Statistics provides the foundational methods for analyzing and modeling data, while computer science provides the computational infrastructure and tools for data storage, processing, and analysis. In addition, computer science provides techniques for data visualization, machine learning, and artificial intelligence that can be used to extract insights and knowledge from large datasets.

Data science involves a wide range of activities, including data acquisition, cleaning and preprocessing, exploratory data analysis, modeling and prediction, data visualization, and communication of results. It requires a strong foundation in statistics, programming, and domain-specific knowledge, as well as expertise in data management, data visualization, and machine learning.

In summary, data science is a multidisciplinary field that combines statistical and computational methods to extract insights and knowledge from data. It draws on a wide range of disciplines, including statistics, computer science, mathematics, and domain-specific knowledge, and requires a diverse set of skills and expertise.

###
###
###
###
###
###
###
###
###
###
###
###
###
###
###
###
###
###
###
###