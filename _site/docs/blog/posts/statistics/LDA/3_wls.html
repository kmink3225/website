<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kwangmin Kim">
<meta name="dcterms.date" content="2023-03-25">
<meta name="description" content="template">

<title>Kwangmin Kim - LDA (3) - Weight Least Square &amp; REML</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../.././images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Kwangmin Kim</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/CV/index.html">
 <span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/projects/index.html">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/blog/index.html">
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../about.html">
 <span class="menu-text">Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kmink3225"><i class="bi bi-github" role="img" aria-label="Github">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kwangmin-kim-a5241b200/"><i class="bi bi-linkedin" role="img" aria-label="Linkedin">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../docs/blog/index.html"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#go-to-project-content-list" id="toc-go-to-project-content-list" class="nav-link active" data-scroll-target="#go-to-project-content-list"><span class="toc-section-number">1</span>  Go to Project Content List</a></li>
  <li><a href="#go-to-blog-content-list" id="toc-go-to-blog-content-list" class="nav-link" data-scroll-target="#go-to-blog-content-list"><span class="toc-section-number">2</span>  Go to Blog Content List</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">LDA (3) - Weight Least Square &amp; REML</h1>
<p class="subtitle lead">Overview</p>
  <div class="quarto-categories">
    <div class="quarto-category">Statistics</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>template</p>
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kwangmin Kim </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 25, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<ul class="nav nav-pills" id="language-tab" role="tablist">
<li class="nav-item" role="presentation">
<button class="nav-link active" id="Korean-tab" data-bs-toggle="tab" data-bs-target="#Korean" type="button" role="tab" aria-controls="Korean" aria-selected="true">
Korean
</button>
</li>
<li class="nav-item" role="presentation">
<button class="nav-link" id="English-tab" data-bs-toggle="tab" data-bs-target="#English" type="button" role="tab" aria-controls="knitr" aria-selected="false">
English
</button>
</li>
<div class="tab-content" id="language-tabcontent">

<div id="Korean" class="tab-pane fade show active" role="tabpanel" aria-labelledby="Korean-tab">
<div id="Korean" class="tab-pane fade show active" role="tabpanel" aria-labelledby="Korean-tab">
<section id="notations" class="level2" data-number="0.1">
<h2 data-number="0.1" class="anchored" data-anchor-id="notations"><span class="header-section-number">0.1</span> Notations</h2>
<ul>
<li><span class="math inline">\(y_{ij}\)</span> : the univariate response (i.e.&nbsp;scalar) for the <span class="math inline">\(i\)</span> th subject at the <span class="math inline">\(j\)</span> th occasion or measurement
<ul>
<li>later when I use the vector case, I will re-define this notation, but focus on the scalar case for now.</li>
</ul></li>
<li><span class="math inline">\(x_{ij}\)</span> : the predictor at time <span class="math inline">\(t_{ij}\)</span>, which is either a scalr or vector.
<ul>
<li>a scalar case: <span class="math inline">\(x_{ij}\)</span> where <span class="math inline">\(i\)</span> is the <span class="math inline">\(i\)</span> th subject, and <span class="math inline">\(j\)</span> is the <span class="math inline">\(j\)</span> th measurement.</li>
<li>a vector case: <span class="math inline">\(x_{ijk}\)</span> where <span class="math inline">\(i\)</span> is the <span class="math inline">\(i\)</span> th subject, <span class="math inline">\(j\)</span> is the <span class="math inline">\(j\)</span> th measurement, and <span class="math inline">\(k \in [1,p]\)</span> is the <span class="math inline">\(k\)</span> th predictor.</li>
<li>sometimes, covariate for different measurements could be the same. In this case, the notation could be written in <span class="math inline">\(x_{i}\)</span>
<ul>
<li>ex) a gender does not change over time in the most cases.</li>
</ul></li>
</ul></li>
<li><span class="math inline">\(i=1, \dots, m\)</span> : i is the index for the <span class="math inline">\(i\)</span> th subject</li>
<li><span class="math inline">\(j=1, \dots, n_i\)</span> : j is the index for the <span class="math inline">\(j\)</span> th measurement of the <span class="math inline">\(i\)</span> th subject
<ul>
<li><span class="math inline">\({n_i}\)</span> is the number of measurements of the <span class="math inline">\(i\)</span> th subject, each <span class="math inline">\({n_i}\)</span> does not have to the same.</li>
<li>balanced desgin: <span class="math inline">\({n_i}\)</span> is the same.</li>
<li>unbalanced desgin: <span class="math inline">\({n_i}\)</span> is different.</li>
</ul></li>
<li><span class="math inline">\(\mathbf y_i\)</span> : a vector (not a matrix), <span class="math inline">\((y_{i1},y_{i2},\dots ,y_{in_i})\)</span> of the <span class="math inline">\(i\)</span> th subject</li>
<li><span class="math inline">\(\mathbf Y\)</span> : the reponse matrix</li>
<li><span class="math inline">\(\mathbf X\)</span> : the predictor matrix</li>
<li><span class="math inline">\(\text{E}(y_{ij})\)</span> : <span class="math inline">\(\mu_{ij}\)</span></li>
<li><span class="math inline">\(\text{E}(\mathbf y_i)\)</span> : <span class="math inline">\(\mathbf \mu_{i}\)</span></li>
<li><span class="math inline">\(\text{Var}(\mathbf y_i)\)</span> : <span class="math inline">\(\text{Var}(\mathbf y_i)\)</span> is a variance-covariance matrix of the different measurement for the <span class="math inline">\(i\)</span> th subject
<ul>
<li>for now, we do not care of the variance covariance of the different subjects because we assume that the measurements of different subjects are indpendent. <span class="math display">\[
\begin{bmatrix}
\text{Var}(y_{i1}) &amp; \text{Cov}( y_{i1}, y_{i2}) &amp; \dots &amp; \text{Cov}( y_{i1}, y_{in_i}) \\
                           &amp; \text{Var}( y_{i2}) &amp; \dots &amp; \text{Cov}( y_{i2}, y_{in_i}) \\
                             &amp;                           &amp; \ddots &amp; \vdots \\
                             &amp;&amp;                            \dots &amp; \text{Var}( y_{in_i})
\end{bmatrix}
\]</span></li>
</ul></li>
</ul>
</section>
<section id="ols-vs-gls" class="level2" data-number="0.2">
<h2 data-number="0.2" class="anchored" data-anchor-id="ols-vs-gls"><span class="header-section-number">0.2</span> OLS vs GLS</h2>
<p>OLS (Ordinary Least Squares) and GLS (Generalized Least Squares) are both methods of regression analysis used to model the relationship between a dependent variable and one or more independent variables. The main difference between OLS and GLS is in the assumptions about the errors in the model.</p>
<section id="ols" class="level3" data-number="0.2.1">
<h3 data-number="0.2.1" class="anchored" data-anchor-id="ols"><span class="header-section-number">0.2.1</span> OLS</h3>
<p>It assumes that the errors are homoscedastic and independent of each other. OLS is a simpler and more commonly used method, but it may not be appropriate for datasets with non-constant variances or autocorrelation in the errors.</p>
<section id="ols-solutions" class="level4" data-number="0.2.1.1">
<h4 data-number="0.2.1.1" class="anchored" data-anchor-id="ols-solutions"><span class="header-section-number">0.2.1.1</span> OLS Solutions</h4>
<p><span class="math display">\[
\begin{aligned}
  \hat{\beta}&amp;=\min_\beta||y-X\beta||^2 \\
  &amp;=(X^TX)^{-1}X^Ty
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\hat \beta\)</span> is unbiased because if we assume that the errors <span class="math inline">\(\epsilon \sim N(0,\sigma^2I)\)</span>, <span class="math inline">\(E(\hat \beta)=\text{E}((X^TX)^{-1}X^Ty)=(X^TX)^{-1}X^T\text{E}(y)=(X^TX)^{-1}X^TX\beta=\beta\)</span></p>
</section>
</section>
<section id="gls" class="level3" data-number="0.2.2">
<h3 data-number="0.2.2" class="anchored" data-anchor-id="gls"><span class="header-section-number">0.2.2</span> GLS</h3>
<p>It relaxes the assumptions of OLS and allows for heteroscedasticity and autocorrelation in the data. GLS is a more flexible method that can handle heteroscedasticity and autocorrelation in the data, but it requires more complex computations and may be computationally expensive for large datasets.</p>
<section id="generalized-least-squares" class="level4" data-number="0.2.2.1">
<h4 data-number="0.2.2.1" class="anchored" data-anchor-id="generalized-least-squares"><span class="header-section-number">0.2.2.1</span> Generalized Least Squares</h4>
<p>The estimator <span class="math inline">\(\hat{\beta}\)</span>, which is also known as the generalized least squares estimator, is given by <span class="math inline">\(\hat{\beta}=(X^TWX)^{-1}X^TWy\)</span>, where <span class="math inline">\(W\)</span> is a positive definite weighting matrix.</p>
<p><span class="math display">\[
\begin{aligned}
  \hat{\beta}&amp;=\min_\beta||y-X\beta||^2 \\
  &amp;=(X^TWX)^{-1}X^TWy
\end{aligned}
\]</span></p>
<section id="weighted-least-square" class="level5" data-number="0.2.2.1.1">
<h5 data-number="0.2.2.1.1" class="anchored" data-anchor-id="weighted-least-square"><span class="header-section-number">0.2.2.1.1</span> Weighted Least Square</h5>
<p>The weighted least squares (WLS) solution can be obtained by minimizing the sum of squared weighted residuals, given by:</p>
<p><span class="math display">\[
\hat{\beta}=\min_\beta||y-X\beta||^2
\]</span></p>
<p>The WLS solution is given by: <span class="math display">\[
\beta_{WLS} = (X^TWX)^{-1}X^TWy
\]</span></p>
<p>where <span class="math inline">\(X\)</span> is the design matrix, <span class="math inline">\(W\)</span> is a diagonal weight matrix with <span class="math inline">\(w_i\)</span> on the <span class="math inline">\(i\)</span> th diagonal element, and <span class="math inline">\(y\)</span> is the vector of responses. The predicted response <span class="math inline">\(\hat{y}\)</span> can be obtained as <span class="math inline">\(\hat{y} = X\hat{\beta}\)</span>.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that the OLS solution is a special case of WLS when all weights are equal to 1.</p>
</div>
</div>
<ol type="1">
<li>Define the weighted design matrix, <span class="math inline">\(\mathbf{W}\)</span>, as a diagonal matrix of weights, where each diagonal element corresponds to the weight for the corresponding observation.</li>
<li>Define the weighted response vector, <span class="math inline">\(\mathbf{y}_{w}\)</span>, as a vector of the response values multiplied by the square root of the corresponding weight.</li>
<li>Define the weighted parameter estimates, <span class="math inline">\(\hat{\beta}_{w}\)</span>, as the solution to the weighted least squares problem: <span class="math display">\[
\hat{\beta}_w = \operatorname*{arg\,min}_{\beta} (y_w - X\beta)^T W (y_w - X\beta)
\]</span> where <span class="math inline">\(\mathbf{X}\)</span> is the design matrix of predictor variables.</li>
<li>The estimated model can be obtained by substituting the weighted parameter estimates, <span class="math inline">\(\hat{\beta}_{w}\)</span>, into the regression equation:</li>
</ol>
<p><span class="math display">\[
\hat{y}=\mathbf{X}\hat{\beta}_w
\]</span></p>
<p>Let’s start by defining the problem: we have a set of m data points, represented as a matrix <span class="math inline">\(\mathbf{X}\)</span> with dimensions <span class="math inline">\(m \times n\)</span>, where <span class="math inline">\(n\)</span> is the number of independent variables. We also have a corresponding vector <span class="math inline">\(\mathbf y\)</span> with <span class="math inline">\(m\)</span> elements, representing the dependent variable. We want to fit a linear function of the form <span class="math inline">\(\mathbf y = \mathbf{X\beta}+ \mathbf \epsilon\)</span> to the data points, where <span class="math inline">\(\mathbf \beta\)</span> is a vector of coefficients to be determined and <span class="math inline">\(\mathbf \epsilon\)</span> is the residual error.</p>
<p>To perform weighted least squares, we define a weight matrix <span class="math inline">\(\mathbf W\)</span> with dimensions <span class="math inline">\(m \times m\)</span>, where the diagonal elements <span class="math inline">\(w_i\)</span> are the weights for each data point <span class="math inline">\(i\)</span>. Weights are typically chosen to be proportional to the inverse of the variance of the data point, so that data points with smaller variances are given more weight.</p>
<p>Using this weight matrix, the objective function for weighted least squares is defined as follows: <span class="math display">\[
\begin{aligned}
\text{minimize } S &amp;= (y - X\beta)^TW(y - X\beta) \\
&amp;= y^TWy - \beta^TX^TWy - y^TWX\beta + \beta^TX^TWX\beta \\
\frac{\partial S}{\partial \beta} &amp;= -2X^TWy + 2X^TWX\beta = 0 \\
X^TWX\beta &amp;= X^TWy \\
\beta &amp;= (X^TWX)^{-1}X^TWy
\end{aligned}
\]</span></p>
<p>Under the assumptions of the GLS model, the GLS estimator is unbiased, which means on average, the GLS estimator will estimate the true population parameters correctly. To be specific, the GLS estimator is based on the assumption that the errors or residuals follow a multivariate normal distribution with a mean vector of zeros and a covariance matrix that is known up to a scalar factor. If this assumption holds, then the GLS estimator is the Best Linear Unbiased Estimator (BLUE) of the model parameters.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Being a BLUE estimator implies that the GLS estimator has the smallest possible variance among all linear unbiased estimators. Therefore, under the GLS assumptions, the GLS estimator is not only unbiased but also efficient, meaning that it achieves the lowest possible variance of all unbiased estimators.</p>
<p>However, If the assumptions are violated (e.g., the errors are not normally distributed or the covariance structure is misspecified), then the GLS estimator may not be unbiased or efficient.</p>
</div>
</div>
<p><span class="math display">\[
\hat{\beta}_{GLS} = (X^T V^{-1} X)^{-1} X^T V^{-1} Y
\]</span></p>
<p>where <span class="math inline">\(V\)</span> is the known covariance matrix of the errors or residuals.</p>
<p>To show that <span class="math inline">\(\hat{\beta}_{GLS}\)</span> is unbiased, we need to show that:</p>
<p><span class="math display">\[
\operatorname{E}(\hat{\beta}_{GLS}) = \beta
\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is the true population parameter.</p>
<p>Using the linearity of expectation, we have:</p>
<p><span class="math display">\[
\begin{align}
\operatorname{E}(\hat{\beta}_{GLS})
&amp;= \operatorname{E}((X^T W^{-1} X)^{-1} X^T W^{-1} Y)\\
&amp;= (X^T W^{-1} X)^{-1} X^T W^{-1} \operatorname{E}(Y)\\
&amp;= (X^T W^{-1} X)^{-1} X^T W^{-1} X \beta\\
&amp;= (X^T W^{-1} X)^{-1} X^T W^{-1} X (X^T X)^{-1} X^T X \beta\\
&amp;= (X^T X)^{-1} X^T W^{-1} X (X^T X)^{-1} X^T X \beta\\
&amp;= (X^T X)^{-1} X^T W^{-1} Y\\
&amp;= \beta
\end{align}
\]</span></p>
<p>where the second-to-last equality follows from the fact that <span class="math inline">\(X^T W^{-1} X\)</span> is a symmetric positive definite matrix, and hence its inverse can be written as <span class="math inline">\((X^T X)^{-1}\)</span>. Therefore, we have shown that the GLS estimator is unbiased, i.e., its expected value is equal to the true population parameter.</p>
</section>
</section>
<section id="gls-solution" class="level4" data-number="0.2.2.2">
<h4 data-number="0.2.2.2" class="anchored" data-anchor-id="gls-solution"><span class="header-section-number">0.2.2.2</span> GLS Solution</h4>
<p>The Generalized Least Squares (GLS) estimator is obtained by minimizing the weighted sum of squared residuals, where the weights are based on the estimated variance-covariance matrix of the errors.</p>
<p>To derive the GLS estimator, we start with the linear regression model:</p>
<p><span class="math inline">\(Y = X\beta + \epsilon\)</span></p>
<p>where <span class="math inline">\(Y\)</span> is the response variable, <span class="math inline">\(X\)</span> is the design matrix of predictor variables, <span class="math inline">\(\beta\)</span> is a vector of unknown coefficients, and <span class="math inline">\(\epsilon\)</span> is a vector of errors or residuals.</p>
<p>The covariance matrix of the errors is denoted by <span class="math inline">\(V = \text{Cov}(\epsilon)\)</span>, which is assumed to be known up to a scalar factor. Specifically, we assume that <span class="math inline">\(V = \sigma^2 W\)</span>, where <span class="math inline">\(\sigma^2\)</span> is an unknown scalar factor and <span class="math inline">\(W\)</span> is a known positive definite matrix.</p>
<p>The GLS estimator of <span class="math inline">\(\beta\)</span> is obtained by minimizing the weighted sum of squared residuals:</p>
<p><span class="math inline">\(Q(\beta) = (\textbf{Y} - \textbf{X}\beta)^T V^{-1} (\textbf{Y} - \textbf{X}\beta)\)</span></p>
<p>where <span class="math inline">\(\textbf{Y}\)</span> and <span class="math inline">\(\textbf{X}\)</span> are the vectors of observed responses and design matrix of predictors, respectively.</p>
<p>Taking the derivative of <span class="math inline">\(Q(\beta)\)</span> with respect to <span class="math inline">\(\beta\)</span>, and setting it to zero, we get:</p>
<p><span class="math inline">\(\frac{\partial Q(\beta)}{\partial \beta} = -2 \textbf{X}^T V^{-1} (\textbf{Y} - \textbf{X}\beta) = 0\)</span></p>
<p>Solving for <span class="math inline">\(\beta\)</span>, we obtain the GLS estimator:</p>
<p><span class="math inline">\(\hat{\beta}_{GLS} = (\textbf{X}^T V^{-1} \textbf{X})^{-1} \textbf{X}^T V^{-1} \textbf{Y}\)</span></p>
<p>where <span class="math inline">\(V^{-1} = \frac{1}{\sigma^2} W^{-1}\)</span>.</p>
<p>Note that the GLS estimator reduces to the OLS estimator when <span class="math inline">\(V\)</span> is a scalar multiple of the identity matrix, i.e., when the errors have constant variance and are uncorrelated. In this case, <span class="math inline">\(W = I\)</span> and <span class="math inline">\(V = \sigma^2 I\)</span>, and the GLS estimator simplifies to:</p>
<p><span class="math inline">\(\hat{\beta}_{OLS} = (\textbf{X}^T \textbf{X})^{-1} \textbf{X}^T \textbf{Y}\)</span></p>
<p>which is the usual OLS estimator.</p>
</section>
</section>
</section>
<section id="if-wblkdiag-1-in-longitudinal-data-analysis-why-more-efficient-than-ols" class="level2" data-number="0.3">
<h2 data-number="0.3" class="anchored" data-anchor-id="if-wblkdiag-1-in-longitudinal-data-analysis-why-more-efficient-than-ols"><span class="header-section-number">0.3</span> If W=blkdiag()^{-1} in longitudinal data analysis, why more efficient than OLS?</h2>
<p>The expression “blkdiag()^{-1}” means the inverse of a block diagonal matrix where each block is a covariance matrix denoted by Sigma (). Specifically, if we have n covariance matrices _1, _2, …, _n, then the block diagonal matrix is given by:</p>
blkdiag(_1, _2, …, _n) =
<span class="math display">\[\begin{bmatrix} \Sigma_1 &amp; 0 &amp; \cdots &amp; 0 \ 0 &amp; \Sigma_2 &amp; \cdots &amp; 0 \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ 0 &amp; 0 &amp; \cdots &amp; \Sigma_n \end{bmatrix}\]</span>
<p>The inverse of this block diagonal matrix can be computed by taking the inverse of each block matrix and placing them on the diagonal, resulting in the following expression:</p>
[blkdiag(_1, _2, …, _n)]^{-1} =
<span class="math display">\[\begin{bmatrix} \Sigma_1^{-1} &amp; 0 &amp; \cdots &amp; 0 \ 0 &amp; \Sigma_2^{-1} &amp; \cdots &amp; 0 \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ 0 &amp; 0 &amp; \cdots &amp; \Sigma_n^{-1} \end{bmatrix}\]</span>
<p>Therefore, blkdiag()^{-1} is a block diagonal matrix where each block is the inverse of the corresponding covariance matrix, and it can be computed by taking the inverse of each block and placing them on the diagonal.</p>
<p>REML for estimating variance covrainace matrix divides the data into twp parts: irrelevant to sigma and irrelevant to beta (REML estimate)</p>
<p>In longitudinal data analysis, it is common to use a generalized linear mixed model (GLMM) to account for the correlated nature of the data. The GLMM framework includes random effects to capture the individual-level variation and allows for the specification of a covariance structure to model the correlation between the repeated measurements within each individual. When fitting a GLMM, the covariance matrix of the random effects, denoted by , needs to be estimated.</p>
<p>One way to estimate is to use maximum likelihood estimation (MLE), which involves maximizing the likelihood function of the GLMM with respect to the unknown parameters, including . However, the MLE of can be inefficient when the number of repeated measurements per individual is small or the correlation between the repeated measurements is weak.</p>
<p>To improve the efficiency of the MLE of , a weighted likelihood method can be used, where the likelihood function is multiplied by a weight matrix that depends on the estimated covariance matrix. Specifically, the weight matrix is given by W = blkdiag()^{-1}, where blkdiag() is the block diagonal matrix of the estimated covariance matrix . The inverse of blkdiag() is taken because it is a positive definite matrix and its inverse exists.</p>
<p>By weighting the likelihood function with W, the resulting weighted likelihood estimator (WLE) of is more efficient than the MLE because it incorporates additional information about the covariance structure of the data. The WLE of is then used in the GLMM to estimate the other parameters, such as the fixed effects.</p>
<p>In summary, using W = blkdiag()^{-1} as a weight matrix in the GLMM framework can improve the efficiency of the MLE of the covariance matrix and result in more accurate estimates of the other parameters in the model, making it more efficient than the OLS estimator, which assumes independence between the observations.</p>
<section id="family-and-longitudinal-data" class="level3" data-number="0.3.1">
<h3 data-number="0.3.1" class="anchored" data-anchor-id="family-and-longitudinal-data"><span class="header-section-number">0.3.1</span> Family and Longitudinal Data</h3>
<p>In longitudinal data analysis with family data, one common approach to estimating the regression coefficients is to use a linear mixed-effects model, also known as a multilevel model or a hierarchical model. The linear mixed-effects model can handle the within-subject correlation due to repeated measurements over time and the within-family correlation due to shared genetic or environmental factors among family members.</p>
<p>The linear mixed-effects model assumes that the outcome variable Y is a function of the fixed effects X and the random effects b, which can account for the within-subject and within-family correlation. The model can be written as:</p>
<p>Y = Xβ + Zb + ε</p>
<p>where β is the vector of fixed effects coefficients, b is the vector of random effects coefficients, Z is the design matrix for the random effects, and ε is the error term.</p>
<p>To estimate the fixed effects coefficients β, one can use maximum likelihood estimation (MLE) or restricted maximum likelihood estimation (REML) methods. The MLE estimates the variance components for both the random effects and the error term, while the REML estimates the variance components for only the random effects and adjusts for the bias in the likelihood function.</p>
<p>To fit the linear mixed-effects model, one can use software packages such as R, SAS, or Stata, which have functions for fitting linear mixed-effects models with repeated measurements and random effects. In R, for example, one can use the lme4 package to fit the linear mixed-effects model using the lmer() function. The output of the function includes the estimated fixed effects coefficients β and the estimated variance components for the random effects and the error term.</p>
<p>Overall, estimating the fixed effects coefficients β of family data with repeated measurements in longitudinal data analysis requires fitting a linear mixed-effects model that accounts for the within-subject and within-family correlation, and the choice of model depends on the research question and assumptions of the data.</p>
<p>In longitudinal data analysis with repeated measurements and correlated subjects, the variance-covariance matrix of the responses can be modeled using a mixed-effects model or a generalized estimating equation (GEE) model.</p>
<p>In the mixed-effects model, the variance-covariance matrix of the responses is decomposed into two components: the within-subject covariance and the between-subject covariance. The within-subject covariance accounts for the correlation between the repeated measurements within the same subject, while the between-subject covariance accounts for the correlation between subjects. The mixed-effects model allows for the inclusion of both fixed effects and random effects, which can model the mean and the variance structure of the responses.</p>
<p>In the GEE model, the variance-covariance matrix of the responses is modeled using a working correlation matrix, which specifies the correlation structure between the observations. The GEE model allows for the inclusion of fixed effects, but not random effects, and estimates the population-averaged mean and the variance structure of the responses.</p>
<p>To design the variance-covariance matrix of the responses in a mixed-effects model or a GEE model, one needs to specify the correlation structure between the repeated measurements within the same subject and between subjects. Common correlation structures for the within-subject covariance include the autoregressive (AR), the exchangeable, and the unstructured covariance structures, while common correlation structures for the between-subject covariance include the independent and the compound symmetry covariance structures.</p>
<p>The choice of correlation structure depends on the research question and assumptions of the data. For example, the AR(1) correlation structure assumes that the correlation between two measurements decreases as the time lag between them increases, while the exchangeable correlation structure assumes that all measurements within the same subject are equally correlated. The choice of correlation structure can be evaluated using statistical criteria such as the Akaike information criterion (AIC) or the Bayesian information criterion (BIC).</p>
<p>Overall, designing the variance-covariance matrix of the responses in longitudinal data analysis with repeated measurements and correlated subjects requires specifying the correlation structure between the repeated measurements within the same subject and between subjects, and the choice of model depends on the research question and assumptions of the data.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this file contains code for linear marginal models for longitudinal data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># We test different covariance patterns and show how to fit model with WLS and REML</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># By Gen Li, 1/1/2018</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># also check lme4</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(nlme)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>opposites <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">"https://stats.idre.ucla.edu/stat/r/examples/alda/data/opposites_pp.txt"</span>,<span class="at">header=</span><span class="cn">TRUE</span>,<span class="at">sep=</span><span class="st">","</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(opposites)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># spaghetti plot</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">ggplot</span>(opposites, <span class="fu">aes</span>(time, opp, <span class="at">group=</span>id)) <span class="sc">+</span> <span class="fu">geom_line</span>()</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit different cov model with REML</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="do">###################################################</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># unstructured covariance for marginal model</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>unstruct <span class="ot">&lt;-</span> <span class="fu">gls</span>(opp<span class="sc">~</span>time<span class="sc">*</span>ccog,opposites, <span class="at">correlation=</span><span class="fu">corSymm</span>(<span class="at">form =</span> <span class="sc">~</span> <span class="dv">1</span> <span class="sc">|</span>id),  <span class="at">weights=</span><span class="fu">varIdent</span>(<span class="at">form =</span> <span class="sc">~</span> <span class="dv">1</span><span class="sc">|</span> wave),<span class="at">method=</span><span class="st">"REML"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># corSymm(form = ~ 1 |id) : the same covariance across different measurement, same correlation matrix for different subjects</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># check ?gls ?corClasses ?corSymm</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(unstruct) <span class="co"># focus on corr, var, (weight)</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>unstruct<span class="sc">$</span>modelStruct<span class="sc">$</span>corStruct <span class="co"># corr</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>unstruct<span class="sc">$</span>modelStruct<span class="sc">$</span>varStruct <span class="co"># variance:weight</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>unstruct<span class="sc">$</span>sigma <span class="co"># standard deviation</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="fu">cov2cor</span>(unstruct<span class="sc">$</span>varBeta)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># compound symmetry</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>comsym <span class="ot">&lt;-</span> <span class="fu">gls</span>(opp<span class="sc">~</span>time,opposites, <span class="at">correlation=</span><span class="fu">corCompSymm</span>(<span class="at">form =</span> <span class="sc">~</span> <span class="dv">1</span><span class="sc">|</span>id),   <span class="at">weights=</span><span class="fu">varIdent</span>(<span class="at">form =</span> <span class="sc">~</span> <span class="dv">1</span><span class="sc">|</span> wave), <span class="at">method=</span><span class="st">"REML"</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(comsym)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="fu">corMatrix</span>(comsym<span class="sc">$</span>modelStruct<span class="sc">$</span>corStruct)[[<span class="dv">1</span>]]</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co"># AR(1)</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>auto1 <span class="ot">&lt;-</span> <span class="fu">gls</span>(opp<span class="sc">~</span>time ,opposites, <span class="at">correlation=</span><span class="fu">corAR1</span>(<span class="at">form =</span> <span class="sc">~</span> <span class="dv">1</span> <span class="sc">|</span>id), <span class="at">method=</span><span class="st">"REML"</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(auto1)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="fu">corMatrix</span>(auto1<span class="sc">$</span>modelStruct<span class="sc">$</span>corStruct)[[<span class="dv">1</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Longitudinal Study vs Cross-Sectional Study Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>A cross-sectional study found that older people smoke more.</p>
<p>Possible explanations:</p>
<ul>
<li>People tend to smoke more when they get older.</li>
<li>Older people grew up in an environment where the harm of smoking was less widely accepted. In other words, when older people were younger, smoking was more socially acceptable and its harmful effects were not well-known or well-publicized. As a result, they may have started smoking earlier in life and developed a stronger habit or addiction. This explanation implies that younger people today may be less likely to smoke because they are growing up in an environment where smoking is less socially acceptable and the risks are more widely known.</li>
</ul>
<p>LDA can distinguish the effect due to aging (i.e., changes over time within subject) from cohort effects (i.e., difference between subjects at baseline). Cross-sectional study cannot.</p>
</div>
</div>
</section>
</section>
<section id="advantages-of-longitudinal-data-analysis" class="level2" data-number="0.4">
<h2 data-number="0.4" class="anchored" data-anchor-id="advantages-of-longitudinal-data-analysis"><span class="header-section-number">0.4</span> Advantages of Longitudinal Data Analysis</h2>
<ul>
<li>Each subject can serve as his/her own control. Influence of genetic make-up, environmental exposures, and maybe unmeasured characteristics tend to persist over time.
<ul>
<li>in certain types of studies or experiments, individuals can be used as their own comparison group. This means that the same person is tested or measured at different points in time, and the differences observed can be attributed to changes over time rather than to differences between individuals.</li>
<li>For example, in a study looking at the effect of a new medication on blood pressure, each participant’s blood pressure before and after taking the medication would be compared to determine if there was a change. By using the same participant as their own control, the effects of genetic factors, environmental exposures, and other individual differences that might influence blood pressure would be minimized.</li>
<li>However, even when using this approach, some individual differences that are not directly measured or controlled for may persist over time and influence the results. These could include factors such as diet, stress levels, or other lifestyle habits that could impact the outcome being measured.</li>
</ul></li>
<li>Distinguish the degree of variation in <span class="math inline">\(Y\)</span> across time within a subject from the variation in <span class="math inline">\(Y\)</span> between subjects. With repeated values, one can borrow strength across time for the person of interest as well as across people.
<ul>
<li>when you have repeated measurements of a variable (Y) for the same person over time, you can use the information from those repeated measurements to better estimate the true value of Y for that person at any given time point. This is known as borrowing strength across time.</li>
<li>Additionally, you can also use the information from the repeated measurements of Y across different people to better estimate the true value of Y for a particular time point across the population. This is known as borrowing strength across people.</li>
<li>By doing both, you can distinguish the degree of variation in Y across time within a subject (i.e., how much Y varies for a particular person over time) from the variation in Y between subjects (i.e., how much Y varies between different people at a particular time point).</li>
</ul></li>
<li>Increased power, by repeated measurements. The repeated measurements from the same subject are rarely perfectly correlated. Hence, longitudinal studies are more powerful than cross-sectional studies.
<ul>
<li>Longitudinal studies are more powerful than cross-sectional studies because they allow researchers to directly model and account for the within-subject correlation among repeated measurements. In other words, longitudinal studies take advantage of the fact that individuals are their own controls by measuring outcomes at multiple time points, which allows for a more accurate estimation of the true effect of an exposure or intervention.</li>
<li>In contrast, cross-sectional studies only measure outcomes at a single time point, which makes it difficult to distinguish between within-subject variability and between-subject variability. In a cross-sectional study, any observed differences between groups may be due to differences in the underlying populations, or due to differences in the timing of the outcome measurement, or both.</li>
<li>Furthermore, longitudinal studies can also provide information on the rate of change in the outcome over time, which can be important in understanding disease progression, treatment effects, or the impact of other time-dependent factors.</li>
</ul></li>
<li>Therefore, because longitudinal studies allow for a more accurate estimation of the true effect of an exposure or intervention and provide more information about disease progression, they are generally considered more powerful than cross-sectional studies.</li>
</ul>
<section id="specialty-of-lda" class="level3" data-number="0.4.1">
<h3 data-number="0.4.1" class="anchored" data-anchor-id="specialty-of-lda"><span class="header-section-number">0.4.1</span> Specialty of LDA</h3>
<p>LDA requires special statistical methods because the set of observations on one subject tends to be inter-correlated.</p>
<p><a href="./childhood readbility.PNG">Copied from Diggle et al.&nbsp;(2002, page 2).</a></p>
<ul>
<li>Consider the example, variation of readability of child as they get aged.
<ul>
<li>Assume this is a longitudinal study with two measurements per child at two age or time points.</li>
<li>The two measurements per subject may be highly correlated.</li>
<li>If we use cross-sectional methods to analyze the data, we may not be able to distinguish changes over time within individual and difference among people in their baseline levels.
<ul>
<li>Only plot (a) is from cross sectional study. Not using connected lines might mislead the time trend within subjects.</li>
</ul></li>
</ul></li>
<li>In general, repeated observations <span class="math inline">\(y_{i1}, \dots , y_{in_i}\)</span> for subject <span class="math inline">\(i\)</span> are likely to be correlated, so the independence assumption is violated.</li>
<li>The standard regression methods (ignoring correlation) may lead to
<ul>
<li>Incorrect inference
<ul>
<li>the violation of the independence assumption means that the errors in the model are correlated across observations, and this correlation can bias the estimated coefficients.</li>
<li>The errors in the model are correlated across observations when there is some form of dependence or clustering in the data. This means that the error term in one observation is related to the error terms in other observations, either through some underlying factor or because of the way the data is collected.</li>
<li>When the errors are correlated across observations, the estimated coefficients from standard regression methods may be biased because the regression model assumes that the errors are independent.</li>
<li>The bias in the estimated coefficients can arise in several ways:
<ul>
<li>The standard errors of the estimated coefficients will be too small, which can lead to overconfidence in the results.</li>
<li>The estimated coefficients may not reflect the true relationships between the dependent variable and the independent variables, as the correlation between the errors can distort the estimates.</li>
<li>The estimates of the standard errors will be biased, leading to incorrect inference in hypothesis testing and confidence interval construction.</li>
</ul></li>
<li>To sum up, failing to account for the correlation between errors can lead to incorrect and imprecise estimates of the coefficients and standard errors in a regression model.</li>
</ul></li>
<li>Inefficient estimates of <span class="math inline">\(\beta\)</span>
<ul>
<li>the estimates are less precise than they could be if the correlation between observations were taken into account.</li>
<li>The standard errors of the estimates will be too large, which means that confidence intervals will be wider and hypothesis tests will have less power.</li>
</ul></li>
<li>Oversight of important correlation structure
<ul>
<li>the standard regression methods may miss important patterns of correlation in the data that could be used to improve the accuracy and precision of the estimates.</li>
<li>For example, if there is a time trend in the data that is not accounted for, the standard errors of the estimates may be too large, and the estimates may not capture the true effect of the independent variables.</li>
<li>Accounting for the correlation structure in the data can lead to more accurate and precise estimates, and can also help identify interesting patterns and relationships that might otherwise be missed.</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
</div>
</div>
<div id="English" class="tab-pane fade" role="tabpanel" aria-labelledby="English-tab">
<div id="English" class="tab-pane fade" role="tabpanel" aria-labelledby="English-tab">

</div>
</div>
<section id="go-to-project-content-list" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Go to Project Content List</h1>
<p><a href="./docs/projects/index.qmd">Project Content List</a></p>
</section>
<section id="go-to-blog-content-list" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Go to Blog Content List</h1>
<p><a href="./docs/blog/posts/content_list.qmd">Blog Content List</a></p>


</section>

</div></ul></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>