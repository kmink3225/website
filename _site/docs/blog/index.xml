<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Kwangmin Kim</title>
<link>kmink3225.netlify.app/docs/blog/index.html</link>
<atom:link href="kmink3225.netlify.app/docs/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>blog</description>
<generator>quarto-1.2.313</generator>
<lastBuildDate>Tue, 31 Dec 2999 15:00:00 GMT</lastBuildDate>
<item>
  <title>Blog Content List</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/content_list.html</link>
  <description><![CDATA[ 



<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Scalars are denoted with a lower-case letter (ex a ) or a non-bolded lower-case Greek letter (ex <img src="https://latex.codecogs.com/png.latex?%5Calpha"> ).</li>
<li>Vectors are denoted using a bold-faced lower-case letter (ex <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20a">).</li>
<li>Matrices are denoted using a bold-faced upper-case letter (ex <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20%5Cphi">) or a bold-faced upper-case Greek letter (ex <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20%5CPhi">).</li>
<li>Tensors are denoted using a bold-faced upper-case letter with multiple subscripts or superscripts, indicating the number of indices and the dimensions of the tensor along each axis.
<ul>
<li>A second-order tensor (also known as a matrix) <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> with dimensions <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m"> can be represented as: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A_%7Bij%7D"> where <img src="https://latex.codecogs.com/png.latex?i%20=%201,%5Cdots,m"> and <img src="https://latex.codecogs.com/png.latex?j%20=%201,%5Cdots,n">, which are the indices that run over the rows and columns of the matrix, respectively.</li>
<li>A third-order tensor <img src="https://latex.codecogs.com/png.latex?T"> with dimensions <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m%20%5Ctimes%20p"> can be represented as: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A_%7Bijk%7D"> where <img src="https://latex.codecogs.com/png.latex?i%20=%201,%5Cdots,m">, <img src="https://latex.codecogs.com/png.latex?j%20=%201,%5Cdots,n">, which are <img src="https://latex.codecogs.com/png.latex?i">, and <img src="https://latex.codecogs.com/png.latex?k%20=%201,%5Cdots,p"> <img src="https://latex.codecogs.com/png.latex?j">, and <img src="https://latex.codecogs.com/png.latex?k">, which are the indices that run over the three dimensions of the tensor.</li>
</ul></li>
</ul>
</div>
</div>
<section id="contents" class="level1">
<h1>Contents</h1>
<ul>
<li><a href="../../../docs/blog/posts/DL/guide_map/index.html">Deep Learning</a></li>
<li><a href="../../../docs/blog/posts/ML/guide_map/index.html">Machine Learning</a></li>
<li><a href="../../../docs/blog/posts/Mathmatics/guide_map/index.html">Mathematics</a></li>
<li><a href="../../../docs/blog/posts/statistics/guide_map/index.html">Statistics</a></li>
<li><a href="../../../docs/blog/posts/Engineering/guide_map/index.html">Engineering</a></li>
<li><a href="../../../docs/blog/posts/Patent/guide_map/index.html">Patent</a></li>
<li><a href="../../../docs/blog/posts/Language/guide_map/index.html">Language</a></li>
<li><a href="../../../docs/blog/posts/Surveilance/guide_map/index.html">Surveilance</a></li>
</ul>
</section>
<section id="reference" class="level1">
<h1>Reference</h1>
<ul>
<li>Statistics
<ul>
<li>George Casella &amp; Rogeer L. Berger - Statistcal Inference, 2nd Edition</li>
<li>Dobson and Barnett (2008) An Introduction to Generalized Linear Model. 3rd Ed. Chapman &amp; Hall.</li>
<li>Fitzmaurice, Laird and Ware (2011) Applied Longitudinal Analysis. 2nd Ed. Wiley.</li>
<li>Hosmer, Lemeshow and May (2008) Applied Survival Analysis. 2nd Ed. Wiley.</li>
<li>슬기로운 통계생활 - https://www.youtube.com/<span class="citation" data-cites="statisticsplaybook">@statisticsplaybook</span></li>
<li>슬기로운 통계생활 - https://github.com/statisticsplaybook</li>
<li>Fast Campus, Coursera, Inflearn</li>
<li>그 외 다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Mathematics
<ul>
<li>James Stewart - Calculus Early Transcedentals, 7th Eidition &amp; any James Stewart series</li>
<li>GILBERT STRANG - Introduction to Linear Algebra, 4th Edition.</li>
<li>임장환 - 머신러닝, 인공지능, 컴퓨터 비전 전공자를 위한 최적화 이론</li>
<li>Fast Campus, Coursera, Inflearn</li>
<li><a href="https://www.youtube.com/playlist?list=PLaqQvlCBe8vIkIEb4GX2ZZ1A4tFYeXR5W">8일간의 선형대수학 기초(이상준 경희대 교수)</a></li>
<li><a href="https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/">Linear Algebra(Prof.&nbsp;Gilbert Strang, MIT Open Courseware)</a></li>
<li><a href="https://github.com/fastai/numerical-linear-algebra/blob/master/README.md">Computational Linear Algebra for Coders</a></li>
<li><a href="http://immersivemath.com/ila/">Immersive linear Algebra</a></li>
<li><a href="https://www.3blue1brown.com/topics/linear-algebra">3blue1brown</a></li>
<li>그 외 다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Machine Learning
<ul>
<li>Gareth M. James, Daniela Witten, Trevor Hastie, Robert Tibshirani - An Introduction to Statistical Learning: With Applications in R 2nd Edition</li>
<li>Trevor Hastie, Robert Tibshirani, Jerome H. Friedman - The Elements of Statistical Learning 2nd Edition</li>
<li>Fast Campus, Coursera, Inflearn</li>
<li>그 외 다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Deep Learning
<ul>
<li>Saito Koki - Deep Learning from Scratch 1,2,3 (밑바닥부터 시작하는 딥러닝 1,2,3)</li>
<li>조준우 - 머신러닝·딥러닝에 필요한 기초 수학 with 파이썬</li>
<li>조준우 - https://github.com/metamath1/noviceml</li>
<li>동빈나 - https://www.youtube.com/c/dongbinna</li>
<li>혁펜하임 - https://www.youtube.com/channel/UCcbPAIfCa4q0x7x8yFXmBag</li>
<li>Fast Campus, Coursera, Inflearn</li>
<li>다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Engineering
<ul>
<li>Fast Campus, Coursera, Inflearn</li>
<li>그 외 다수의 Youtube, and Documents from Googling</li>
</ul></li>
</ul>


</section>

 ]]></description>
  <category>All List</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/content_list.html</guid>
  <pubDate>Tue, 31 Dec 2999 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Statistics</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/statistics/guide_map/index.html</link>
  <description><![CDATA[ 



<p><strong>(Draft)</strong></p>
<section id="contents-list" class="level1">
<h1>Contents List</h1>
<section id="basic" class="level2">
<h2 class="anchored" data-anchor-id="basic">Basic</h2>
<section id="probability-theory" class="level3">
<h3 class="anchored" data-anchor-id="probability-theory">Probability Theory</h3>
<ul>
<li>2023-02-05, Set Theory</li>
<li>2023-02-05, [Basics of Probability Theory - Axiomatic Foundations]</li>
<li>2023-02-05, [Basics of Probability Theory - Calculus of Probabilities]</li>
<li>2023-02-05, <a href="../../../../../docs/blog/posts/statistics/2023-02-05_probability/index.html">Basics of Probability Theory - Probability</a></li>
<li>2023-02-05, <a href="../../../../../docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html">Conditional Probability</a></li>
<li>2023-02-05, [Independence]</li>
<li>2023-02-05, <a href="../../../../../docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html">Bayes’ Rule</a></li>
<li>2023-02-05, Random Variable</li>
<li>1111-11-11, Probability Distribution</li>
</ul>
</section>
<section id="transformations-and-expectations" class="level3">
<h3 class="anchored" data-anchor-id="transformations-and-expectations">Transformations and Expectations</h3>
<ul>
<li>2023-02-21, <a href="../../../../../docs/blog/posts/statistics/2023-02-21_transformation/index.html">Transformation of Random Variables</a></li>
<li>1111-11-11, Expected Value vs Realizaed Value</li>
<li>1111-11-11, Variance</li>
<li>1111-11-11, Covariance and Correlation</li>
<li>2023-02-28, <a href="../../../../../docs/blog/posts/statistics/2023-02-28_mgf/index.html">Moment Generating Function, MGF</a></li>
</ul>
</section>
<section id="exponential-family-distributions" class="level3">
<h3 class="anchored" data-anchor-id="exponential-family-distributions">Exponential Family Distributions</h3>
<ul>
<li>Discrete Random Variable
<ul>
<li>2023-02-27,<a href="../../../../../docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html">Bernoulli Distribution</a></li>
<li>2023-02-28,<a href="../../../../../docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html">Binomial Distribution</a></li>
<li>2023-03-01,<a href="../../../../../docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html">Poisson Distribution</a></li>
<li>2023-03-01,<a href="../../../../../docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html">Geometric Distribution</a></li>
<li>1111-11-11, Hypergeometric Distribution</li>
</ul></li>
<li>Continuous Random Variable
<ul>
<li>1111-11-11, Normal Distribution</li>
<li>1111-11-11, Exponential Distribution</li>
<li>1111-11-11, Beta Distribution</li>
<li>1111-11-11, Chi-squared Distribution</li>
</ul></li>
<li>1111-11-11,</li>
</ul>
</section>
<section id="multiple-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="multiple-random-variables">Multiple Random Variables</h3>
<ul>
<li>1111-11-11, Joint Distribution and Marginal Distribution</li>
</ul>
</section>
<section id="point-estimation" class="level3">
<h3 class="anchored" data-anchor-id="point-estimation">Point Estimation</h3>
<ul>
<li>1111-11-11, Estimation Methods - Method of Moments</li>
<li>2023-03-29, Estimation Methods - <a href="../../../../../docs/blog/posts/statistics/2023-03-25_MLE/index.html">Maximum Likelihood Estimation &amp; Statistical Bias</a></li>
<li>1111-11-11, Estimation Methods - Bayesian Estimation</li>
<li>1111-11-11, Estimation Methods - The EM Algorithm</li>
<li>1111-11-11, Evaluation Methods of Estimators - Mean Squared Error</li>
<li>1111-11-11, Evaluation Methods of Estimators - Best Unbiased Estimators</li>
<li>1111-11-11, Evaluation Methods of Estimators - Sufficiency and Unbiasedness</li>
<li>1111-11-11, Evaluation Methods of Estimators - Loss Function Optimality</li>
</ul>
</section>
</section>
<section id="hypothesis-testing" class="level2">
<h2 class="anchored" data-anchor-id="hypothesis-testing">Hypothesis Testing</h2>
<ul>
<li>1111-11-11, Hypothesis Testing</li>
<li>1111-11-11, Permutation Test</li>
</ul>
<section id="methods-of-finding-tests" class="level3">
<h3 class="anchored" data-anchor-id="methods-of-finding-tests">Methods of Finding Tests</h3>
<ul>
<li>1111-11-11, Likelihood Ratio Tests</li>
<li>1111-11-11, Bayesian Tests</li>
<li>1111-11-11, Union-Intersection and Intersection-Union Tets</li>
</ul>
</section>
<section id="methods-of-evaluating-tests" class="level3">
<h3 class="anchored" data-anchor-id="methods-of-evaluating-tests">Methods of Evaluating Tests</h3>
<ul>
<li>1111-11-11, Power</li>
<li>1111-11-11, Error Proabilities and the Power Function</li>
<li>1111-11-11, Most Powerful Tests</li>
<li>2022-12-28, <a href="../../../../../docs/blog/posts/statistics/2022-12-08-P-value/index.html">p-values</a></li>
<li>1111-11-11, Loss Function Optimality</li>
<li>1111-11-11, Multiple Testing</li>
<li>1111-11-11, Sample Size Calculation</li>
<li>1111-11-11, A/B Testing</li>
<li>2023-01-07, <a href="../../../../../docs/blog/posts/statistics/2023-01-07-anova/index.html">ANOVA</a>
<ul>
<li>2023-01-27, <a href="../2023-01-27_ANCOVA/">ANCOVA</a></li>
<li>2023-01-27, <a href="../2023-01-27_rmANOVA/">repeated measures ANOVA</a></li>
<li>2023-01-28, <a href="../2023-01-28_MANOVA/">MANOVA</a></li>
</ul></li>
</ul>
</section>
</section>
<section id="categorical-data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="categorical-data-analysis">Categorical Data Analysis</h2>
<ul>
<li>1111-11-11, Introduction</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>2022-12-28,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>2023-01-07,</li>
<li>2023-01-27,</li>
<li>2023-01-27,</li>
<li>2023-01-28,</li>
</ul>
</section>
<section id="regression" class="level2">
<h2 class="anchored" data-anchor-id="regression">Regression</h2>
<ul>
<li>1111-11-11, Least Square and Simple Linear Regression</li>
<li>1111-11-11, Multiple Linear Regression</li>
</ul>
<section id="generalized-linear-models" class="level3">
<h3 class="anchored" data-anchor-id="generalized-linear-models">Generalized Linear Models</h3>
<ul>
<li>1111-11-11, Logistic Regression</li>
<li>1111-11-11, Multinomial Regression</li>
<li>1111-11-11, Poisson Regression</li>
<li>1111-11-11, Poisson Regression</li>
<li>1111-11-11, Poisson Regression</li>
</ul>
</section>
</section>
<section id="longitudinal-data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="longitudinal-data-analysis">Longitudinal Data Analysis</h2>
<ul>
<li>2023-03-23, <a href="../../../../../docs/blog/posts/statistics/LDA/1_intro.html">LDA (1) - Intro</a></li>
<li>2023-03-23, <a href="../../../../../docs/blog/posts/statistics/LDA/2_covariance_model.html">LDA (2) - Concepts &amp; Covariance Models</a></li>
<li>2023-03-25, <a href="../LDA/intro.qmd">LDA (3) - WLS &amp; REML</a></li>
<li>2023-03-25, <a href="../LDA/intro.qmd">LDA (4) - Respiratory Infection Data Example</a></li>
<li>2023-03-28, <a href="../LDA/intro.qmd">LDA (5) - Epileptic Seizures Data Example</a></li>
</ul>
<section id="mixed-models" class="level3">
<h3 class="anchored" data-anchor-id="mixed-models">Mixed Models</h3>
<ul>
<li>1111-11-11, Linear Mixed Models</li>
</ul>
</section>
</section>
<section id="generalized-additive-models" class="level2">
<h2 class="anchored" data-anchor-id="generalized-additive-models">Generalized Additive Models</h2>
</section>
<section id="survival-analysis" class="level2">
<h2 class="anchored" data-anchor-id="survival-analysis">Survival Analysis</h2>
<ul>
<li>1111-11-11, Cox-Hazard Model</li>
</ul>


</section>
</section>

 ]]></description>
  <category>Statistics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/statistics/guide_map/index.html</guid>
  <pubDate>Fri, 30 Apr 2100 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Machine Learning</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/ML/guide_map/index.html</link>
  <description><![CDATA[ 



<p><strong>(Draft)</strong></p>
<section id="contents-list" class="level1">
<h1>Contents List</h1>
<section id="basic" class="level2">
<h2 class="anchored" data-anchor-id="basic">Basic</h2>
<section id="r" class="level3">
<h3 class="anchored" data-anchor-id="r">R</h3>
</section>
<section id="python" class="level3">
<h3 class="anchored" data-anchor-id="python">Python</h3>
<section id="tensor-flow-framework" class="level4">
<h4 class="anchored" data-anchor-id="tensor-flow-framework">Tensor Flow Framework</h4>
<ul>
<li>2023-02-03, <a href="../../../../../docs/blog/posts/ML/2023-02-03_tf_introduction/index.html">Tensor Flow Introduction</a></li>
</ul>
</section>
<section id="pytorch-framework" class="level4">
<h4 class="anchored" data-anchor-id="pytorch-framework">Pytorch Framework</h4>
<ul>
<li>2023-02-03, <a href="../../../../../docs/blog/posts/ML/2023-02-03_pytorch_introduction/index.html">Pytorch Introduction</a></li>
</ul>
</section>
</section>
</section>
<section id="machine-learning-methods" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning-methods">Machine Learning Methods</h2>
<section id="supervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="supervised-learning">Supervised Learning</h3>
<ul>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Logistic Regression]</li>
<li>0000-00=00, [Generative Models]
<ul>
<li>0000-00=00, [Linear Discriminant Analysis]</li>
<li>0000-00=00, [Quadratic Discriminant Analysis]</li>
<li>0000-00=00, [Naive Bayes]</li>
</ul></li>
<li>0000-00=00, [Resampling Methods]</li>
<li>0000-00=00, [Regularization]</li>
<li>0000-00=00, [Smoothing]</li>
<li>0000-00=00, [Tree Based Methods]</li>
<li>0000-00=00, [Support Vector Machine]</li>
<li>0000-00=00, [PCR]</li>
<li>0000-00=00, [PLS]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
</ul>
</section>
<section id="unupervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="unupervised-learning">Unupervised Learning</h3>
<ul>
<li>0000-00=00, [PCA]</li>
<li>0000-00=00, [K means clustering]</li>
<li>0000-00=00, [Hierarchical Clustering]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
</ul>


</section>
</section>
</section>

 ]]></description>
  <category>ML</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/ML/guide_map/index.html</guid>
  <pubDate>Wed, 31 Mar 2100 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Mathematics</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/guide_map/index.html</link>
  <description><![CDATA[ 



<section id="contents" class="level1">
<h1>Contents</h1>
<section id="basic" class="level2">
<h2 class="anchored" data-anchor-id="basic">Basic</h2>
<ul>
<li>2023-03-24, <a href="../../../../../docs/blog/posts/Mathmatics/variables/index.html">Variable types</a></li>
<li>1111-11-11, <a href="../../../../../docs/blog/posts/Mathmatics/function/index.html">Function</a>
<ul>
<li>2023-01-31, <a href="../../../../../docs/blog/posts/Mathmatics/function/univariable_scalar_function.html">Function (1) - Univariable Scalar Function (One to One)</a></li>
<li>2023-01-31, <a href="../../../../../docs/blog/posts/Mathmatics/function/multivariable_scalar_function.html">Function (2) - Multi-variable Scalar Function (Many to One)</a></li>
<li>2023-01-31, <a href="../../../../../docs/blog/posts/Mathmatics/function/univariable_vector_function.html">Function (3) - Univariable Vector Function (One to Many)</a></li>
<li>2023-01-31, <a href="../../../../../docs/blog/posts/Mathmatics/function/mutivariable_vector_function.html">Function (4) - Multi-variable Vector Function (Many to Many)</a></li>
<li>2023-02-18, <a href="../../../../../docs/blog/posts/Mathmatics/function/composite_function.html">Function (5) - Composite Function</a></li>
</ul></li>
<li>2023-02-18, <a href="../../../../../docs/blog/posts/Mathmatics/transformation/index.html">Transformations of Functions</a></li>
<li>1111-11-11, Vector &amp; Matrix</li>
<li>2023-03-15, <a href="../epsilon_delta/">Limit, <img src="https://latex.codecogs.com/png.latex?%5Cepsilon-%5Cdelta"> Method</a></li>
<li>Differentiation
<ul>
<li>2023-02-04, <a href="../../../../../docs/blog/posts/Mathmatics/differentiation/2023-02-04_uni_derivative.html">Derivative (1) - Univariable Scalar Funtion</a></li>
<li>1111-11-11, <a href="../../../../../docs/blog/posts/Mathmatics/differentiation/2023-02-10_composite_partial_derivative.html">Derivative (2) - Chain Rule &amp; Partial Derivative</a></li>
<li>1111-11-11, Derivative (3) - Higher Order Derivative</li>
<li>1111-11-11, Derivative (4) - Mean Value Theorem</li>
<li>1111-11-11, Derivative (5) - Gradient</li>
</ul></li>
<li>2023-03-15, <a href="../../../../../docs/blog/posts/Mathmatics/taylor_series/index.html">Talyer’s Series</a></li>
<li>1111-11-11, Gradient Direction</li>
<li>1111-11-11, Random Variable</li>
<li>1111-11-11, Probability Distribution</li>
<li>1111-11-11, Information Theory - Entropy</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
</ul>
</section>
<section id="linear-algebra" class="level2">
<h2 class="anchored" data-anchor-id="linear-algebra">Linear Algebra</h2>
<ul>
<li>2023-03-30, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/01.basic_vector.html">Basics (1) - Vector Operations</a></li>
<li>2023-03-30, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/02.basic_matrix.html">Basics (2) - Matrix Operations</a></li>
<li>2023-03-30, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/03.basic_special_matrix.html">Basics (3) - Special Matrix</a></li>
<li>2023-04-14, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/04.lineqr_equations.html">Lineqr Equations</a></li>
<li>2023-04-14, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html">Vector Space and Subspaces</a></li>
<li>2023-04-21, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/06.orthogonality.html">Orthogonality</a></li>
<li>1111-11-11, [Determinants]</li>
<li>1111-11-11, [Eigen Value &amp; Eigen Vector]</li>
<li>1111-11-11, [Linear Transformations]</li>
<li>1111-11-11, Basis, Dimension, &amp; Rank</li>
<li>1111-11-11,</li>
<li>1111-11-11, Eigen Decomposition</li>
<li>1111-11-11, Singular Value Decomposition (SVD)</li>
<li>1111-11-11, Group</li>
<li>1111-11-11, Rotation &amp; Group</li>
<li>2023-04-02, <a href="../linear_algebra/quadratic_form.qmd">Matrix Transformation (5) - Quadratic Form</a></li>
<li>2023-04-02, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/derivative_matrix_vector.html">Matrix Calculus (1) - Quadratic Form</a></li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
</ul>
</section>
<section id="optimization" class="level2">
<h2 class="anchored" data-anchor-id="optimization">Optimization</h2>
<ul>
<li>2023-03-23, <a href="../../../../../docs/blog/posts/Mathmatics/optimization/minimizer.html">Minimizer &amp; Minimum</a></li>
<li>1111-11-11, Convex Set</li>
<li>1111-11-11, Convex Function</li>
<li>1111-11-11, Unconstrained Optimization</li>
<li>1111-11-11, Non-linear Least Square</li>
<li>1111-11-11, Largrange Multiplier Method
<ul>
<li>1111-11-11, Largrange Primal Function</li>
<li>1111-11-11, Largrange Dual Function</li>
<li>1111-11-11, KKT conditions</li>
</ul></li>
<li>1111-11-11, Gradient Descent Optimizers</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
</ul>
</section>
<section id="reference" class="level2">
<h2 class="anchored" data-anchor-id="reference">Reference</h2>
<ul>
<li>Statistics
<ul>
<li>George Casella &amp; Rogeer L. Berger - Statistcal Inference, 2nd Edition</li>
<li>슬기로운 통계생활 - https://www.youtube.com/<span class="citation" data-cites="statisticsplaybook">@statisticsplaybook</span></li>
<li>슬기로운 통계생활 - https://github.com/statisticsplaybook</li>
<li>다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Mathematics
<ul>
<li>James Stewart - Calculus Early Transcedentals, 7th Eidition</li>
<li>any James Stewart series</li>
<li>임장환 - 머신러닝, 인공지능, 컴퓨터 비전 전공자를 위한 최적화 이론</li>
<li>다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Deep Learning
<ul>
<li>조준우 - 머신러닝·딥러닝에 필요한 기초 수학 with 파이썬</li>
<li>조준우 - https://github.com/metamath1/noviceml</li>
<li>동빈나 - https://www.youtube.com/c/dongbinna</li>
<li>혁펜하임 - https://www.youtube.com/channel/UCcbPAIfCa4q0x7x8yFXmBag</li>
<li>다수의 Youtube, and Documents from Googling</li>
</ul></li>
</ul>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/guide_map/index.html</guid>
  <pubDate>Sun, 28 Feb 2100 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Deep Learning</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/DL/guide_map/index.html</link>
  <description><![CDATA[ 



<p><strong>(Draft)</strong></p>
<section id="contents-list" class="level1">
<h1>Contents List</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<ul>
<li>1111-11-11, Artificial Intelligence</li>
<li>1111-11-11, Perceptron</li>
<li>1111-11-11, Artificial Neural Netwroks (ANN)
<ul>
<li>1111-11-11, activation functions</li>
<li>1111-11-11, output layer design</li>
</ul></li>
<li>1111-11-11, loss function</li>
<li>1111-11-11, numerical differentiation</li>
<li>1111-11-11, gradient descent</li>
<li>1111-11-11, backpropagation</li>
<li>1111-11-11, optimizer
<ul>
<li>1111-11-11, stochastic gradient descent</li>
<li>1111-11-11, momentum</li>
<li>1111-11-11, adaGrad</li>
<li>1111-11-11, adam</li>
<li>1111-11-11, weight initalization</li>
</ul></li>
<li>1111-11-11, batch normalization</li>
<li>1111-11-11, dropout</li>
<li>1111-11-11, tuning parameter</li>
<li>1111-11-11, auto-encoder</li>
<li>1111-11-11, stacked auto-encoder</li>
<li>1111-11-11, denoising auto-encoder(DAE)</li>
</ul>
<section id="convolutional-neural-network-cnn" class="level3">
<h3 class="anchored" data-anchor-id="convolutional-neural-network-cnn">Convolutional Neural Network (CNN)</h3>
<ul>
<li>2023-03-10, <a href="../../../../../docs/blog/posts/DL/2023-03-10_cnn/index.html">CNN (1) - Concept</a></li>
<li>2023-03-10, <a href="">CNN (2) - Practice</a></li>
</ul>
</section>
<section id="natural-language-process-nlp" class="level3">
<h3 class="anchored" data-anchor-id="natural-language-process-nlp">Natural Language Process (NLP)</h3>
<ul>
<li>1111-11-11, word2vec</li>
<li>1111-11-11, improved word2vec</li>
</ul>
</section>
<section id="recurrent-neural-network-rnn" class="level3">
<h3 class="anchored" data-anchor-id="recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</h3>
</section>
<section id="gate-recurrent-unit-gru" class="level3">
<h3 class="anchored" data-anchor-id="gate-recurrent-unit-gru">Gate Recurrent Unit (GRU)</h3>
</section>
<section id="long-short-term-memory-lstm" class="level3">
<h3 class="anchored" data-anchor-id="long-short-term-memory-lstm">Long Short-Term Memory (LSTM)</h3>
</section>
<section id="attention-transformer" class="level3">
<h3 class="anchored" data-anchor-id="attention-transformer">Attention (Transformer)</h3>
</section>
<section id="bidirectional-encoder-representations-from-transformers-bert" class="level3">
<h3 class="anchored" data-anchor-id="bidirectional-encoder-representations-from-transformers-bert">Bidirectional Encoder Representations from Transformers (BERT)</h3>
</section>
<section id="generative-pre-training-transformer-gpt" class="level3">
<h3 class="anchored" data-anchor-id="generative-pre-training-transformer-gpt">Generative Pre-training Transformer (GPT)</h3>


</section>
</section>
</section>

 ]]></description>
  <category>DL</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/DL/guide_map/index.html</guid>
  <pubDate>Thu, 31 Dec 2099 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Engineering</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Engineering/guide_map/index.html</link>
  <description><![CDATA[ 



<section id="it-terminology" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="it-terminology"><span class="header-section-number">1</span> IT Terminology</h2>
<ul>
<li>0000-00-00, Terminology</li>
</ul>
</section>
<section id="data-structure" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="data-structure"><span class="header-section-number">2</span> Data Structure</h2>
<ul>
<li>2023-01-17, <a href="../2023-01-17_data_structure_overview/">Overview</a></li>
<li>2023-01-18, <a href="../2023-01-18_array/">Array</a></li>
<li>2023-01-18, <a href="../2023-01-18_linked_list/">Linked List</a></li>
<li>2023-01-18, <a href="../2023-01-18_python_list/">Python List</a></li>
<li>2023-01-19, <a href="../2023-01-19_stack/">Stack</a></li>
<li>2023-01-19, <a href="../2023-01-19_queue/">Queue</a></li>
<li>2023-01-26, <a href="../2023-01-19_deque/">Deque</a></li>
<li>2023-01-26, <a href="../2023-01-20_binary_search_tree/">Binary Search Tree</a></li>
<li>2023-01-20, <a href="../2023-01-20_priority_queue/">Priority Queue</a></li>
<li>2023-01-20, <a href="../2023-01-20_graph/">Graph</a></li>
</ul>
</section>
<section id="conda" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="conda"><span class="header-section-number">3</span> Conda</h2>
</section>
<section id="docker" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="docker"><span class="header-section-number">4</span> Docker</h2>
<ul>
<li>2023-01-30, Docker Install</li>
<li>2023-01-31, Docker Compose</li>
<li>2023-02-01, Docker Container</li>
</ul>
</section>
<section id="dynamic-documentation" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="dynamic-documentation"><span class="header-section-number">5</span> Dynamic Documentation</h2>
<ul>
<li>2023-01-19, <a href="https://quarto.org/docs/get-started/">Quarto</a></li>
<li>2023-01-19, <a href="https://github.com/yihui/xaringan">xaringan[R]</a></li>
<li>2023-01-19, <a href="https://bookdown.org/yihui/bookdown/get-started.html">Bookdown[R]</a></li>
<li>2023-01-19, <a href="https://decile-team-distil.readthedocs.io/en/latest/index.html">DISTL</a></li>
<li>2023-01-26, <a href="https://www.sphinx-doc.org/en/master/">Sphinx[Python]</a></li>
</ul>
</section>
<section id="aws-cloud" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="aws-cloud"><span class="header-section-number">6</span> AWS Cloud</h2>
<p>Coursera Course: AWS Fundamentals</p>
<ul>
<li>2023-03-09, <a href="../../../../../docs/blog/posts/Engineering/2023-03-02_aws/computing_networking.html">Computing and Networking</a></li>
<li>2023-03-12, <a href="../../../../../docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html">Storage and Database</a></li>
<li>2023-03-26, <a href="../../../../../docs/blog/posts/Engineering/2023-03-02_aws/monitoring_sharedresponsibility.html">Monitoring and SharedResponsibility</a></li>
<li>2023-04-05, <a href="../../../../../docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html">Infrastructure Security</a></li>
</ul>
</section>
<section id="azure-cloud" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="azure-cloud"><span class="header-section-number">7</span> Azure Cloud</h2>
</section>
<section id="data-modeling" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="data-modeling"><span class="header-section-number">8</span> Data Modeling</h2>
</section>
<section id="apache-airflow" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="apache-airflow"><span class="header-section-number">9</span> Apache Airflow</h2>
</section>
<section id="apache-spark" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="apache-spark"><span class="header-section-number">10</span> Apache Spark</h2>
</section>
<section id="front-end" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="front-end"><span class="header-section-number">11</span> Front End</h2>
</section>
<section id="back-end" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="back-end"><span class="header-section-number">12</span> Back End</h2>


</section>

 ]]></description>
  <category>Engineering</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Engineering/guide_map/index.html</guid>
  <pubDate>Thu, 31 Dec 2099 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Language</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Language/guide_map/index.html</link>
  <description><![CDATA[ 



<p><strong>(Draft)</strong></p>
<section id="contents-list" class="level1">
<h1>Contents List</h1>
<section id="r" class="level2">
<h2 class="anchored" data-anchor-id="r">R</h2>
<ul>
<li>1111-11-11, tidyverse
<ul>
<li>1111-11-11, dplyr</li>
<li>1111-11-11, ggplot2</li>
<li>1111-11-11, tidyr</li>
<li>1111-11-11, readr</li>
<li>1111-11-11, purrr</li>
<li>1111-11-11, tibble</li>
<li>1111-11-11, stringr</li>
<li>1111-11-11, forcats</li>
</ul></li>
<li>1111-11-11, tidymodels</li>
<li>1111-11-11, R shiny</li>
<li>1111-11-11, mosaic</li>
</ul>
</section>
<section id="python" class="level2">
<h2 class="anchored" data-anchor-id="python">Python</h2>
<ul>
<li>1111-11-11, numpy</li>
<li>1111-11-11, pandas</li>
<li>1111-11-11, matplotlib</li>
<li>1111-11-11, seaborn</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
</ul>
</section>
<section id="sql" class="level2">
<h2 class="anchored" data-anchor-id="sql">SQL</h2>
<section id="sqlite" class="level3">
<h3 class="anchored" data-anchor-id="sqlite">SQLite</h3>
</section>
<section id="oracle-sql" class="level3">
<h3 class="anchored" data-anchor-id="oracle-sql">Oracle SQL</h3>
</section>
<section id="ms-sql" class="level3">
<h3 class="anchored" data-anchor-id="ms-sql">MS-SQL</h3>
</section>
<section id="postgre-sql" class="level3">
<h3 class="anchored" data-anchor-id="postgre-sql">Postgre SQL</h3>
</section>
</section>
<section id="linux" class="level2">
<h2 class="anchored" data-anchor-id="linux">Linux</h2>
</section>
<section id="powershell" class="level2">
<h2 class="anchored" data-anchor-id="powershell">Powershell</h2>
</section>
<section id="c" class="level2">
<h2 class="anchored" data-anchor-id="c">C++</h2>
</section>
<section id="javascript" class="level2">
<h2 class="anchored" data-anchor-id="javascript">Javascript</h2>


</section>
</section>

 ]]></description>
  <category>Language</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Language/guide_map/index.html</guid>
  <pubDate>Thu, 31 Dec 2099 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Patent</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Patent/guide_map/index.html</link>
  <description><![CDATA[ 



<p><strong>(Draft)</strong></p>
<section id="contents-list" class="level1">
<h1>Contents List</h1>
<section id="basic" class="level2">
<h2 class="anchored" data-anchor-id="basic">Basic</h2>


</section>
</section>

 ]]></description>
  <category>Patent</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Patent/guide_map/index.html</guid>
  <pubDate>Thu, 31 Dec 2099 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Validation</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Surveilance/guide_map/index.html</link>
  <description><![CDATA[ 



<section id="sgs" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="sgs"><span class="header-section-number">1</span> SGS</h2>
<ul>
<li>0000-00-00, EN62304</li>
</ul>
</section>
<section id="fda" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="fda"><span class="header-section-number">2</span> FDA</h2>
<ul>
<li>2023-01-27, <a href="../../../../../docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html">General Principles of SW Validation</a></li>
<li>2023-01-27, <a href="../../../../../docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html">General Principles of SW Validation - Diagram Summary</a></li>
<li>1111-11-11, Guidance for the Content of Premarket Submissions for Software Contained in Medical Devices</li>
</ul>
</section>
<section id="dhf" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="dhf"><span class="header-section-number">3</span> DHF</h2>
</section>
<section id="public-health" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="public-health"><span class="header-section-number">4</span> Public Health</h2>
</section>
<section id="wet-lab" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="wet-lab"><span class="header-section-number">5</span> Wet Lab</h2>
<ul>
<li>0000-00-00, PCR (Polymerase Chain Reaction) Experiment</li>
</ul>


</section>

 ]]></description>
  <category>Surveilance</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Surveilance/guide_map/index.html</guid>
  <pubDate>Sat, 31 Dec 2089 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Eigenvalue</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/09.eigenvalues.html</link>
  <description><![CDATA[ 



<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;">library</span>(tidyverse)</span>
<span id="cb1-2"><span class="fu" style="color: #4758AB;">library</span>(mosaic)</span>
<span id="cb1-3"><span class="fu" style="color: #4758AB;">library</span>(mvtnorm)</span></code></pre></div>
</details>
</div>
<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Eigenvalues could be used for understanding characteristics or properties of a square matrix and determining what type of a quadratic form the matrix belongs to.</p>
<section id="definition" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="definition"><span class="header-section-number">1.1</span> Definition</h2>
<div id="def-eigenvalue" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Eigenvalues and Eigenvectors) </strong></span>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be an <img src="https://latex.codecogs.com/png.latex?n%20%C3%97%20n"> square matrix. A scalar <img src="https://latex.codecogs.com/png.latex?%5Clambda"> is called an eigenvalue of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> if there exists a non-zero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> such that <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BAv%7D=%5Clambda%5Cmathbf%7Bv%7D%0A"></p>
<p>Such a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> is called an eigenvector corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda">.</p>
</div>
</section>
<section id="properties" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="properties"><span class="header-section-number">1.2</span> Properties</h2>
<ul>
<li>Eigenvalues are scalar values: Eigenvalues are scalars and represent the scaling factor by which the corresponding eigenvectors are stretched or shrunk when multiplied by a matrix.</li>
<li>When a matrix is ​​expressed in quadratic form, there exists a symmetric matrix that satisfies the uniqueness of the quadratic form and all eigenvalues ​​corresponding to the symmetric matrix having real numbers as elements are real numbers. Depending on the signs of the eigenvalues, a quadratic form can be classified into 5 types:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is said to be a positive definite (PD) matrix if the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are all positive. <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20%3E%200%20%5Ctext%7B%20iff%20%7D%20%5Clambda_%7B%5Ctext%7Bmin%7D%7D(%5Cmathbf%7BA%7D)%20%3E%200%0A"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is said to be a positive semi-definite (PSD) matrix if the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are not negative. <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20%5Cge%200%20%5Ctext%7B%20iff%20%7D%20%5Clambda_%7B%5Ctext%7Bmin%7D%7D(%5Cmathbf%7BA%7D)%20%5Cge%200%0A"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is said to be a negative definite (ND) matrix if the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are all negative. <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20%3C%200%20%5Ctext%7B%20iff%20%7D%20%5Clambda_%7B%5Ctext%7Bmax%7D%7D(%5Cmathbf%7BA%7D)%20%3C%200%0A"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is said to be a negative semi-definite (NSD) matrix if the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are not positive. <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20%5Cle%200%20%5Ctext%7B%20iff%20%7D%20%5Clambda_%7B%5Ctext%7Bmax%7D%7D(%5Cmathbf%7BA%7D)%20%5Cle%200%0A"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is said to be a indefinite (NSD) matrix if the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> have both signs. <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20%5Clessgtr%200%20%5Ctext%7B%20iff%20%7D%20%5Clambda_%7B%5Ctext%7Bmax%7D%7D(%5Cmathbf%7BA%7D)%20%5Clessgtr%200%0A"></li>
</ul></li>
<li>Sum of eigenvalues: The sum of the eigenvalues of a matrix is equal to the trace of the matrix, where the trace is the sum of the diagonal elements. <img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bi=1%7D%5En%20%5Clambda_i%20=%20%5Ctext%7Btr%7D(%5Cmathbf%7BA%7D)%0A">.</li>
<li>Product of eigenvalues: The product of the eigenvalues of a matrix is equal to the determinant of the matrix. <img src="https://latex.codecogs.com/png.latex?%0A%5Cprod_%7Bi=1%7D%5En%20%5Clambda_i%20=%20%5Cdet(%5Cmathbf%7BA%7D)=%7C%5Cmathbf%7BA%7D%7C%0A">.<br>
</li>
<li>Eigenvalues are solutions to the characteristic equation: For a square matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> of size <img src="https://latex.codecogs.com/png.latex?n%5Ctimes%20n">, the eigenvalues <img src="https://latex.codecogs.com/png.latex?%5Clambda"> satisfy the characteristic equation <img src="https://latex.codecogs.com/png.latex?%5Cdet(%5Cmathbf%7BA%7D%20-%20%5Clambda%5Cmathbf%7BI%7D)%20=%200">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D"> is the identity matrix of size <img src="https://latex.codecogs.com/png.latex?n%5Ctimes%20n">.
<ul>
<li>The characteristic equation is an equation associated with a square matrix that helps determine its eigenvalues, which are crucial for understanding the behavior and properties of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. <img src="https://latex.codecogs.com/png.latex?%0A%5Cdet(%5Cmathbf%7BA%7D%20-%20%5Clambda%5Cmathbf%7BI%7D)%20=%200%0A"> The equation indicates that the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20-%20%5Clambda%5Cmathbf%7BI%7D"> does not have full rank and has a nontrivial null space.</li>
</ul></li>
<li>Eigenvalues of similar matrices: Similar matrices have the same eigenvalues. If two matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D"> are similar (i.e., <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BPBP%7D%5E%7B-1%7D"> for some invertible matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BP%7D">), then they have the same eigenvalues.</li>
<li>Eigenvalues of triangular matrices: The eigenvalues of a triangular matrix are equal to its diagonal entries. In other words, for an upper triangular matrix, the eigenvalues are the elements on its main diagonal.</li>
</ul>
</section>
<section id="example" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="example"><span class="header-section-number">1.3</span> Example</h2>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be the matrix</p>
<p>To find the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, we solve the characteristic equation <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdet%7D(%5Cmathbf%20A%20-%20%5Clambda%20%5Cmathbf%20I%20)%20=%200">, where I is the n × n identity matrix.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%5Ctext%7Bdet%7D(%5Cmathbf%20A%20-%20%5Clambda%20%5Cmathbf%20I%20)%0A%20%20&amp;=%0A%20%20%20%20%5Cbegin%7Bvmatrix%7D%0A%20%20%20%203%20-%20%5Clambda%20&amp;%201%20%5C%5C%0A%20%20%20%201%20&amp;%203%20-%20%5Clambda%0A%20%20%20%20%5Cend%7Bvmatrix%7D%20%5C%5C%0A%20%20&amp;=%0A%20%20(3%20-%20%5Clambda)(3%20-%20%5Clambda)%20-%201%20%5C%5C%0A%20%20&amp;=%20%5Clambda%5E2%20-%206%5Clambda%20+%208%20=%200%0A%5Cend%7Balign*%7D%0A"></p>
<p>Solving this quadratic equation gives us the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">: <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202"> and <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">.</p>
<p>To find the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202">, we solve the equation <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%20A%20-%202%20%5Cmathbf%20I)%5Cmathbf%7Bv%7D%20=%20%5Cmathbf%7B0%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20I"> is the <img src="https://latex.codecogs.com/png.latex?2%20%5Ctimes%202"> identity matrix.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20(%5Cmathbf%20A%20-%202%20%5Cmathbf%20I)%5Cmathbf%7Bv%7D%20=%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%201%20&amp;%201%20%5C%5C%0A%20%20%20%20%20%201%20&amp;%201%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20x%20%5C%5C%0A%20%20%20%20y%0A%20%20%5Cend%7Bbmatrix%7D%20=%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%200%20%5C%5C%0A%20%20%20%200%0A%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p>Solving this system of equations gives us the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202">: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_1%7D%20=%20%5Cbegin%7Bbmatrix%7D%20-1%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"></p>
<p>Similarly, for <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">, we solve the equation <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%20A%20-%204%5Cmathbf%20I)%5Cmathbf%7Bv%7D"> = <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D"> to get the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_2%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"></p>
<div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">R</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Python</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">PD <span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>),<span class="at" style="color: #657422;">ncol=</span><span class="dv" style="color: #AD0000;">2</span>,<span class="at" style="color: #657422;">byrow =</span> <span class="cn" style="color: #8f5902;">TRUE</span>)</span>
<span id="cb2-2">PSD <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span>),<span class="at" style="color: #657422;">ncol=</span><span class="dv" style="color: #AD0000;">2</span>,<span class="at" style="color: #657422;">byrow =</span> <span class="cn" style="color: #8f5902;">TRUE</span>)</span>
<span id="cb2-3">ND <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">0</span>,<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>),<span class="at" style="color: #657422;">ncol=</span><span class="dv" style="color: #AD0000;">2</span>,<span class="at" style="color: #657422;">byrow =</span> <span class="cn" style="color: #8f5902;">TRUE</span>)</span>
<span id="cb2-4">NSD <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>),<span class="at" style="color: #657422;">ncol=</span><span class="dv" style="color: #AD0000;">2</span>,<span class="at" style="color: #657422;">byrow =</span> <span class="cn" style="color: #8f5902;">TRUE</span>)</span>
<span id="cb2-5">Ind <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>),<span class="at" style="color: #657422;">ncol=</span><span class="dv" style="color: #AD0000;">2</span>,<span class="at" style="color: #657422;">byrow =</span> <span class="cn" style="color: #8f5902;">TRUE</span>)</span>
<span id="cb2-6"></span>
<span id="cb2-7">sym_mat <span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">6</span>,<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>,<span class="dv" style="color: #AD0000;">12</span>,<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>,<span class="dv" style="color: #AD0000;">6</span>,<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">12</span>,<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">3</span>),<span class="at" style="color: #657422;">ncol=</span><span class="dv" style="color: #AD0000;">3</span>,<span class="at" style="color: #657422;">byrow =</span> <span class="cn" style="color: #8f5902;">TRUE</span>)</span>
<span id="cb2-8"><span class="fu" style="color: #4758AB;">eigen</span>(sym_mat)<span class="sc" style="color: #5E5E5E;">$</span>values</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 18.138954  4.556363 -7.695317</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><span class="co" style="color: #5E5E5E;"># trace</span></span>
<span id="cb4-2"><span class="fu" style="color: #4758AB;">diag</span>(sym_mat)<span class="sc" style="color: #5E5E5E;">%&gt;%</span><span class="fu" style="color: #4758AB;">sum</span>()</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 15</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><span class="fu" style="color: #4758AB;">sum</span>(<span class="fu" style="color: #4758AB;">eigen</span>(sym_mat)<span class="sc" style="color: #5E5E5E;">$</span>values)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 15</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><span class="co" style="color: #5E5E5E;"># product of eigenvalues : more convenient than using a for loop</span></span>
<span id="cb8-2"><span class="fu" style="color: #4758AB;">exp</span>(<span class="fu" style="color: #4758AB;">sum</span>(<span class="fu" style="color: #4758AB;">log</span>(<span class="fu" style="color: #4758AB;">eigen</span>(sym_mat)<span class="sc" style="color: #5E5E5E;">$</span>values)))</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] NaN</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><span class="fu" style="color: #4758AB;">det</span>(sym_mat)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -636</code></pre>
</div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb12-2"></span>
<span id="cb12-3"><span class="co" style="color: #5E5E5E;"># Define the matrix A</span></span>
<span id="cb12-4">A <span class="op" style="color: #5E5E5E;">=</span> np.array([[<span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">1</span>], [<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">3</span>]])</span>
<span id="cb12-5"></span>
<span id="cb12-6"><span class="co" style="color: #5E5E5E;"># Calculate the eigenvalues and eigenvectors</span></span>
<span id="cb12-7">eigenvalues, eigenvectors <span class="op" style="color: #5E5E5E;">=</span> np.linalg.eig(A)</span>
<span id="cb12-8"></span>
<span id="cb12-9"><span class="co" style="color: #5E5E5E;"># Print the eigenvalues</span></span>
<span id="cb12-10"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Eigenvalues:"</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Eigenvalues:</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="cf" style="color: #003B4F;">for</span> eigenvalue <span class="kw" style="color: #003B4F;">in</span> eigenvalues:</span>
<span id="cb14-2">    <span class="bu" style="color: null;">print</span>(eigenvalue)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1.381966011250105
3.618033988749895</code></pre>
</div>
</div>
</div>
</div>
</div>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/09.eigenvalues.html</guid>
  <pubDate>Fri, 26 May 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Measures of Risk: Relative Risk &amp; Odds Ratio</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Epidemilogy/relative_risk_odds_ratio.html</link>
  <description><![CDATA[ 



<ul class="nav nav-pills" id="language-tab">
<li class="nav-item">
<button class="nav-link active" id="Korean-tab" data-bs-toggle="tab" data-bs-target="#Korean" type="button" aria-controls="Korean" aria-selected="true">
Korean
</button>
</li>
<li class="nav-item">
<button class="nav-link" id="English-tab" data-bs-toggle="tab" data-bs-target="#English" type="button" aria-controls="knitr" aria-selected="false">
English
</button>
</li>
<div class="tab-content" id="language-tabcontent">

<div id="Korean" class="tab-pane fade show active" aria-labelledby="Korean-tab">
<div id="Korean" class="tab-pane fade show active" aria-labelledby="Korean-tab">
<p>Knowing the relative risk and odds ratio provides important insights beyond just looking at the counts in a contingency table.</p>
<section id="motivation" class="level2" data-number="0.1">
<h2 data-number="0.1" class="anchored" data-anchor-id="motivation"><span class="header-section-number">0.1</span> Motivation</h2>
<p>Relative Risk (RR) and Odds Ratio (OR) provides a more comprehensive understanding of the relationship between an exposure and an outcome by quantifying the association between the exposure and the outcome.</p>
<p><strong>Relative Risk(RR)</strong></p>
<ul>
<li>Measures the strength of association between an exposure and an outcome.</li>
<li>Indicates the ratio of the risk of developing the outcome in the exposed group compared to the unexposed group.</li>
<li>Helps determine the magnitude of the effect of an exposure on the outcome.</li>
<li>Enables comparison of the risk between different groups or populations.</li>
<li>Allows for assessing the impact of interventions or treatments on the risk of the outcome.</li>
</ul>
<p><strong>Relative Risk(OR)</strong></p>
<ul>
<li>Estimates the odds of an event occurring in one group compared to another group.</li>
<li>Widely used in case-control studies and logistic regression.</li>
<li>Useful when the outcome is rare, as it approximates the relative risk.</li>
<li>Can be used to assess the strength of association between an exposure and an outcome.</li>
<li>Enables comparison of the odds between different groups or populations.</li>
</ul>
</section>
<section id="relative-risk" class="level2" data-number="0.2">
<h2 data-number="0.2" class="anchored" data-anchor-id="relative-risk"><span class="header-section-number">0.2</span> Relative Risk</h2>
<div id="def-risk" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Risk) </strong></span>The risk can be represented as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Barray%7D%7Bc%7Cc%7Cc%7D%0A&amp;%20%5Ctext%7BEvent%7D%20&amp;%20%5Ctext%7BNon-Event%7D%20%5C%5C%20%5Chline%0A%5Ctext%7BExposed%7D%20&amp;%20a%20&amp;%20b%20%5C%5C%0A%5Ctext%7BUnexposed%7D%20&amp;%20c%20&amp;%20d%20%5C%5C%0A%5Cend%7Barray%7D%0A"></p>
<p>The risk of the event occurring in the exposed group is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRisk%7D_%5Ctext%7Bexposed%7D%20=%20%5Cfrac%7Ba%7D%7Ba%20+%20b%7D%0A"></p>
<p>Similarly, the risk of the event occurring in the unexposed group is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRisk%7D_%5Ctext%7Bunexposed%7D%20=%20%5Cfrac%7Bc%7D%7Bc%20+%20d%7D%0A"></p>
</div>
<section id="properties" class="level3" data-number="0.2.1">
<h3 data-number="0.2.1" class="anchored" data-anchor-id="properties"><span class="header-section-number">0.2.1</span> Properties</h3>
<ul>
<li>Probability: Risk is a measure of the probability or likelihood of an event or outcome occurring. It quantifies the chance that an undesired event will happen.</li>
<li>Magnitude: Risk can vary in terms of its magnitude, reflecting the potential impact or severity of the event. High-risk situations involve events with significant consequences, while low-risk situations involve events with minor consequences.</li>
<li>Contextual Dependence: Risk is influenced by the specific context or domain under consideration. The same event may be perceived as higher or lower risk depending on the circumstances, individuals involved, and external factors.</li>
<li>Subjectivity: Risk perception can be subjective, varying from person to person. Individuals may perceive and assess risk differently based on their experiences, knowledge, beliefs, and personal characteristics.</li>
<li>Uncertainty: Risk assessment often involves dealing with uncertainties, as it is challenging to predict outcomes with absolute certainty. Uncertainty arises from limited information, variability, and potential unknown factors.</li>
<li>Trade-offs: Risk often involves trade-offs, where accepting or managing one risk may introduce or mitigate another. Decision-makers may need to consider and weigh different risks against each other to make informed choices.</li>
<li>Time Dependency: Risk can change over time. It may evolve due to external factors, interventions, or natural processes. Long-term risks may require considering trends, projections, and potential future scenarios.</li>
</ul>
</section>
<section id="interpretation" class="level3" data-number="0.2.2">
<h3 data-number="0.2.2" class="anchored" data-anchor-id="interpretation"><span class="header-section-number">0.2.2</span> Interpretation</h3>
<section id="risk-in-exposed-group" class="level4" data-number="0.2.2.1">
<h4 data-number="0.2.2.1" class="anchored" data-anchor-id="risk-in-exposed-group"><span class="header-section-number">0.2.2.1</span> Risk in Exposed Group</h4>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRisk%7D_%5Ctext%7Bexposed%7D%20=%20%5Cfrac%7Ba%7D%7Ba%20+%20b%7D%0A"></p>
<p>This represents the proportion of individuals in the exposed group who experience the event.</p>
<ul>
<li>If the risk is low (close to 0), it indicates a lower likelihood of the event occurring in the exposed group.</li>
<li>If the risk is high (close to 1), it suggests a higher likelihood of the event occurring in the exposed group.</li>
</ul>
</section>
<section id="risk-in-unexposed-group" class="level4" data-number="0.2.2.2">
<h4 data-number="0.2.2.2" class="anchored" data-anchor-id="risk-in-unexposed-group"><span class="header-section-number">0.2.2.2</span> Risk in Unexposed Group</h4>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRisk%7D_%5Ctext%7Bexposed%7D%20=%20%5Cfrac%7Ba%7D%7Ba%20+%20b%7D%0A"></p>
<p>This represents the proportion of individuals in the unexposed group who experience the event.</p>
<ul>
<li>If the risk is low (close to 0), it indicates a lower likelihood of the event occurring in the unexposed group.</li>
<li>If the risk is high (close to 1), it suggests a higher likelihood of the event occurring in the unexposed group.</li>
</ul>
</section>
<section id="comparing-risks" class="level4" data-number="0.2.2.3">
<h4 data-number="0.2.2.3" class="anchored" data-anchor-id="comparing-risks"><span class="header-section-number">0.2.2.3</span> Comparing Risks</h4>
<p>Compare the risks between the exposed and unexposed groups to assess the association between the exposure and the event.</p>
<ul>
<li>If the risk in the exposed group is significantly higher than the risk in the unexposed group, it suggests a positive association. The exposure may be a risk factor for the event.</li>
<li>If the risk in the exposed group is significantly lower than the risk in the unexposed group, it suggests a negative association. The exposure may be protective against the event.</li>
<li>If the risks are similar between the exposed and unexposed groups, it suggests no significant association between the exposure and the event.</li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Interpreting risk requires considering other factors such as study design, sample size, confidence intervals, and potential confounders. Additionally, the specific context and domain of the study should be taken into account when interpreting risk.</p>
<p>Remember to interpret risk in conjunction with other measures such as relative risk, odds ratio, and confidence intervals to gain a comprehensive understanding of the relationship between exposure and event occurrence.</p>
</div>
</div>
<div id="def-relative_risk" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (Relative Risk) </strong></span>the relative risk (RR) is calculated by dividing the risk in the exposed group by the risk in the unexposed group.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRelative%20Risk%20(RR)%7D%20=%20%5Cfrac%7B%7B%5Ctext%7BRisk%20in%20exposed%20group%7D%7D%7D%7B%7B%5Ctext%7BRisk%20in%20unexposed%20group%7D%7D%7D%0A"></p>
<p>The relative risk (RR) can be calculated as the formula: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRR%7D%20=%20%5Cfrac%7B%7B%5Cfrac%7Ba%7D%7B%7Ba+b%7D%7D%7D%7D%7B%7B%5Cfrac%7Bc%7D%7B%7Bc+d%7D%7D%7D%7D%0A"></p>
</div>
</section>
</section>
<section id="properties-1" class="level3" data-number="0.2.3">
<h3 data-number="0.2.3" class="anchored" data-anchor-id="properties-1"><span class="header-section-number">0.2.3</span> Properties</h3>
<ul>
<li>Comparison of Risks: Relative risk compares the risk of an outcome between two groups exposed to different levels of a factor or intervention. It provides a quantitative measure of the association between exposure and outcome.</li>
<li>Interpretability: Relative risk is easily interpretable as a ratio of risks. An RR of 1 indicates no difference in risk between the exposed and unexposed groups. An RR greater than 1 suggests an increased risk associated with exposure, while an RR less than 1 indicates a decreased risk.</li>
<li>Contextual Interpretation: The interpretation of relative risk depends on the specific context, exposure, outcome, and population under study.</li>
<li>Causality: Relative risk is a useful measure for assessing causal relationships between exposures and outcomes, although it does not establish causality alone. Additional evidence from other study designs and criteria for causality should be considered.</li>
<li>Directionality: Relative risk indicates the direction of the association between exposure and outcome. An RR greater than 1 signifies a positive association, indicating that exposure increases the risk of the outcome. An RR less than 1 indicates a negative association, suggesting a decreased risk with exposure.</li>
<li>Strength of Association: The magnitude of the relative risk reflects the strength of the association between exposure and outcome.</li>
<li>Confidence Interval: Relative risk is often reported with a confidence interval (CI) to quantify the uncertainty around the point estimate.</li>
<li>Temporality: Relative risk is typically measured in prospective or retrospective cohort studies, where exposure is assessed before the outcome occurrence. This temporal relationship supports the assessment of causality between exposure and outcome.</li>
<li>Effect Modification: Relative risk can be used to evaluate effect modification, where the strength of the association between exposure and outcome varies based on the levels of another factor. Stratified analyses can help identify effect modifiers.</li>
<li>Generalizability: The generalizability of relative risk depends on the study population, exposure assessment methods, outcome measurement, and other characteristics of the study. Generalizability should be considered when interpreting and applying relative risk estimates to other populations or settings.</li>
</ul>
</section>
<section id="interpretation-1" class="level3" data-number="0.2.4">
<h3 data-number="0.2.4" class="anchored" data-anchor-id="interpretation-1"><span class="header-section-number">0.2.4</span> Interpretation</h3>
<ul>
<li>RR = 1: If the relative risk is equal to 1, it indicates no difference in risk between the exposed and unexposed groups. In other words, the exposure does not have an impact on the outcome.</li>
<li>RR &gt; 1: A relative risk greater than 1 signifies an increased risk associated with exposure. This suggests that the exposed group has a higher risk of experiencing the outcome compared to the unexposed group.</li>
<li>RR &lt; 1: A relative risk less than 1 indicates a decreased risk associated with exposure. This suggests that the exposed group has a lower risk of experiencing the outcome compared to the unexposed group.</li>
<li>Magnitude: The higher the RR value, the stronger the association.
<ul>
<li>risk effect: an RR of 1.5 indicates a 50% higher risk in the exposed group compared to the unexposed group.</li>
<li>protective effect: an RR of 0.5 indicates a 50% lower risk in the exposed group compared to the unexposed group.</li>
</ul></li>
<li>Direction
<ul>
<li>An RR &gt; 1 indicates a positive association, suggesting that the exposure is a risk factor for the outcome.</li>
<li>An RR &lt; 1 indicates a negative association, suggesting that the exposure is protective against the outcome.</li>
</ul></li>
<li>Confidence Interval (CI): The relative risk is often reported with a confidence interval, which indicates the range of plausible values for the true relative risk. If the CI includes 1, it suggests that the observed association is not statistically significant, and the risk difference between the groups may be due to chance.</li>
<li>Clinical and Public Health Significance: When interpreting relative risk, consider the clinical or public health significance of the observed association. A small or modest relative risk may have limited practical importance, whereas a large relative risk may have substantial implications for intervention or preventive measures.</li>
</ul>
</section>
<section id="example" class="level3" data-number="0.2.5">
<h3 data-number="0.2.5" class="anchored" data-anchor-id="example"><span class="header-section-number">0.2.5</span> Example</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Barray%7D%7Bc%7Cc%7Cc%7Cc%7D%0A&amp;%20&amp;%20%5Ctext%7BOutcome%7D%20&amp;%20%5C%5C%0A&amp;%20&amp;%20%5Ctext%7BYes%7D%20&amp;%20%5Ctext%7BNo%7D%20%5C%5C%0A%5Chline%0A%5Ctext%7BExposure%7D%20&amp;%20%5Ctext%7BExposed%7D%20&amp;%2050%20&amp;%20100%20%5C%5C%0A&amp;%20%5Ctext%7BUnexposed%7D%20&amp;%2030%20&amp;%20120%20%5C%5C%0A%5Cend%7Barray%7D%0A"></p>
<p>The contingency table above represents the relationship between exposure and outcome. <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRR%7D%20=%20%5Cfrac%7B%7B%5Cfrac%7B%7B50%7D%7D%7B%7B50+100%7D%7D%7D%7D%7B%7B%5Cfrac%7B%7B30%7D%7D%7B%7B30+120%7D%7D%7D%7D%20=%20%5Cfrac%7B%7B50%20%5Ctimes%20(30+120)%7D%7D%7B%7B30%20%5Ctimes%20(50+100)%7D%7D%0A"></p>
<p>To interpret the relative risk:</p>
<ul>
<li>If RR = 1, there is no difference in risk between the exposed and unexposed groups.</li>
<li>If RR &gt; 1, the exposed group has a higher risk of the outcome compared to the unexposed group.</li>
<li>If RR &lt; 1, the exposed group has a lower risk of the outcome compared to the unexposed group.</li>
</ul>
<p>Let’s calculate the relative risk using the given values:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRR%7D%20=%20%5Cfrac%7B%7B50%20%5Ctimes%20(30+120)%7D%7D%7B%7B30%20%5Ctimes%20(50+100)%7D%7D%20=%20%5Cfrac%7B%7B50%20%5Ctimes%20150%7D%7D%7B%7B30%20%5Ctimes%20150%7D%7D%20=%20%5Cfrac%7B%7B7500%7D%7D%7B%7B4500%7D%7D%20=%201.67%0A"></p>
<p>Based on the calculated RR of 1.67, we can conclude that the exposed group has a 1.67 times higher risk of the outcome compared to the unexposed group.</p>
</section>
</section>
<section id="odds-ratio" class="level2" data-number="0.3">
<h2 data-number="0.3" class="anchored" data-anchor-id="odds-ratio"><span class="header-section-number">0.3</span> Odds Ratio</h2>
<div id="def-odds" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (Odds) </strong></span>The odds are defined as the probability that the event will occur divided by the probability that the event will not occur a.k.a <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Ctext%7BP%7D(%5Ctext%7BSucess%7D)%7D%7B%5Ctext%7BP%7D(%5Ctext%7BFailure%7D)%7D">. The odds can be denoted as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Ctext%7BP%7D(X=%5Ctext%7Bthe%20event%20will%20occur%7D)%7D%7B%5Ctext%7BP%7D(%5Coverline%7BX%7D=%5Ctext%7Bthe%20event%20will%20not%20occur%7D)%7D%20=%20%5Cfrac%7B%5Ctext%7BP%7D(X)%7D%7B1-%5Ctext%7BP%7D(X)%7D%0A">.</p>
<p>In other words, odds is a ratio of probabilities.</p>
<p>The odds can be calcuated as the formula:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Barray%7D%7Bc%7Cc%7Cc%7D%0A&amp;%20%5Ctext%7BEvent%7D%20&amp;%20%5Ctext%7BNon-Event%7D%20%5C%5C%20%5Chline%0A%5Ctext%7BExposed%7D%20&amp;%20a%20&amp;%20b%20%5C%5C%0A%5Ctext%7BUnexposed%7D%20&amp;%20c%20&amp;%20d%20%5C%5C%0A%5Cend%7Barray%7D%0A"></p>
<p>The odds of the event occurring in the exposed group are given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BOdds%7D_%5Ctext%7Bexposed%7D%20=%20%5Cfrac%7Ba%7D%7Bb%7D%0A"></p>
<p>Similarly, the odds of the event occurring in the unexposed group are given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BOdds%7D_%5Ctext%7Bunexposed%7D%20=%20%5Cfrac%7Bc%7D%7Bd%7D%0A"></p>
</div>
<section id="properties-2" class="level3" data-number="0.3.1">
<h3 data-number="0.3.1" class="anchored" data-anchor-id="properties-2"><span class="header-section-number">0.3.1</span> Properties</h3>
<ul>
<li>Ratio of Probabilities: Odds represent the ratio of the probability of an event occurring to the probability of the event not occurring. Mathematically, odds are defined as the probability of an event divided by the probability of its complement.</li>
<li>Range: Odds <img src="https://latex.codecogs.com/png.latex?%5Cin%20%5B0,%5Cinfty)">. An event with a probability of 0 corresponds to odds of 0, while an event with a probability of 1 corresponds to odds of infinity. Values between 0 and 1 represent odds less than 1, indicating the event is less likely to occur than not.</li>
<li>Interpretability: Odds are often interpreted as the number of times an event is likely to occur compared to the number of times it is unlikely to occur. For example, odds of 2:1 indicate that the event is twice as likely to occur as not to occur.</li>
<li>Non-symmetry: Odds are not symmetric around 1. For example, odds of 2:1 and 1:2 represent different scenarios, with the former indicating a higher likelihood of the event and the latter indicating a higher likelihood of the complement.</li>
<li>Independence: When events are independent, the odds of their joint occurrence can be calculated by multiplying the individual odds. In contrast, probabilities cannot be directly multiplied for independent events.</li>
</ul>
<p>Odds Ratios: The ratio of two odds is called an odds ratio. Odds ratios are commonly used to measure the association between exposure and outcome in case-control studies and logistic regression analysis.</p>
<p>Logarithmic Transformation: Odds are often transformed using logarithms for ease of analysis. The log-odds, also known as the logit, is commonly used in logistic regression models.</p>
<p>Probability-Odds Conversion: Odds can be converted back to probabilities using the formula: probability = odds / (1 + odds). This conversion allows for the interpretation of odds in terms of probabilities.</p>
<p>Common Use in Gambling: Odds have widespread use in the gambling industry to represent the likelihood of specific outcomes in games of chance, such as sports betting and casino games.</p>
<p>Interpretation Challenges: Odds are not as intuitively interpretable as probabilities, particularly when the odds deviate significantly from 1. Care must be taken when interpreting odds to avoid miscommunication or misunderstanding.</p>
<p>Understanding these properties of odds is essential for their proper application and interpretation in various fields of study. It is crucial to consider the specific context, statistical methods, and other measures of association when working with odds.</p>
</section>
</section>
</div>
</div>
<div id="English" class="tab-pane fade" aria-labelledby="English-tab">
<div id="English" class="tab-pane fade" aria-labelledby="English-tab">

</div>
</div>
<section id="go-to-project-content-list" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Go to Project Content List</h1>
<p><a href="./docs/projects/index.qmd">Project Content List</a></p>
</section>
<section id="go-to-blog-content-list" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Go to Blog Content List</h1>
<p><a href="../../content_list.qmd">Blog Content List</a></p>


</section>

</div></ul> ]]></description>
  <category>Epidemiology</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Epidemilogy/relative_risk_odds_ratio.html</guid>
  <pubDate>Mon, 22 May 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Important Concepts in Data Science</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/concepts/concepts.html</link>
  <description><![CDATA[ 



<section id="statistics" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="statistics"><span class="header-section-number">1</span> Statistics</h2>
<section id="what-is-the-central-limit-theorem-and-why-is-it-important" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="what-is-the-central-limit-theorem-and-why-is-it-important"><span class="header-section-number">1.1</span> WHAT IS THE CENTRAL LIMIT THEOREM AND WHY IS IT IMPORTANT?</h3>
<p>The central limit theorem is a fundamental concept in probability theory and statistics. It states that if a large enough sample size is taken from a population, the distribution of the sample means will be approximately normal, regardless of the shape of the original population distribution.</p>
<p>In simpler terms, this means that as the sample size increases, the sample mean will become a more accurate representation of the population mean, and the distribution of the sample means will become more bell-shaped and symmetrical, resembling a normal distribution.</p>
<p>The central limit theorem is important because it allows us to make statistical inferences about a population based on a sample, even if we don’t know the exact shape of the population distribution. It also helps us to understand the behavior of the sample mean and standard deviation, which are key parameters used in many statistical tests and models.</p>
<section id="examples" class="level4" data-number="1.1.1">
<h4 data-number="1.1.1" class="anchored" data-anchor-id="examples"><span class="header-section-number">1.1.1</span> Examples</h4>
<p><strong>Example1: Average of Body Weights In Population</strong> Let’s say you want to know the average body weight of all people in your country. It would be impractical to measure every single person’s weight, so you take a sample of 1000 people. The central limit theorem tells us that if we take enough samples (more than 30) of size 1000, the means of those samples will be normally distributed. This allows us to make inferences about the population mean based on our sample mean.</p>
<p><strong>Example2: Quality Control</strong> Suppose a factory produces a large number of identical products, and the weight of each product follows a normal distribution with a mean of 500 grams and a standard deviation of 10 grams. The factory wants to ensure that the average weight of a batch of 100 products is within a certain range, say between 498 and 502 grams, with a confidence level of 95%.</p>
<p>Using the central limit theorem, we can take random samples of 100 products from the population, calculate the mean weight of each sample, and create a sampling distribution of the means. Since the sample size is sufficiently large (n &gt; 30), the central limit theorem tells us that the sampling distribution of the means will be approximately normal, with a mean of 500 grams and a standard deviation of 1 gram (calculated as the population standard deviation divided by the square root of the sample size).</p>
<p>We can then use this information to calculate the probability that the average weight of a batch of 100 products will fall within the desired range. Using a standard normal distribution table or software, we can find the z-score for each endpoint of the range (498 and 502), and calculate the probability that a randomly selected sample of 100 products will have a mean weight within this range. If the probability is at least 95%, we can conclude that the factory’s quality control standards are met.</p>
</section>
</section>
<section id="what-is-sampling-how-many-sampling-methods-are-there" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="what-is-sampling-how-many-sampling-methods-are-there"><span class="header-section-number">1.2</span> WHAT IS SAMPLING? HOW MANY SAMPLING METHODS ARE THERE?</h3>
<p>The purpose of sampling is to obtain a representative subset of the population that can be used to make inferences or draw conclusions about the larger population.</p>
<p>There are several sampling methods, including:</p>
<section id="probabilistic-sampling" class="level4" data-number="1.2.1">
<h4 data-number="1.2.1" class="anchored" data-anchor-id="probabilistic-sampling"><span class="header-section-number">1.2.1</span> Probabilistic Sampling</h4>
<ul>
<li>Simple Random Sampling: In this method, each individual or unit in the population has an equal chance of being selected for the sample. This is often done using random number generators or by using a table of random numbers.</li>
<li>Stratified Sampling: This method involves dividing the population into subgroups, or strata, based on some characteristic (such as age, gender, or geographic location), and then selecting a random sample from each stratum. This ensures that the sample is representative of the population with respect to the characteristic of interest.</li>
<li>Cluster Sampling: This method involves dividing the population into clusters, such as households or schools, and then selecting a random sample of clusters to survey. All individuals or units within each selected cluster are then surveyed.</li>
<li>Systematic Sampling: In this method, a random starting point is chosen from the population, and then every nth individual or unit is selected for the sample, where n is a predetermined interval.</li>
</ul>
</section>
<section id="non-probabilistic-sampling" class="level4" data-number="1.2.2">
<h4 data-number="1.2.2" class="anchored" data-anchor-id="non-probabilistic-sampling"><span class="header-section-number">1.2.2</span> Non-Probabilistic Sampling</h4>
<ul>
<li>Convenience Sampling: This method involves selecting individuals or units from the population based on convenience or availability, rather than using a formal sampling technique. While convenient, this method can lead to biased results and is generally not recommended for research purposes.</li>
<li>Snowball Sampling: This method is often used when studying hard-to-reach populations or groups with limited accessibility. It involves starting with a small group of individuals who meet certain criteria, and then having them refer other individuals who meet the same criteria. This process continues until the desired sample size is reached.</li>
</ul>
</section>
</section>
<section id="what-is-the-difference-between-type-i-vs-type-ii-error" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="what-is-the-difference-between-type-i-vs-type-ii-error"><span class="header-section-number">1.3</span> WHAT IS THE DIFFERENCE BETWEEN TYPE I VS TYPE II ERROR?</h3>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 31%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Truth Table</th>
<th style="text-align: center;">Reject <img src="https://latex.codecogs.com/png.latex?H_%7Bnull%7D"></th>
<th style="text-align: center;">Fail to Reject <img src="https://latex.codecogs.com/png.latex?H_%7Bnull%7D"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?H_%7Bnull%7D"> = True</td>
<td style="text-align: center;">Type I Error, <img src="https://latex.codecogs.com/png.latex?%5Calpha">, FP</td>
<td style="text-align: center;">Correct Decision (1 - <img src="https://latex.codecogs.com/png.latex?%5Calpha">), TN</td>
</tr>
<tr class="even">
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?H_%7Bnull%7D"> = False</td>
<td style="text-align: center;">Correct Decision (Power, 1-<img src="https://latex.codecogs.com/png.latex?%5Cbeta">) TP</td>
<td style="text-align: center;">Type II Error, <img src="https://latex.codecogs.com/png.latex?%5Cbeta">, FN</td>
</tr>
</tbody>
</table>
<p>In hypothesis testing, a Type I error occurs when we reject a null hypothesis that is actually true. In other words, we conclude that there is a significant effect or difference when there really isn’t one. This error is also known as a false positive.</p>
<p>On the other hand, a Type II error occurs when we fail to reject a null hypothesis that is actually false. In other words, we conclude that there is no significant effect or difference when there actually is one. This error is also known as a false negative.</p>
</section>
<section id="what-is-linear-regression" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="what-is-linear-regression"><span class="header-section-number">1.4</span> WHAT IS LINEAR REGRESSION?</h3>
<p>Linear regression is a statistical technique used to model the relationship between a dependent variable (also known as the response variable) and one or more independent variables (also known as predictors or explanatory variables). The goal of linear regression is to find the best linear relationship between the dependent variable and the independent variable(s) that can be used to predict the values of the dependent variable.</p>
<p>In linear regression, the p-value, coefficient, and R-squared value are important components that can help to evaluate the statistical significance and predictive power of the model:</p>
<p>P-value: The p-value measures the probability of obtaining a result as extreme as the one observed, assuming that the null hypothesis is true. In the context of linear regression, the null hypothesis is that the coefficient of the independent variable is zero, indicating no linear relationship between the independent variable and the dependent variable. A low p-value (typically less than 0.05) indicates that the relationship between the independent variable and the dependent variable is statistically significant, and that the null hypothesis can be rejected.</p>
<p>Coefficient: The coefficient is the slope of the line that represents the linear relationship between the independent variable and the dependent variable. It measures the amount of change in the dependent variable that can be attributed to a one-unit change in the independent variable, while holding all other variables constant. A positive coefficient indicates a positive relationship between the independent variable and the dependent variable, while a negative coefficient indicates a negative relationship.</p>
<p>R-squared value: The R-squared value measures the proportion of the variation in the dependent variable that is explained by the variation in the independent variable(s). It is a measure of the goodness-of-fit of the model, and it ranges from 0 to 1. A high R-squared value indicates that the model explains a large proportion of the variation in the dependent variable, and that it is a good predictor of the dependent variable.</p>
<p>The significance of each of these components in linear regression is as follows:</p>
<p>P-value: It helps to determine whether the relationship between the independent variable and the dependent variable is statistically significant, and whether the model can be used to make valid predictions.</p>
<p>Coefficient: It helps to determine the direction and strength of the linear relationship between the independent variable and the dependent variable, and to make predictions about the dependent variable based on changes in the independent variable.</p>
<p>R-squared value: It helps to evaluate the overall fit of the model, and to compare different models to determine which one is the best predictor of the dependent variable.</p>
</section>
<section id="what-is-selection-bias" class="level3" data-number="1.5">
<h3 data-number="1.5" class="anchored" data-anchor-id="what-is-selection-bias"><span class="header-section-number">1.5</span> WHAT IS SELECTION BIAS?</h3>
<p>Selection bias is a type of bias that occurs when the process of selecting individuals or groups to participate in a study or analysis is not random or representative of the population of interest. Selection bias can lead to biased estimates of the population parameters and affect the internal and external validity of the study.</p>
<p>Selection bias can occur in different stages of a study or analysis, such as:</p>
<p>Sampling bias: Occurs when the sample is not representative of the population of interest. For example, if a study on the prevalence of a disease only includes participants from a certain demographic or geographic area, the results may not be generalizable to the entire population.</p>
<p>Non-response bias: Occurs when the response rate is low, or participants who decline to participate are different from those who agree to participate. This can lead to an unrepresentative sample and biased estimates.</p>
<p>Survivorship bias: Occurs when the sample only includes individuals or groups that have survived or persisted through a certain selection process, such as a follow-up study. This can lead to an overestimation of the survival rates or other outcomes.</p>
<p>Berkson’s bias: Occurs when the sample is selected based on a condition that is related to the exposure or outcome of interest, such as hospital-based samples. This can lead to an underestimation of the association between the exposure and outcome.</p>
<p>Selection bias can be minimized by using random sampling techniques and ensuring that the sample is representative of the population of interest. It is also important to analyze and report any potential biases and limitations of the study to ensure that the results are interpreted correctly.</p>
</section>
<section id="section" class="level3" data-number="1.6">
<h3 data-number="1.6" class="anchored" data-anchor-id="section"><span class="header-section-number">1.6</span> </h3>
</section>
<section id="section-1" class="level3" data-number="1.7">
<h3 data-number="1.7" class="anchored" data-anchor-id="section-1"><span class="header-section-number">1.7</span> </h3>
</section>
<section id="section-2" class="level3" data-number="1.8">
<h3 data-number="1.8" class="anchored" data-anchor-id="section-2"><span class="header-section-number">1.8</span> </h3>
</section>
<section id="section-3" class="level3" data-number="1.9">
<h3 data-number="1.9" class="anchored" data-anchor-id="section-3"><span class="header-section-number">1.9</span> </h3>
</section>
<section id="section-4" class="level3" data-number="1.10">
<h3 data-number="1.10" class="anchored" data-anchor-id="section-4"><span class="header-section-number">1.10</span> </h3>
</section>
<section id="section-5" class="level3" data-number="1.11">
<h3 data-number="1.11" class="anchored" data-anchor-id="section-5"><span class="header-section-number">1.11</span> </h3>
</section>
<section id="section-6" class="level3" data-number="1.12">
<h3 data-number="1.12" class="anchored" data-anchor-id="section-6"><span class="header-section-number">1.12</span> </h3>
</section>
<section id="section-7" class="level3" data-number="1.13">
<h3 data-number="1.13" class="anchored" data-anchor-id="section-7"><span class="header-section-number">1.13</span> </h3>
</section>
<section id="section-8" class="level3" data-number="1.14">
<h3 data-number="1.14" class="anchored" data-anchor-id="section-8"><span class="header-section-number">1.14</span> </h3>
</section>
<section id="section-9" class="level3" data-number="1.15">
<h3 data-number="1.15" class="anchored" data-anchor-id="section-9"><span class="header-section-number">1.15</span> </h3>
</section>
<section id="section-10" class="level3" data-number="1.16">
<h3 data-number="1.16" class="anchored" data-anchor-id="section-10"><span class="header-section-number">1.16</span> </h3>
</section>
<section id="section-11" class="level3" data-number="1.17">
<h3 data-number="1.17" class="anchored" data-anchor-id="section-11"><span class="header-section-number">1.17</span> </h3>
</section>
</section>
<section id="data-science" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="data-science"><span class="header-section-number">2</span> Data Science</h2>
<section id="what-is-data-science" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="what-is-data-science"><span class="header-section-number">2.1</span> WHAT IS DATA SCIENCE?</h3>
<p>Data science is an interdisciplinary field that involves the use of statistical, computational, and domain-specific knowledge and techniques to extract insights and knowledge from data. It combines methods and concepts from statistics, computer science, mathematics, and domain-specific knowledge to solve complex data-related problems.</p>
<p>Statistics and computer science are two important components of data science. Statistics provides the foundational methods for analyzing and modeling data, while computer science provides the computational infrastructure and tools for data storage, processing, and analysis. In addition, computer science provides techniques for data visualization, machine learning, and artificial intelligence that can be used to extract insights and knowledge from large datasets.</p>
<p>Data science involves a wide range of activities, including data acquisition, cleaning and preprocessing, exploratory data analysis, modeling and prediction, data visualization, and communication of results. It requires a strong foundation in statistics, programming, and domain-specific knowledge, as well as expertise in data management, data visualization, and machine learning.</p>
<p>In summary, data science is a multidisciplinary field that combines statistical and computational methods to extract insights and knowledge from data. It draws on a wide range of disciplines, including statistics, computer science, mathematics, and domain-specific knowledge, and requires a diverse set of skills and expertise.</p>
</section>
<section id="section-12" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="section-12"><span class="header-section-number">2.2</span> </h3>
</section>
<section id="section-13" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="section-13"><span class="header-section-number">2.3</span> </h3>
</section>
<section id="section-14" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="section-14"><span class="header-section-number">2.4</span> </h3>
</section>
<section id="section-15" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="section-15"><span class="header-section-number">2.5</span> </h3>
</section>
<section id="section-16" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="section-16"><span class="header-section-number">2.6</span> </h3>
</section>
<section id="section-17" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="section-17"><span class="header-section-number">2.7</span> </h3>
</section>
<section id="section-18" class="level3" data-number="2.8">
<h3 data-number="2.8" class="anchored" data-anchor-id="section-18"><span class="header-section-number">2.8</span> </h3>
</section>
<section id="section-19" class="level3" data-number="2.9">
<h3 data-number="2.9" class="anchored" data-anchor-id="section-19"><span class="header-section-number">2.9</span> </h3>
</section>
<section id="section-20" class="level3" data-number="2.10">
<h3 data-number="2.10" class="anchored" data-anchor-id="section-20"><span class="header-section-number">2.10</span> </h3>
</section>
<section id="section-21" class="level3" data-number="2.11">
<h3 data-number="2.11" class="anchored" data-anchor-id="section-21"><span class="header-section-number">2.11</span> </h3>
</section>
<section id="section-22" class="level3" data-number="2.12">
<h3 data-number="2.12" class="anchored" data-anchor-id="section-22"><span class="header-section-number">2.12</span> </h3>
</section>
<section id="section-23" class="level3" data-number="2.13">
<h3 data-number="2.13" class="anchored" data-anchor-id="section-23"><span class="header-section-number">2.13</span> </h3>
</section>
<section id="section-24" class="level3" data-number="2.14">
<h3 data-number="2.14" class="anchored" data-anchor-id="section-24"><span class="header-section-number">2.14</span> </h3>
</section>
<section id="section-25" class="level3" data-number="2.15">
<h3 data-number="2.15" class="anchored" data-anchor-id="section-25"><span class="header-section-number">2.15</span> </h3>
</section>
<section id="section-26" class="level3" data-number="2.16">
<h3 data-number="2.16" class="anchored" data-anchor-id="section-26"><span class="header-section-number">2.16</span> </h3>
</section>
<section id="section-27" class="level3" data-number="2.17">
<h3 data-number="2.17" class="anchored" data-anchor-id="section-27"><span class="header-section-number">2.17</span> </h3>
</section>
<section id="section-28" class="level3" data-number="2.18">
<h3 data-number="2.18" class="anchored" data-anchor-id="section-28"><span class="header-section-number">2.18</span> </h3>
</section>
<section id="section-29" class="level3" data-number="2.19">
<h3 data-number="2.19" class="anchored" data-anchor-id="section-29"><span class="header-section-number">2.19</span> </h3>
</section>
<section id="section-30" class="level3" data-number="2.20">
<h3 data-number="2.20" class="anchored" data-anchor-id="section-30"><span class="header-section-number">2.20</span> </h3>
</section>
<section id="section-31" class="level3" data-number="2.21">
<h3 data-number="2.21" class="anchored" data-anchor-id="section-31"><span class="header-section-number">2.21</span> </h3>


</section>
</section>

 ]]></description>
  <category>DS Concepts</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/concepts/concepts.html</guid>
  <pubDate>Fri, 21 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Orthogonality</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/06.orthogonality.html</link>
  <description><![CDATA[ 



<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> matplotlib.animation <span class="im" style="color: #00769E;">as</span> animation</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">import</span> matplotlib_inline</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">from</span> mpl_toolkits.mplot3d <span class="im" style="color: #00769E;">import</span> Axes3D</span>
<span id="cb1-6"><span class="im" style="color: #00769E;">import</span> sympy <span class="im" style="color: #00769E;">as</span> sym <span class="co" style="color: #5E5E5E;"># for RREF</span></span>
<span id="cb1-7"><span class="im" style="color: #00769E;">import</span> scipy.linalg <span class="co" style="color: #5E5E5E;"># for LU</span></span>
<span id="cb1-8"><span class="im" style="color: #00769E;">import</span> matplotlib.gridspec <span class="im" style="color: #00769E;">as</span> gridspec <span class="co" style="color: #5E5E5E;"># used to create non-regular subplots</span></span>
<span id="cb1-9"><span class="im" style="color: #00769E;">from</span> scipy.linalg <span class="im" style="color: #00769E;">import</span> lstsq <span class="co" style="color: #5E5E5E;"># for least square example</span></span></code></pre></div>
</details>
</div>
<section id="orthogonality" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Orthogonality</h1>
<ul>
<li>Orthogonality of the Four Subspaces
<ul>
<li>Orthogonal Vectors</li>
<li>Orthogonal Subspaces
<ul>
<li>Orthogonal Components</li>
</ul></li>
<li>Orthogonal Bases</li>
<li>Orthogonal Matrices</li>
</ul></li>
<li>Orthogonal Vector Decomposition,</li>
<li>QR decomposition
<ul>
<li>‘Q’ stands for an orthogonal matrix, and</li>
<li>‘R’ stands for an upper triangular matrix.</li>
</ul></li>
<li>Gram-Schmidt Decomposition,</li>
<li>Eigen Decomposition, and</li>
<li>Singular Value Decomposition</li>
</ul>
<section id="orthogonality-of-the-four-subspaces" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="orthogonality-of-the-four-subspaces"><span class="header-section-number">1.1</span> Orthogonality of the Four Subspaces</h2>
<p>The four subspaces: vectors, subspaces, orthogonal bases, and orthogonal matrices</p>
<section id="orthogonal-vectors" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="orthogonal-vectors"><span class="header-section-number">1.1.1</span> Orthogonal Vectors</h3>
<div id="def-orthogonalVec" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 </strong></span>Two vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En"> are said to be orthogonal if their dot product is zero:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bv%7D%20%5Ccdot%20%5Cmathbf%7Bw%7D%20=%20%5Csum_%7Bi=1%7D%5En%20v_i%20w_i%20=%200%20%5Ctext%7B%20and%20%7D%7C%7C%5Cmathbf%7Bv%7D%7C%7C%5E2+%7C%7C%5Cmathbf%7Bw%7D%7C%7C%5E2=%7C%7C%5Cmathbf%7Bv%7D+%5Cmathbf%7Bw%7D%7C%7C%5E2%0A"></p>
</div>
<p>Geometrically, two vectors are orthogonal if they are perpendicular to each other.</p>
<p>Orthogonality is an important concept in linear algebra and has many applications, including in the construction of orthonormal bases and in least-squares regression.</p>
<section id="examples" class="level4" data-number="1.1.1.1">
<h4 data-number="1.1.1.1" class="anchored" data-anchor-id="examples"><span class="header-section-number">1.1.1.1</span> Examples</h4>
<p><strong>Example1</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5C%5C%200%20%5Cend%7Bbmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%201%20%5C%5C%200%20%5Cend%7Bbmatrix%7D"> are orthogonal because <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%20%5Ccdot%20%5Cmathbf%7Bw%7D%20=%201%20%5Ccdot%200%20+%200%20%5Ccdot%201%20+%200%20%5Ccdot%200%20=%200">. These vectors are also perpendicular to each other in 3D space.</p>
<p><strong>Example2</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%20-2%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"> are orthogonal because <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%20%5Ccdot%20%5Cmathbf%7Bw%7D%20=%201%20%5Ccdot%201%20+%201%20%5Ccdot%20(-2)%20+%201%20%5Ccdot%201%20=%200">. These vectors are also perpendicular to each other in 3D space.</p>
<p><strong>Example3</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%202%20%5Cend%7Bbmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D%20=%20%5Cbegin%7Bbmatrix%7D%20-2%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"> are orthogonal because <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%20%5Ccdot%20%5Cmathbf%7Bw%7D%20=%201%20%5Ccdot%20(-2)%20+%202%20%5Ccdot%201%20=%200">. These vectors are also perpendicular to each other in 2D space.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;"># Example 1</span></span>
<span id="cb2-2">v1 <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb2-3">w1 <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb2-4"><span class="bu" style="color: null;">print</span>(np.dot(v1, w1))  <span class="co" style="color: #5E5E5E;"># Output: 0</span></span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="co" style="color: #5E5E5E;"># Example 2</span></span>
<span id="cb2-7">v2 <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb2-8">w2 <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="dv" style="color: #AD0000;">1</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb2-9"><span class="bu" style="color: null;">print</span>(np.dot(v2, w2))  <span class="co" style="color: #5E5E5E;"># Output: 0</span></span>
<span id="cb2-10"></span>
<span id="cb2-11"><span class="co" style="color: #5E5E5E;"># Example 3</span></span>
<span id="cb2-12">v3 <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>])</span>
<span id="cb2-13">w3 <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb2-14"><span class="bu" style="color: null;">print</span>(np.dot(v3, w3))  <span class="co" style="color: #5E5E5E;"># Output: 0</span></span>
<span id="cb2-15"></span>
<span id="cb2-16"><span class="co" style="color: #5E5E5E;"># Example 1</span></span>
<span id="cb2-17">v2 <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb2-18">w2 <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="dv" style="color: #AD0000;">1</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb2-19">fig <span class="op" style="color: #5E5E5E;">=</span> plt.figure()</span>
<span id="cb2-20">ax <span class="op" style="color: #5E5E5E;">=</span> fig.add_subplot(<span class="dv" style="color: #AD0000;">111</span>, projection<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'3d'</span>)</span>
<span id="cb2-21">ax.quiver(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, v2[<span class="dv" style="color: #AD0000;">0</span>], v2[<span class="dv" style="color: #AD0000;">1</span>], v2[<span class="dv" style="color: #AD0000;">2</span>], colors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'b'</span>, arrow_length_ratio<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.1</span>)</span>
<span id="cb2-22">ax.quiver(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, w2[<span class="dv" style="color: #AD0000;">0</span>], w2[<span class="dv" style="color: #AD0000;">1</span>], w2[<span class="dv" style="color: #AD0000;">2</span>], colors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'r'</span>, arrow_length_ratio<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.1</span>)</span>
<span id="cb2-23">ax.set_xlim([<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>])</span>
<span id="cb2-24">ax.set_ylim([<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>])</span>
<span id="cb2-25">ax.set_zlim([<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>])</span>
<span id="cb2-26">ax.set_title(<span class="st" style="color: #20794D;">"Example 1: Orthogonal Vectors"</span>)</span>
<span id="cb2-27">ax.legend([<span class="st" style="color: #20794D;">"v"</span>, <span class="st" style="color: #20794D;">"w"</span>])</span>
<span id="cb2-28">plt.show()</span>
<span id="cb2-29"></span>
<span id="cb2-30"><span class="co" style="color: #5E5E5E;"># Example 2</span></span>
<span id="cb2-31">v1 <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb2-32">w1 <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb2-33">plt.quiver(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, v1[<span class="dv" style="color: #AD0000;">0</span>], v1[<span class="dv" style="color: #AD0000;">1</span>], angles<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale_units<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'b'</span>)</span>
<span id="cb2-34">plt.quiver(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, w1[<span class="dv" style="color: #AD0000;">0</span>], w1[<span class="dv" style="color: #AD0000;">1</span>], angles<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale_units<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'r'</span>)</span>
<span id="cb2-35">plt.xlim(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb2-36">plt.ylim(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb2-37">plt.title(<span class="st" style="color: #20794D;">"Example 2: Orthogonal Vectors"</span>)</span>
<span id="cb2-38">plt.legend([<span class="st" style="color: #20794D;">"v"</span>, <span class="st" style="color: #20794D;">"w"</span>])</span>
<span id="cb2-39">plt.grid(<span class="va" style="color: #111111;">True</span>)</span>
<span id="cb2-40">plt.show()</span>
<span id="cb2-41"></span>
<span id="cb2-42"></span>
<span id="cb2-43"><span class="co" style="color: #5E5E5E;"># Example 3</span></span>
<span id="cb2-44">v3 <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>])</span>
<span id="cb2-45">w3 <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb2-46">plt.quiver(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, v3[<span class="dv" style="color: #AD0000;">0</span>], v3[<span class="dv" style="color: #AD0000;">1</span>], angles<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale_units<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'b'</span>)</span>
<span id="cb2-47">plt.quiver(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, w3[<span class="dv" style="color: #AD0000;">0</span>], w3[<span class="dv" style="color: #AD0000;">1</span>], angles<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale_units<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'r'</span>)</span>
<span id="cb2-48">plt.xlim(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">3</span>)</span>
<span id="cb2-49">plt.ylim(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">3</span>)</span>
<span id="cb2-50">plt.title(<span class="st" style="color: #20794D;">"Example 3: Orthogonal Vectors"</span>)</span>
<span id="cb2-51">plt.legend([<span class="st" style="color: #20794D;">"v"</span>, <span class="st" style="color: #20794D;">"w"</span>])</span>
<span id="cb2-52">plt.grid(<span class="va" style="color: #111111;">True</span>)</span>
<span id="cb2-53">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>0
0
0</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/06.orthogonality_files/figure-html/cell-3-output-2.png" width="408" height="419"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/06.orthogonality_files/figure-html/cell-3-output-3.png" width="592" height="431"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/06.orthogonality_files/figure-html/cell-3-output-4.png" width="573" height="431"></p>
</div>
</div>
</section>
<section id="properties" class="level4" data-number="1.1.1.2">
<h4 data-number="1.1.1.2" class="anchored" data-anchor-id="properties"><span class="header-section-number">1.1.1.2</span> Properties</h4>
<ul>
<li><p>Orthogonal vectors have a dot product of zero: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bv%7D%20%5Ccdot%20%5Cmathbf%7Bw%7D%20=%200%0A"></p></li>
<li><p>The magnitude (length) of the projection of a vector onto an orthogonal vector is given by: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bproj%7D_%7B%5Cmathbf%7Bw%7D%7D(%5Cmathbf%7Bv%7D)%20=%20%5Cfrac%7B%5Cmathbf%7Bv%7D%20%5Ccdot%20%5Cmathbf%7Bw%7D%7D%7B%5C%7C%5Cmathbf%7Bw%7D%5C%7C%5E2%7D%20%5Cmathbf%7Bw%7D%20=%200%0A"></p></li>
<li><p>The Pythagorean theorem holds for orthogonal vectors: <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%5Cmathbf%7Bv%7D%20+%20%5Cmathbf%7Bw%7D%5C%7C%5E2%20=%20%5C%7C%5Cmathbf%7Bv%7D%5C%7C%5E2%20+%20%5C%7C%5Cmathbf%7Bw%7D%5C%7C%5E2%0A"></p></li>
<li><p>The angle between two orthogonal vectors is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpi%7D%7B2%7D"> radians or <img src="https://latex.codecogs.com/png.latex?90"> degrees: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta%20=%20%5Cfrac%7B%5Cpi%7D%7B2%7D%0A"></p></li>
<li><p>Orthogonal vectors are linearly independent, which means that no vector in the span of one vector can be expressed as a linear combination of the other vector: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bspan%7D%5C%7B%5Cmathbf%7Bv%7D%5C%7D%20%5Ccap%20%5Ctext%7Bspan%7D%5C%7B%5Cmathbf%7Bw%7D%5C%7D%20=%20%5C%7B%5Cmathbf%7B0%7D%5C%7D%0A"></p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="kmink3225.netlify.app/images/linear_algebra/orthogonal_space.PNG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Orthogonal Space-Gilbert Strang: Introduction to Linear Algebra</figcaption><p></p>
</figure>
</div>
<ul>
<li>The row space is perpendicular to the nullspace</li>
<li>The column space is perpendicular to the nullspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET">.
<ul>
<li>This peroperty <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> plays a key role in solving the equation <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx=b%7D"> but <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D"> is outside the column space (meaning we can’t solve the equation directly). In this case, we use the nullspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET"> to find the “least-squares” solution, which gives us the smallest possible error <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%20=%20b%20-%20Ax%7D"> in the solution.</li>
</ul></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D"> is outside the column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, there is no exact solution to the equation <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%20=%20b%7D">. Instead, we seek a solution that minimizes the error <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%20=%20b%20-%20Ax%7D">. The least-squares solution achieves this by finding the projection of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D"> onto the column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. It turns out that the projection of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D"> onto the column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is exactly equal to the solution of the equation <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7BA%7D%5ET%5Cmathbf%7Bb%7D">, which can be solved using the nullspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET">.</p>
<p>In summary, the statement “the column space is perpendicular to the nullspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET">” tells us that the column space and nullspace are orthogonal (i.e., perpendicular) subspaces, and this fact allows us to use the nullspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET"> to find the least-squares solution to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%20=%20b%7D">.</p>
</div>
</div>
<section id="least-square-example" class="level5" data-number="1.1.1.2.1">
<h5 data-number="1.1.1.2.1" class="anchored" data-anchor-id="least-square-example"><span class="header-section-number">1.1.1.2.1</span> Least Square Example</h5>
<p>Suppose we have a system of equations <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7Bb%7D"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%201"> vector, and we want to find the least squares solution to this system (i.e., the solution that minimizes the residual <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7BAx%7D%20-%20%5Cmathbf%7Bb%7D%7C">). If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> has linearly independent columns, then we can solve for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> using the formula <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20(%5Cmathbf%7BA%7D%5ET%5Cmathbf%7BA%7D)%5E%7B-1%7D%5Cmathbf%7BA%7D%5ET%5Cmathbf%7Bb%7D">.</p>
<p>However, if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> does not have linearly independent columns, then we can use the fact that the column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is perpendicular to the nullspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET"> to find the least squares solution.</p>
<p>To do this, we first find a basis for the column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and a basis for the nullspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET">. Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BP%7D"> be the projection matrix onto the column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, given by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BP%7D%20=%20%5Cmathbf%7BA%7D(%5Cmathbf%7BA%7D%5ET%5Cmathbf%7BA%7D)%5E%7B-1%7D%5Cmathbf%7BA%7D%5ET">. Then the least squares solution to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7Bb%7D"> is given by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cmathbf%7BP%7D%5Cmathbf%7Bb%7D">, and the residual <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D%20=%20%5Cmathbf%7Bb%7D%20-%20%5Cmathbf%7BAx%7D"> is in the nullspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET">.</p>
</section>
</section>
</section>
<section id="orthogonal-subspaces" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2" class="anchored" data-anchor-id="orthogonal-subspaces"><span class="header-section-number">1.1.2</span> Orthogonal Subspaces</h3>
<div id="def-orthogonalSubspaces" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 </strong></span>Two subspaces <img src="https://latex.codecogs.com/png.latex?U"> and <img src="https://latex.codecogs.com/png.latex?V"> of a vector space <img src="https://latex.codecogs.com/png.latex?W"> are said to be orthogonal subspaces if every vector in <img src="https://latex.codecogs.com/png.latex?U"> is orthogonal to every vector in <img src="https://latex.codecogs.com/png.latex?V">. Symbolically, we write <img src="https://latex.codecogs.com/png.latex?U%20%5Cperp%20V"> if and only if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20%5Ccdot%20%5Cmathbf%7Bv%7D%20=%200"> for all <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20%5Cin%20U"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%20%5Cin%20V">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20u%5ET%20%5Cmathbf%20v%20=%200%0A"></p>
</div>
<p>Every vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> in the nullspace is perpendicular to every row of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, because <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx=0%7D">. The <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Bnull%7D(%5Cmathbf%7BA%7D)"> and the row space <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCol%7D(%5Cmathbf%7BA%7D%5ET)"> are orthogonal subspaces of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En"></p>
<p>Every vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D"> in the nullspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET"> is perpendicular to every column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. The left <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Bnull%7D(%5Cmathbf%7BA%7D%5ET)"> and the column space <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCol%7D(%5Cmathbf%7BA%7D)"> are orthogonal subspaces in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="kmink3225.netlify.app/images/linear_algebra/Null(A)_Col(A^T)_orthogonal.PNG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Null(A) <img src="https://latex.codecogs.com/png.latex?%5Cperp"> Col(A^T)-Gilbert Strang: Introduction to Linear Algebra</figcaption><p></p>
</figure>
</div>
<section id="examples-1" class="level4" data-number="1.1.2.1">
<h4 data-number="1.1.2.1" class="anchored" data-anchor-id="examples-1"><span class="header-section-number">1.1.2.1</span> Examples</h4>
<p><strong>Example1</strong> Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%201%20%5C%5C%20-1%20%5Cend%7Bbmatrix%7D"> be two vectors in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3">. Then the subspaces <img src="https://latex.codecogs.com/png.latex?U%20=%20%5Ctext%7Bspan%7D%5C%7B%5Cmathbf%7Bu%7D%5C%7D"> and <img src="https://latex.codecogs.com/png.latex?V%20=%20%5Ctext%7Bspan%7D%5C%7B%5Cmathbf%7Bv%7D%5C%7D"> are orthogonal subspaces, since <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20%5Ccdot%20%5Cmathbf%7Bv%7D%20=%200">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AU%20=%20%5Ctext%7Bspan%7D%5C%7B%5Cmathbf%7Bu%7D%5C%7D%20=%20%5Ctext%7Bspan%7D%5Cleft%5C%7B%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%5Cright%5C%7D,%20%5Cquad%0AV%20=%20%5Ctext%7Bspan%7D%5C%7B%5Cmathbf%7Bv%7D%5C%7D%20=%20%5Ctext%7Bspan%7D%5Cleft%5C%7B%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%201%20%5C%5C%20-1%20%5Cend%7Bbmatrix%7D%5Cright%5C%7D%0A"></p>
<p><strong>Example2</strong> Let <img src="https://latex.codecogs.com/png.latex?U"> be the subspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3"> spanned by the vectors <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%202%20%5C%5C%200%20%5Cend%7Bbmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5C%5C%20-2%20%5Cend%7Bbmatrix%7D">, and let <img src="https://latex.codecogs.com/png.latex?V"> be the subspace spanned by the vector <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%202%20%5C%5C%20-1%20%5C%5C%201%20%5Cend%7Bbmatrix%7D">. Then <img src="https://latex.codecogs.com/png.latex?U"> and <img src="https://latex.codecogs.com/png.latex?V"> are orthogonal subspaces, since every vector in <img src="https://latex.codecogs.com/png.latex?U"> is orthogonal to every vector in <img src="https://latex.codecogs.com/png.latex?V">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AU%20=%20%5Ctext%7Bspan%7D%5Cleft%5C%7B%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%202%20%5C%5C%200%20%5Cend%7Bbmatrix%7D,%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5C%5C%20-2%20%5Cend%7Bbmatrix%7D%5Cright%5C%7D,%20%5Cquad%0AV%20=%20%5Ctext%7Bspan%7D%5Cleft%5C%7B%5Cbegin%7Bbmatrix%7D%202%20%5C%5C%20-1%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%5Cright%5C%7D%0A"></p>
<p><strong>Example3</strong> Let <img src="https://latex.codecogs.com/png.latex?U"> and <img src="https://latex.codecogs.com/png.latex?V"> be the subspaces of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E2"> spanned by the vectors <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%20-1%20%5Cend%7Bbmatrix%7D">, respectively. Then <img src="https://latex.codecogs.com/png.latex?U"> and <img src="https://latex.codecogs.com/png.latex?V"> are orthogonal subspaces, since <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20&amp;%201%20%5Cend%7Bbmatrix%7D%20%5Ccdot%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%20-1%20%5Cend%7Bbmatrix%7D%20=%200">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AU%20=%20%5Ctext%7Bspan%7D%5Cleft%5C%7B%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%5Cright%5C%7D,%20%5Cquad%0AV%20=%20%5Ctext%7Bspan%7D%5Cleft%5C%7B%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%20-1%20%5Cend%7Bbmatrix%7D%5Cright%5C%7D%0A"></p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">u <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb4-2">v <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="dv" style="color: #AD0000;">1</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb4-3"></span>
<span id="cb4-4"><span class="co" style="color: #5E5E5E;"># plot vectors</span></span>
<span id="cb4-5">plt.figure()</span>
<span id="cb4-6">plt.plot([<span class="dv" style="color: #AD0000;">0</span>, u[<span class="dv" style="color: #AD0000;">0</span>]], [<span class="dv" style="color: #AD0000;">0</span>, u[<span class="dv" style="color: #AD0000;">1</span>]], <span class="st" style="color: #20794D;">'b'</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="vs" style="color: #20794D;">r'$\mathbf</span><span class="sc" style="color: #5E5E5E;">{u}</span><span class="vs" style="color: #20794D;">$'</span>)</span>
<span id="cb4-7">plt.plot([<span class="dv" style="color: #AD0000;">0</span>, v[<span class="dv" style="color: #AD0000;">0</span>]], [<span class="dv" style="color: #AD0000;">0</span>, v[<span class="dv" style="color: #AD0000;">1</span>]], <span class="st" style="color: #20794D;">'r'</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="vs" style="color: #20794D;">r'$\mathbf</span><span class="sc" style="color: #5E5E5E;">{v}</span><span class="vs" style="color: #20794D;">$'</span>)</span>
<span id="cb4-8">plt.legend()</span>
<span id="cb4-9"></span>
<span id="cb4-10"><span class="co" style="color: #5E5E5E;"># plot subspaces</span></span>
<span id="cb4-11">plt.axline((<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>), slope<span class="op" style="color: #5E5E5E;">=</span>u[<span class="dv" style="color: #AD0000;">1</span>]<span class="op" style="color: #5E5E5E;">/</span>u[<span class="dv" style="color: #AD0000;">0</span>], color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'b'</span>, linestyle<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'--'</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="vs" style="color: #20794D;">r'$U$'</span>)</span>
<span id="cb4-12">plt.axline((<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>), slope<span class="op" style="color: #5E5E5E;">=</span>v[<span class="dv" style="color: #AD0000;">1</span>]<span class="op" style="color: #5E5E5E;">/</span>v[<span class="dv" style="color: #AD0000;">0</span>], color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'r'</span>, linestyle<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'--'</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="vs" style="color: #20794D;">r'$V$'</span>)</span>
<span id="cb4-13"></span>
<span id="cb4-14">plt.xlim(<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.5</span>, <span class="fl" style="color: #AD0000;">1.5</span>)</span>
<span id="cb4-15">plt.ylim(<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.5</span>, <span class="fl" style="color: #AD0000;">1.5</span>)</span>
<span id="cb4-16">plt.gca().set_aspect(<span class="st" style="color: #20794D;">'equal'</span>, adjustable<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'box'</span>)</span>
<span id="cb4-17">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/06.orthogonality_files/figure-html/cell-6-output-1.png" width="441" height="416"></p>
</div>
</div>
<p>The vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> are in blue and red, respectively, and the subspaces <img src="https://latex.codecogs.com/png.latex?U"> and <img src="https://latex.codecogs.com/png.latex?V"> as dashed lines with corresponding colors. Since <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20%5Ccdot%20%5Cmathbf%7Bv%7D%20=%200">, the subspaces are orthogonal.</p>
</section>
</section>
<section id="orthogonal-complements" class="level3" data-number="1.1.3">
<h3 data-number="1.1.3" class="anchored" data-anchor-id="orthogonal-complements"><span class="header-section-number">1.1.3</span> Orthogonal Complements</h3>
<p>Given a subspace <img src="https://latex.codecogs.com/png.latex?V"> of a vector space <img src="https://latex.codecogs.com/png.latex?W">, we can decompose any vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D%20%5Cin%20W"> into two orthogonal components, one in <img src="https://latex.codecogs.com/png.latex?V"> and one in the orthogonal complement of <img src="https://latex.codecogs.com/png.latex?V">.</p>
<div id="def-orthogonalComplements" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 </strong></span>Let <img src="https://latex.codecogs.com/png.latex?V"> be a subspace of a vector space <img src="https://latex.codecogs.com/png.latex?W">. The orthogonal complement of <img src="https://latex.codecogs.com/png.latex?V">, denoted by <img src="https://latex.codecogs.com/png.latex?V%5E%5Cperp">, is the set of all vectors in <img src="https://latex.codecogs.com/png.latex?W"> that are orthogonal to every vector in <img src="https://latex.codecogs.com/png.latex?V">. That is, the orthogonal complement of <img src="https://latex.codecogs.com/png.latex?V"> is the set of vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20w%20%5Cin%20W"> such that the inner product between <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20w"> and any vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v%20%5Cin%20V"> is equal to zero: <img src="https://latex.codecogs.com/png.latex?%0AV%5E%5Cperp%20=%20%5C%7B%20%5Cmathbf%20w%20%5Cin%20W%20%7C%20%5Clangle%20%5Cmathbf%20w,%5Cmathbf%20v%20%5Crangle%20=%200,%20%5Cforall%20%5Cmathbf%20v%20%5Cin%20V%20%5C%7D.%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?V%5E%5Cperp"> represents the orthogonal complement of the subspace <img src="https://latex.codecogs.com/png.latex?V">, <img src="https://latex.codecogs.com/png.latex?w"> and <img src="https://latex.codecogs.com/png.latex?v"> are vectors in the subspaces <img src="https://latex.codecogs.com/png.latex?W"> and <img src="https://latex.codecogs.com/png.latex?V"> respectively, and <img src="https://latex.codecogs.com/png.latex?%5Clangle%20%5Crangle"> denote the inner product between two vectors.</p>
<p>In other words, The orthogonal complement of a subspace <img src="https://latex.codecogs.com/png.latex?V"> contains every vector that is perpendicular to <img src="https://latex.codecogs.com/png.latex?V">. This orthogonal subspace is denoted by <img src="https://latex.codecogs.com/png.latex?V%5E%5Cperp">.</p>
</div>
<p>We can then decompose any vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D%20%5Cin%20W"> into two orthogonal components as follows: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20w%20=%20%5Cmathbf%20w_%7BV%7D%20+%5Cmathbf%20w_%7BV%5E%7B%5Cperp%7D%7D%20%20%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D_%7BV%7D"> is the orthogonal projection of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> onto <img src="https://latex.codecogs.com/png.latex?V">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D_%7BV%5E%5Cperp%7D"> is the orthogonal projection of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> onto <img src="https://latex.codecogs.com/png.latex?V%5E%5Cperp">.</p>
<p>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v"> is orthogonal to the nullspace, it must be in the row space.</p>
<div id="thm-orthogonalComplement" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 </strong></span>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be a matrix and let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20W%20=%20%5Coperatorname%7BCol%7D(A)">. Then, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20W%5E%7B%5Cperp%7D%20=%20%5Coperatorname%7BNull%7D(%5Cmathbf%20A)">.</p>
</div>
<p>By the proposition, computing the orthogonal complement of a span means solving a system of linear equations.</p>
<section id="example" class="level4" data-number="1.1.3.1">
<h4 data-number="1.1.3.1" class="anchored" data-anchor-id="example"><span class="header-section-number">1.1.3.1</span> Example</h4>
<p>Let <img src="https://latex.codecogs.com/png.latex?W%20=%20%5Cmathbb%7BR%7D%5E3"> and <img src="https://latex.codecogs.com/png.latex?V"> be the subspace spanned by the vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1%20=%20(1,0,0)"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_2%20=%20(0,1,1)">. Then, we can find a basis for <img src="https://latex.codecogs.com/png.latex?V%5E%5Cperp"> by solving the system of equations</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%201%20&amp;%200%20&amp;%200%20%5C%5C%200%20&amp;%201%20&amp;%201%20%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x%20%5C%5C%20y%20%5C%5C%20z%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%200%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>Compute <img src="https://latex.codecogs.com/png.latex?W%5E%7B%5Cperp%7D">, where <img src="https://latex.codecogs.com/png.latex?W=%5Coperatorname%7BSpan%7D%5Cleft%5C%7B%20%5Cbegin%7Bbmatrix%7D1%20%5C%5C%200%20%5C%5C%200%20%5Cend%7Bbmatrix%7D,%5Cbegin%7Bbmatrix%7D0%20%5C%5C%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%20%5Cright%5C%7D"></p>
<p>Compute <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BNull%7D(%5Cbegin%7Bbmatrix%7D%201%20&amp;%200%20&amp;%200%20%5C%5C%200%20&amp;%201%20&amp;%201%20%5Cend%7Bbmatrix%7D)"> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Ax%20&amp;=%200%20%5C%5C%0Ay%20+%20z%20&amp;=%200%5C%5C%0AW%5E%7B%5Cperp%7D&amp;=%5Coperatorname%7B%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%20-z%20%5C%5C%20z%5Cend%7Bbmatrix%7D%7D%0A%5Cend%7Balign*%7D%0A">, which has the unique solution <img src="https://latex.codecogs.com/png.latex?x%20=%200">, <img src="https://latex.codecogs.com/png.latex?y%20=%20-z">. Therefore, the subspace <img src="https://latex.codecogs.com/png.latex?V%5E%5Cperp"> is spanned by the vector <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%20-1%20%5C%5C%201%20%5Cend%7Bbmatrix%7D">.</p>
<p>To see this, note that any vector <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%20x%20%5C%5C%20y%20%5C%5C%20z%20%5Cend%7Bbmatrix%7D"> in <img src="https://latex.codecogs.com/png.latex?V%5E%5Cperp"> must satisfy <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%20x%20%5C%5C%20y%20%5C%5C%20z%20%5Cend%7Bbmatrix%7D%20%5Ccdot%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5C%5C%200%20%5Cend%7Bbmatrix%7D%20=%200"> and <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%20x%20%5C%5C%20y%20%5C%5C%20z%20%5Cend%7Bbmatrix%7D%20%5Ccdot%20%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%20=%200">. These conditions can be rewritten as the equations <img src="https://latex.codecogs.com/png.latex?x%20=%200"> and <img src="https://latex.codecogs.com/png.latex?y%20=%20-z">, respectively. Therefore, any vector in <img src="https://latex.codecogs.com/png.latex?V%5E%5Cperp"> must have the form <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%20-z%20%5C%5C%20z%20%5Cend%7Bbmatrix%7D">, and it is easy to check that <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%20-1%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"> satisfies this equation and is linearly independent from <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_2">, so it is a basis for <img src="https://latex.codecogs.com/png.latex?V%5E%5Cperp">.</p>
<p>Therefore, we have <img src="https://latex.codecogs.com/png.latex?V%5E%5Cperp%20=%20%5Coperatorname%7Bspan%7D%5Cleft(%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%20-1%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%5Cright)">.</p>
<p>Given any vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D%20=%20%5Cbegin%7Bbmatrix%7D%20w_1%20%5C%5C%20w_2%20%5C%5C%20w_3%20%5Cend%7Bbmatrix%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E3">, we can decompose it into two orthogonal components as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20w%20=%20%5Cmathbf%20w_%7BV%7D%20+%5Cmathbf%20w_%7BV%5E%7B%5Cperp%7D%7D%20%20%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D_V"> is the projection of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> onto <img src="https://latex.codecogs.com/png.latex?V">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D_%7BV%5E%5Cperp%7D"> is the projection of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> onto <img src="https://latex.codecogs.com/png.latex?V%5E%5Cperp">.</p>
<p>To compute <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D_V">, note that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> can be written as a linear combination of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_2"> as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbf%7Bw%7D%20&amp;=%20%5Cfrac%7B%5Cmathbf%7Bw%7D%20%5Ccdot%20%5Cmathbf%7Bv%7D_1%7D%7B%7C%7C%5Cmathbf%7Bv%7D_1%7C%7C%5E2%7D%20%5Cmathbf%7Bv%7D_1%20+%20%5Cfrac%7B%5Cmathbf%7Bw%7D%20%5Ccdot%20%5Cmathbf%7Bv%7D_2%7D%7B%7C%7C%5Cmathbf%7Bv%7D_2%7C%7C%5E2%7D%20%5Cmathbf%7Bv%7D_2%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B1%5E2%7D%5Cbegin%7Bbmatrix%7D1%20%5C%5C%200%20%5C%5C%200%20%5Cend%7Bbmatrix%7D+%20%5Cfrac%7B1%7D%7B2%7D%5Cbegin%7Bbmatrix%7D0%20%5C%5C%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%20%5C%5C%0A&amp;=%20%5Cbegin%7Bbmatrix%7D1%20%5C%5C%20%5Cfrac%7B1%7D%7B2%7D%20%5C%5C%20%5Cfrac%7B1%7D%7B2%7D%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p>Therefore, the subspace spanned by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> is the line passing through the point <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D1%20%5C%5C%20%5Cfrac%7B1%7D%7B2%7D%20%5C%5C%20%5Cfrac%7B1%7D%7B2%7D%5Cend%7Bbmatrix%7D"> in the direction of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D">, which is <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bspan%7D%5Cleft%20%5C%7B%5Cbegin%7Bbmatrix%7D1%20%5C%5C%20%5Cfrac%7B1%7D%7B2%7D%20%5C%5C%20%5Cfrac%7B1%7D%7B2%7D%5Cend%7Bbmatrix%7D%5Cright%20%5C%7D">.</p>
<p>The projection of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> onto <img src="https://latex.codecogs.com/png.latex?V"> is given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bw%7D_V%20=%20%5Coperatorname%7Bproj%7D_%7BV%7D(%5Cmathbf%7Bw%7D)%20=%20%5Cfrac%7B%5Clangle%20%5Cmathbf%7Bw%7D,%5Cmathbf%7Bv%7D_1%5Crangle%7D%7B%5C%7C%5Cmathbf%7Bv%7D_1%5C%7C%5E2%7D%20%5Cmathbf%7Bv%7D_1%20+%20%5Cfrac%7B%5Clangle%20%5Cmathbf%7Bw%7D,%5Cmathbf%7Bv%7D_2%5Crangle%7D%7B%5C%7C%5Cmathbf%7Bv%7D_2%5C%7C%5E2%7D%20%5Cmathbf%7Bv%7D_2%20=%20%5Cfrac%7B1%7D%7B1%5E2+0%5E2+0%5E2%7D%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5C%5C%200%5Cend%7Bbmatrix%7D%20=%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5C%5C%200%5Cend%7Bbmatrix%7D%0A"></p>
<p>The projection of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> onto <img src="https://latex.codecogs.com/png.latex?V%5E%5Cperp"> is given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20w%20=%20%5Cmathbf%20w_%7BV%7D%20-%5Cmathbf%20w_%7BV%5E%7B%5Cperp%7D%7D%20%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%202%20%5C%5C%203%20%5Cend%7Bbmatrix%7D%20-%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5C%5C%200%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%202%20%5C%5C%203%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>Therefore, we have decomposed <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> into two orthogonal components as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5C%5C%200%20%5Cend%7Bbmatrix%7D%20+%20%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%202%20%5C%5C%203%20%5Cend%7Bbmatrix%7D">.</p>
<div id="thm-fundamentralThm" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 </strong></span>The Fundamental Theorem.<br>
Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be an <img src="https://latex.codecogs.com/png.latex?m%5Ctimes%20n"> matrix over <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D">. Then,</p>
<ul>
<li>The column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, denoted <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCol%7D(%5Cmathbf%20A)">, is a subspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Em">.</li>
<li>The null space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, denoted <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BNull%7D(%5Cmathbf%20A)">, is a subspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En">.</li>
<li>The orthogonal complement of <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCol%7D(%5Cmathbf%20A)">, denoted <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCol%7D(%5Cmathbf%20A)%5E%5Cperp">, is equal to <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BNull%7D(%5Cmathbf%20A)">.</li>
<li>The orthogonal complement of <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BNull%7D(%5Cmathbf%20A)">, denoted <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BNull%7D(%5Cmathbf%20A)%5E%5Cperp">, is equal to <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCol%7D(%5Cmathbf%20A)">.</li>
</ul>
<p>In other words, we have the following orthogonal decomposition of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%20R%5En%20=%20N(%5Cmathbf%20A)%20%5Coplus%20C(%5Cmathbf%20A)%5E%5Cperp%0A"></p>
<p>and the following orthogonal decomposition of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Em">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%20R%5Em%20=%20C(%5Cmathbf%20A)%20%5Coplus%20N(%5Cmathbf%20A)%5E%5Cperp%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Coplus"> denotes the direct sum of subspaces.</p>
</div>
<p>The Fundamental Theorem states that for a given matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, the column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> and the null space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> are orthogonal complements of each other. In other words, every vector in the null space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is orthogonal to every vector in the column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, and vice versa. This means that any vector in the domain of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> can be uniquely decomposed as the sum of a vector in the column space and a vector in the null space.</p>
<p>The point of <strong>complements</strong> is that every <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x"> can be split into a row space component <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x%5Er"> and a nullspace component <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x%5En">. When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> multiplies <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x%20=%20%5Cmathbf%20x%5Er%20+%20%5Cmathbf%20x%5En">, Figure 4.3 shows what happens:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="kmink3225.netlify.app/images/linear_algebra/nullspace_complement.PNG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Null Space Complement-Gilbert Strang: Introduction to Linear Algebra</figcaption><p></p>
</figure>
</div>
<ul>
<li>The nullspace component goes to zero: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D_n%20=%20%5Cmathbf%7B0%7D">.</li>
<li>The row space component goes to the column space: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D_r%20=%20%5Cmathbf%7BAx%7D"> Ax r = Ax.</li>
<li>Every vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20b"> in the column space comes from one and only one vector in the row space.</li>
<li>pseudoinverse: there is an <img src="https://latex.codecogs.com/png.latex?r"> by <img src="https://latex.codecogs.com/png.latex?r"> invertible matrix hiding inside <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, if we throwaway the two nullspaces. From the row space to the column space, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is invertible.</li>
</ul>
</section>
<section id="exmaple" class="level4" data-number="1.1.3.2">
<h4 data-number="1.1.3.2" class="anchored" data-anchor-id="exmaple"><span class="header-section-number">1.1.3.2</span> Exmaple</h4>
<p><strong>Example1</strong> Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix with rank <img src="https://latex.codecogs.com/png.latex?r">. Then, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En"> can be decomposed as <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En%20=%20N(%5Cmathbf%20A)%20%5Coplus%20N(%5Cmathbf%20A)%5E%7B%5Cperp%7D">, where <img src="https://latex.codecogs.com/png.latex?N(%5Cmathbf%20A)"> is the null space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, <img src="https://latex.codecogs.com/png.latex?N(%5Cmathbf%20A)%5E%7B%5Cperp%7D"> is its orthogonal complement, and <img src="https://latex.codecogs.com/png.latex?%5Coplus"> denotes the direct sum. This means that any vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En"> can be written uniquely as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%20=%20%5Cmathbf%7Bv%7D_1%20+%20%5Cmathbf%7Bv%7D_2">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1%20%5Cin%20N(%5Cmathbf%20A)"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_2%20%5Cin%20N(%5Cmathbf%20A)%5E%7B%5Cperp%7D">.</p>
<p><strong>Example2</strong> Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix with rank <img src="https://latex.codecogs.com/png.latex?r">. Then, the column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, denoted <img src="https://latex.codecogs.com/png.latex?C(%5Cmathbf%20A)">, is equal to the orthogonal complement of the null space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5ET">, i.e., <img src="https://latex.codecogs.com/png.latex?C(%5Cmathbf%20A)%20=%20N(%5Cmathbf%20A%5ET)%5E%7B%5Cperp%7D">.</p>
<p><strong>Example3</strong> Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix with rank <img src="https://latex.codecogs.com/png.latex?r">, and let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D%20%5Cin%20%5Cmathbb%7BR%7D%5Em"> be a vector. Then, the system of linear equations <img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D%20=%20%5Cmathbf%7Bb%7D"> has a solution if and only if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D%20%5Cin%20C(%5Cmathbf%20A)">. Moreover, if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_0"> is a particular solution to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5Cmathbf%7Bx%7D%20=%20%5Cmathbf%7Bb%7D">, then the set of all solutions is given by <img src="https://latex.codecogs.com/png.latex?%7B%5Cmathbf%7Bx%7D_0%20+%20%5Cmathbf%7Bv%7D%20:%20%5Cmathbf%7Bv%7D%20%5Cin%20N(%5Cmathbf%20A)%7D">, i.e., it is the affine space consisting of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_0"> plus the null space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">.</p>
<p><strong>Example4</strong> Every diagonal matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20D"> has a diagonal submatrix consisting of its first <img src="https://latex.codecogs.com/png.latex?r"> diagonal entries that is <img src="https://latex.codecogs.com/png.latex?r%20%5Ctimes%20r"> and invertible for any <img src="https://latex.codecogs.com/png.latex?r"> between <img src="https://latex.codecogs.com/png.latex?1"> and the size of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20D">. For example, consider the diagonal matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20D%20=%20%5Cbegin%7Bbmatrix%7D%202%20&amp;%200%20&amp;%200%20%5C%5C%200%20&amp;%203%20&amp;%200%20%5C%5C%200%20&amp;%200%20&amp;%204%20%5Cend%7Bbmatrix%7D">. The <img src="https://latex.codecogs.com/png.latex?2%20%5Ctimes%202"> diagonal submatrix consisting of the first two diagonal entries, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20D'%20=%20%5Cbegin%7Bbmatrix%7D%202%20&amp;%200%20%5C%5C%200%20&amp;%203%20%5Cend%7Bbmatrix%7D">, is invertible since its diagonal entries are nonzero. Similarly, the <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203"> diagonal submatrix consisting of all the diagonal entries, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20D''%20=%20%5Cbegin%7Bbmatrix%7D%202%20&amp;%200%20&amp;%200%20%5C%5C%200%20&amp;%203%20&amp;%200%20%5C%5C%200%20&amp;%200%20&amp;%204%20%5Cend%7Bbmatrix%7D">, is also invertible since all of its diagonal entries are nonzero. This example illustrates the fact that every diagonal matrix has an invertible diagonal submatrix of any size between <img src="https://latex.codecogs.com/png.latex?1"> and the size of the matrix.</p>
<p><strong>Example5</strong> Consider the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A=%5Cbegin%7Bbmatrix%7D1%20&amp;%202%20&amp;%203%20%5C%5C%204%20&amp;%205%20&amp;%206%20%5Cend%7Bbmatrix%7D">. We want to find the right bases for <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E2"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> becomes a diagonal matrix.</p>
<p>We begin by computing <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5ET%5Cmathbf%20A">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A%5ET%20%5Cmathbf%20A%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%204%20%5C%5C%202%20&amp;%205%20%5C%5C%203%20&amp;%206%20%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20&amp;%203%20%5C%5C%204%20&amp;%205%20&amp;%206%20%5Cend%7Bbmatrix%7D%20%20=%20%5Cbegin%7Bbmatrix%7D%2017%20&amp;%2022%20&amp;%2027%20%5C%5C%2022%20&amp;%2029%20&amp;%2036%20%5C%5C%2027%20&amp;%2036%20&amp;%2045%5Cend%7Bbmatrix%7D%0A"></p>
<p>The eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5ET%5Cmathbf%20A"> are <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%200">, <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%201">, and <img src="https://latex.codecogs.com/png.latex?%5Clambda_3%20=%2090">. We can find the corresponding eigenvectors as follows:</p>
<ul>
<li>For <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%200">, we solve <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%20A%5ET%5Cmathbf%20A%20-%20%5Clambda_1%5Cmathbf%20I)%5Cmathbf%20v%20=%200">, which gives us the equation <img src="https://latex.codecogs.com/png.latex?17x%20+%2022y%20+%2027z%20=%200">. One possible eigenvector is <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%20-2%20%5C%5C%201%20%5C%5C%200%20%5Cend%7Bbmatrix%7D">.</li>
<li>For <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%201">, we solve <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%20A%5ET%5Cmathbf%20A%20-%20%5Clambda_2%5Cmathbf%20I)%5Cmathbf%20v%20=%200">, which gives us the equation <img src="https://latex.codecogs.com/png.latex?16x%20+%2020y%20+%2024z%20=%200">. One possible eigenvector is <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%203%20%5C%5C%20-2%20%5C%5C%200%20%5Cend%7Bbmatrix%7D">.</li>
<li>For <img src="https://latex.codecogs.com/png.latex?%5Clambda_3%20=%2090">, we solve <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%20A%5ET%5Cmathbf%20A%20-%20%5Clambda_3%20%5Cmathbf%20I)%5Cmathbf%20v%20=%200">, which gives us the equation <img src="https://latex.codecogs.com/png.latex?-2x%20+%20y%20+%20z%20=%200">. One possible eigenvector is <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%202%20%5C%5C%20-1%20%5Cend%7Bbmatrix%7D">.</li>
</ul>
<p>We normalize these eigenvectors to obtain an orthonormal basis for <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20v_1%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B5%7D%7D%5Cbegin%7Bbmatrix%7D%20-2%20%5C%5C%201%20%5C%5C%200%20%5Cend%7Bbmatrix%7D,%20%5Cquad%20%5Cmathbf%20v_2%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B13%7D%7D%5Cbegin%7Bbmatrix%7D%203%20%5C%5C%20-2%20%5C%5C%200%20%5Cend%7Bbmatrix%7D,%20%5Cquad%20%5Cmathbf%20v_3%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B6%7D%7D%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%202%20%5C%5C%20-1%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>Next, we compute <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAv%7D_i"> for each <img src="https://latex.codecogs.com/png.latex?i=1,2,3">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BAv%7D_1%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20&amp;%203%20%5C%5C%204%20&amp;%205%20&amp;%206%20%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D%20%5Cfrac%7B-2%7D%7B%5Csqrt%7B5%7D%7D%20%5C%5C%20%5Cfrac%7B1%7D%7B%5Csqrt%7B5%7D%7D%20%5C%5C%200%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cfrac%7B-2%7D%7B%5Csqrt%7B5%7D%7D%20%5C%5C%20%5Cfrac%7B8%7D%7B%5Csqrt%7B5%7D%7D%20%5Cend%7Bbmatrix%7D%20=%20%5Cfrac%7B2%7D%7B%5Csqrt%7B5%7D%7D%5Cbegin%7Bbmatrix%7D%20-1%20%5C%5C%202%20%5Cend%7Bbmatrix%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BAv%7D_2%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20&amp;%203%20%5C%5C%204%20&amp;%205%20&amp;%206%20%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D%20-0.6931%20%5C%5C%20-0.1184%20%5C%5C%200.7107%20%5Cend%7Bbmatrix%7D%20%5Capprox%20%5Cbegin%7Bbmatrix%7D%20-3.1623%20%5C%5C%20-7.4162%20%5Cend%7Bbmatrix%7D%20%5Capprox%20-3.1623v_1,%0A"></p>
<p>and</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BAv%7D_3%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20&amp;%203%20%5C%5C%204%20&amp;%205%20&amp;%206%20%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D%20-0.6931%20%5C%5C%200.6646%20%5C%5C%20-0.2774%20%5Cend%7Bbmatrix%7D%20%5Capprox%20%5Cbegin%7Bbmatrix%7D%20-4.7246%20%5C%5C%204.6707%20%5Cend%7Bbmatrix%7D%20%5Capprox%204.6707v_1,%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1=%5Cbegin%7Bbmatrix%7D%200.2673%20%5C%5C%200.5345%20%5C%5C%200.8018%20%5Cend%7Bbmatrix%7D">.</p>
<p>Therefore, we can take <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1"> as the first column of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D">, and the normalized eigenvectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v_2"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v_3"> as the second and third columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20V">, respectively. Then we can define <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BU=AV%5CSigma%7D%5E%7B-1%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20%5CSigma"> is the diagonal matrix with the square roots of the nonzero eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5ET%5Cmathbf%20A"> as its entries.</p>
<p>&lt; 여기서 부터 다시 볼것&gt; Thus, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BU%5CSigma%20V%7D%5ET%20=%20%5Cbegin%7Bbmatrix%7D%20-0.231%20%5C%20%5C%20%5C%200.9730%20%5C%5C%20-0.5253%20%5C%20%5C%20%5C%200.0806%20%5C%5C%20-0.8196%20-0.9195%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%209.4868%20&amp;%200%20&amp;%200%20%5C%5C%200%20&amp;%201%20&amp;%200%20%5C%5C%200%20&amp;%200%20&amp;%200%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20-0.2673%20%5C%20%5C%20%5C%200.5345%20%5C%20%5C%20%5C%200.8018%20%5C%5C%20-0.6931%20%5C%20%5C%20%5C%20-0.1184%20%5C%20%5C%20%5C%200.7107%20%5C%5C%20-0.6646%20%5C%20%5C%20%5C%200.7774%20%5C%20%5C%20%5C%20-0.2774%20%5Cend%7Bbmatrix%7D.%0A"> This gives us the diagonalization <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A=%5Cmathbf%7BQDQ%7D%5E%7B-1%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D=%5Cmathbf%7BU%5CSigma%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BD%7D=%5Cmathbf%7BV%7D%5ET">. Therefore, by choosing the appropriate bases for <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E2"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3"> given by the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q">, we can make <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> a diagonal matrix.</p>
<p>&lt; 여기까지&gt;</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A%20=%20%5Cmathbf%7BU%5CSigma%20V%7D%5ET%20=%20%5Cbegin%7Bbmatrix%7D%20-0.231%20&amp;%200.973%20&amp;%200%20%5C%5C%200.732%20&amp;%200.182%20&amp;%20-0.655%20%5C%5C%200.641%20&amp;%200.136%20&amp;%200.755%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%203.89%20&amp;%200%20&amp;%200%20%5C%5C%200%20&amp;%201.27%20&amp;%200%20%5C%5C%200%20&amp;%200%20&amp;%200.43%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20-0.227%20&amp;%20-0.592%20&amp;%20-0.773%20%5C%5C%20-0.904%20&amp;%200.275%20&amp;%200.329%20%5C%5C%200.361%20&amp;%200.758%20&amp;%20-0.541%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>This is known as the Singular Value Decomposition (SVD) of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">. The diagonal matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20%5CSigma"> contains the singular values of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, which are the square roots of the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5ET%5Cmathbf%20A">. These values represent the importance of the corresponding singular vectors in the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">.</p>
<p>The SVD can be used for a variety of applications, including data compression, dimensionality reduction, and image processing. It is also used in machine learning and data science for tasks such as collaborative filtering, recommender systems, and principal component analysis.</p>
</section>
</section>
<section id="combining-bases-from-subspaces" class="level3" data-number="1.1.4">
<h3 data-number="1.1.4" class="anchored" data-anchor-id="combining-bases-from-subspaces"><span class="header-section-number">1.1.4</span> Combining bases from Subspaces</h3>
<p><strong>Any <img src="https://latex.codecogs.com/png.latex?n"> independent vectors in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En"> must span <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">. So, they are a basis.</strong></p>
<p>In <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">, a set of <img src="https://latex.codecogs.com/png.latex?n"> independent vectors is said to span <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En"> if any vector in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En"> can be expressed as a linear combination of these <img src="https://latex.codecogs.com/png.latex?n"> vectors. This means that the <img src="https://latex.codecogs.com/png.latex?n"> vectors are sufficient to represent any vector in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">.</p>
<p>To see why this is the case, consider that any vector in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En"> can be represented as a column vector with <img src="https://latex.codecogs.com/png.latex?n"> entries. By definition, each entry can be written as a linear combination of the entries of the <img src="https://latex.codecogs.com/png.latex?n"> independent vectors. Therefore, the entire column vector can be expressed as a linear combination of the <img src="https://latex.codecogs.com/png.latex?n"> independent vectors. Since this is true for any vector in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">, the set of <img src="https://latex.codecogs.com/png.latex?n"> independent vectors must span <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">.</p>
<p>Moreover, if a set of <img src="https://latex.codecogs.com/png.latex?n"> independent vectors spans <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">, then they are a basis for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">. This means that the <img src="https://latex.codecogs.com/png.latex?n"> vectors are linearly independent and also span <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">. By definition, a basis is a set of vectors that can be used to represent any vector in a space and that is linearly independent. So, any set of <img src="https://latex.codecogs.com/png.latex?n"> independent vectors that spans <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En"> is a basis for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">.</p>
<p><strong>Any <img src="https://latex.codecogs.com/png.latex?n"> vectors that span <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En"> must be independent. So, they are a basis.</strong></p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%7Bv_1,v_2,%5Cdots,v_n%7D"> be a set of <img src="https://latex.codecogs.com/png.latex?n"> vectors that span <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">. This means that any vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x"> in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En"> can be expressed as a linear combination of the vectors in <img src="https://latex.codecogs.com/png.latex?%7Bv_1,v_2,%5Cdots,v_n%7D">, i.e., there exist scalars <img src="https://latex.codecogs.com/png.latex?a_1,a_2,%5Cdots,a_n"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x%20=%20a_1v_1+a_2v_2+%5Ccdots+a_nv_n">.</p>
<p>Now suppose that the vectors in <img src="https://latex.codecogs.com/png.latex?%7Bv_1,v_2,%5Cdots,v_n%7D"> are not independent. Then there exist scalars <img src="https://latex.codecogs.com/png.latex?b_1,b_2,%5Cdots,b_n">, not all zero, such that <img src="https://latex.codecogs.com/png.latex?b_1v_1+b_2v_2+%5Ccdots+b_nv_n=%5Cmathbf%200">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%200"> denotes the zero vector in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">.</p>
<p>We can rewrite this equation as <img src="https://latex.codecogs.com/png.latex?a_1v_1+a_2v_2+%5Ccdots+a_nv_n=%5Cmathbf%200">, where <img src="https://latex.codecogs.com/png.latex?a_i=-b_i"> for <img src="https://latex.codecogs.com/png.latex?i=1,2,%5Cdots,n">. But this implies that the vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x=%5Cmathbf%200"> can be expressed as a nontrivial linear combination of the vectors in <img src="https://latex.codecogs.com/png.latex?%7Bv_1,v_2,%5Cdots,v_n%7D">, which contradicts the assumption that these vectors span <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">.</p>
<p>Therefore, the vectors <img src="https://latex.codecogs.com/png.latex?%7Bv_1,v_2,%5Cdots,v_n%7D"> must be independent. Since they span <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">, they form a basis for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">.</p>
<p><strong>If the <img src="https://latex.codecogs.com/png.latex?n"> columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> are independent, they span <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">. So Ax=b is solvable</strong></p>
<p>If the <img src="https://latex.codecogs.com/png.latex?n"> columns of a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> are independent, then they span <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">, which means that any vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20b"> in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En"> can be expressed as a linear combination of the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">.</p>
<p>Suppose we have a system of linear equations <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7Bb%7D">. If the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> are independent, then we can find a unique linear combination of the columns that equals <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20b">. In other words, we can solve the system of equations for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x">. This means that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7Bb%7D"> is solvable for any vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20b"> in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">.</p>
<p>Therefore, if the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> are independent, the equation <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7Bb%7D"> is solvable for any <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20b%20%5Cin%20%5Cmathbf%20R%5En">, and the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> form a basis for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">.</p>
<p><strong>If the <img src="https://latex.codecogs.com/png.latex?n"> columns span <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">, they are independent. So <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx=b%7D"> has only one solution.</strong></p>
<p>If the <img src="https://latex.codecogs.com/png.latex?n"> columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> span <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">, it means that any vector in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En"> can be expressed as a linear combination of those columns. Mathematically, if we denote the <img src="https://latex.codecogs.com/png.latex?n"> columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20a_1,%20%5Cmathbf%20a_2,%20%5Cdots,%20%5Cmathbf%20a_n">, then for any vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20b%20%5Cin%20%5Cmathbf%20R%5En">, there exist scalars <img src="https://latex.codecogs.com/png.latex?x_1,%20x_2,%20%5Cdots,%20x_n"> such that:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bb%7D%20=%20x_1%20%5Cmathbf%7Ba%7D_1%20+%20x_2%20%5Cmathbf%7Ba%7D_2%20+%20%5Ccdots%20+%20x_n%20%5Cmathbf%7Ba%7D_n%0A"></p>
<p>Now, let’s assume that the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> are not independent. This means that there exist scalars <img src="https://latex.codecogs.com/png.latex?x_1,%20x_2,%20%5Cdots,%20x_n">, not all zero, such that:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_1%20%5Cmathbf%7Ba%7D_1%20+%20x_2%20%5Cmathbf%7Ba%7D_2%20+%20%5Ccdots%20+%20x_n%20%5Cmathbf%7Ba%7D_n%20=%20%5Cmathbf%7B0%7D%0A"></p>
<p>This implies that the homogeneous system <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D=%5Cmathbf%200"> has a nontrivial solution, since we can choose <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%20x_2%20%5C%20%5Cvdots%20%5C%20x_n%20%5Cend%7Bbmatrix%7D%20%5Cneq%20%5Cmathbf%200"> as a solution.</p>
<p>However, this contradicts the assumption that the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> span <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">. If there exists a nontrivial solution <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x"> to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D=%5Cmathbf%200">, it means that the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> do not span the entire <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En"> space, because they are not able to generate the zero vector. Therefore, the assumption that the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> are not independent leads to a contradiction.</p>
<p>Hence, we conclude that the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> must be independent if they span <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R%5En">. This also implies that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is invertible, since the equation <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D=%5Cmathbf%20b"> has a unique solution for any <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20b%20%5Cin%20%5Cmathbf%20R%5En">.</p>
</section>
<section id="example-1" class="level3" data-number="1.1.5">
<h3 data-number="1.1.5" class="anchored" data-anchor-id="example-1"><span class="header-section-number">1.1.5</span> Example</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20v_1=%5Cbegin%7Bbmatrix%7D%201%20%5C%5C0%5C%5C1%20%5Cend%7Bbmatrix%7D%20%5Cquad%20%5Cmathbf%20v_2=%5Cbegin%7Bbmatrix%7D%200%20%5C%5C1%5C%5C1%20%5Cend%7Bbmatrix%7D%20%5Cquad%20%20%5Cmathbf%20v_3=%5Cbegin%7Bbmatrix%7D%201%20%5C%5C1%5C%5C0%20%5Cend%7Bbmatrix%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Aq_1%20&amp;=%20%5Cfrac%7Bv_1%7D%7B%7Cv_1%7C%7D%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5C%5C%201%20%5Cend%7Bbmatrix%7D,%20%5C%5C%0Au_2%20&amp;=%20v_2%20-%20%5Clangle%20v_2,%20q_1%20%5Crangle%20q_1%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%20-%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20%5C%5C%201%20%5C%5C%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20%5Cend%7Bbmatrix%7D,%20%5C%5C%0Aq_2%20&amp;=%20%5Cfrac%7Bu_2%7D%7B%7Cu_2%7C%7D%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20%5Cbegin%7Bbmatrix%7D%20-1%20%5C%5C%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D,%20%5C%5C%0Au_3%20&amp;=%20v_3%20-%20%5Clangle%20v_3,%20q_1%20%5Crangle%20q_1%20-%20%5Clangle%20v_3,%20q_2%20%5Crangle%20q_2%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5C%5C%200%20%5Cend%7Bbmatrix%7D%20-%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%20-%20%5Cfrac%7B1%7D%7B%5Csqrt%7B6%7D%7D%5Cbegin%7Bbmatrix%7D%20-1%20%5C%5C%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%20%5C%5C%20%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%20%5C%20-%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%20%5Cend%7Bbmatrix%7D,%20%5C%5C%0Aq_3%20&amp;=%20%5Cfrac%7Bu_3%7D%7B%7Cu_3%7C%7D%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5C%5C%20-1%20%5Cend%7Bbmatrix%7D.%0A%5Cend%7Balign*%7D%0A"></p>
<p>Therefore, the orthogonal matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BQ%7D=%0A%5Cbegin%7Bbmatrix%7D%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20&amp;%20-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20&amp;%200%5C%5C%0A0%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B6%7D%7D%5C%5C%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20&amp;%20-%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="orthogonal-matrices" class="level3" data-number="1.1.6">
<h3 data-number="1.1.6" class="anchored" data-anchor-id="orthogonal-matrices"><span class="header-section-number">1.1.6</span> Orthogonal Matrices</h3>
<div id="def-orthogonalMatrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 </strong></span>An <img src="https://latex.codecogs.com/png.latex?n%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D"> is orthogonal if its columns <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20q"> form an orthonormal set. That is, the columns <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20q"> of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D"> satisfy</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clangle%5Cmathbf%7Bq%7D_i,%5Cmathbf%7Bq%7D_j%5Crangle%20=%0A%5Cbegin%7Bcases%7D%0A0%20%5Ctext%7B%20if%20%7D%20i%20%5Cne%20j%20%5C%5C%0A1%20%5Ctext%7B%20if%20%7D%20i%20=%20j%0A%5Cend%7Bcases%7D%0A"></p>
<p>We can organize all of the dot products amongst all pairs of columns by premultiplying the matrix by its transpose. Since matrix multiplication is defined as dot products between all rows of the left matrix with all columns of the right matrix,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BQ%7D%5Cmathbf%7BQ%7D%5ET=%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7BQ%7D=%5Cmathbf%7BI%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D"> is the <img src="https://latex.codecogs.com/png.latex?n%5Ctimes%20n"> identity matrix.</p>
</div>
</section>
<section id="properties-1" class="level3" data-number="1.1.7">
<h3 data-number="1.1.7" class="anchored" data-anchor-id="properties-1"><span class="header-section-number">1.1.7</span> Properties</h3>
<ul>
<li>Orthogonal columns: all columns are pair-wise orthogonal</li>
<li>Unit-norm columns: the norm (geometric length) of each column is exactly 1.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7BQ%7D=%5Cmathbf%7BQ%7D%5Cmathbf%7BQ%7D%5ET=%5Cmathbf%7BI%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D"> is the identity matrix of appropriate size.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D%5ET=%5Cmathbf%7BQ%7D%5E%7B-1%7D">
<ul>
<li>Great propoerty because the matrix inverse is tedious and prone to numerical inaccuracies, whereas the matrix transpose is fast and accurate.</li>
</ul></li>
<li>The determinant of an orthogonal matrix is either <img src="https://latex.codecogs.com/png.latex?1"> or <img src="https://latex.codecogs.com/png.latex?-1">.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D"> is orthogonal, then its columns form an orthonormal set, i.e., the columns are pairwise orthogonal and each column has unit length.</li>
<li>Orthogonal matrices preserve lengths and angles. If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D"> are two vectors, then <img src="https://latex.codecogs.com/png.latex?%7C%7C%5Cmathbf%7BQx%7D%7C%7C=%7C%7C%5Cmathbf%7Bx%7D%7C%7C"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%5Cmathbf%7By%7D=(%5Cmathbf%7BQx%7D)%5ET(%5Cmathbf%7BQy%7D)">.</li>
</ul>
</section>
<section id="example-2" class="level3" data-number="1.1.8">
<h3 data-number="1.1.8" class="anchored" data-anchor-id="example-2"><span class="header-section-number">1.1.8</span> Example</h3>
<p><strong>Example1</strong> Orthogonal matrices include rotation matrices and reflection matrices.</p>
<p>the <img src="https://latex.codecogs.com/png.latex?2%5Ctimes%202"> matrix and the <img src="https://latex.codecogs.com/png.latex?3%5Ctimes%203"> matrix: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%5Ccos%20%5Ctheta%20&amp;%20-%5Csin%20%5Ctheta%20%5C%5C%0A%5Csin%20%5Ctheta%20&amp;%20%5Ccos%20%5Ctheta%0A%5Cend%7Bbmatrix%7D%0A%5Cquad%0A%5Cbegin%7Bbmatrix%7D%0A%5Ccos%20%5Ctheta%20&amp;%20-%5Csin%20%5Ctheta%20&amp;%200%20%5C%5C%0A%5Csin%20%5Ctheta%20&amp;%20%5Ccos%20%5Ctheta%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%201%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>is an orthogonal matrix that rotates a vector counterclockwise by an angle <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> regardless of the rotation angle (as long as the same rotation angle is used in all matrix elements).</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;"># Pure rotation matrix</span></span>
<span id="cb5-2"></span>
<span id="cb5-3"><span class="co" style="color: #5E5E5E;"># angle to rotate by</span></span>
<span id="cb5-4">th <span class="op" style="color: #5E5E5E;">=</span> np.pi<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">5</span></span>
<span id="cb5-5"></span>
<span id="cb5-6"><span class="co" style="color: #5E5E5E;"># transformation matrix</span></span>
<span id="cb5-7">T <span class="op" style="color: #5E5E5E;">=</span> np.array([ </span>
<span id="cb5-8">              [ np.cos(th),np.sin(th)],</span>
<span id="cb5-9">              [<span class="op" style="color: #5E5E5E;">-</span>np.sin(th),np.cos(th)]</span>
<span id="cb5-10">            ])</span>
<span id="cb5-11"></span>
<span id="cb5-12"></span>
<span id="cb5-13"><span class="co" style="color: #5E5E5E;"># original dots are a vertical line</span></span>
<span id="cb5-14">x <span class="op" style="color: #5E5E5E;">=</span> np.linspace(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">20</span>)</span>
<span id="cb5-15">origPoints <span class="op" style="color: #5E5E5E;">=</span> np.vstack( (np.zeros(x.shape),x) )</span>
<span id="cb5-16"></span>
<span id="cb5-17"></span>
<span id="cb5-18"><span class="co" style="color: #5E5E5E;"># apply the transformation</span></span>
<span id="cb5-19">transformedPoints <span class="op" style="color: #5E5E5E;">=</span> T <span class="op" style="color: #5E5E5E;">@</span> origPoints</span>
<span id="cb5-20"></span>
<span id="cb5-21"></span>
<span id="cb5-22"><span class="co" style="color: #5E5E5E;"># plot the points</span></span>
<span id="cb5-23">plt.figure(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">6</span>,<span class="dv" style="color: #AD0000;">6</span>))</span>
<span id="cb5-24">plt.plot(origPoints[<span class="dv" style="color: #AD0000;">0</span>,:],origPoints[<span class="dv" style="color: #AD0000;">1</span>,:],<span class="st" style="color: #20794D;">'ko'</span>,label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'Original'</span>)</span>
<span id="cb5-25">plt.plot(transformedPoints[<span class="dv" style="color: #AD0000;">0</span>,:],transformedPoints[<span class="dv" style="color: #AD0000;">1</span>,:],<span class="st" style="color: #20794D;">'s'</span>,color<span class="op" style="color: #5E5E5E;">=</span>[<span class="fl" style="color: #AD0000;">.7</span>,<span class="fl" style="color: #AD0000;">.7</span>,<span class="fl" style="color: #AD0000;">.7</span>],label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'Transformed'</span>)</span>
<span id="cb5-26"></span>
<span id="cb5-27">plt.axis(<span class="st" style="color: #20794D;">'square'</span>)</span>
<span id="cb5-28">plt.xlim([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.2</span>,<span class="fl" style="color: #AD0000;">1.2</span>])</span>
<span id="cb5-29">plt.ylim([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.2</span>,<span class="fl" style="color: #AD0000;">1.2</span>])</span>
<span id="cb5-30">plt.legend()</span>
<span id="cb5-31">plt.title(<span class="ss" style="color: #20794D;">f'Rotation by </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>rad2deg(th)<span class="sc" style="color: #5E5E5E;">:.0f}</span><span class="ss" style="color: #20794D;"> degrees.'</span>)</span>
<span id="cb5-32">plt.show()</span>
<span id="cb5-33"></span>
<span id="cb5-34"><span class="co" style="color: #5E5E5E;"># Animating transformations</span></span>
<span id="cb5-35"><span class="co" style="color: #5E5E5E;"># function to update the axis on each iteration</span></span>
<span id="cb5-36"><span class="kw" style="color: #003B4F;">def</span> aframe(ph):</span>
<span id="cb5-37"></span>
<span id="cb5-38">  <span class="co" style="color: #5E5E5E;"># create the transformation matrix</span></span>
<span id="cb5-39">  T <span class="op" style="color: #5E5E5E;">=</span> np.array([</span>
<span id="cb5-40">                 [  <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">-</span>ph ],</span>
<span id="cb5-41">                 [  <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>    ]</span>
<span id="cb5-42">                ])</span>
<span id="cb5-43">  </span>
<span id="cb5-44">  <span class="co" style="color: #5E5E5E;"># apply the transformation to the points using matrix multiplication</span></span>
<span id="cb5-45">  P <span class="op" style="color: #5E5E5E;">=</span> T<span class="op" style="color: #5E5E5E;">@</span>points</span>
<span id="cb5-46"></span>
<span id="cb5-47">  <span class="co" style="color: #5E5E5E;"># update the dots</span></span>
<span id="cb5-48">  plth.set_xdata(P[<span class="dv" style="color: #AD0000;">0</span>,:])</span>
<span id="cb5-49">  plth.set_ydata(P[<span class="dv" style="color: #AD0000;">1</span>,:])</span>
<span id="cb5-50"></span>
<span id="cb5-51">  <span class="co" style="color: #5E5E5E;"># export the plot handles</span></span>
<span id="cb5-52">  <span class="cf" style="color: #003B4F;">return</span> plth</span>
<span id="cb5-53"></span>
<span id="cb5-54"></span>
<span id="cb5-55"><span class="co" style="color: #5E5E5E;"># define XY points</span></span>
<span id="cb5-56">theta  <span class="op" style="color: #5E5E5E;">=</span> np.linspace(<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">*</span>np.pi,<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb5-57">points <span class="op" style="color: #5E5E5E;">=</span> np.vstack((np.sin(theta),np.cos(theta)))</span>
<span id="cb5-58"></span>
<span id="cb5-59"></span>
<span id="cb5-60"><span class="co" style="color: #5E5E5E;"># setup figure</span></span>
<span id="cb5-61">fig,ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;">1</span>,figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">12</span>,<span class="dv" style="color: #AD0000;">6</span>))</span>
<span id="cb5-62">plth,  <span class="op" style="color: #5E5E5E;">=</span> ax.plot(np.cos(x),np.sin(x),<span class="st" style="color: #20794D;">'ko'</span>)</span>
<span id="cb5-63">ax.set_aspect(<span class="st" style="color: #20794D;">'equal'</span>)</span>
<span id="cb5-64">ax.set_xlim([<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">2</span>])</span>
<span id="cb5-65">ax.set_ylim([<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">2</span>])</span>
<span id="cb5-66"></span>
<span id="cb5-67"><span class="co" style="color: #5E5E5E;"># define values for transformation (note: clip off the final point for a smooth animation loop)</span></span>
<span id="cb5-68">phi <span class="op" style="color: #5E5E5E;">=</span> np.linspace(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">40</span>,<span class="dv" style="color: #AD0000;">40</span>)<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb5-69"></span>
<span id="cb5-70"><span class="co" style="color: #5E5E5E;"># run animation!</span></span>
<span id="cb5-71">animation.FuncAnimation(fig, aframe, phi, interval<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>, repeat<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/06.orthogonality_files/figure-html/cell-7-output-1.png" width="504" height="505"></p>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>&lt;matplotlib.animation.FuncAnimation at 0x1b470bc8ac0&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/06.orthogonality_files/figure-html/cell-7-output-3.png" width="515" height="490"></p>
</div>
</div>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbf%7BQ%7D&amp;=%5Cbegin%7Bbmatrix%7D%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20&amp;%20-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5C%5C%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D"> is orthogonal by computing <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7BQ%7D">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7BQ%7D&amp;=%5Cbegin%7Bbmatrix%7D%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5C%5C%0A-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%0A%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20&amp;%20-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5C%5C%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%0A%5Cend%7Bbmatrix%7D%5C%5C%0A&amp;=%5Cbegin%7Bbmatrix%7D%0A%5Cfrac%7B1%7D%7B2%7D+%5Cfrac%7B1%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D+%5Cfrac%7B1%7D%7B2%7D%5C%5C%0A-%5Cfrac%7B1%7D%7B2%7D+%5Cfrac%7B1%7D%7B2%7D%20&amp;%20%5Cfrac%7B1%7D%7B2%7D+%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%5C%5C%0A&amp;=%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%5C%5C%0A0%20&amp;%201%0A%5Cend%7Bbmatrix%7D%5C%5C%0A&amp;=%5Cmathbf%7BI%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p>Therefore, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D"> is an orthogonal matrix.</p>
<p><strong>Example 2</strong> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20&amp;%200%20%5C%5C%0A0%20&amp;%20-1%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%20-1%0A%5Cend%7Bbmatrix%7D%0A">,</p>
<p>which is an orthogonal matrix that reflects a vector across the <img src="https://latex.codecogs.com/png.latex?x">-axis.</p>
<p><strong>Example 3</strong> the identity matrix is an example of an orthogonal matrix</p>
<p><strong>Exmaple 4</strong> Permutation matrices are also orthogonal. Permutation matrices are used to exchange rows of a matrix.</p>
</section>
</section>
<section id="orthogonal-bases" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="orthogonal-bases"><span class="header-section-number">1.2</span> Orthogonal Bases</h2>
<p>An orthogonal basis is a set of vectors that are pairwise orthogonal (perpendicular) and each vector is non-zero.</p>
<div id="def-orthogonalBases" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 </strong></span>A set of vectors <img src="https://latex.codecogs.com/png.latex?%7B%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cdots,%20%5Cmathbf%7Bv%7D_n%7D"> is said to be an orthogonal basis of a vector space <img src="https://latex.codecogs.com/png.latex?V"> if: 1. Each vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_i"> is non-zero. 2. Each vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_i"> is orthogonal (perpendicular) to every other vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_j">, <img src="https://latex.codecogs.com/png.latex?i%20%5Cneq%20j">. In other words, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_i%20%5Ccdot%20%5Cmathbf%7Bv%7D_j%20=%200"> for <img src="https://latex.codecogs.com/png.latex?i%20%5Cneq%20j">.</p>
</div>
<section id="example-3" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="example-3"><span class="header-section-number">1.2.1</span> Example</h3>
<p><strong>Example1</strong> Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%202%20%5C%5C%203%20%5Cend%7Bbmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20=%20%5Cbegin%7Bbmatrix%7D%20-1%20%5C%200%20%5C%201%20%5Cend%7Bbmatrix%7D">. We can check if these vectors form an orthogonal basis by computing their dot product:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bv%7D%20%5Ccdot%20%5Cmathbf%7Bu%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%202%20%5C%5C%203%20%5Cend%7Bbmatrix%7D%20%5Ccdot%20%5Cbegin%7Bbmatrix%7D%20-1%20%5C%5C%200%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%20=%20(-1)%20%5Ccdot%201%20+%20(0)%20%5Ccdot%202%20+%20(1)%20%5Ccdot%203%20=%202%0A"></p>
<p>Since the dot product is not zero, we can conclude that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> do not form an orthogonal basis.</p>
<p><strong>Example2</strong></p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%202%20%5C%5C%200%20%5Cend%7Bbmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20=%20%5Cbegin%7Bbmatrix%7D%20-2%20%5C%5C%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D">. To check if they form an orthogonal basis, we need to compute their dot product:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bv%7D%20%5Ccdot%20%5Cmathbf%7Bu%7D%20=%20(1)(-2)%20+%20(2)(1)%20+%20(0)(1)%20=%20-2%20+%202%20+%200%20=%200%0A"></p>
<p>Since the dot product is zero, we know that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> are orthogonal. We can also check that they are both nonzero and linearly independent by computing their norms:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%7C%5Cmathbf%7Bv%7D%7C%7C%20=%20%5Csqrt%7B1%5E2%20+%202%5E2%20+%200%5E2%7D%20=%20%5Csqrt%7B5%7D%20%5Cne%200%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%7C%5Cmathbf%7Bw%7D%7C%7C%20=%20%5Csqrt%7B(-2)%5E2%20+%201%5E2%20+%201%5E2%7D%20=%20%5Csqrt%7B6%7D%20%5Cne%200%0A"></p>
<p>Therefore, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> form an orthogonal basis for <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3">.</p>
</section>
</section>
<section id="gram-schmidt-gs-or-g-s" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="gram-schmidt-gs-or-g-s"><span class="header-section-number">1.3</span> Gram-Schmidt (GS or G-S)</h2>
<p>The Gram-Schmidt procedure is a method of transforming a nonorthogonal matrix into an orthogonal matrix by orthonormalizing a set of linearly independent vectors in an inner product space, usually the Euclidean space <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En">. The process takes a sequence of vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v_1,%20%5Cmathbf%20v_2,%20%5Cdots,%20%5Cmathbf%20v_n"> and constructs an orthonormal sequence <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20q_1,%20%5Cmathbf%20q_2,%20%5Cdots,%20%5Cmathbf%20q_n"> that spans the same subspace as the original sequence.</p>
<p>The Gram-Schmidt procedure is useful for understanding orthogonal vector decomposition when programming and implementing the QR decomposition algorithm, and GS is the right way to conceptualize how and why QR decomposition works even if the low-level implementation is slightly different.</p>
<div id="def-gramSchmidt" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6 </strong></span>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v_1,%20%5Cmathbf%20v_2,%20%5Cdots,%20%5Cmathbf%20v_n"> be a sequence of linearly independent vectors in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En">. Define <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20q_1"> to be the unit vector in the direction of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v_1">, i.e., <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20q_1%20=%20%5Cfrac%7B%5Cmathbf%20v_1%7D%7B%7C%5Cmathbf%20v_1%7C%7D">. For <img src="https://latex.codecogs.com/png.latex?k%20=%202,%203,%20%5Cdots,%20n">, define <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20q_k"> as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20q_k%20=%20%5Cfrac%7B%5Cmathbf%20u_k%7D%7B%7C%5Cmathbf%20u_k%7C%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20u_k=%5Cmathbf%20v_k-%5Csum_%7Bj=1%7D%5E%7Bk-1%7D%5Clangle%20%5Cmathbf%20v_k,%5Cmathbf%20q_j%20%5Crangle%5Cmathbf%20q_j"></p>
</div>
<p>The vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20u_k"> is the projection of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v_k"> onto the subspace orthogonal to <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bspan%7D%7B%5Cmathbf%20q_1,%20%5Cmathbf%20q_2,%20%5Cdots,%20%5Cmathbf%20q_%7Bk-1%7D%7D">.</p>
<p>The Gram-Schmidt process produces an orthonormal basis <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20q_1,%20%5Cmathbf%20q_2,%20%5Cdots,%20%5Cmathbf%20q_n"> for <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bspan%7D%7B%5Cmathbf%20v_1,%20%5Cmathbf%20v_2,%20%5Cdots,%20%5Cmathbf%20v_n%7D">. The matrix whose columns are <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20q_1,%20%5Cmathbf%20q_2,%20%5Cdots,%20%5Cmathbf%20q_n"> is an orthogonal matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D"> is transformed into <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D"> according to the following algorithm:</p>
<p>For all column vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%20%5Cin%20V"> starting from the first (leftmost) and moving systematically to the last (rightmost):</p>
<ol type="1">
<li>Orthogonalize <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v_k"> to all previous columns in matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q"> using orthogonal vector decomposition. That is, compute the component of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v_k"> that is perpendicular to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20q_%7Bk-1%7D,%20%5Cmathbf%20q_%7Bk-2%7D">, and so on down to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20q_%7B1%7D">. The orthogonalized vector is called <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v%5E%7B*%7D_k">. :::{.callout-note} The first column vector is not orthogonalized because there are no preceeding vectors; therefore, you begin with the following normalization step. :::</li>
<li>Normalize <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v%5E%7B*%7D_k"> to unit length. This is now <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20q_%7Bk%7D">, the <img src="https://latex.codecogs.com/png.latex?k"> th column in matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q">.</li>
</ol>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;"># # Define a 4x4 random matrix</span></span>
<span id="cb7-2"><span class="co" style="color: #5E5E5E;"># A = np.random.rand(4,4)</span></span>
<span id="cb7-3"><span class="co" style="color: #5E5E5E;"># </span></span>
<span id="cb7-4"><span class="co" style="color: #5E5E5E;"># # Gram-Schmidt procedure</span></span>
<span id="cb7-5"><span class="co" style="color: #5E5E5E;"># Q = np.zeros_like(A)</span></span>
<span id="cb7-6"><span class="co" style="color: #5E5E5E;"># for j in range(A.shape[1]):</span></span>
<span id="cb7-7"><span class="co" style="color: #5E5E5E;">#     v = A[:,j]</span></span>
<span id="cb7-8"><span class="co" style="color: #5E5E5E;">#     for i in range(j):</span></span>
<span id="cb7-9"><span class="co" style="color: #5E5E5E;">#         q = Q[:,i]</span></span>
<span id="cb7-10"><span class="co" style="color: #5E5E5E;">#         R[i,j] = np.dot(q,v)</span></span>
<span id="cb7-11"><span class="co" style="color: #5E5E5E;">#         v -= R[i,j]*q</span></span>
<span id="cb7-12"><span class="co" style="color: #5E5E5E;">#     R[j,j] = np.linalg.norm(v)</span></span>
<span id="cb7-13"><span class="co" style="color: #5E5E5E;">#     Q[:,j] = v/R[j,j]</span></span>
<span id="cb7-14"><span class="co" style="color: #5E5E5E;"># </span></span>
<span id="cb7-15"><span class="co" style="color: #5E5E5E;"># # Check answer against Q from np.linalg.qr</span></span>
<span id="cb7-16"><span class="co" style="color: #5E5E5E;"># Q_np, R_np = np.linalg.qr(A)</span></span>
<span id="cb7-17"><span class="co" style="color: #5E5E5E;"># diff = Q - Q_np</span></span>
<span id="cb7-18"><span class="co" style="color: #5E5E5E;"># sum_Q = Q + Q_np</span></span>
<span id="cb7-19"><span class="co" style="color: #5E5E5E;"># </span></span>
<span id="cb7-20"><span class="co" style="color: #5E5E5E;"># print("Q matrix (Gram-Schmidt):\n", Q)</span></span>
<span id="cb7-21"><span class="co" style="color: #5E5E5E;"># print("Q matrix (numpy):\n", Q_np)</span></span>
<span id="cb7-22"><span class="co" style="color: #5E5E5E;"># print("Difference between Q and Q_np:\n", diff)</span></span>
<span id="cb7-23"><span class="co" style="color: #5E5E5E;"># print("Sum of Q and Q_np:\n", sum_Q)</span></span>
<span id="cb7-24"></span>
<span id="cb7-25"></span>
<span id="cb7-26"><span class="co" style="color: #5E5E5E;"># create the matrix </span></span>
<span id="cb7-27">m <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">4</span></span>
<span id="cb7-28">n <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">4</span></span>
<span id="cb7-29">A <span class="op" style="color: #5E5E5E;">=</span> np.random.randn(m,n)</span>
<span id="cb7-30"></span>
<span id="cb7-31"><span class="co" style="color: #5E5E5E;"># initialize</span></span>
<span id="cb7-32">Q <span class="op" style="color: #5E5E5E;">=</span> np.zeros((m,n))</span>
<span id="cb7-33"></span>
<span id="cb7-34"></span>
<span id="cb7-35"><span class="co" style="color: #5E5E5E;"># the GS algo</span></span>
<span id="cb7-36"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(n):</span>
<span id="cb7-37">    </span>
<span id="cb7-38">    <span class="co" style="color: #5E5E5E;"># initialize</span></span>
<span id="cb7-39">    Q[:,i] <span class="op" style="color: #5E5E5E;">=</span> A[:,i]</span>
<span id="cb7-40">    </span>
<span id="cb7-41">    <span class="co" style="color: #5E5E5E;"># orthogonalize</span></span>
<span id="cb7-42">    a <span class="op" style="color: #5E5E5E;">=</span> A[:,i] <span class="co" style="color: #5E5E5E;"># convenience</span></span>
<span id="cb7-43">    <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(i): <span class="co" style="color: #5E5E5E;"># only to earlier cols</span></span>
<span id="cb7-44">        q <span class="op" style="color: #5E5E5E;">=</span> Q[:,j] <span class="co" style="color: #5E5E5E;"># convenience</span></span>
<span id="cb7-45">        Q[:,i]<span class="op" style="color: #5E5E5E;">=</span>Q[:,i]<span class="op" style="color: #5E5E5E;">-</span>np.dot(a,q)<span class="op" style="color: #5E5E5E;">/</span>np.dot(q,q)<span class="op" style="color: #5E5E5E;">*</span>q</span>
<span id="cb7-46">    </span>
<span id="cb7-47">    <span class="co" style="color: #5E5E5E;"># normalize</span></span>
<span id="cb7-48">    Q[:,i] <span class="op" style="color: #5E5E5E;">=</span> Q[:,i] <span class="op" style="color: #5E5E5E;">/</span> np.linalg.norm(Q[:,i])</span>
<span id="cb7-49"></span>
<span id="cb7-50">    </span>
<span id="cb7-51"><span class="co" style="color: #5E5E5E;"># "real" QR decomposition for comparison</span></span>
<span id="cb7-52">Q_np,R <span class="op" style="color: #5E5E5E;">=</span> np.linalg.qr(A)</span>
<span id="cb7-53"></span>
<span id="cb7-54"><span class="co" style="color: #5E5E5E;"># note the possible sign differences.</span></span>
<span id="cb7-55"><span class="co" style="color: #5E5E5E;"># seemingly non-zero columns will be 0 when adding</span></span>
<span id="cb7-56"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Q matrix (Gram-Schmidt):</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>, Q)</span>
<span id="cb7-57"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Q matrix (numpy):</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>, Q_np)</span>
<span id="cb7-58"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Difference between Q and Q_np:</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>, np.<span class="bu" style="color: null;">round</span>( Q<span class="op" style="color: #5E5E5E;">-</span>Q_np ,<span class="dv" style="color: #AD0000;">10</span>) ), <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">' '</span>)</span>
<span id="cb7-59"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Sum of Q and Q_np:</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>, np.<span class="bu" style="color: null;">round</span>( Q<span class="op" style="color: #5E5E5E;">+</span>Q_np ,<span class="dv" style="color: #AD0000;">10</span>) )</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Q matrix (Gram-Schmidt):
 [[-0.03234158  0.05410685 -0.40803588  0.91078713]
 [-0.24740686  0.64842745  0.67349859  0.25442355]
 [-0.6308443  -0.67102789  0.34870012  0.17368149]
 [-0.73469676  0.35543756 -0.50824659 -0.27490036]]
Q matrix (numpy):
 [[-0.03234158 -0.05410685  0.40803588 -0.91078713]
 [-0.24740686 -0.64842745 -0.67349859 -0.25442355]
 [-0.6308443   0.67102789 -0.34870012 -0.17368149]
 [-0.73469676 -0.35543756  0.50824659  0.27490036]]
Difference between Q and Q_np:
 [[-0.          0.10821371 -0.81607176  1.82157425]
 [-0.          1.2968549   1.34699717  0.5088471 ]
 [-0.         -1.34205579  0.69740024  0.34736298]
 [-0.          0.71087511 -1.01649319 -0.54980072]]
 
Sum of Q and Q_np:
 [[-0.06468316  0.         -0.         -0.        ]
 [-0.49481372  0.         -0.         -0.        ]
 [-1.2616886   0.         -0.          0.        ]
 [-1.46939353  0.          0.         -0.        ]]</code></pre>
</div>
</div>
</section>
<section id="qr-decomposition" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="qr-decomposition"><span class="header-section-number">1.4</span> QR Decomposition</h2>
<p>QR decomposition is a factorization of a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> into the product of an orthogonal matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D"> and an upper triangular matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D=%5Cmathbf%7BQR%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D"> has orthonormal columns and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D"> is an upper triangular matrix.</p>
<p>The process of finding the QR decomposition of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> involves the Gram-Schmidt orthogonalization process, which produces an orthonormal basis for the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> as mentioned earlier. The columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D"> are the orthonormal basis vectors, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D"> is the matrix that expresses the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> in terms of the orthonormal basis vectors.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbf%7BA%7D&amp;=%5Cmathbf%7BQR%7D%5C%5C%0A%5Cmathbf%7BQ%7D%5ET%20%5Cmathbf%7BA%7D&amp;=%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7BQR%7D%5C%5C%0A%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7BA%7D&amp;=%5Cmathbf%7BR%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p>The algorithm for computing the QR decomposition of a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is as follows:</p>
<ol type="1">
<li>Apply the Gram-Schmidt orthogonalization process to the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> to obtain an orthonormal basis for the column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. Let the resulting matrix be denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D">.</li>
<li>Compute the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BQ%7D%5Cmathbf%7BR%7D">. This can be done by solving the linear system <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D%20=%20%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7BA%7D">, which expresses the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> in terms of the orthonormal basis vectors.</li>
</ol>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;"># create a random matrix</span></span>
<span id="cb9-2">A <span class="op" style="color: #5E5E5E;">=</span> np.random.randn(<span class="dv" style="color: #AD0000;">6</span>,<span class="dv" style="color: #AD0000;">6</span>)</span>
<span id="cb9-3"></span>
<span id="cb9-4"><span class="co" style="color: #5E5E5E;"># QR decomposition</span></span>
<span id="cb9-5">Q,R <span class="op" style="color: #5E5E5E;">=</span> np.linalg.qr(A)</span>
<span id="cb9-6"></span>
<span id="cb9-7"></span>
<span id="cb9-8"></span>
<span id="cb9-9"><span class="co" style="color: #5E5E5E;"># show the matrices</span></span>
<span id="cb9-10">fig <span class="op" style="color: #5E5E5E;">=</span> plt.figure(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">10</span>,<span class="dv" style="color: #AD0000;">6</span>))</span>
<span id="cb9-11">axs <span class="op" style="color: #5E5E5E;">=</span> [<span class="dv" style="color: #AD0000;">0</span>]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">5</span></span>
<span id="cb9-12">c <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.5</span> <span class="co" style="color: #5E5E5E;"># color limits</span></span>
<span id="cb9-13"></span>
<span id="cb9-14">gs1 <span class="op" style="color: #5E5E5E;">=</span> gridspec.GridSpec(<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">6</span>)</span>
<span id="cb9-15">axs[<span class="dv" style="color: #AD0000;">0</span>] <span class="op" style="color: #5E5E5E;">=</span> plt.subplot(gs1[<span class="dv" style="color: #AD0000;">0</span>,:<span class="dv" style="color: #AD0000;">2</span>])</span>
<span id="cb9-16">axs[<span class="dv" style="color: #AD0000;">0</span>].imshow(A,vmin<span class="op" style="color: #5E5E5E;">=-</span>c,vmax<span class="op" style="color: #5E5E5E;">=</span>c,cmap<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'gray'</span>)</span>
<span id="cb9-17">axs[<span class="dv" style="color: #AD0000;">0</span>].set_title(<span class="st" style="color: #20794D;">'A'</span>,fontweight<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'bold'</span>)</span>
<span id="cb9-18"></span>
<span id="cb9-19">axs[<span class="dv" style="color: #AD0000;">1</span>] <span class="op" style="color: #5E5E5E;">=</span> plt.subplot(gs1[<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">2</span>:<span class="dv" style="color: #AD0000;">4</span>])</span>
<span id="cb9-20">axs[<span class="dv" style="color: #AD0000;">1</span>].imshow(Q,vmin<span class="op" style="color: #5E5E5E;">=-</span>c,vmax<span class="op" style="color: #5E5E5E;">=</span>c,cmap<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'gray'</span>)</span>
<span id="cb9-21">axs[<span class="dv" style="color: #AD0000;">1</span>].set_title(<span class="st" style="color: #20794D;">'Q'</span>,fontweight<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'bold'</span>)</span>
<span id="cb9-22"></span>
<span id="cb9-23">axs[<span class="dv" style="color: #AD0000;">2</span>] <span class="op" style="color: #5E5E5E;">=</span> plt.subplot(gs1[<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">4</span>:<span class="dv" style="color: #AD0000;">6</span>])</span>
<span id="cb9-24">axs[<span class="dv" style="color: #AD0000;">2</span>].imshow(R,vmin<span class="op" style="color: #5E5E5E;">=-</span>c,vmax<span class="op" style="color: #5E5E5E;">=</span>c,cmap<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'gray'</span>)</span>
<span id="cb9-25">axs[<span class="dv" style="color: #AD0000;">2</span>].set_title(<span class="st" style="color: #20794D;">'R'</span>,fontweight<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'bold'</span>)</span>
<span id="cb9-26"></span>
<span id="cb9-27">axs[<span class="dv" style="color: #AD0000;">3</span>] <span class="op" style="color: #5E5E5E;">=</span> plt.subplot(gs1[<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span>:<span class="dv" style="color: #AD0000;">3</span>])</span>
<span id="cb9-28">axs[<span class="dv" style="color: #AD0000;">3</span>].imshow(A <span class="op" style="color: #5E5E5E;">-</span> Q<span class="op" style="color: #5E5E5E;">@</span>R,vmin<span class="op" style="color: #5E5E5E;">=-</span>c,vmax<span class="op" style="color: #5E5E5E;">=</span>c,cmap<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'gray'</span>)</span>
<span id="cb9-29">axs[<span class="dv" style="color: #AD0000;">3</span>].set_title(<span class="st" style="color: #20794D;">'A - QR'</span>,fontweight<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'bold'</span>)</span>
<span id="cb9-30"></span>
<span id="cb9-31">axs[<span class="dv" style="color: #AD0000;">4</span>] <span class="op" style="color: #5E5E5E;">=</span> plt.subplot(gs1[<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">3</span>:<span class="dv" style="color: #AD0000;">5</span>])</span>
<span id="cb9-32">axs[<span class="dv" style="color: #AD0000;">4</span>].imshow(Q.T<span class="op" style="color: #5E5E5E;">@</span>Q,cmap<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'gray'</span>)</span>
<span id="cb9-33">axs[<span class="dv" style="color: #AD0000;">4</span>].set_title(<span class="vs" style="color: #20794D;">r'$\mathbf</span><span class="sc" style="color: #5E5E5E;">{Q}</span><span class="vs" style="color: #20794D;">^</span><span class="sc" style="color: #5E5E5E;">{T}</span><span class="vs" style="color: #20794D;">\mathbf</span><span class="sc" style="color: #5E5E5E;">{Q}</span><span class="vs" style="color: #20794D;">$'</span>,fontweight<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'bold'</span>)</span>
<span id="cb9-34"></span>
<span id="cb9-35"><span class="co" style="color: #5E5E5E;"># remove ticks from all axes</span></span>
<span id="cb9-36"><span class="cf" style="color: #003B4F;">for</span> a <span class="kw" style="color: #003B4F;">in</span> axs:</span>
<span id="cb9-37">  a.set_xticks([])</span>
<span id="cb9-38">  a.set_yticks([])</span>
<span id="cb9-39"></span>
<span id="cb9-40">plt.tight_layout()</span>
<span id="cb9-41">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/06.orthogonality_files/figure-html/cell-9-output-1.png" width="894" height="566"></p>
</div>
</div>
<section id="examples-2" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="examples-2"><span class="header-section-number">1.4.1</span> Examples</h3>
<ol type="1">
<li>Consider the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%202%20&amp;%203%20%5C%5C%203%20&amp;%204%20%5Cend%7Bbmatrix%7D">. To find its QR decomposition, we first apply the Gram-Schmidt process to obtain an orthonormal basis for its column space:</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20q_1%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B14%7D%7D%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%202%20%5C%5C%203%5Cend%7Bbmatrix%7D%20%5Cquad%20%5Cmathbf%20q_2%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cbegin%7Bbmatrix%7D%20-2%20%5C%5C%201%20%5C%5C%200%5Cend%7Bbmatrix%7D%0A"></p>
<p>The resulting orthogonal matrix is <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cmathbf%7BQ%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B14%7D%7D%20&amp;%20-%5Cfrac%7B2%7D%7B%5Csqrt%7B28%7D%7D%20%5C%5C%0A%5Cfrac%7B2%7D%7B%5Csqrt%7B14%7D%7D%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20%5C%5C%0A%5Cfrac%7B3%7D%7B%5Csqrt%7B14%7D%7D%20&amp;%200%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D%0A">.</p>
<p>Next, we compute the upper triangular matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D"> by solving <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D%20=%20%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7BA%7D">. This gives <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BR%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A%5Csqrt%7B14%7D%20&amp;%20%5Cfrac%7B11%7D%7B%5Csqrt%7B14%7D%7D%20%5C%5C%0A0%20&amp;%20%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B%5Csqrt%7B7%7D%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>Therefore, the QR decomposition of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is given by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BQ%7D%5Cmathbf%7BR%7D">.</p>
<ol start="2" type="1">
<li>Consider the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%20-1%20&amp;%200%20%5C%5C%201%20&amp;%200%20&amp;%201%20%5C%5C%201%20&amp;%201%20&amp;%200%20%5Cend%7Bbmatrix%7D">. Applying the Gram-Schmidt process to its columns yields the orthonormal basis vectors:</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbf%20q_1%20&amp;=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D,%20%5C%5C%0A%5Cmathbf%20u_2%20&amp;=%20%5Cmathbf%20v_2%20-%20%5Clangle%20%5Cmathbf%20v_2,%20%5Cmathbf%20q_1%20%5Crangle%20%5Cmathbf%20q_1%20=%20%5Cbegin%7Bbmatrix%7D%20-1%20%5C%5C%200%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%20-%20%5Cfrac%7B1%7D%7B3%7D%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20-%5Cfrac%7B4%7D%7B3%7D%20%5C%5C%20-%5Cfrac%7B1%7D%7B3%7D%20%5C%5C%20%5Cfrac%7B2%7D%7B3%7D%20%5Cend%7Bbmatrix%7D,%20%5C%5C%0A%5Cmathbf%20q_2%20&amp;=%20%5Cfrac%7Bu_2%7D%7B%7Cu_2%7C%7D%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20%5Cbegin%7Bbmatrix%7D%20-2%20%5C%5C%20-1%20%5C%5C%201%20%5Cend%7Bbmatrix%7D,%20%5C%5C%0A%5Cmathbf%20u_3%20&amp;=%20%5Cmathbf%20v_3%20-%20%5Clangle%20%5Cmathbf%20v_3,%20%5Cmathbf%20q_1%20%5Crangle%20%5Cmathbf%20q_1%20-%20%5Clangle%20%5Cmathbf%20v_3,%20%5Cmathbf%20q_2%20%5Crangle%20%5Cmathbf%20q_2%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%201%20%5C%5C%200%20%5Cend%7Bbmatrix%7D%20-%20%5Cfrac%7B1%7D%7B3%7D%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%20-%20%5Cfrac%7B1%7D%7B2%7D%5Cbegin%7Bbmatrix%7D%20-2%20%5C%5C%20-1%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cfrac%7B1%7D%7B3%7D%20%5C%5C%20-%5Cfrac%7B1%7D%7B3%7D%20%5C%5C%20-%5Cfrac%7B1%7D%7B3%7D%20%5Cend%7Bbmatrix%7D,%20%5C%5C%0A%5Cmathbf%20q_3%20&amp;=%20%5Cfrac%7B%5Cmathbf%20u_3%7D%7B%7C%5Cmathbf%20u_3%7C%7D%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%20-1%20%5C%5C%20-1%20%5Cend%7Bbmatrix%7D.%0A%5Cend%7Balign*%7D%0A"></p>
<p>So the QR decomposition of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BQR%7D"> where</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA=QR%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%20&amp;%20-%5Cfrac%7B1%7D%7B%5Csqrt%7B6%7D%7D%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20%5C%5C%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B6%7D%7D%20&amp;%20-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20%5C%5C%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%20&amp;%20%5Cfrac%7B2%7D%7B%5Csqrt%7B6%7D%7D%20&amp;%200%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>and</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BR%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A%5Csqrt%7B3%7D%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%20&amp;%20%5Csqrt%7B3%7D%20%5C%5C%0A0%20&amp;%20%5Cfrac%7B2%7D%7B%5Csqrt%7B6%7D%7D%20&amp;%20-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20%5C%5C%0A0%20&amp;%200%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A">.</p>
<p>So, <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%20-1%20&amp;%200%20%5C%5C%0A1%20&amp;%200%20%20&amp;%201%20%5C%5C%0A1%20&amp;%201%20%20&amp;%200%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%20&amp;%20-%5Cfrac%7B1%7D%7B%5Csqrt%7B6%7D%7D%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20%5C%5C%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B6%7D%7D%20&amp;%20-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20%5C%5C%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%20&amp;%20%5Cfrac%7B2%7D%7B%5Csqrt%7B6%7D%7D%20&amp;%200%20%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%5Csqrt%7B3%7D%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%20&amp;%20%5Csqrt%7B3%7D%20%5C%5C%0A0%20&amp;%20%5Cfrac%7B2%7D%7B%5Csqrt%7B6%7D%7D%20&amp;%20-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%20%5C%5C%0A0%20&amp;%200%20&amp;%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%0A%5Cend%7Bbmatrix%7D%0A=%0A%5Cmathbf%7BQR%7D%0A"></p>
<p>In Python,</p>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">A <span class="op" style="color: #5E5E5E;">=</span> np.array([[<span class="dv" style="color: #AD0000;">1</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>], [<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>], [<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>]])</span>
<span id="cb10-2">Q, R <span class="op" style="color: #5E5E5E;">=</span> np.linalg.qr(A)</span>
<span id="cb10-3"></span>
<span id="cb10-4"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Q =</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>, Q)</span>
<span id="cb10-5"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"R =</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>, R)</span>
<span id="cb10-6"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"QR =</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>, Q <span class="op" style="color: #5E5E5E;">@</span> R)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Q =
 [[-5.77350269e-01  7.07106781e-01  4.08248290e-01]
 [-5.77350269e-01  5.55111512e-17 -8.16496581e-01]
 [-5.77350269e-01 -7.07106781e-01  4.08248290e-01]]
R =
 [[-1.73205081e+00  0.00000000e+00 -5.77350269e-01]
 [ 0.00000000e+00 -1.41421356e+00  1.11022302e-16]
 [ 0.00000000e+00  0.00000000e+00 -8.16496581e-01]]
QR =
 [[ 1.00000000e+00 -1.00000000e+00  3.55968130e-17]
 [ 1.00000000e+00 -7.85046229e-17  1.00000000e+00]
 [ 1.00000000e+00  1.00000000e+00 -3.01008243e-17]]</code></pre>
</div>
</div>
<p>The QR decomposition of a matrix is not unique, So the <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R"> could be different from the latex ones.</p>
</section>
<section id="sizes-of-mathbf-q-and-mathbf-r" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="sizes-of-mathbf-q-and-mathbf-r"><span class="header-section-number">1.4.2</span> Sizes of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R"></h3>
<p>The sizes of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R"> depend on the size of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> and on whether the QR decomposition is <em>reduced</em> or <em>full</em>.</p>
<p>For a tall matrix <img src="https://latex.codecogs.com/png.latex?(m%20%3E%20n)">, do we create a <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q"> matrix with n columns or m columns?</p>
<ul>
<li>Economy or Reduced: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q_%7Bm%5Ctimes%20n%7D"> (tall <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q">)</li>
<li>Full or Complete: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q_%7Bm%5Ctimes%20m%7D"> (square <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q">)
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q"> can be square when <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is tall (<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q"> can have more columns than <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">)</li>
<li>In python, the option of <code>np.linalg.qr(A,'complete')</code> is ‘complete’, which produces a full QR decomposition.</li>
<li>The option of <code>np.linalg.qr(A,'reduced')</code> is ‘reduced’, which is the default, gives the economy-mode QR decomposition, in which <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q"> is the same size as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">.</li>
</ul></li>
<li>Likewise, the rank of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q"> is always the maximum possible rank, which is <img src="https://latex.codecogs.com/png.latex?m"> for all square <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q"> matrices and <img src="https://latex.codecogs.com/png.latex?n"> for the economy <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q">. The rank of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R"> is the same as the rank of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">.
<ul>
<li>the difference of <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Brank%7D(%5Cmathbf%20A)%20and%20%5Coperatorname%7Brank%7D(%5Cmathbf%20Q)"> means <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q"> spans all of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%20R%5Em"> even if the <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Bcol%7D(%5Cmathbf%20A)"> is only a lower-dimensional subspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%20R%5Em">.</li>
</ul></li>
<li><strong>non-uniqueness</strong>: QR decomposition is not unique for all matrix sizes and ranks. (<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%20=%20%5Cmathbf%20Q_1%20%5Cmathbf%20R_1"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%20=%20%5Cmathbf%20Q_2%5Cmathbf%20R_2"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q_1%20%5Cne%20%5Cmathbf%20Q_2">).</li>
<li><strong>uniqueness with constraints</strong>: QR decomposition can be made unique given additional constraints (e.g., positive values on the diagonals of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R">)</li>
</ul>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">A <span class="op" style="color: #5E5E5E;">=</span> np.array([ [<span class="dv" style="color: #AD0000;">1</span>,<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>] ]).T</span>
<span id="cb12-2">Q,R <span class="op" style="color: #5E5E5E;">=</span> np.linalg.qr(A,<span class="st" style="color: #20794D;">'complete'</span>)</span>
<span id="cb12-3">Q<span class="op" style="color: #5E5E5E;">*</span>np.sqrt(<span class="dv" style="color: #AD0000;">2</span>) <span class="co" style="color: #5E5E5E;"># scaled by sqrt(2) to get integers</span></span>
<span id="cb12-4"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"A =</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>, A)</span>
<span id="cb12-5"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Q =</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>, Q)</span>
<span id="cb12-6"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"R =</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>, R)</span>
<span id="cb12-7"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Q*np.sqrt(2)=</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>, Q<span class="op" style="color: #5E5E5E;">*</span>np.sqrt(<span class="dv" style="color: #AD0000;">2</span>))</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>A =
 [[ 1]
 [-1]]
Q =
 [[-0.70710678  0.70710678]
 [ 0.70710678  0.70710678]]
R =
 [[-1.41421356]
 [ 0.        ]]
Q*np.sqrt(2)=
 [[-1.  1.]
 [ 1.  1.]]</code></pre>
</div>
</div>
</section>
<section id="upper-triangle-of-mathbf-r" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="upper-triangle-of-mathbf-r"><span class="header-section-number">1.4.3</span> Upper-triangle of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R"></h3>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R"> comes from the formula <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7BA%20=%20R%7D">.</li>
<li>The lower triangle of a product matrix comprises dot products between later rows of the left matrix and earlier columns of the right matrix.</li>
<li>The rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D%5ET"> are the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D">.</li>
</ul>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="co" style="color: #5E5E5E;"># Define matrix A</span></span>
<span id="cb14-2">A <span class="op" style="color: #5E5E5E;">=</span> np.array([[<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">3</span>], [<span class="dv" style="color: #AD0000;">4</span>, <span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">6</span>], [<span class="dv" style="color: #AD0000;">7</span>, <span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">9</span>]])</span>
<span id="cb14-3"></span>
<span id="cb14-4"><span class="co" style="color: #5E5E5E;"># Compute QR decomposition of A</span></span>
<span id="cb14-5">Q, R <span class="op" style="color: #5E5E5E;">=</span> np.linalg.qr(A)</span>
<span id="cb14-6"></span>
<span id="cb14-7"><span class="co" style="color: #5E5E5E;"># Compute Q^T * A</span></span>
<span id="cb14-8">QtA <span class="op" style="color: #5E5E5E;">=</span> np.matmul(Q.T, A)</span>
<span id="cb14-9"></span>
<span id="cb14-10"><span class="co" style="color: #5E5E5E;"># Extract upper triangle of R</span></span>
<span id="cb14-11">upper_R <span class="op" style="color: #5E5E5E;">=</span> np.triu(R)</span>
<span id="cb14-12"></span>
<span id="cb14-13"><span class="co" style="color: #5E5E5E;"># Compute dot products between later rows of Q^T and earlier columns of A</span></span>
<span id="cb14-14">dot_products <span class="op" style="color: #5E5E5E;">=</span> np.zeros((<span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">3</span>))</span>
<span id="cb14-15"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">3</span>):</span>
<span id="cb14-16">    <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(i):</span>
<span id="cb14-17">        dot_products[i, j] <span class="op" style="color: #5E5E5E;">=</span> np.dot(Q.T[i], A[:, j])</span>
<span id="cb14-18"></span>
<span id="cb14-19"><span class="co" style="color: #5E5E5E;"># Print results</span></span>
<span id="cb14-20"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"A = "</span>)</span>
<span id="cb14-21"><span class="bu" style="color: null;">print</span>(A)</span>
<span id="cb14-22"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Q = "</span>)</span>
<span id="cb14-23"><span class="bu" style="color: null;">print</span>(Q)</span>
<span id="cb14-24"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"R = "</span>)</span>
<span id="cb14-25"><span class="bu" style="color: null;">print</span>(R)</span>
<span id="cb14-26"><span class="bu" style="color: null;">print</span>(<span class="vs" style="color: #20794D;">r"$Q^T * A = $"</span>)</span>
<span id="cb14-27"><span class="bu" style="color: null;">print</span>(QtA)</span>
<span id="cb14-28"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Upper-triangle of R = "</span>)</span>
<span id="cb14-29"><span class="bu" style="color: null;">print</span>(upper_R)</span>
<span id="cb14-30"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Dot products between later rows of Q^T and earlier columns of A = "</span>)</span>
<span id="cb14-31"><span class="bu" style="color: null;">print</span>(dot_products)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>A = 
[[1 2 3]
 [4 5 6]
 [7 8 9]]
Q = 
[[-0.12309149  0.90453403  0.40824829]
 [-0.49236596  0.30151134 -0.81649658]
 [-0.86164044 -0.30151134  0.40824829]]
R = 
[[-8.12403840e+00 -9.60113630e+00 -1.10782342e+01]
 [ 0.00000000e+00  9.04534034e-01  1.80906807e+00]
 [ 0.00000000e+00  0.00000000e+00 -8.88178420e-16]]
$Q^T * A = $
[[-8.12403840e+00 -9.60113630e+00 -1.10782342e+01]
 [ 1.55431223e-15  9.04534034e-01  1.80906807e+00]
 [ 6.10622664e-16 -4.44089210e-16 -1.05471187e-15]]
Upper-triangle of R = 
[[-8.12403840e+00 -9.60113630e+00 -1.10782342e+01]
 [ 0.00000000e+00  9.04534034e-01  1.80906807e+00]
 [ 0.00000000e+00  0.00000000e+00 -8.88178420e-16]]
Dot products between later rows of Q^T and earlier columns of A = 
[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]
 [ 1.77635684e-15  0.00000000e+00  0.00000000e+00]
 [ 4.44089210e-16 -8.88178420e-16  0.00000000e+00]]</code></pre>
</div>
</div>
<p>The lower triangle of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D"> comprises dot products of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q%5ET%5Cmathbf%20A"> (between later rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D%5ET"> and earlier columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">) :::{.callout-note} Note that the lower triangle of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D"> is zero, because the first column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is orthogonal to the remaining columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. Thus, the pairs of vectors used to form the lower triangle of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D"> are orthogonal. On the other hand, the upper triangle of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D"> comes from the dot product of later rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D"> and earlier columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. Specifically, the <img src="https://latex.codecogs.com/png.latex?(2,1)"> entry of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D"> is the dot product of the second row of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D"> with the first column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"></p>
<p>If columns <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j"> of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> were already orthogonal, then the corresponding <img src="https://latex.codecogs.com/png.latex?(i,j)"> th element in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R"> would be zero. In fact, if you compute the QR decomposition of an orthogonal matrix, then <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R"> will be a diagonal matrix in which the diagonal elements are the norms of each column in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">. That means that if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%20=%20Q%7D">, then <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%20=%20I%7D">, which is obvious from the equation solved for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D"> :::</p>
</section>
<section id="qr-and-inverses" class="level3" data-number="1.4.4">
<h3 data-number="1.4.4" class="anchored" data-anchor-id="qr-and-inverses"><span class="header-section-number">1.4.4</span> QR and Inverses</h3>
<p>QR decomposition provides a more numerically stable way to compute the matrix inverse, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5E%7B-1%7D"> of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> because <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q"> is numerically stable due to the Householder reflection algorithm, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20R"> is numerically stable because it simply results from matrix multiplication.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%5Cmathbf%7BA%7D&amp;=%5Cmathbf%7BQR%7D%5C%5C%0A%20%20%5Cmathbf%7BA%7D%5E%7B-1%7D&amp;=(%5Cmathbf%7BQR%7D)%5E%7B-1%7D%5C%5C%0A%20%20%5Cmathbf%7BA%7D%5E%7B-1%7D&amp;=%5Cmathbf%7BR%7D%5E%7B-1%7D%5Cmathbf%7BQ%7D%5E%7B-1%7D%5C%5C%0A%20%20%5Cmathbf%7BA%7D%5E%7B-1%7D&amp;=%5Cmathbf%7BR%7D%5E%7B-1%7D%5Cmathbf%7BQ%7D%5E%7BT%7D%0A%5Cend%7Balign*%7D%0A"></p>
</section>
</section>
<section id="projections" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="projections"><span class="header-section-number">1.5</span> Projections</h2>
</section>
<section id="least-squares-approximations" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="least-squares-approximations"><span class="header-section-number">1.6</span> Least Squares Approximations</h2>
</section>
<section id="orthogonal-bases-and-gram-schmidt" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="orthogonal-bases-and-gram-schmidt"><span class="header-section-number">1.7</span> Orthogonal Bases and Gram-Schmidt</h2>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/06.orthogonality.html</guid>
  <pubDate>Thu, 20 Apr 2023 15:00:00 GMT</pubDate>
  <media:content url="kmink3225.netlify.app/images/linear_algebra/orthogonal_space.PNG" medium="image"/>
</item>
<item>
  <title>Vector Space</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html</link>
  <description><![CDATA[ 



<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> mpl_toolkits.mplot3d <span class="im" style="color: #00769E;">import</span> Axes3D</span></code></pre></div>
</details>
</div>
<section id="vector-space" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Vector Space</h1>
<p>The perspective of looking into linear algebra change from ‘numbers’ to ‘vector space’ through ‘vectors’. Here, instead of looking at individual columns, we will observe <strong>spaces of vecctors</strong> and <strong>their subspace</strong> to better understand <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D=%5Cmathbf%7Bb%7D">. From this part, the fundamental theorem of linear algebra appears.</p>
<section id="definition" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="definition"><span class="header-section-number">1.1</span> Definition</h2>
<div id="def-VSnotation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 </strong></span>The space <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7Bn%7D"> consists of all column vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v"> with <img src="https://latex.codecogs.com/png.latex?n"> components. The space <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BC%7D%5E%7Bn%7D"> consists of all column vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v"> with <img src="https://latex.codecogs.com/png.latex?n"> components. where <img src="https://latex.codecogs.com/png.latex?n=1,2,3,%5Cldots">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> stands for a set of real numbers, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BC%7D"> stands for a set of complex numbers.</p>
</div>
<p>For example, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7B5%7D"> contains all column vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v"> with <img src="https://latex.codecogs.com/png.latex?5"> components, which is called ‘5-dimensional space’</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D2%20%5C%5C%204%20%5Cend%7Bbmatrix%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B2%7D%20%5Cquad%20%5Cbegin%7Bbmatrix%7D2%20&amp;%202%20&amp;%2043%20&amp;%2056%20&amp;%204%20%5Cend%7Bbmatrix%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B5%7D%20%5Cquad%20%5Cbegin%7Bbmatrix%7D2+4i%20%5C%5C%204-21i%20%5Cend%7Bbmatrix%7D%20%5Cin%20%5Cmathbb%7BC%7D%5E%7B2%7D%0A"></p>
<p>A vector space is a set <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D"> equipped with two operations:</p>
<ul>
<li>vector addition and</li>
<li>scalar multiplication</li>
</ul>
<p>,which satisfy the following properties for all vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D,%20%5Cmathbf%7Bv%7D,%20%5Cmathbf%7Bw%7D"> in <img src="https://latex.codecogs.com/png.latex?V"> and all scalars <img src="https://latex.codecogs.com/png.latex?c,%20d">:</p>
<ol type="1">
<li>Closure under vector addition: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D%20%5Cin%20V"></li>
<li>Associativity of vector addition: <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D)%20+%20%5Cmathbf%7Bw%7D%20=%20%5Cmathbf%7Bu%7D%20+%20(%5Cmathbf%7Bv%7D%20+%20%5Cmathbf%7Bw%7D)"></li>
<li>Identity element of vector addition: There exists a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D"> in <img src="https://latex.codecogs.com/png.latex?V"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7B0%7D%20=%20%5Cmathbf%7Bu%7D"> for all <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> in <img src="https://latex.codecogs.com/png.latex?V"></li>
<li>Existence of additive inverse: For each <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> in <img src="https://latex.codecogs.com/png.latex?V">, there exists a vector <img src="https://latex.codecogs.com/png.latex?-%5Cmathbf%7Bu%7D"> in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20(-%5Cmathbf%7Bu%7D)%20=%20%5Cmathbf%7B0%7D"></li>
<li>Closure under scalar multiplication: <img src="https://latex.codecogs.com/png.latex?c%5Cmathbf%7Bu%7D%20%5Cin%20V"></li>
<li>Distributive law of scalar multiplication with respect to scalar addition: <img src="https://latex.codecogs.com/png.latex?(c%20+%20d)%5Cmathbf%7Bu%7D%20=%20c%5Cmathbf%7Bu%7D%20+%20d%5Cmathbf%7Bu%7D"></li>
<li>Distributive law of scalar multiplication with respect to vector addition: <img src="https://latex.codecogs.com/png.latex?c(%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D)%20=%20c%5Cmathbf%7Bu%7D%20+%20c%5Cmathbf%7Bv%7D"></li>
<li>Associativity of scalar multiplication: <img src="https://latex.codecogs.com/png.latex?(cd)%5Cmathbf%7Bu%7D%20=%20c(d%5Cmathbf%7Bu%7D)"></li>
<li>Identity element of scalar multiplication: <img src="https://latex.codecogs.com/png.latex?1%5Cmathbf%7Bu%7D%20=%20%5Cmathbf%7Bu%7D"></li>
</ol>
<p>The <strong>closure</strong> or <strong>inside the vector space</strong> mean that the result of the properties stays in the space.</p>
<section id="example" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="example"><span class="header-section-number">1.1.1</span> Example</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?V%20=%20%5Cmathbb%7BR%7D%5E3">, the set of all 3-dimensional real vectors. The vector space <img src="https://latex.codecogs.com/png.latex?V"> is equipped with vector addition and scalar multiplication defined as usual component-wise. Vector addition and scalar multiplication satisfy all the properties listed in the definition of a vector space.</p>
</section>
<section id="three-special-vector-spaces" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2" class="anchored" data-anchor-id="three-special-vector-spaces"><span class="header-section-number">1.1.2</span> Three Special Vector Spaces</h3>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BM%7D"> the vector space of all real 3 by 2 matrices
<ul>
<li>the vectors <img src="https://latex.codecogs.com/png.latex?%5Cin%20%5Cmathbb%7BM%7D"> are really matrices.</li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BF%7D"> the vector space of all real functions f(x)
<ul>
<li>the vectors <img src="https://latex.codecogs.com/png.latex?%5Cin%20%5Cmathbb%7BF%7D"> are really functions.</li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BZ%7D"> the vector space that consists only of a zero vectors.
<ul>
<li>the vectors <img src="https://latex.codecogs.com/png.latex?%5Cin%20%5Cmathbb%7BZ%7D"> are used for the addition <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D+%5Cmathbf%7B0%7D=%5Cmathbf%7B0%7D"></li>
</ul></li>
</ul>
</section>
</section>
<section id="subspaces" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="subspaces"><span class="header-section-number">1.2</span> Subspaces</h2>
<section id="definition-1" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="definition-1"><span class="header-section-number">1.2.1</span> Definition</h3>
<div id="def-subspace" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 </strong></span>A subset <img src="https://latex.codecogs.com/png.latex?W"> of a vector space <img src="https://latex.codecogs.com/png.latex?V"> over a field <img src="https://latex.codecogs.com/png.latex?F"> is called a subspace of <img src="https://latex.codecogs.com/png.latex?V"> if <img src="https://latex.codecogs.com/png.latex?W"> is also a vector space over <img src="https://latex.codecogs.com/png.latex?F"> under the same vector addition and scalar multiplication operations defined on <img src="https://latex.codecogs.com/png.latex?V">. A subspace of a vector space is a set of vectors (including <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%200">) that satisifies two requirements: if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20u"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v"> in the subspace and <img src="https://latex.codecogs.com/png.latex?c"> is any scalar, then</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v%20+%20%5Cmathbf%20w"> is in the subspace</li>
<li><img src="https://latex.codecogs.com/png.latex?c%20%5Cmathbf%20v"> is in the subspace.</li>
</ol>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Field
</div>
</div>
<div class="callout-body-container callout-body">
<p>A field is a set <img src="https://latex.codecogs.com/png.latex?F"> along with two operations, addition (<img src="https://latex.codecogs.com/png.latex?+">) and multiplication (<img src="https://latex.codecogs.com/png.latex?%5Ccdot">), satisfying the following properties:</p>
<ul>
<li>Closure: For all <img src="https://latex.codecogs.com/png.latex?a,%20b%20%5Cin%20F">, both <img src="https://latex.codecogs.com/png.latex?a%20+%20b"> and <img src="https://latex.codecogs.com/png.latex?a%20%5Ccdot%20b"> are in <img src="https://latex.codecogs.com/png.latex?F">.</li>
<li>Associativity: For all <img src="https://latex.codecogs.com/png.latex?a,%20b,%20c%20%5Cin%20F">, <img src="https://latex.codecogs.com/png.latex?(a%20+%20b)%20+%20c%20=%20a%20+%20(b%20+%20c)"> and <img src="https://latex.codecogs.com/png.latex?(a%20%5Ccdot%20b)%20%5Ccdot%20c%20=%20a%20%5Ccdot%20(b%20%5Ccdot%20c)">.</li>
<li>Commutativity: For all <img src="https://latex.codecogs.com/png.latex?a,%20b%20%5Cin%20F">, <img src="https://latex.codecogs.com/png.latex?a%20+%20b%20=%20b%20+%20a"> and <img src="https://latex.codecogs.com/png.latex?a%20%5Ccdot%20b%20=%20b%20%5Ccdot%20a">.</li>
<li>Identity: There exists an element <img src="https://latex.codecogs.com/png.latex?0%20%5Cin%20F"> such that for all <img src="https://latex.codecogs.com/png.latex?a%20%5Cin%20F">, <img src="https://latex.codecogs.com/png.latex?a%20+%200%20=%20a">. There exists an element <img src="https://latex.codecogs.com/png.latex?1%20%5Cin%20F"> such that for all <img src="https://latex.codecogs.com/png.latex?a%20%5Cin%20F">, <img src="https://latex.codecogs.com/png.latex?a%20%5Ccdot%201%20=%20a">.</li>
<li>Inverse: For every <img src="https://latex.codecogs.com/png.latex?a%20%5Cin%20F"> with <img src="https://latex.codecogs.com/png.latex?a%20%5Cneq%200">, there exists an element <img src="https://latex.codecogs.com/png.latex?b%20%5Cin%20F"> such that <img src="https://latex.codecogs.com/png.latex?a%20+%20b%20=%200">. For every <img src="https://latex.codecogs.com/png.latex?a%20%5Cin%20F"> with <img src="https://latex.codecogs.com/png.latex?a%20%5Cneq%200">, there exists an element <img src="https://latex.codecogs.com/png.latex?c%20%5Cin%20F"> such that <img src="https://latex.codecogs.com/png.latex?a%20%5Ccdot%20c%20=%201">.</li>
</ul>
<section id="example-1" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="example-1"><span class="header-section-number">1.2.2</span> Example</h3>
<ul>
<li>The set of real numbers <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> with addition and multiplication.</li>
<li>The set of complex numbers <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BC%7D"> with addition and multiplication.</li>
<li>The set of rational numbers <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BQ%7D"> with addition and multiplication.</li>
<li>The set of integers modulo a prime number <img src="https://latex.codecogs.com/png.latex?p">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BZ%7D_p">, with addition and multiplication modulo <img src="https://latex.codecogs.com/png.latex?p">.</li>
</ul>
</section>
</div>
</div>
</section>
<section id="properties" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="properties"><span class="header-section-number">1.2.3</span> Properties</h3>
<p>A subset <img src="https://latex.codecogs.com/png.latex?W"> of a vector space <img src="https://latex.codecogs.com/png.latex?V"> is called a subspace of <img src="https://latex.codecogs.com/png.latex?V"> if it satisfies the following properties:</p>
<ul>
<li>Closure under Addition: For all <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D,%20%5Cmathbf%7Bv%7D%20%5Cin%20W">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D%20%5Cin%20W">.</li>
<li>Closure under Scalar Multiplication: For all <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20%5Cin%20W"> and <img src="https://latex.codecogs.com/png.latex?c%20%5Cin%20%5Cmathbb%7BR%7D"> (or any field), <img src="https://latex.codecogs.com/png.latex?c%5Cmathbf%7Bu%7D%20%5Cin%20W">.</li>
<li>Contains the Zero Vector: The zero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D"> of <img src="https://latex.codecogs.com/png.latex?V"> is in <img src="https://latex.codecogs.com/png.latex?W">.</li>
<li>Every subspace contains the zero vector</li>
<li>Lines through the origin are also subspaces. (If we try to keep only part of a plane or line, the requirements for a subspace don’t hold.)</li>
<li>A subspace containing <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20u"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v"> must contain all linear combinations <img src="https://latex.codecogs.com/png.latex?c%5Cmathbf%20v%20+d%5Cmathbf%20w"></li>
</ul>
</section>
<section id="example-2" class="level3" data-number="1.2.4">
<h3 data-number="1.2.4" class="anchored" data-anchor-id="example-2"><span class="header-section-number">1.2.4</span> Example</h3>
<ol type="1">
<li>The set of all real-valued <img src="https://latex.codecogs.com/png.latex?n">-dimensional column vectors, denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En">, is a subspace of the vector space of all <img src="https://latex.codecogs.com/png.latex?n">-dimensional column vectors, denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%201%7D">, with closure under addition and scalar multiplication.</li>
</ol>
<ul>
<li>Subset <img src="https://latex.codecogs.com/png.latex?V_3%20=%20%7B(x,%20y,%20z)%20%5Cin%20%5Cmathbb%7BR%7D%5E3%20:%20x%20+%20y%20+%20z%20=%200%7D"> of vector space <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3">: <img src="https://latex.codecogs.com/png.latex?V_3%20=%20%5C%7B(x,%20y,%20z)%20%5Cin%20%5Cmathbb%7BR%7D%5E3%20:%20x%20+%20y%20+%20z%20=%200%5C%7D"></li>
</ul>
<ol start="2" type="1">
<li>The set of all <img src="https://latex.codecogs.com/png.latex?2%20%5Ctimes%202"> symmetric matrices, denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BS%7D%5E2">, is a subspace of the vector space of all <img src="https://latex.codecogs.com/png.latex?2%20%5Ctimes%202"> matrices, denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7B2%20%5Ctimes%202%7D">, with closure under addition and scalar multiplication.</li>
</ol>
<ul>
<li>Subset <img src="https://latex.codecogs.com/png.latex?V_1%20=%20%7B(x,%20y)%20%5Cin%20%5Cmathbb%7BR%7D%5E2%20:%20x%20+%20y%20=%200%7D"> of vector space <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E2">: <img src="https://latex.codecogs.com/png.latex?V_1%20=%20%5C%7B(x,%20y)%20%5Cin%20%5Cmathbb%7BR%7D%5E2%20:%20x%20+%20y%20=%200%5C%7D"></li>
</ul>
<ol start="3" type="1">
<li>The set of all polynomials of degree at most <img src="https://latex.codecogs.com/png.latex?n">, denoted as <img src="https://latex.codecogs.com/png.latex?P_n">, is a subspace of the vector space of all polynomials, denoted as <img src="https://latex.codecogs.com/png.latex?P">, with closure under addition and scalar multiplication.</li>
</ol>
<ul>
<li>Subset <img src="https://latex.codecogs.com/png.latex?V_2%20=%20%7Bp(x)%20%5Cin%20%5Cmathbb%7BR%7D%5Bx%5D%20:%20%5Ctext%7Bdeg%7D(p(x))%20%5Cleq%203%7D"> of vector space <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Bx%5D">: <img src="https://latex.codecogs.com/png.latex?V_2%20=%20%5C%7Bp(x)%20%5Cin%20%5Cmathbb%7BR%7D%5Bx%5D%20:%20%5Ctext%7Bdeg%7D(p(x))%20%5Cleq%203%5C%7D"></li>
<li>“deg” stands for “degree” and refers to the degree of a polynomial. In the example, <img src="https://latex.codecogs.com/png.latex?V_2%20=%20%7Bp(x)%20%5Cin%20%5Cmathbb%7BR%7D%5Bx%5D%20:%20%5Ctext%7Bdeg%7D(p(x))%20%5Cleq%203%7D">, <img src="https://latex.codecogs.com/png.latex?p(x)"> represents a polynomial, and <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdeg%7D(p(x))"> represents the degree of the polynomial <img src="https://latex.codecogs.com/png.latex?p(x)">. The condition <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdeg%7D(p(x))%20%5Cleq%203"> specifies that the subset <img src="https://latex.codecogs.com/png.latex?V_2"> includes all polynomials of degree 3 or lower in the vector space <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Bx%5D"> of polynomials with real coefficients.</li>
</ul>
<ol start="4" type="1">
<li>All upper triangular matrices <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7Da&amp;b%5C%5C0&amp;d%5Cend%7Bbmatrix%7D%20%5Cin%20%5Cmathbb%20M"> are a subspace</li>
<li>All diagonal matrices <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7Da&amp;0%5C%5C0&amp;d%5Cend%7Bbmatrix%7D%20%5Cin%20%5Cmathbb%20M"> are a subspace</li>
</ol>
</section>
</section>
<section id="the-column-space-of-mathbf-a" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="the-column-space-of-mathbf-a"><span class="header-section-number">1.3</span> The Column Space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"></h2>
<section id="definition-2" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="definition-2"><span class="header-section-number">1.3.1</span> Definition</h3>
<div id="def-subspace" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 </strong></span>Given a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> with columns <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Ba%7D_1,%20%5Cmathbf%7Ba%7D_2,%20%5Cldots,%20%5Cmathbf%7Ba%7D_n">, the column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bcol%7D(%5Cmathbf%20A)">, is the set of all possible linear combinations of the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">. The combinations are all possible vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D">. In other words, <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Ctext%7Bcol%7D(%5Cmathbf%20A)%20&amp;=%20%5Ctext%7Bspan%7D%5C%7B%5Cmathbf%7Ba%7D_1,%20%5Cmathbf%7Ba%7D_2,%20%5Cldots,%20%5Cmathbf%7Ba%7D_n%5C%7D%20%5C%5C%0AS%20&amp;=%20%5Ctext%7B%20set%20of%20vectors%20in%20%7D%20V%20(%5Ctext%7B%20probably%20not%20a%20subspace%20%7D)%20%5C%5C%0ASS%20&amp;=%20%5Ctext%7B%20all%20combinations%20of%20vectors%20in%20%7D%20S%20%5C%5C%0ASS%20&amp;=%20%5Ctext%7B%20all%20%7D%20c_1%5Cmathbf%7Bv%7D_1+%5Cldots+c_n%5Cmathbf%7Bv%7D_n%20%5C%5C%0A%20%20&amp;=%20%5Ctext%7B%20the%20subspace%20of%20%7D%20V%20%5Ctext%7B%20spanned%20by%20%7D%20S%0A%5Cend%7Balign*%7D%0A"></p>
</div>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>To solve <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Ax=%5Cmathbf%20b"> is to express <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20b"> as a combination of the columns <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20a">. <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20b"> has to be in <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bcol%7D(%5Cmathbf%20A)"> or no solution.
<ul>
<li>The system <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Ax=%5Cmathbf%20b"> is solvable if and only if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20b"> is in <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bcol%7D(%5Cmathbf%20A)">.</li>
</ul></li>
<li>The column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> or <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bcol%7D(%5Cmathbf%20A)"> is a subspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%20R%5E%7Bm%7D"> not <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%20R%5E%7Bn%7D">.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="kmink3225.netlify.app/images/linear_algebra/column_space.PNG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Gilbert Strang - Introduction to Linear Algebra</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="example-3" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="example-3"><span class="header-section-number">1.3.2</span> Example</h3>
<ol type="1">
<li>Consider the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%20=%20%5Cbegin%7Bbmatrix%7D1%20&amp;%202%20&amp;%203%20%5C%5C4%20&amp;%205%20&amp;%206%20%5C%5C7%20&amp;%208%20&amp;%209%20%5Cend%7Bbmatrix%7D"></li>
</ol>
<p>The column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is the span of its columns: <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bcol%7D(%5Cmathbf%20A)%20=%20%5Ctext%7Bspan%7D%5C%7B%5Cmathbf%7Ba%7D_1,%20%5Cmathbf%7Ba%7D_2,%20%5Cmathbf%7Ba%7D_3%5C%7D"></p>
<ol start="2" type="1">
<li>Describe the column spaces for the following matrices</li>
</ol>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20I%20=%20%5Cbegin%7Bbmatrix%7D1%20&amp;%200%20%5C%5C0%20&amp;%201%20%5Cend%7Bbmatrix%7D">
<ul>
<li>The column space of the identity matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BCol%7D(%5Cmathbf%7BI%7D)">, spans the entire space <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E2">, as it includes all possible linear combinations of the standard unit vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D_1"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D_2">. Thus, <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bcol%7D(%5Cmathbf%20A)"> is <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%20R%5E2"></li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%20=%20%5Cbegin%7Bbmatrix%7D1%20&amp;%202%20%5C%5C2%20&amp;%204%20%5Cend%7Bbmatrix%7D">
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BCol%7D(%5Cmathbf%7BA%7D)">, is the subspace spanned by the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. Since the second column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a scalar multiple (twice) of the first column, the column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a one-dimensional subspace, which is the line in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E2"> that lies along the direction of the first column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bcol%7D(%5Cmathbf%20A)%20=%20%5Ctext%7Bspan%7D%5Cleft%5C%7B%5Cbegin%7Bbmatrix%7D1%20%5C%5C%202%5Cend%7Bbmatrix%7D%5Cright%5C%7D"></li>
<li>The equation <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%20=%20b%7D"> is only solvable when <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D"> is on the line.</li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20B%20=%20%5Cbegin%7Bbmatrix%7D1%20&amp;%202%20&amp;%204%5C%5C0%20&amp;%200%20&amp;%204%20%5Cend%7Bbmatrix%7D">
<ul>
<li>These column vectors span the column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D">. Since column 2 is a scalar multiple of column 1, and column 3 is a linear combination of columns 1 and 2, the column space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D"> is the span of the first two column vectors, which is a two-dimensional subspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E2">, <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bcol%7D(%5Cmathbf%20B)%20=%20%5Ctext%7Bspan%7D%5C%7B%5Cmathbf%7Bb%7D_1,%20%5Cmathbf%7Bb%7D_2,%20%5Cmathbf%7Bb%7D_3%5C%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%20R%5E2">.</li>
</ul></li>
</ul>
</section>
<section id="properties-1" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="properties-1"><span class="header-section-number">1.3.3</span> Properties</h3>
<p>The column space of a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bcol%7D(%5Cmathbf%7BA%7D)"> or <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bspan%7D%7B%5Cmathbf%7Ba%7D_1,%20%5Cmathbf%7Ba%7D_2,%20%5Cldots,%20%5Cmathbf%7Ba%7D_n%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Ba%7D_i"> represents the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, has several important properties:</p>
<p>Consists of all possible linear combinations: The column space consists of all possible linear combinations of the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. In other words, any vector in <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bcol%7D(%5Cmathbf%7BA%7D)"> can be expressed as a linear combination of the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<section id="is-a-subspace" class="level4" data-number="1.3.3.1">
<h4 data-number="1.3.3.1" class="anchored" data-anchor-id="is-a-subspace"><span class="header-section-number">1.3.3.1</span> Is a subspace</h4>
<p>The column space is a subspace of the vector space <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Em">, meaning it is closed under vector addition and scalar multiplication. This property allows for the column space to be used in linear combinations and as a solution space for non-homogeneous linear equations.</p>
</section>
<section id="is-spanned-by-the-columns-of-mathbfa" class="level4" data-number="1.3.3.2">
<h4 data-number="1.3.3.2" class="anchored" data-anchor-id="is-spanned-by-the-columns-of-mathbfa"><span class="header-section-number">1.3.3.2</span> Is spanned by the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"></h4>
<p>The column space is spanned by the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, meaning any vector in <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bcol%7D(%5Cmathbf%7BA%7D)"> can be expressed as a linear combination of the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
</section>
<section id="has-the-same-dimensionality-as-the-row-space" class="level4" data-number="1.3.3.3">
<h4 data-number="1.3.3.3" class="anchored" data-anchor-id="has-the-same-dimensionality-as-the-row-space"><span class="header-section-number">1.3.3.3</span> Has the same dimensionality as the row space</h4>
<p>The dimensionality of the column space, denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdim%7D(%5Ctext%7Bcol%7D(%5Cmathbf%7BA%7D))">, is equal to the dimensionality of the row space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdim%7D(%5Ctext%7Brow%7D(%5Cmathbf%7BA%7D))">. This property is known as the rank-nullity theorem.</p>
</section>
<section id="has-the-same-dimensionality-as-the-row-space-1" class="level4" data-number="1.3.3.4">
<h4 data-number="1.3.3.4" class="anchored" data-anchor-id="has-the-same-dimensionality-as-the-row-space-1"><span class="header-section-number">1.3.3.4</span> Has the same dimensionality as the row space</h4>
<p>The dimensionality of the column space, denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdim%7D(%5Ctext%7Bcol%7D(%5Cmathbf%7BA%7D))">, is equal to the dimensionality of the row space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdim%7D(%5Ctext%7Brow%7D(%5Cmathbf%7BA%7D))">. This property is known as the rank-nullity theorem. Is orthogonal to the nullspace: The column space is orthogonal to the nullspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, meaning any vector in the column space is orthogonal to any vector in the nullspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. This property is known as the fundamental theorem of linear algebra.</p>
</section>
</section>
</section>
<section id="nullspace-of-solving" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="nullspace-of-solving"><span class="header-section-number">1.4</span> Nullspace of : Solving </h2>
<section id="definition-3" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="definition-3"><span class="header-section-number">1.4.1</span> Definition</h3>
<div id="def-nullspace" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 </strong></span>The nullspace of a matrix refers to the set of all vectors that, when multiplied by the matrix, result in the zero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D">. For a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, the nullspace is denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bnull%7D(%5Cmathbf%7BA%7D)"> and defined as follows: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bnull%7D(%5Cmathbf%7BA%7D)%20=%20%5C%7B%5Cmathbf%7Bx%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En%20%5Cmid%20%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7B0%7D%5C%7D%0A"></p>
<p>The nullspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is the set of all solutions <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> to the homogeneous equation <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7B0%7D">.</p>
</div>
<p>All the solutions <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20%5Cin%20%5Cmathbb%20R%5En">, so the <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bnull%7D(%5Cmathbf%7BA%7D)"> is a subspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%20R%5En">, and <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCol%7D(%5Cmathbf%20A)"> is a subspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%20R%5Em"> (from Gilbere Strang) If the right side <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D"> is not <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D">, the solutions of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx=b%7D"> do not form a subspace. The vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx=0%7D"> is only a solution if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb=0%7D">. When the set of solutions does not include <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx=0%7D">, it cannot be a subspace. Section 3.4 will show how the solutions to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx=b%7D"> (if there are any solutions) are shifted away from the origin by one particular solution.</p>
</section>
<section id="example-4" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="example-4"><span class="header-section-number">1.4.2</span> Example</h3>
<section id="example-1-1" class="level4" data-number="1.4.2.1">
<h4 data-number="1.4.2.1" class="anchored" data-anchor-id="example-1-1"><span class="header-section-number">1.4.2.1</span> Example 1</h4>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20&amp;%203%20%5Cend%7Bbmatrix%7D%20%5Cquad%20%5Cmathbf%7BAx=0%7D%20%5C%5C%0A%5Ctext%7Bnull%7D(%5Cmathbf%7BA%7D)%20=%20%5C%7B%5Cmathbf%7Bx%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En%20%5Cmid%20%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7B0%7D%5C%7D%20%5Ctext%7B%20is%20a%20subspace%20of%20%7D%5Cmathbb%20R%5E3%0A%5Cend%7Balign*%7D%0A"></p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;"># Define the coefficients of the planes</span></span>
<span id="cb2-2">a1, b1, c1, d1 <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb2-3">a2, b2, c2, d2 <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">6</span></span>
<span id="cb2-4"></span>
<span id="cb2-5">x <span class="op" style="color: #5E5E5E;">=</span> np.linspace(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb2-6">y <span class="op" style="color: #5E5E5E;">=</span> np.linspace(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb2-7">X, Y <span class="op" style="color: #5E5E5E;">=</span> np.meshgrid(x, y)</span>
<span id="cb2-8"></span>
<span id="cb2-9"><span class="co" style="color: #5E5E5E;"># Compute the corresponding z values for each x, y pair for the first plane</span></span>
<span id="cb2-10">Z1 <span class="op" style="color: #5E5E5E;">=</span> (<span class="op" style="color: #5E5E5E;">-</span>a1<span class="op" style="color: #5E5E5E;">*</span>X <span class="op" style="color: #5E5E5E;">-</span> b1<span class="op" style="color: #5E5E5E;">*</span>Y <span class="op" style="color: #5E5E5E;">-</span> d1) <span class="op" style="color: #5E5E5E;">/</span> c1</span>
<span id="cb2-11"></span>
<span id="cb2-12"><span class="co" style="color: #5E5E5E;"># Compute the corresponding z values for each x, y pair for the second plane</span></span>
<span id="cb2-13">Z2 <span class="op" style="color: #5E5E5E;">=</span> (<span class="op" style="color: #5E5E5E;">-</span>a2<span class="op" style="color: #5E5E5E;">*</span>X <span class="op" style="color: #5E5E5E;">-</span> b2<span class="op" style="color: #5E5E5E;">*</span>Y <span class="op" style="color: #5E5E5E;">-</span> d2) <span class="op" style="color: #5E5E5E;">/</span> c2</span>
<span id="cb2-14"></span>
<span id="cb2-15"><span class="co" style="color: #5E5E5E;"># Check if the origin is in the blue plane</span></span>
<span id="cb2-16">origin_in_blue_plane <span class="op" style="color: #5E5E5E;">=</span> a1<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">0</span> <span class="op" style="color: #5E5E5E;">+</span> b1<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">0</span> <span class="op" style="color: #5E5E5E;">+</span> c1<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">0</span> <span class="op" style="color: #5E5E5E;">+</span> d1 <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb2-17"></span>
<span id="cb2-18"><span class="co" style="color: #5E5E5E;"># Check if the origin is in the red plane</span></span>
<span id="cb2-19">origin_in_red_plane <span class="op" style="color: #5E5E5E;">=</span> a2<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">0</span> <span class="op" style="color: #5E5E5E;">+</span> b2<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">0</span> <span class="op" style="color: #5E5E5E;">+</span> c2<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">0</span> <span class="op" style="color: #5E5E5E;">+</span> d2 <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb2-20"></span>
<span id="cb2-21"><span class="co" style="color: #5E5E5E;"># Create a 3D plot</span></span>
<span id="cb2-22">fig <span class="op" style="color: #5E5E5E;">=</span> plt.figure()</span>
<span id="cb2-23">ax <span class="op" style="color: #5E5E5E;">=</span> fig.add_subplot(<span class="dv" style="color: #AD0000;">111</span>, projection<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'3d'</span>)</span>
<span id="cb2-24"></span>
<span id="cb2-25"><span class="co" style="color: #5E5E5E;"># Plot the first plane</span></span>
<span id="cb2-26">ax.plot_surface(X, Y, Z1, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'blue'</span>, alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>)</span>
<span id="cb2-27"></span>
<span id="cb2-28"><span class="co" style="color: #5E5E5E;"># Plot the second plane</span></span>
<span id="cb2-29">ax.plot_surface(X, Y, Z2, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'red'</span>, alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>)</span>
<span id="cb2-30"></span>
<span id="cb2-31"><span class="co" style="color: #5E5E5E;"># Plot the origin as a black point</span></span>
<span id="cb2-32">ax.scatter(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'black'</span>, s<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb2-33"></span>
<span id="cb2-34"></span>
<span id="cb2-35"><span class="co" style="color: #5E5E5E;"># Highlight the origin in the blue plane</span></span>
<span id="cb2-36"><span class="cf" style="color: #003B4F;">if</span> origin_in_blue_plane:</span>
<span id="cb2-37">    ax.scatter(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'blue'</span>, s<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="vs" style="color: #20794D;">r'Origin in the $x+2y+3z=0$ Plane'</span>)</span>
<span id="cb2-38"></span>
<span id="cb2-39"><span class="co" style="color: #5E5E5E;"># Highlight the origin in the red plane</span></span>
<span id="cb2-40"><span class="cf" style="color: #003B4F;">if</span> origin_in_red_plane:</span>
<span id="cb2-41">    ax.scatter(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'red'</span>, s<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="vs" style="color: #20794D;">r'Origin in the $x+2y+3z=6$ Plane'</span>)</span>
<span id="cb2-42"></span>
<span id="cb2-43"><span class="co" style="color: #5E5E5E;"># Set labels and title</span></span>
<span id="cb2-44">ax.set_xlabel(<span class="st" style="color: #20794D;">'X'</span>)</span>
<span id="cb2-45">ax.set_ylabel(<span class="st" style="color: #20794D;">'Y'</span>)</span>
<span id="cb2-46">ax.set_zlabel(<span class="st" style="color: #20794D;">'Z'</span>)</span>
<span id="cb2-47">ax.set_title(<span class="vs" style="color: #20794D;">r'Example of Nullspace: $x+2y+3z=0$ vs $x+2y+3z=6$'</span>)</span>
<span id="cb2-48"></span>
<span id="cb2-49"><span class="co" style="color: #5E5E5E;"># Add a legend</span></span>
<span id="cb2-50">ax.legend()</span>
<span id="cb2-51"></span>
<span id="cb2-52"><span class="co" style="color: #5E5E5E;"># Show the plot</span></span>
<span id="cb2-53">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/05.vector_space_files/figure-html/cell-3-output-1.png" width="464" height="411"></p>
</div>
</div>
</section>
<section id="example-2-1" class="level4" data-number="1.4.2.2">
<h4 data-number="1.4.2.2" class="anchored" data-anchor-id="example-2-1"><span class="header-section-number">1.4.2.2</span> Example 2</h4>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%203%20&amp;%206%20%5C%5C%202%20&amp;%204%20%5Cend%7Bbmatrix%7D%20%5Cquad%0A%5Ctext%7Bnull%7D(%5Cmathbf%7BA%7D)%20=%20%5Cleft%5C%7B%20%5Cbegin%7Bbmatrix%7D%20-2t%20%5C%5C%20t%20%5Cend%7Bbmatrix%7D%20%5Cmid%20t%20%5Cin%20%5Cmathbb%7BR%7D%20%5Cright%5C%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p>The null space of a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bnull%7D(%5Cmathbf%7BA%7D)">, is the set of all vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> that satisfy the equation <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7B0%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D"> is the zero vector.</p>
<p>For the given matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%203%20&amp;%206%20%5C%5C%202%20&amp;%204%20%5Cend%7Bbmatrix%7D">, we can find the null space by solving the equation <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7B0%7D">.</p>
<p>Let’s set up the augmented matrix <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7BA%7D%20%7C%20%5Cmathbf%7B0%7D%5D"> and perform Gaussian elimination to find the row-reduced echelon form (RREF):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5B%20%5Cbegin%7Barray%7D%7Bccc%7Cc%7D%0A1%20&amp;%202%20&amp;%200%20&amp;%200%20%5C%5C%0A3%20&amp;%206%20&amp;%200%20&amp;%200%20%5C%5C%0A2%20&amp;%204%20&amp;%200%20&amp;%200%20%5C%5C%0A%5Cend%7Barray%7D%20%5Cright%5D%0A"></p>
<p>Row 2 is equal to Row 1 multiplied by 3, and Row 3 is equal to Row 1 multiplied by 2. Thus, Row 3 is redundant and can be eliminated.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%7Bccc%7Cc%7D%0A1%20&amp;%202%20&amp;%200%20&amp;%200%20%5C%5C%0A3%20&amp;%206%20&amp;%200%20&amp;%200%20%5C%5C%0A2%20&amp;%204%20&amp;%200%20&amp;%200%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%203%20&amp;%206%20%5C%5C%202%20&amp;%204%20%5Cend%7Bbmatrix%7D%20%5Cquad%0A%5Ctext%7Bnull%7D(%5Cmathbf%7BA%7D)%20=%20%5Cleft%5C%7B%20%5Cbegin%7Bbmatrix%7D%20-2t%20%5C%5C%20t%20%5Cend%7Bbmatrix%7D%20%5Cmid%20t%20%5Cin%20%5Cmathbb%7BR%7D%20%5Cright%5C%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbf%7BB%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%200%20&amp;%200%20%5C%5C%200%20&amp;%200%20&amp;%201%20%5Cend%7Bbmatrix%7D%20%5Cquad%0A%5Ctext%7Bnull%7D(%5Cmathbf%7BB%7D)%20=%20%5Cleft%5C%7B%20%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%20t%20%5C%5C%200%20%5Cend%7Bbmatrix%7D%20%5Cmid%20t%20%5Cin%20%5Cmathbb%7BR%7D%20%5Cright%5C%7D%0A%5Cend%7Balign*%7D%0A"></p>
</section>
</section>
<section id="properties-2" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="properties-2"><span class="header-section-number">1.4.3</span> Properties</h3>
<section id="nullspace-contains-the-zero-vector" class="level4" data-number="1.4.3.1">
<h4 data-number="1.4.3.1" class="anchored" data-anchor-id="nullspace-contains-the-zero-vector"><span class="header-section-number">1.4.3.1</span> Nullspace Contains the Zero Vector</h4>
<p>The nullspace always contains the zero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D">, since <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5Cmathbf%7B0%7D%20=%20%5Cmathbf%7B0%7D">.</p>
</section>
<section id="nullspace-is-a-subspace" class="level4" data-number="1.4.3.2">
<h4 data-number="1.4.3.2" class="anchored" data-anchor-id="nullspace-is-a-subspace"><span class="header-section-number">1.4.3.2</span> Nullspace Is a Subspace</h4>
<p>The nullspace is a subspace of the vector space <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En">, meaning it is closed under vector addition and scalar multiplication. This property allows for the nullspace to be used in linear combinations and as a solution space for homogeneous linear equations.</p>
</section>
<section id="nullspace-is-orthogonal-to-row-space" class="level4" data-number="1.4.3.3">
<h4 data-number="1.4.3.3" class="anchored" data-anchor-id="nullspace-is-orthogonal-to-row-space"><span class="header-section-number">1.4.3.3</span> Nullspace Is Orthogonal to Row Space</h4>
<p>The nullspace is orthogonal to the row space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, meaning any vector in the nullspace is orthogonal to all vectors in the row space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
</section>
<section id="nullspace-can-be-non-mpty" class="level4" data-number="1.4.3.4">
<h4 data-number="1.4.3.4" class="anchored" data-anchor-id="nullspace-can-be-non-mpty"><span class="header-section-number">1.4.3.4</span> Nullspace Can Be Non-mpty</h4>
<p>The nullspace can be non-empty, meaning there can be non-trivial solutions to the homogeneous equation <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7B0%7D">.</p>
</section>
<section id="dimensionality" class="level4" data-number="1.4.3.5">
<h4 data-number="1.4.3.5" class="anchored" data-anchor-id="dimensionality"><span class="header-section-number">1.4.3.5</span> Dimensionality</h4>
<p>The dimensionality of the nullspace, denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdim%7D(%5Ctext%7Bnull%7D(%5Cmathbf%7BA%7D))">, is also known as the nullity of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. The nullity of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is equal to the number of linearly independent columns or the number of free variables in the row-reduced echelon form (RREF) of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
</section>
</section>
</section>
<section id="rank-and-row-reduced-form" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="rank-and-row-reduced-form"><span class="header-section-number">1.5</span> Rank and Row Reduced Form</h2>
</section>
<section id="complete-solution" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="complete-solution"><span class="header-section-number">1.6</span> Complete Solution</h2>
</section>
<section id="independence-basis-and-dimension" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="independence-basis-and-dimension"><span class="header-section-number">1.7</span> Independence, Basis, and Dimension</h2>
</section>
<section id="dimensions-of-four-subspaces" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="dimensions-of-four-subspaces"><span class="header-section-number">1.8</span> Dimensions of Four Subspaces</h2>
</section>
<section id="basis-vector" class="level2" data-number="1.9">
<h2 data-number="1.9" class="anchored" data-anchor-id="basis-vector"><span class="header-section-number">1.9</span> Basis Vector</h2>
<p>A basis for a vector space <img src="https://latex.codecogs.com/png.latex?V"> is a set of vectors <img src="https://latex.codecogs.com/png.latex?%7Bv_1,%20v_2,%20%5Cldots,%20v_n%7D"> that spans <img src="https://latex.codecogs.com/png.latex?V"> and is linearly independent. In other words, every vector in <img src="https://latex.codecogs.com/png.latex?V"> can be expressed as a linear combination of the basis vectors, and the basis vectors are linearly independent, meaning that no basis vector can be written as a linear combination of the other basis vectors.</p>
<p>Mathematically, a set of vectors <img src="https://latex.codecogs.com/png.latex?%7Bv_1,%20v_2,%20%5Cldots,%20v_n%7D"> is a basis for a vector space <img src="https://latex.codecogs.com/png.latex?V"> if it satisfies the following conditions:</p>
<p>The vectors <img src="https://latex.codecogs.com/png.latex?%7Bv_1,%20v_2,%20%5Cldots,%20v_n%7D"> span <img src="https://latex.codecogs.com/png.latex?V">, which means that for any vector <img src="https://latex.codecogs.com/png.latex?v"> in <img src="https://latex.codecogs.com/png.latex?V">, there exist scalars <img src="https://latex.codecogs.com/png.latex?c_1,%20c_2,%20%5Cldots,%20c_n"> such that <img src="https://latex.codecogs.com/png.latex?v%20=%20c_1%20v_1%20+%20c_2%20v_2%20+%20%5Cldots%20+%20c_n%20v_n">.</p>
<p>The vectors <img src="https://latex.codecogs.com/png.latex?%7Bv_1,%20v_2,%20%5Cldots,%20v_n%7D"> are linearly independent, which means that the only solution to the equation <img src="https://latex.codecogs.com/png.latex?c_1%20v_1%20+%20c_2%20v_2%20+%20%5Cldots%20+%20c_n%20v_n%20=%200"> is <img src="https://latex.codecogs.com/png.latex?c_1%20=%20c_2%20=%20%5Cldots%20=%20c_n%20=%200">.</p>
<section id="example-5" class="level3" data-number="1.9.1">
<h3 data-number="1.9.1" class="anchored" data-anchor-id="example-5"><span class="header-section-number">1.9.1</span> Example</h3>
<p>Consider the vector space <img src="https://latex.codecogs.com/png.latex?V%20=%20%5Cmathbb%7BR%7D%5E3">, the set of all real-valued vectors with three components. A basis for <img src="https://latex.codecogs.com/png.latex?V"> can be given by the following set of vectors:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Av_1%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20%5C%5C%200%20%5C%5C%200%0A%5Cend%7Bbmatrix%7D,%20%5Cquad%0Av_2%20=%20%5Cbegin%7Bbmatrix%7D%0A0%20%5C%5C%201%20%5C%5C%200%0A%5Cend%7Bbmatrix%7D,%20%5Cquad%0Av_3%20=%20%5Cbegin%7Bbmatrix%7D%0A0%20%5C%5C%200%20%5C%5C%201%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>These three vectors form a basis for <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3">, as they span <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3"> (any vector in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3"> can be expressed as a linear combination of these vectors) and they are linearly independent (the only solution to the equation <img src="https://latex.codecogs.com/png.latex?c_1%20v_1%20+%20c_2%20v_2%20+%20c_3%20v_3%20=%200"> is <img src="https://latex.codecogs.com/png.latex?c_1%20=%20c_2%20=%20c_3%20=%200">).</p>
<p>Basis vectors are fundamental in defining and understanding vector spaces because they provide a way to express any vector in the vector space as a linear combination of basis vectors. The properties of basis vectors, such as linear independence, spanning set, minimal set, and unique representation, are essential in establishing the foundational concepts of vector spaces and their properties. By using basis vectors, we can represent vectors in a vector space in a concise and systematic way, and they provide a basis for studying and analyzing vector spaces in various mathematical and practical applications.</p>
</section>
<section id="properties-3" class="level3" data-number="1.9.2">
<h3 data-number="1.9.2" class="anchored" data-anchor-id="properties-3"><span class="header-section-number">1.9.2</span> Properties</h3>
<ul>
<li>Linear independence: A set of basis vectors is linearly independent, meaning that no vector in the set can be expressed as a linear combination of the others. In other words, the coefficients of the basis vectors in any linear combination that equals the zero vector are all zero.</li>
<li>Spanning set: The set of basis vectors spans the entire vector space, meaning that any vector in the vector space can be expressed as a linear combination of the basis vectors. In other words, the basis vectors “span” the vector space by forming a basis for it.</li>
<li>Minimal set: The set of basis vectors is minimal, meaning that no vector can be removed from the set without changing the span of the vector space. In other words, the basis vectors form the smallest possible set that can generate the entire vector space.</li>
<li>Unique representation: Any vector in the vector space can be uniquely represented as a linear combination of the basis vectors. This means that there is only one way to express a vector as a linear combination of the basis vectors, ensuring that the representation is unique.</li>
</ul>
</section>
</section>
<section id="subspace" class="level2" data-number="1.10">
<h2 data-number="1.10" class="anchored" data-anchor-id="subspace"><span class="header-section-number">1.10</span> Subspace</h2>
<section id="definition-of-a-subspace" class="level3" data-number="1.10.1">
<h3 data-number="1.10.1" class="anchored" data-anchor-id="definition-of-a-subspace"><span class="header-section-number">1.10.1</span> Definition of a subspace</h3>
<p>A subset <img src="https://latex.codecogs.com/png.latex?W"> of <img src="https://latex.codecogs.com/png.latex?V"> is called a subspace of <img src="https://latex.codecogs.com/png.latex?V"> if it satisfies the following three conditions: 1. <img src="https://latex.codecogs.com/png.latex?W"> contains the zero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D"> of <img src="https://latex.codecogs.com/png.latex?V">. 2. <img src="https://latex.codecogs.com/png.latex?W"> is closed under vector addition, i.e., for any vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D,%20%5Cmathbf%7Bv%7D%20%5Cin%20W">, their sum <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D"> is also in <img src="https://latex.codecogs.com/png.latex?W">. 3. <img src="https://latex.codecogs.com/png.latex?W"> is closed under scalar multiplication, i.e., for any vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20%5Cin%20W"> and any scalar <img src="https://latex.codecogs.com/png.latex?c">, their product <img src="https://latex.codecogs.com/png.latex?c%5Cmathbf%7Bu%7D"> is also in <img src="https://latex.codecogs.com/png.latex?W">.</p>
</section>
<section id="example-6" class="level3" data-number="1.10.2">
<h3 data-number="1.10.2" class="anchored" data-anchor-id="example-6"><span class="header-section-number">1.10.2</span> Example</h3>
<p>Consider the vector space <img src="https://latex.codecogs.com/png.latex?V%20=%20%5Cmathbb%7BR%7D%5E3"> with standard vector addition and scalar multiplication. Let <img src="https://latex.codecogs.com/png.latex?W"> be the subset of <img src="https://latex.codecogs.com/png.latex?V"> consisting of all vectors of the form <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%20x%20%5C%5C%20y%20%5C%5C%200%20%5Cend%7Bbmatrix%7D">, where <img src="https://latex.codecogs.com/png.latex?x,%20y"> are real numbers. We can express <img src="https://latex.codecogs.com/png.latex?W"> as: <img src="https://latex.codecogs.com/png.latex?W%20=%20%5Cleft%5C%7B%20%5Cbegin%7Bbmatrix%7D%20x%20%5C%5C%20y%20%5C%5C%200%20%5Cend%7Bbmatrix%7D%20%5C,%20%5Cmiddle%7C%20%5C,%20x,%20y%20%5Cin%20%5Cmathbb%7BR%7D%20%5Cright%5C%7D"> Then <img src="https://latex.codecogs.com/png.latex?W"> is a subspace of <img src="https://latex.codecogs.com/png.latex?V"> because it satisfies the three conditions mentioned above.</p>
</section>
<section id="properties-4" class="level3" data-number="1.10.3">
<h3 data-number="1.10.3" class="anchored" data-anchor-id="properties-4"><span class="header-section-number">1.10.3</span> Properties</h3>
<p>A subspace is a subset of a vector space that retains the structure of a vector space. Here are some properties of a subspace in relation to a vector space:</p>
<ol type="1">
<li>Contains the zero vector: A subspace must contain the zero vector (denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D">) of the vector space it is a subset of. This is because the zero vector is required for closure under vector addition and scalar multiplication.</li>
<li>Closed under vector addition: If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D,%20%5Cmathbf%7Bv%7D%20%5Cin%20W">, then <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D%20%5Cin%20W">.</li>
<li>Closed under scalar multiplication: If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20%5Cin%20W"> and <img src="https://latex.codecogs.com/png.latex?c"> is a scalar, then <img src="https://latex.codecogs.com/png.latex?c%5Cmathbf%7Bu%7D%20%5Cin%20W">.</li>
</ol>
<p>These properties provide a way to describe subsets of vector spaces that inherit certain properties from the parent vector space. Subspaces are useful for understanding the structure, properties, and behavior of vector spaces in a more focused and simplified manner. Some reasons why we need subspaces are:</p>
<ul>
<li>Simplification and abstraction: Subspaces allow us to simplify the study of vector spaces by focusing on smaller, more manageable subsets that share similar properties. This abstraction can help us understand the fundamental structure and behavior of vector spaces without getting bogged down by the complexity of the entire vector space.</li>
<li>Study of special cases: Subspaces can represent special cases or special structures within a vector space that are of particular interest.
<ul>
<li>For example, subspaces can represent sub-spaces of solutions to linear systems of equations, eigenspaces associated with eigenvalues of matrices, or orthogonal subspaces related to orthogonality and projections.</li>
</ul></li>
<li>Applications in various fields: Subspaces have numerous applications in various fields, such as physics, computer graphics, data analysis, signal processing, and optimization, among others. Subspaces provide a mathematical framework for modeling, analyzing, and solving problems in these fields.</li>
<li>Computational efficiency: Subspaces can be used in algorithms and techniques for solving problems involving vector spaces in a computationally efficient manner. Techniques such as subspace methods, subspace approximation, and subspace projection can be employed to reduce the computational complexity of certain problems by working within lower-dimensional subspaces.</li>
</ul>
</section>
<section id="span" class="level3" data-number="1.10.4">
<h3 data-number="1.10.4" class="anchored" data-anchor-id="span"><span class="header-section-number">1.10.4</span> Span</h3>
</section>
<section id="definition-4" class="level3" data-number="1.10.5">
<h3 data-number="1.10.5" class="anchored" data-anchor-id="definition-4"><span class="header-section-number">1.10.5</span> Definition</h3>
<p>The span of a set of vectors <img src="https://latex.codecogs.com/png.latex?%7Bv_1,%20v_2,%20%5Cldots,%20v_n%7D"> in a vector space <img src="https://latex.codecogs.com/png.latex?V">, denoted by <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bspan%7D%7Bv_1,%20v_2,%20%5Cldots,%20v_n%7D">, is the set of all possible linear combinations of these vectors. Mathematically, it is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bspan%7D%5C%7B%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cldots,%20%5Cmathbf%7Bv%7D_n%5C%7D%20=%20%5Cleft%5C%7B%20c_1%5Cmathbf%7Bv%7D_1%20+%20c_2%5Cmathbf%7Bv%7D_2%20+%20%5Cldots%20+%20c_n%5Cmathbf%7Bv%7D_n%20%7C%20c_1,%20c_2,%20%5Cldots,%20c_n%20%5Cin%20%5Cmathbb%7BR%7D%20%5Cright%5C%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?c_1,%20c_2,%20%5Cldots,%20c_n"> are scalar coefficients and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> represents the set of real numbers.</p>
<p>Example: Let’s consider the set of vectors <img src="https://latex.codecogs.com/png.latex?v_1">, <img src="https://latex.codecogs.com/png.latex?v_2">, and <img src="https://latex.codecogs.com/png.latex?v_3"> defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%200%20%5C%200%20%5Cend%7Bbmatrix%7D,%20%5Cquad%20%5Cmathbf%7Bv%7D_2%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%201%20%5C%200%20%5Cend%7Bbmatrix%7D,%20%5Cquad%20%5Cmathbf%7Bv%7D_3%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%200%20%5C%201%20%5Cend%7Bbmatrix%7D"></p>
<p>The span of these vectors, denoted by <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bspan%7D%7B%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cmathbf%7Bv%7D_3%7D">, is the set of all possible linear combinations of these vectors, which in this case is the entire three-dimensional vector space <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3">, since any vector in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3"> can be expressed as a linear combination of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_2">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_3">.</p>
</section>
<section id="proporties" class="level3" data-number="1.10.6">
<h3 data-number="1.10.6" class="anchored" data-anchor-id="proporties"><span class="header-section-number">1.10.6</span> Proporties</h3>
<ul>
<li>Span is a subspace: The span of a set of vectors is always a subspace of the vector space in which the vectors belong. This means that it satisfies all the properties of a vector space, including closure under vector addition and scalar multiplication.</li>
<li>Span is the smallest subspace: The span of a set of vectors is the smallest subspace that contains all the vectors in the set. It is the intersection of all subspaces that contain the vectors, and thus it forms the smallest subspace that spans the set of vectors.</li>
<li>Span is closed under linear combinations: The span of a set of vectors is closed under linear combinations of the vectors. This means that any linear combination of the vectors in the set will also be in the span.</li>
<li>Span generates the entire vector space: The span of a set of vectors is capable of generating the entire vector space to which the vectors belong. This means that any vector in the vector space can be expressed as a linear combination of the vectors in the span.</li>
<li>Span is unique: The span of a set of vectors is unique, meaning that it is uniquely determined by the set of vectors being spanned. However, the span may be different for different sets of vectors.</li>
</ul>
<p>These properties make the concept of span important in linear algebra, as it allows us to understand the space spanned by a set of vectors and the relationships between vectors within a vector space.</p>
<p>The concept of span is important in linear algebra because of:</p>
<ul>
<li>Understanding vector space: The span of a set of vectors helps us understand the space that can be generated by those vectors within a vector space. It provides insight into the range of possible values and combinations that can be obtained using the given set of vectors.</li>
<li>Solving systems of linear equations: Span is closely related to solving systems of linear equations. The solutions to a system of linear equations can be expressed as linear combinations of the vectors in the span of the coefficient matrix. By finding the span of a set of vectors, we can determine the possible solutions to a system of linear equations and understand the relationship between different solutions.</li>
<li>Basis for vector spaces: The span of a set of vectors can form a basis for a vector space. A basis is a set of linearly independent vectors that span the entire vector space. By finding the span of a set of vectors, we can determine if they form a basis for a vector space, and if so, use them as a foundation for understanding and manipulating vectors within that space.</li>
<li>Vector space operations: Span is closed under vector space operations, such as vector addition and scalar multiplication. This property allows us to perform operations on vectors within the span and obtain new vectors that are still within the span. It also allows us to express any vector in the vector space as a linear combination of vectors in the span.</li>
<li>Dimensionality and rank: The span of a set of vectors is closely related to the dimensionality and rank of a vector space. The dimension of a vector space is the number of linearly independent vectors in a basis for that space, and the rank of a matrix is the dimension of the span of its column vectors. Understanding the span of vectors can help us determine the dimensionality and rank of a vector space, which has applications in areas such as data analysis, machine learning, and signal processing.</li>
</ul>
<p>Span provides insights into the space spanned by a set of vectors, the relationships between vectors within a vector space, and the possible solutions to systems of linear equations. It also serves as a basis for vector spaces and facilitates vector space operations, and is closely related to the dimensionality and rank of a vector space.</p>
</section>
</section>
<section id="dimensionality-and-rank" class="level2" data-number="1.11">
<h2 data-number="1.11" class="anchored" data-anchor-id="dimensionality-and-rank"><span class="header-section-number">1.11</span> Dimensionality and Rank</h2>
<section id="definition-5" class="level3" data-number="1.11.1">
<h3 data-number="1.11.1" class="anchored" data-anchor-id="definition-5"><span class="header-section-number">1.11.1</span> Definition</h3>
<ul>
<li>The dimensionality of a vector space is defined as the number of linearly independent vectors in its basis.</li>
<li>The rank of a matrix is defined as the maximum number of linearly independent rows or columns in the matrix.</li>
</ul>
</section>
<section id="example-7" class="level3" data-number="1.11.2">
<h3 data-number="1.11.2" class="anchored" data-anchor-id="example-7"><span class="header-section-number">1.11.2</span> Example</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?V"> be a vector space with a basis <img src="https://latex.codecogs.com/png.latex?%7B%20%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cldots,%20%5Cmathbf%7Bv%7D_n%20%7D">. The dimensionality of <img src="https://latex.codecogs.com/png.latex?V">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdim%7D(V)">, is the number of linearly independent vectors in its basis, which is equal to <img src="https://latex.codecogs.com/png.latex?n">. Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix. The rank of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Brank%7D(%5Cmathbf%20A)">, is the maximum number of linearly independent rows or columns in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">. It can also be defined as the dimensionality of the column space or row space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">.</p>
</section>
<section id="properties-5" class="level3" data-number="1.11.3">
<h3 data-number="1.11.3" class="anchored" data-anchor-id="properties-5"><span class="header-section-number">1.11.3</span> Properties</h3>
<section id="dimensionality-1" class="level4" data-number="1.11.3.1">
<h4 data-number="1.11.3.1" class="anchored" data-anchor-id="dimensionality-1"><span class="header-section-number">1.11.3.1</span> Dimensionality</h4>
<ul>
<li>Dimensionality refers to the number of elements or components in a vector or the size of a vector space.</li>
<li>The dimensionality of a vector space is always a positive integer.</li>
<li>Dimensionality is additive, meaning that the dimensionality of the direct sum of vector spaces is equal to the sum of their individual dimensionality.</li>
<li>A set of vectors is linearly independent if and only if the dimensionality of the vector space they span is equal to the number of vectors in the set.</li>
</ul>
</section>
<section id="rank" class="level4" data-number="1.11.3.2">
<h4 data-number="1.11.3.2" class="anchored" data-anchor-id="rank"><span class="header-section-number">1.11.3.2</span> Rank</h4>
<ul>
<li>The rank of a matrix is the maximum number of linearly independent rows or columns in the matrix.</li>
<li>The rank of a matrix is always a non-negative integer.</li>
<li>The rank of a matrix is equal to the dimensionality of its column space and row space.</li>
<li>The rank of a matrix is invariant under elementary row and column operations.</li>
<li>The rank of a matrix is less than or equal to the minimum of the number of rows and columns in the matrix.</li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In some contexts, the term “dimensionality” may also refer to the dimensionality of the column space or row space of a matrix, which is equivalent to the rank of the matrix.</p>
</div>
</div>
</section>
</section>
</section>
<section id="column-space" class="level2" data-number="1.12">
<h2 data-number="1.12" class="anchored" data-anchor-id="column-space"><span class="header-section-number">1.12</span> Column Space</h2>
<section id="definition-6" class="level3" data-number="1.12.1">
<h3 data-number="1.12.1" class="anchored" data-anchor-id="definition-6"><span class="header-section-number">1.12.1</span> Definition</h3>
<p>The column space of a matrix is the subspace spanned by its column vectors. It is denoted by <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BCol%7D(%5Cmathbf%20A)"> or <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bspan%7D%7B%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cldots,%20%5Cmathbf%7Bv%7D_n%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cldots,%20%5Cmathbf%7Bv%7D_n"> are the column vectors of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">.</p>
</section>
<section id="example-8" class="level3" data-number="1.12.2">
<h3 data-number="1.12.2" class="anchored" data-anchor-id="example-8"><span class="header-section-number">1.12.2</span> Example</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%20%5C%5C%0A4%20&amp;%205%20&amp;%206%20%5C%5C%0A7%20&amp;%208%20&amp;%209%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>The column space of matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, denoted by <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BCol%7D(%5Cmathbf%20A)"> or <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bspan%7D%5C%7B%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cmathbf%7Bv%7D_3%5C%7D">, is the subspace spanned by its column vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1%20=%20%5B1,%204,%207%5D%5ET">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_2%20=%20%5B2,%205,%208%5D%5ET">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_3%20=%20%5B3,%206,%209%5D%5ET">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BCol%7D(%5Cmathbf%7BA%7D)%20=%20%5Ctext%7Bspan%7D%5Cleft%5C%7B%0A%5Cbegin%7Bbmatrix%7D%0A1%20%5C%5C%204%20%5C%5C%207%20%5C%5C%0A%5Cend%7Bbmatrix%7D,%0A%5Cbegin%7Bbmatrix%7D%0A2%20%5C%5C%205%20%5C%5C%208%20%5C%5C%0A%5Cend%7Bbmatrix%7D,%0A%5Cbegin%7Bbmatrix%7D%0A3%20%5C%5C%206%20%5C%5C%209%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cright%5C%7D%0A"></p>
</section>
<section id="properties-6" class="level3" data-number="1.12.3">
<h3 data-number="1.12.3" class="anchored" data-anchor-id="properties-6"><span class="header-section-number">1.12.3</span> Properties</h3>
<ul>
<li>It is a subspace: The column space of a matrix is a subspace of the vector space in which the matrix’s columns reside. This means it is closed under vector addition and scalar multiplication, and contains the zero vector.</li>
<li>It is spanned by the columns of the matrix: The column space of a matrix is the span of its column vectors. In other words, it is the smallest subspace that contains all the column vectors of the matrix.</li>
<li>It is the range of the corresponding linear transformation: The column space of a matrix is the range of the linear transformation associated with that matrix. This means it contains all possible output vectors that can be obtained by applying the linear transformation to input vectors.</li>
<li>It has the same dimension as the rank of the matrix: The dimension of the column space of a matrix is equal to the rank of the matrix. This is known as the column space’s dimensionality property.</li>
<li>It provides a basis for the row space and null space: The column space of a matrix provides a basis for both the row space and the null space of the matrix. The row space is the orthogonal complement of the null space, and the column space is the orthogonal complement of the left null space.</li>
<li>It determines the solvability of linear systems: The column space of a coefficient matrix in a system of linear equations determines whether the system has a unique solution, infinitely many solutions, or no solution at all. If the column space spans the entire vector space, the system has a unique solution. If the column space does not span the entire vector space, the system has either infinitely many solutions (if the rank of the matrix is less than the number of variables) or no solution (if the rank of the matrix is less than the number of equations).</li>
</ul>
</section>
</section>
<section id="row-space" class="level2" data-number="1.13">
<h2 data-number="1.13" class="anchored" data-anchor-id="row-space"><span class="header-section-number">1.13</span> Row Space</h2>
<section id="definition-7" class="level3" data-number="1.13.1">
<h3 data-number="1.13.1" class="anchored" data-anchor-id="definition-7"><span class="header-section-number">1.13.1</span> Definition</h3>
<p>The row space of a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BRow%7D(%5Cmathbf%7BA%7D)">, is the subspace spanned by the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRow%7D(%5Cmathbf%7BA%7D)%20=%20%5Ctext%7Bspan%7D%5C%7B%5Cmathbf%7Br%7D_1,%20%5Cmathbf%7Br%7D_2,%20%5Cldots,%20%5Cmathbf%7Br%7D_m%5C%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Br%7D_1,%20%5Cmathbf%7Br%7D_2,%20%5Cldots,%20%5Cmathbf%7Br%7D_m"> are the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
</section>
<section id="example-9" class="level3" data-number="1.13.2">
<h3 data-number="1.13.2" class="anchored" data-anchor-id="example-9"><span class="header-section-number">1.13.2</span> Example</h3>
<p>The row space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is the subspace spanned by the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, which can be expressed as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BRow%7D(%5Cmathbf%7BA%7D)%20=%20%5Ctext%7Bspan%7D%5Cleft%5C%7B%0A%5Cbegin%7Bbmatrix%7D%0A1%20%5C%5C%202%20%5C%5C%203%20%5C%5C%0A%5Cend%7Bbmatrix%7D,%0A%5Cbegin%7Bbmatrix%7D%0A4%20%5C%5C%205%20%5C%5C%206%20%5C%5C%0A%5Cend%7Bbmatrix%7D,%0A%5Cbegin%7Bbmatrix%7D%0A7%20%5C%5C%208%20%5C%5C%209%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cright%5C%7D%0A"></p>
</section>
<section id="properties-7" class="level3" data-number="1.13.3">
<h3 data-number="1.13.3" class="anchored" data-anchor-id="properties-7"><span class="header-section-number">1.13.3</span> Properties</h3>
<ul>
<li>It is a subspace: The row space of a matrix is a subspace of the vector space in which the matrix’s rows reside. This means it is closed under vector addition and scalar multiplication, and contains the zero vector.</li>
<li>It is spanned by the rows of the matrix: The row space of a matrix is the span of its row vectors. In other words, it is the smallest subspace that contains all the row vectors of the matrix.</li>
<li>It is the range of the corresponding linear transformation: The row space of a matrix is the range of the linear transformation associated with the transpose of that matrix. This means it contains all possible output vectors that can be obtained by applying the transpose of the linear transformation to input vectors.</li>
<li>It has the same dimension as the rank of the matrix: The dimension of the row space of a matrix is equal to the rank of the matrix. This is known as the row space’s dimensionality property.</li>
<li>It provides a basis for the null space and left null space: The row space of a matrix provides a basis for both the null space (kernel) and the left null space (cokernel) of the matrix. The null space is the orthogonal complement of the row space, and the left null space is the orthogonal complement of the column space.</li>
<li>It determines the row-rank and column-rank equality: The row space of a matrix determines the row-rank and column-rank equality property, which states that the number of linearly independent rows (the row-rank) is equal to the number of linearly independent columns (the column-rank) of the matrix.</li>
<li>It determines the solvability of linear systems: The row space of a coefficient matrix in a system of linear equations determines whether the system has a unique solution, infinitely many solutions, or no solution at all. If the row space spans the entire vector space, the system has a unique solution. If the row space does not span the entire vector space, the system has either infinitely many solutions (if the rank of the matrix is less than the number of variables) or no solution (if the rank of the matrix is less than the number of equations).</li>
</ul>
</section>
</section>
<section id="null-space" class="level2" data-number="1.14">
<h2 data-number="1.14" class="anchored" data-anchor-id="null-space"><span class="header-section-number">1.14</span> Null Space</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BNull%7D(%5Cmathbf%7BA%7D)%20=%20%5Cleft%5C%7B%20%5Cmathbf%7Bx%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En%20%5C,%20%5Cmiddle%7C%20%5C,%20%5Cmathbf%7BA%7D%5Cmathbf%7Bx%7D%20=%20%5Cmathbf%7B0%7D%20%5Cright%5C%7D%0A"></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BNull%7D"> represents the null space of a matrix,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> represents the given matrix,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> represents a vector in the null space of ,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En"> represents the n-dimensional real vector space, and</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D"> represents the zero vector.</li>
</ul>
<section id="example-10" class="level3" data-number="1.14.1">
<h3 data-number="1.14.1" class="anchored" data-anchor-id="example-10"><span class="header-section-number">1.14.1</span> Example</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%20%5C%5C%0A4%20&amp;%205%20&amp;%206%20%5C%5C%0A7%20&amp;%208%20&amp;%209%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>The null space of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is the set of all vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E3"> that satisfy <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5Cmathbf%7Bx%7D%20=%20%5Cmathbf%7B0%7D">.</p>
</section>
<section id="properties-8" class="level3" data-number="1.14.2">
<h3 data-number="1.14.2" class="anchored" data-anchor-id="properties-8"><span class="header-section-number">1.14.2</span> Properties</h3>
<p>The null space of a matrix, also known as the kernel space, is the set of all vectors that are mapped to the zero vector by the linear transformation associated with the matrix.</p>
<ul>
<li>Contains the zero vector: The null space always contains the zero vector, as any matrix multiplied by the zero vector results in the zero vector.</li>
<li>Subspace property: The null space is a subspace of the vector space from which the vectors are drawn. This means that it is closed under vector addition and scalar multiplication. In other words, if two vectors are in the null space, their sum and any scalar multiple of them will also be in the null space.</li>
<li>Dimensionality: The dimension of the null space is equal to the number of linearly independent solutions to the homogeneous linear system associated with the matrix. This is known as the nullity of the matrix.</li>
<li>Basis: The null space has a basis consisting of linearly independent vectors that span the entire null space. This basis is used to represent all vectors in the null space as linear combinations of the basis vectors.</li>
<li>Relationship to solvability: The null space is directly related to the solvability of a system of linear equations. If the null space contains only the zero vector, the system has a unique solution. If the null space contains non-zero vectors, the system has infinitely many solutions, and the null space vectors represent the general solutions.</li>
<li>Orthogonal complement of row space: The null space is orthogonal to the row space of the matrix. This means that any vector in the null space is orthogonal to every vector in the row space, and vice versa.</li>
<li>Relationship to matrix rank: The dimension of the null space, also known as the nullity, is related to the rank of the matrix through the rank-nullity theorem. The rank of the matrix plus the nullity of the matrix is equal to the number of columns in the matrix.</li>
<li>Computation: The null space can be computed by finding the solutions to the homogeneous linear system <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7B0%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is the coefficient matrix of the system of linear equations.</li>
</ul>
</section>
</section>
<section id="column-space-vs-row-space-vs-null-space" class="level2" data-number="1.15">
<h2 data-number="1.15" class="anchored" data-anchor-id="column-space-vs-row-space-vs-null-space"><span class="header-section-number">1.15</span> Column Space vs Row Space vs Null Space</h2>
<p>While both column space and row space are subspaces associated with a matrix, they serve different roles in linear algebra.</p>
<section id="column-space-vs-row-space" class="level3" data-number="1.15.1">
<h3 data-number="1.15.1" class="anchored" data-anchor-id="column-space-vs-row-space"><span class="header-section-number">1.15.1</span> Column Space vs Row Space</h3>
<ul>
<li>Dimensionality: The dimensionality of the column space and row space can differ. The dimension of the column space is determined by the number of linearly independent columns, which is also known as the rank of the matrix. On the other hand, the dimension of the row space is determined by the number of linearly independent rows of the matrix, which may not necessarily be the same as the rank of the matrix.</li>
<li>Basis and Span: The column space is typically used to find a basis for the range (output space) of a linear transformation associated with the matrix, while the row space is used to find a basis for the null space (kernel) and left null space (cokernel) of the matrix. The column space spans the range of the linear transformation, while the row space spans the orthogonal complement of the null space and left null space.</li>
<li>Solvability of linear systems: The row space of the coefficient matrix in a system of linear equations determines the solvability of the system, while the column space is not directly related to the solvability. Specifically, if the row space spans the entire vector space, the system has a unique solution. If the row space does not span the entire vector space, the system may have infinitely many solutions or no solution. The column space, on the other hand, does not directly determine the solvability of the system.</li>
<li>Transpose relationship: The row space of a matrix is related to the range of the linear transformation associated with the transpose of the matrix, while the column space is directly related to the range of the original matrix. This means that the row space and column space are related through the transpose operation, but they are not identical.</li>
</ul>
</section>
<section id="null-space-1" class="level3" data-number="1.15.2">
<h3 data-number="1.15.2" class="anchored" data-anchor-id="null-space-1"><span class="header-section-number">1.15.2</span> Null Space</h3>
<p>Null space: The null space of a matrix is the set of all vectors that are mapped to the zero vector by the linear transformation associated with the matrix. It represents the subspace of the vector space that consists of solutions to the homogeneous linear system <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D%20=%20%5Cmathbf%7B0%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is the coefficient matrix of the system of linear equations.</p>
</section>
<section id="the-relationships-between-these-spaces" class="level3" data-number="1.15.3">
<h3 data-number="1.15.3" class="anchored" data-anchor-id="the-relationships-between-these-spaces"><span class="header-section-number">1.15.3</span> The relationships between these spaces</h3>
<ul>
<li>The column space and row space are related, as they are orthogonal complements of each other. This means that any vector in the row space is orthogonal to any vector in the null space, and vice versa. This relationship is known as the row-space-null-space decomposition.</li>
<li>The dimension of the column space is equal to the rank of the matrix, which is the maximum number of linearly independent columns or rows. Similarly, the dimension of the row space is also equal to the rank of the matrix.</li>
<li>The null space is orthogonal to both the column space and the row space. This means that any vector in the null space is orthogonal to any vector in the column space and the row space.</li>
<li>The null space is useful in determining the solvability of a system of linear equations. If the null space contains only the zero vector, the system has a unique solution. If the null space contains non-zero vectors, the system has infinitely many solutions.</li>
</ul>


</section>
</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html</guid>
  <pubDate>Sun, 09 Apr 2023 15:00:00 GMT</pubDate>
  <media:content url="kmink3225.netlify.app/images/linear_algebra/column_space.PNG" medium="image"/>
</item>
<item>
  <title>Infrastructure Security</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html</link>
  <description><![CDATA[ 



<ul class="nav nav-pills" id="language-tab">
<li class="nav-item">
<button class="nav-link active" id="Korean-tab" data-bs-toggle="tab" data-bs-target="#Korean" type="button" aria-controls="Korean" aria-selected="true">
Korean
</button>
</li>
<li class="nav-item">
<button class="nav-link" id="English-tab" data-bs-toggle="tab" data-bs-target="#English" type="button" aria-controls="knitr" aria-selected="false">
English
</button>
</li>
<div class="tab-content" id="language-tabcontent">

<div id="Korean" class="tab-pane fade show active" aria-labelledby="Korean-tab">
<div id="Korean" class="tab-pane fade show active" aria-labelledby="Korean-tab">
<section id="network-isolation" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="network-isolation"><span class="header-section-number">1</span> Network Isolation</h2>
<p>AWS has implemented network isolation through a <strong>limited number of access points</strong> to the cloud, allowing for comprehensive monitoring of inbound and outbound communications and network traffic.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>inbound and outbound communications involve observing and analyzing the data and traffic that is entering or leaving the AWS network to ensure security and compliance.</p>
</div>
</div>
<ul>
<li>Endpoints are URLs that serve as entry points for web services.</li>
<li>Some services do not support regions (like IAM), so their endpoints do not include a region. But, some services (like US-West-2) do support regions.</li>
<li>AWS offers Amazon Virtual Private Cloud (VPC) as a private network within the AWS Cloud that provides isolation from other customers and from the internet
<ul>
<li>VPC allows you to allocate IP address spaces and build a private infrastructure with networks isolated from the internet.</li>
<li>You can also connect your on-premises environment or other VPN infrastructures to VPC using IPSec tunnels and AWS Direct Connect.</li>
<li>VPC allows resources to communicate with the internet if desired.</li>
</ul></li>
<li>Building a fort on a barren planet to protect themselves and the bees, and further isolating hives inside the fort itself is similar to the concept of isolating resources within a secure environment, such as Amazon VPC, to protect them from potential external threats.</li>
<li>Network Isolation VPC
<ul>
<li>the concept of Virtual Private Cloud (VPC) in AWS is a way to logically separate your AWS infrastructure from other customers.</li>
<li>VPC is like creating a fort around your AWS account and isolating resources into hives, using subnets or logical subdivision of IPs.
<ul>
<li>ex) EC2 instances are able to access the internet and be accessed from by putting them in a public subnet via Network Access Control Lists (NACLs), which are used to control inbound and outbound traffic at the subnet level.</li>
</ul></li>
<li>security groups
<ul>
<li>act as firewalls for EC2 instances by controlling both inbound and outbound traffic at the instance level.</li>
<li>This fine-grained access is defined by allow rules and looks</li>
</ul></li>
</ul></li>
<li>how to secure traffic between VPCs in AWS using VPC endpoints and route tables?
<ul>
<li>further secure communication between Virtual Private Clouds (VPCs) in AWS
<ul>
<li>It compares the traditional method of sending traffic between VPCs through the internet with the use of private links, which allow for direct communication between VPCs within the AWS infrastructure, resulting in a safer path of travel for data.</li>
<li>the concept of route tables in VPCs
<ul>
<li>route tables contain <strong>rules or routes</strong> used to determine where network traffic is directed, and the option to create custom route tables for routing traffic according to specific requirements.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="detective-controls" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="detective-controls"><span class="header-section-number">2</span> Detective Controls</h2>
<ul>
<li>Detective controls: 감사(auditing), 자동화된 분석, 경보를 가능하게 하는 사건 모니터링 및 블록 처리의 지속적인 개선
<ul>
<li>잠재적인 보안 위협 또는 사건을 식별할 수 있는 능력 향상</li>
<li>governance frameworks에 필수적</li>
<li>법이나 compliance 준수 의무, 보안 작업 등의 개선을 지원</li>
</ul></li>
<li>Different types of detective controls
<ul>
<li>자산 인벤토리의 작성(conducting an inventory of AWS resources)</li>
<li>내부 감사(internal auditing)</li>
<li>정보 시스템과 관련된 제어가 정책 및 요구사항 충족하는지 검사</li>
</ul></li>
<li>이상 활동 범위를 식별하고 이해하는데 도움이 됨</li>
</ul>
</section>
<section id="auditing" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="auditing"><span class="header-section-number">3</span> Auditing</h2>
<ul>
<li>AWS infrastructure의 보안 및 compliance를 향상시키는 AWS services (일부는 무료, 일부는 유료)
<ul>
<li>AWS CloudTrail: AWS infrastructure와 interact 하는 사람 추적 가능 (잘못된 변경/데이터 유출 추적에 도움)</li>
<li>AWS Config: configuration 관리 및 변경 기록, 모든 실제 config 세부 사항의 inventory 제공</li>
<li>AWS Inspector: 자동 보안 평가 실행
<ul>
<li>deploy된 applications의 보안 및 compliance를 향상시키기 위해, best practies와의 차이, EC2 instances의 노출, 취약점 등을 체크</li>
</ul></li>
<li>Trusted Advisor
<ul>
<li>AWS resources의 프로비저닝 보조 - best practices를 사용해서 리포트 제공
<ul>
<li>리포트 항목: 비용 최적화, 성능, 보안, 장애 허용 정도, 서비스 제한</li>
<li>조사 또는 실행을 위해, 심각한 수준(녹색,주황,적색)에 따라 권장 사항 제공</li>
</ul></li>
<li>Security section: S3 bucket의 권한, 보안 그룹, IAM 사용, root 계정의 MFA, 노출된 access keys, IAM 비밀번호 정책 등을 스캔</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="monitoring-cloudwatch-and-cloudwatch-log" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="monitoring-cloudwatch-and-cloudwatch-log"><span class="header-section-number">4</span> Monitoring CloudWatch and CloudWatch Log</h2>
<ul>
<li>Monitoring: Infrastructure와 관련된 데이터와 통계를 수집, 추적, 표시하는 과정</li>
<li>AWS의 CloudWatch: metrics repository역할 - repository에 넣은 metrics 기반으로 통계 검색
<ul>
<li>사용자가 정한 threshold를 넘었을때 경보 생성 가능</li>
<li>특정 기간 동안 하나의 metric을 감시 → threshold와 비교한 metric의 상대적인 값에 따라 하나 이상의 특정 action 수행 가능</li>
</ul></li>
<li>CloudWatch Logs: 여러 resources의 log files을 모니터링, 저장, 접근 가능한 tool
<ul>
<li>application, 서버 OS의 로그 수집 및 저장</li>
<li>CloudTrail 사용해서 API activity 수집</li>
<li>Amazon Route 53(Amazon의 DNS 웹 서비스)의 DNS queries를 기록</li>
<li>S3의 로그 데이터 저장</li>
</ul></li>
<li>CloudWatch Logs Insights: 로그 데이터를 interactive하게 검색하고 분석</li>
</ul>
</section>
<section id="monitoring-guard-duty-and-security-hub" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="monitoring-guard-duty-and-security-hub"><span class="header-section-number">5</span> Monitoring Guard Duty and Security Hub</h2>
<ul>
<li>Amazon GuardDuty: 위협 감지 서비스
<ul>
<li>AWS 계정 및 리소스에 대한 허가되지 않거나 악성인 행동들을 계속 모니터링</li>
<li>머신 러닝, 이상 감지, integrated threat intelligence를 사용해서 잠재적인 위협을 식별하고 우선 순위를 정함</li>
<li>여러 AWS data resources에서 수백억건의 사건 분석</li>
<li>잠재적인 위협을 세 단계(low, medium, high) 심각 수준으로 나눠서 대응 우선순위 결정</li>
<li>HTTPs API, CLI tools, Amazon CloudWatch events를 제공해서 보안 관련 발견에 대한 자동화된 보안 제공 지원</li>
</ul></li>
<li>Security Hub: 여러 AWS service의 보안 경고나 발견을 모으고, 정리하고, 우선 순위를 정함 → 통합 dashboards에서 시각화 요약 제공
<ul>
<li>AWS best practies 및 업계 표준을 기반으로, compliance check 자동화를 통해 환경을 지속적으로 모니터링할 수 있도록 함</li>
</ul></li>
</ul>
</section>
</div>
</div>
<div id="English" class="tab-pane fade" aria-labelledby="English-tab">
<div id="English" class="tab-pane fade" aria-labelledby="English-tab">

</div>
</div>
<section id="back-to-content-list" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="back-to-content-list"><span class="header-section-number">6</span> Back to Content List</h2>
<ul>
<li><a href="../../../../../docs/blog/posts/content_list.html">Global Blog Content List</a></li>
</ul>


</section>

</div></ul> ]]></description>
  <category>Engineering</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html</guid>
  <pubDate>Tue, 04 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Matrix Transformation (5) - Quadratic Form</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/08.quadratic_form.html</link>
  <description><![CDATA[ 



<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;">library</span>(tidyverse)</span>
<span id="cb1-2"><span class="fu" style="color: #4758AB;">library</span>(mosaic)</span>
<span id="cb1-3"><span class="fu" style="color: #4758AB;">library</span>(mvtnorm)</span></code></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb2-2"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span></code></pre></div>
</details>
</div>
<section id="background-knowledge" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="background-knowledge"><span class="header-section-number">1</span> Background Knowledge</h2>
<section id="linear-transformation" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="linear-transformation"><span class="header-section-number">1.1</span> Linear Transformation</h3>
<p>A linear transformation is a function <img src="https://latex.codecogs.com/png.latex?T"> that maps vectors from one vector space to another, and it satisfies the following two properties:</p>
<ul>
<li>Additivity: For any two vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> in the domain of <img src="https://latex.codecogs.com/png.latex?T">, <img src="https://latex.codecogs.com/png.latex?T(%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D)%20=%20T(%5Cmathbf%7Bu%7D)%20+%20T(%5Cmathbf%7Bv%7D)">.</li>
<li>Homogeneity: For any vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> in the domain of <img src="https://latex.codecogs.com/png.latex?T"> and any scalar <img src="https://latex.codecogs.com/png.latex?c">, <img src="https://latex.codecogs.com/png.latex?T(c%5Cmathbf%7Bu%7D)%20=%20cT(%5Cmathbf%7Bu%7D)">.</li>
</ul>
<p>Typical transformations that satisfies the linear transformation are</p>
<ul>
<li>rotation,</li>
<li>scaling,</li>
<li>shear,</li>
<li>reflection, and</li>
<li>projection, etc</li>
</ul>
<section id="example" class="level4" data-number="1.1.1">
<h4 data-number="1.1.1" class="anchored" data-anchor-id="example"><span class="header-section-number">1.1.1</span> Example</h4>
<p><strong>Example1</strong> A linear transformation <img src="https://latex.codecogs.com/png.latex?T:%20%5Cmathbb%7BR%7D%5E2%20-%3E%20%5Cmathbb%7BR%7D%5E2"> defined as <img src="https://latex.codecogs.com/png.latex?T(%5Cmathbf%7Bx%7D,%20%5Cmathbf%7By%7D)%20=%20(2%5Cmathbf%7Bx%7D%20+%20%5Cmathbf%7By%7D,%203%5Cmathbf%7Bx%7D%20-%20%5Cmathbf%7By%7D)">.</p>
<p>The transformation <img src="https://latex.codecogs.com/png.latex?T"> satisfies the additivity property: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AT((%5Cmathbf%7Bx%7D_1,%20%5Cmathbf%7By%7D_1)%20+%20(%5Cmathbf%7Bx%7D_1,%20%5Cmathbf%7By%7D_2))%0A&amp;=%20T(%5Cmathbf%7Bx%7D_1%20+%20%5Cmathbf%7Bx%7D_2,%20%5Cmathbf%7By%7D_1%20+%20%5Cmathbf%7By%7D_2)%20%5C%5C%0A&amp;=%20(2(%5Cmathbf%7Bx%7D_1%20+%20%5Cmathbf%7Bx%7D_2)%20+%20(%5Cmathbf%7By%7D_1%20+%20%5Cmathbf%7By%7D_1),%203(%5Cmathbf%7Bx%7D_1%20+%20%5Cmathbf%7Bx%7D_2)%20-%20(%5Cmathbf%7By%7D_1%20+%20%5Cmathbf%7By%7D_2))%20%5C%5C%0A&amp;=%20(2%5Cmathbf%7Bx%7D_1%20+%20%5Cmathbf%7By%7D_1,%203%5Cmathbf%7Bx%7D_1%20-%20%5Cmathbf%7By%7D_1)%20+%20(2%5Cmathbf%7Bx%7D_2%20+%20%5Cmathbf%7By%7D_1,%203%5Cmathbf%7Bx%7D_2%20-%20%5Cmathbf%7By%7D_2)%20%5C%5C%0A&amp;=%20T(%5Cmathbf%7Bx%7D_1,%20%5Cmathbf%7By%7D_1)%20+%20T(%5Cmathbf%7Bx%7D_2,%20%5Cmathbf%7By%7D_2)%0A%5Cend%7Balign*%7D%0A"></p>
<p>The transformation <img src="https://latex.codecogs.com/png.latex?T"> also satisfies the homogeneity property: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AT(c(%5Cmathbf%7Bx%7D,%20%5Cmathbf%7By%7D))%20&amp;=%20T(c%5Cmathbf%7Bx%7D,%20c%5Cmathbf%7By%7D)%20%5C%5C%0A&amp;=%20(2(c%5Cmathbf%7Bx%7D)%20+%20c%5Cmathbf%7By%7D,%203(c%5Cmathbf%7Bx%7D)%20-%20c%5Cmathbf%7By%7D)%20%5C%5C%0A&amp;=%20c(2%5Cmathbf%7Bx%7D%20+%20%5Cmathbf%7By%7D,%203%5Cmathbf%7Bx%7D%20-%20%5Cmathbf%7By%7D)%20%5C%5C%0A&amp;=%20cT(%5Cmathbf%7Bx%7D,%20%5Cmathbf%7By%7D)%0A%5Cend%7Balign*%7D%0A"></p>
<p>Thus, T is a linear transformation.</p>
<p><strong>Example2</strong> Rotation</p>
<ul>
<li>Additivity: Let’s consider a rotation transformation <img src="https://latex.codecogs.com/png.latex?R"> that rotates vectors in the plane counterclockwise by an angle <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. For any two vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> in the domain of <img src="https://latex.codecogs.com/png.latex?R">, we want to show that <img src="https://latex.codecogs.com/png.latex?R(%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D)%20=%20R(%5Cmathbf%7Bu%7D)%20+%20R(%5Cmathbf%7Bv%7D)">.</li>
</ul>
<p>When we rotate <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> individually, we obtain <img src="https://latex.codecogs.com/png.latex?R(%5Cmathbf%7Bu%7D)"> and <img src="https://latex.codecogs.com/png.latex?R(%5Cmathbf%7Bv%7D)"> respectively. To find <img src="https://latex.codecogs.com/png.latex?R(%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D)">, we rotate the sum of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> by the same angle <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. The result will be <img src="https://latex.codecogs.com/png.latex?R(%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D)">. Geometrically, the sum of two vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> can be represented by placing the initial point of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> at the terminal point of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D">. When we rotate this combined vector, we obtain the same result as rotating <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> individually and adding their rotated versions.</p>
<p>Therefore, <img src="https://latex.codecogs.com/png.latex?R(%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D)%20=%20R(%5Cmathbf%7Bu%7D)%20+%20R(%5Cmathbf%7Bv%7D)">, satisfying the additivity property.</p>
<ul>
<li>Homogeneity: Geometrically, scaling a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> by a scalar c does not affect the angle of rotation. Thus, rotating cu will give us the same result as rotating <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> and scaling the result by <img src="https://latex.codecogs.com/png.latex?c">. Therefore, <img src="https://latex.codecogs.com/png.latex?R(c%5Cmathbf%7Bu%7D)%20=%20cR(%5Cmathbf%7Bu%7D)">, satisfying the homogeneity property.</li>
</ul>
<p>Since rotation satisfies both the additivity and homogeneity properties, we can conclude that rotation is a linear transformation.</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D=(1,0)"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D=(0,1)">. If the two vectors are rotated by <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> using the rotation matrix R <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%5Ccos%5Ctheta%20&amp;%20-%5Csin%5Ctheta%20%5C%5C%0A%5Csin%5Ctheta%20&amp;%20%5Ccos%5Ctheta%0A%5Cend%7Bbmatrix%7D%0A">,</p>
<p>$$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AR%5Cbegin%7Bbmatrix%7D%0Ax%20%5C%5C%0Ay%0A%5Cend%7Bbmatrix%7D%0A&amp;=%20R%5Cbegin%7Bbmatrix%7D%0Ax%5C%5C%0A0%20%5Cend%7Bbmatrix%7D%20+%0AR%5Cbegin%7Bbmatrix%7D%0A0%5C%5C%0Ay%20%5Cend%7Bbmatrix%7D%5C%5C%0A&amp;=%20R%5Cbegin%7Bbmatrix%7D%0A1%5C%5C%0A0%20%5Cend%7Bbmatrix%7Dx%20+%0AR%5Cbegin%7Bbmatrix%7D%0A0%5C%5C%0A1%20%5Cend%7Bbmatrix%7Dy%5C%5C%0A&amp;=%20%5Cbegin%7Bbmatrix%7D%0A%5Ccos%5Ctheta%20%5C%5C%0A%5Csin%5Ctheta%0A%5Cend%7Bbmatrix%7Dx%20+%0A%5Cbegin%7Bbmatrix%7D%0A-%5Csin%5Ctheta%20%5C%5C%0A%5Ccos%5Ctheta%0A%5Cend%7Bbmatrix%7Dy%20%5C%5C%0A&amp;=%0A%5Cbegin%7Bbmatrix%7D%0A%5Ccos%5Ctheta%20&amp;%20-%5Csin%5Ctheta%20%5C%5C%0A%5Csin%5Ctheta%20&amp;%20%5Ccos%5Ctheta%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0Ax%20%5C%5C%0Ay%0A%5Cend%7Bbmatrix%7D%0A%0A%0A%5Cend%7Balign*%7D"> $$</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;"># Define the rotation angle</span></span>
<span id="cb3-2">theta <span class="op" style="color: #5E5E5E;">=</span> np.pi<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">4</span></span>
<span id="cb3-3"></span>
<span id="cb3-4"><span class="co" style="color: #5E5E5E;"># Define the original vector</span></span>
<span id="cb3-5">u <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb3-6">v <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb3-7"></span>
<span id="cb3-8"><span class="co" style="color: #5E5E5E;"># Define the transformation matrix for rotation</span></span>
<span id="cb3-9">rotation_matrix <span class="op" style="color: #5E5E5E;">=</span> np.array([[np.cos(theta), <span class="op" style="color: #5E5E5E;">-</span>np.sin(theta)],</span>
<span id="cb3-10">                            [np.sin(theta), np.cos(theta)]])</span>
<span id="cb3-11"></span>
<span id="cb3-12"><span class="co" style="color: #5E5E5E;"># Apply the rotation transformation to the vector</span></span>
<span id="cb3-13">rotated_u <span class="op" style="color: #5E5E5E;">=</span> np.dot(rotation_matrix, u)</span>
<span id="cb3-14">rotated_v <span class="op" style="color: #5E5E5E;">=</span> np.dot(rotation_matrix, v)</span>
<span id="cb3-15"></span>
<span id="cb3-16"><span class="co" style="color: #5E5E5E;"># Plot the original vector and its rotated version</span></span>
<span id="cb3-17">plt.figure(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">6</span>, <span class="dv" style="color: #AD0000;">6</span>))</span>
<span id="cb3-18">plt.axhline(<span class="dv" style="color: #AD0000;">0</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'black'</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>)</span>
<span id="cb3-19">plt.axvline(<span class="dv" style="color: #AD0000;">0</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'black'</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>)</span>
<span id="cb3-20"></span>
<span id="cb3-21">plt.quiver(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, u[<span class="dv" style="color: #AD0000;">0</span>], u[<span class="dv" style="color: #AD0000;">1</span>], angles<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale_units<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'black'</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'Original Vector (1,0)'</span>)</span>
<span id="cb3-22">plt.quiver(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, v[<span class="dv" style="color: #AD0000;">0</span>], v[<span class="dv" style="color: #AD0000;">1</span>], angles<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale_units<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'gray'</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'Original Vector (0,1)'</span>)</span>
<span id="cb3-23"></span>
<span id="cb3-24">plt.quiver(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, rotated_u[<span class="dv" style="color: #AD0000;">0</span>], rotated_u[<span class="dv" style="color: #AD0000;">1</span>], angles<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale_units<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'blue'</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'Rotated Vector'</span>)</span>
<span id="cb3-25">plt.quiver(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, rotated_v[<span class="dv" style="color: #AD0000;">0</span>], rotated_v[<span class="dv" style="color: #AD0000;">1</span>], angles<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale_units<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'xy'</span>, scale<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'red'</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'Rotated Vector'</span>)</span>
<span id="cb3-26"></span>
<span id="cb3-27"><span class="co" style="color: #5E5E5E;"># Plot settings</span></span>
<span id="cb3-28">plt.xlim(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(-2.0, 2.0)</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">plt.ylim(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(-2.0, 2.0)</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">plt.xlabel(<span class="st" style="color: #20794D;">'x-axis'</span>)</span>
<span id="cb7-2">plt.ylabel(<span class="st" style="color: #20794D;">'y-axis'</span>)</span>
<span id="cb7-3">plt.title(<span class="st" style="color: #20794D;">'Rotation Transformation'</span>)</span>
<span id="cb7-4">plt.legend()</span>
<span id="cb7-5">plt.grid(<span class="va" style="color: #111111;">True</span>)</span>
<span id="cb7-6">plt.show()</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/08.quadratic_form_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="576"></p>
</div>
</div>
</section>
</section>
<section id="conic-equation" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="conic-equation"><span class="header-section-number">1.2</span> Conic Equation</h3>
<p>A conic equation is a second-degree polynomial equation in two variables (<img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y">) that represents a conic section in the Cartesian coordinate system. It can be written in the general form as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AAx%5E2+Bxy+Cy%5E2+Dx+Ey+F=0%0A"></p>
<p>where A,B,C,D,E, and F are constants.</p>
<ul>
<li>the first degree term <img src="https://latex.codecogs.com/png.latex?Dx">, <img src="https://latex.codecogs.com/png.latex?Ey">: indicating the quadratic form is trnaslated (except for a parabola). If <img src="https://latex.codecogs.com/png.latex?D,E=0">, the quadratic form is called a central conic.</li>
<li>the <img src="https://latex.codecogs.com/png.latex?xy"> term, <img src="https://latex.codecogs.com/png.latex?Bxy">: indicating the quadratic form is rotated. If <img src="https://latex.codecogs.com/png.latex?B=0">, it is said the conic equation is in a standard position.</li>
</ul>
<section id="parabola" class="level4" data-number="1.2.1">
<h4 data-number="1.2.1" class="anchored" data-anchor-id="parabola"><span class="header-section-number">1.2.1</span> Parabola</h4>
<p>A parabola is a conic section defined as the locus of points that are equidistant from a fixed point called the focus (<img src="https://latex.codecogs.com/png.latex?F">) and a fixed line called the directrix (that does not pass through the focus point). It can be represented by the equation:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?y%5E2=4px"> with <img src="https://latex.codecogs.com/png.latex?F(p,0)"> and directrix <img src="https://latex.codecogs.com/png.latex?x=-p"></li>
<li><img src="https://latex.codecogs.com/png.latex?x%5E2=4py"> with <img src="https://latex.codecogs.com/png.latex?F(0,p)"> and directrix <img src="https://latex.codecogs.com/png.latex?y=-p"></li>
</ul>
</section>
<section id="eliipse" class="level4" data-number="1.2.2">
<h4 data-number="1.2.2" class="anchored" data-anchor-id="eliipse"><span class="header-section-number">1.2.2</span> Eliipse</h4>
<p>An ellipse is a conic section defined as the locus of points such that the sum of the distances from any point on the ellipse to two fixed points, called the foci (<img src="https://latex.codecogs.com/png.latex?F,F'">), is constant. It can be represented by the equation:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%7B(x-h)%5E2%7D%7D%7B%7Ba%5E2%7D%7D%20+%20%5Cfrac%7B%7B(y-k)%5E2%7D%7D%7B%7Bb%5E2%7D%7D%20=%201%0A"></p>
<p><em><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%7B(x-h)%5E2%7D%7D%7B%7Ba%5E2%7D%7D%20+%20%5Cfrac%7B%7B(y-k)%5E2%7D%7D%7B%7Bb%5E2%7D%7D%20=%201%20%5Ctext%7B%20%7D%20(b%3Ca)"> with the sum of the distances from <img src="https://latex.codecogs.com/png.latex?F,F'"> equal to <img src="https://latex.codecogs.com/png.latex?2a"> and <img src="https://latex.codecogs.com/png.latex?F,F'=(%5Cpm%20%5Csqrt%7Ba%5E2-b%5E2%7D,0)"><br>
</em><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%7B(x-h)%5E2%7D%7D%7B%7Ba%5E2%7D%7D%20+%20%5Cfrac%7B%7B(y-k)%5E2%7D%7D%7B%7Bb%5E2%7D%7D%20=%201%20%5Ctext%7B%20%7D%20(a%3Cb)"> with the sum of the distances from <img src="https://latex.codecogs.com/png.latex?F,F'"> equal to <img src="https://latex.codecogs.com/png.latex?2b"> and <img src="https://latex.codecogs.com/png.latex?F,F'=(0,%5Cpm%20%5Csqrt%7Ba%5E2-b%5E2%7D)"></p>
</section>
<section id="hyperbola" class="level4" data-number="1.2.3">
<h4 data-number="1.2.3" class="anchored" data-anchor-id="hyperbola"><span class="header-section-number">1.2.3</span> Hyperbola</h4>
<p>A hyperbola is a conic section defined as the locus of points such that the absolute value of the difference of the distances from any point on the hyperbola to two fixed points, called the foci (<img src="https://latex.codecogs.com/png.latex?F,F'">), is constant. It can be represented by the equation depending on the orientation of the hyperbola:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%7B(x-h)%5E2%7D%7D%7B%7Ba%5E2%7D%7D%20-%20%5Cfrac%7B%7B(y-k)%5E2%7D%7D%7B%7Bb%5E2%7D%7D%20=%201"> with the difference of the distances from <img src="https://latex.codecogs.com/png.latex?F,F'"> equal to <img src="https://latex.codecogs.com/png.latex?2a">, <img src="https://latex.codecogs.com/png.latex?F,F'=(%5Cpm%20%5Csqrt%7Ba%5E2+b%5E2%7D,0)">, and the asymptotic line <img src="https://latex.codecogs.com/png.latex?y=%5Cpm%5Cfrac%7Bb%7D%7Ba%7D(x-h)+k"><br>
</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%7B(x-h)%5E2%7D%7D%7B%7Ba%5E2%7D%7D%20-%20%5Cfrac%7B%7B(y-k)%5E2%7D%7D%7B%7Bb%5E2%7D%7D%20=%20-1"> with the difference of the distances from <img src="https://latex.codecogs.com/png.latex?F,F'"> equal to <img src="https://latex.codecogs.com/png.latex?2b">, <img src="https://latex.codecogs.com/png.latex?F,F'=(0,%5Cpm%20%5Csqrt%7Ba%5E2+b%5E2%7D)">, and the asymptotic line <img src="https://latex.codecogs.com/png.latex?y=%5Cpm%5Cfrac%7Bb%7D%7Ba%7D(x-h)+k"></li>
</ul>
</section>
</section>
</section>
<section id="quadratic-form" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="quadratic-form"><span class="header-section-number">2</span> Quadratic Form</h2>
<div id="def-quadratic" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Quadratic Form) </strong></span>For a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Bx_1,x_2,%5Cldots,x_n%5D%5ET">, the quadratic form is defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AQ(%5Cmathbf%7Bx%7D)%20=%20%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> symmetric matrix.</p>
</div>
<p>Here, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET"> represents the transpose of the vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"> represents the dot product of the vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> with itself after the transformation by the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>The quadratic form can be used to represent a quadratic equation using vectors and matrices. For example, let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Bx_1,x_2%5D%5ET"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be a <img src="https://latex.codecogs.com/png.latex?2%20%5Ctimes%202"> symmetric matrix given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A%20=%20%5Cbegin%7Bbmatrix%7D%202&amp;1%20%5C%5C%201&amp;3%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>Then, the quadratic form <img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)"> can be written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbf%7BQ%7D(%5Cmathbf%7Bx%7D)%20&amp;=%0A%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A2%20&amp;%201%20%5C%5C%0A1%20&amp;%203%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0Ax_1%20%5C%5C%20x_2%0A%5Cend%7Bbmatrix%7D%20%5C%5C%0A&amp;=%20%5Cbegin%7Bbmatrix%7D%202x_1%20+%20x_2%20&amp;%20x_1%20+%203x_2%20%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D%20%5C%5C%0A&amp;=%202x_1%5E2%20+%202x_1x_2%20+%203x_2%5E2%0A%5Cend%7Balign*%7D%0A"></p>
<p>Here, we can see that the quadratic form can be represented as a polynomial function of degree 2 in the variables <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2"> with the coefficients given by the entries of the symmetric matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Non-uniqueness
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> of the quadratic form <img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)"> is not uniuqe if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is not symmetric. Different choices of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> can give rise to the same quadratic form. For example, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20B%20=%20%5Cbegin%7Bbmatrix%7D%202&amp;1.5%20%5C%5C%200.5&amp;3%20%5Cend%7Bbmatrix%7D"> makes the <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q(%5Cmathbf%20x)%20=%202x_1%5E2+2x_1x_2+3x_2%5E2"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20Q(%5Cmathbf%20x)%20=%20%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BB%7D%20%5Cmathbf%7Bx%7D=%0A%5Cbegin%7Bbmatrix%7D%0Ax_1%20&amp;%20x_2%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A2&amp;1.5%20%5C%5C%200.5&amp;3%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0Ax_1%20%5C%5C%20x_2%0A%5Cend%7Bbmatrix%7D=2x_1%5E2+2x_1x_2+3x_2%5E2%0A"></p>
<p>There are infinitely many cases that resulting in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q(%5Cmathbf%20x)%20=%202x_1%5E2+2x_1x_2+3x_2%5E2"> with any combination of <img src="https://latex.codecogs.com/png.latex?b_%7B12%7D,b_%7B21%7D"> such that <img src="https://latex.codecogs.com/png.latex?b_%7B12%7D%20+%20b_%7B21%7D%20=2">.</p>
<p>Another example of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20Q(%5Cmathbf%20x)=%20%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D=%204x_1%5E2+7x_1x_2+9x_2%5E2"> gives the possible multiple <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D"> matrices such as <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BB%7D_1%20=%20%5Cbegin%7Bbmatrix%7D%204&amp;4%20%5C%5C%203&amp;9%20%5Cend%7Bbmatrix%7D%20%5Cquad%20%5Cmathbf%7BB%7D_2%20=%20%5Cbegin%7Bbmatrix%7D%204&amp;3.5%20%5C%5C%203.5&amp;9%20%5Cend%7Bbmatrix%7D%20%5Cquad%20%5Cmathbf%7BB%7D_3%20=%20%5Cbegin%7Bbmatrix%7D%204&amp;1%20%5C%5C%206&amp;9%20%5Cend%7Bbmatrix%7D%20%5Cquad%20%5Cmathbf%7BB%7D_4%20=%20%5Cbegin%7Bbmatrix%7D%204&amp;2%20%5C%5C%205&amp;9%20%5Cend%7Bbmatrix%7D%20%5Cquad%20%5Ctext%7Betc%7D%0A">.</p>
<p><strong>But there are several constraints that can be imposed on <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> to make it unique</strong>:</p>
<ul>
<li>Symmetric: If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is required to be symmetric, i.e., <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BA%7D%5ET">, then it can be shown that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is unique.
<ul>
<li>Suppose <img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)=x_1%5E2+x_2%5E2+x_3%5E2">. Then the associated matrix is <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D=%5Cbegin%7Bbmatrix%7D1&amp;0&amp;0%5C%5C0&amp;1&amp;0%5C%5C0&amp;0&amp;1%5Cend%7Bbmatrix%7D">.</li>
</ul></li>
<li>Positive definite: If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is required to be positive definite, i.e., <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D%20%3E%200"> for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, then it can be shown that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is unique.
<ul>
<li>Suppose <img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)=2x_1%5E2+3x_2%5E2+4x_3%5E2+4x_1x_2-4x_1x_3+8x_2x_3">. Then the associated matrix is <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D=%5Cbegin%7Bbmatrix%7D2&amp;2&amp;-2%5C%5C2&amp;3&amp;4%5C%5C-2&amp;4&amp;4%5Cend%7Bbmatrix%7D">.</li>
</ul></li>
<li>Diagonal: If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is required to be diagonal, then it is unique up to the order of the diagonal elements.
<ul>
<li>Consider the quadratic form <img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)%20=%20x_1%5E2%20-%204x_1x_2%20+%204x_2%5E2%20+%204x_3%5E2">. To express <img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)"> in the form of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%5Cmathbf%7BA%7D%5Cmathbf%7Bx%7D">, we first define the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D=%5Cbegin%7Bbmatrix%7D1&amp;-2&amp;0%5C%5C-2&amp;3&amp;0%5C%5C0&amp;0&amp;4%5Cend%7Bbmatrix%7D">. Then we have <img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)%20=%20%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D=%5Cbegin%7Bbmatrix%7Dx_1&amp;x_2&amp;x_3%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D1&amp;-2&amp;0%5C%5C-2&amp;3&amp;0%5C%5C0&amp;0&amp;4%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%5C%5Cx_2%5C%5Cx_3%5Cend%7Bbmatrix%7D">. So, the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is unique, and it is the one we defined above.</li>
</ul></li>
</ul>
<p>To find the symmetrtic matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> among these candidates, pick one of the candidates matrices and conduct the following operation: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D=%5Cfrac%7B1%7D%7B2%7D(%5Cmathbf%7BB%7D_i+%5Cmathbf%7BB%7D_i%5ET)%0A"></p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1">B <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">6</span>, <span class="dv" style="color: #AD0000;">65</span>, <span class="dv" style="color: #AD0000;">98</span>, <span class="dv" style="color: #AD0000;">5</span>), <span class="at" style="color: #657422;">ncol =</span> <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb8-2">A <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fl" style="color: #AD0000;">0.5</span> <span class="sc" style="color: #5E5E5E;">*</span> (B <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">t</span>(B))</span>
<span id="cb8-3"><span class="fu" style="color: #4758AB;">print</span>(<span class="st" style="color: #20794D;">"B="</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "B="</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><span class="fu" style="color: #4758AB;">print</span>(B)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2]
[1,]    6   98
[2,]   65    5</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><span class="fu" style="color: #4758AB;">print</span>(<span class="st" style="color: #20794D;">"A="</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "A="</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><span class="fu" style="color: #4758AB;">print</span>(A)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2]
[1,]  6.0 81.5
[2,] 81.5  5.0</code></pre>
</div>
</div>
</div>
</div>
<section id="properties" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="properties"><span class="header-section-number">2.1</span> Properties</h3>
<ul>
<li>Linearity: <img src="https://latex.codecogs.com/png.latex?Q(a%5Cmathbf%7Bx%7D%20+%20b%5Cmathbf%7By%7D)%20=%20aQ(%5Cmathbf%7Bx%7D)%20+%20bQ(%5Cmathbf%7By%7D)">, where <img src="https://latex.codecogs.com/png.latex?a,b"> are scalars and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D,%5Cmathbf%7By%7D"> are vectors.</li>
<li>Symmetry: <img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)%20=%20Q(%5Cmathbf%7Bx%7D%5ET)"> for all vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</li>
<li>Homogeneity: <img src="https://latex.codecogs.com/png.latex?Q(k%5Cmathbf%7Bx%7D)%20=%20k%5E2%20Q(%5Cmathbf%7Bx%7D)"> for all vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> and scalar <img src="https://latex.codecogs.com/png.latex?k">.</li>
<li>5 Types of Quadratic Forms
<ol type="1">
<li>Positive Definite: <img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)%20%3E%200"> for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</li>
<li>Positive Semi-Definite: <img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)%20%5Cgeq%200"> for all vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</li>
<li>Negative Definite: <img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)%20%3C%200"> for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</li>
<li>Negative Semi-Definite: <img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)%20%5Cleq%200"> for all vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</li>
<li>Indefinite: <img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)"> takes both positive and negative values for some nonzero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</li>
</ol></li>
</ul>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1">plot_quadratic_form <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(formula) {</span>
<span id="cb16-2">    <span class="fu" style="color: #4758AB;">plotFun</span>(formula,</span>
<span id="cb16-3">        <span class="at" style="color: #657422;">x1.lim =</span> <span class="fu" style="color: #4758AB;">range</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">10</span>, <span class="dv" style="color: #AD0000;">10</span>),</span>
<span id="cb16-4">        <span class="at" style="color: #657422;">x2.lim =</span> <span class="fu" style="color: #4758AB;">range</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">10</span>, <span class="dv" style="color: #AD0000;">10</span>),</span>
<span id="cb16-5">        <span class="at" style="color: #657422;">surface =</span> <span class="cn" style="color: #8f5902;">TRUE</span>,</span>
<span id="cb16-6">        <span class="at" style="color: #657422;">xlab =</span> <span class="fu" style="color: #4758AB;">expression</span>(x[<span class="dv" style="color: #AD0000;">1</span>]),</span>
<span id="cb16-7">        <span class="at" style="color: #657422;">ylab =</span> <span class="fu" style="color: #4758AB;">expression</span>(x[<span class="dv" style="color: #AD0000;">2</span>]),</span>
<span id="cb16-8">        <span class="at" style="color: #657422;">zlab =</span> <span class="fu" style="color: #4758AB;">expression</span>(<span class="fu" style="color: #4758AB;">Q</span>(x[<span class="dv" style="color: #AD0000;">1</span>], x[<span class="dv" style="color: #AD0000;">2</span>]))</span>
<span id="cb16-9">    )</span>
<span id="cb16-10">}</span></code></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><span class="do" style="color: #5E5E5E;
font-style: italic;">## PDM: positive definite matrix</span></span>
<span id="cb17-2"><span class="do" style="color: #5E5E5E;
font-style: italic;">## PSDM: positive semi-definite matrix</span></span>
<span id="cb17-3"><span class="do" style="color: #5E5E5E;
font-style: italic;">## NDM: negative definite matrix</span></span>
<span id="cb17-4"><span class="do" style="color: #5E5E5E;
font-style: italic;">## NSDM: negative semi-definite matrix</span></span>
<span id="cb17-5"><span class="do" style="color: #5E5E5E;
font-style: italic;">## IDM: indefinte matrix</span></span>
<span id="cb17-6"></span>
<span id="cb17-7">PDM <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>), <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb17-8">PSDM <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>), <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb17-9">NDM <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>), <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb17-10">NSDM <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>), <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb17-11">IDM <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>), <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb17-12"></span>
<span id="cb17-13"><span class="co" style="color: #5E5E5E;"># positive definite</span></span>
<span id="cb17-14"><span class="fu" style="color: #4758AB;">plot_quadratic_form</span>(x1<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">+</span> x2<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">~</span> x1 <span class="sc" style="color: #5E5E5E;">&amp;</span> x2)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/08.quadratic_form_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><span class="co" style="color: #5E5E5E;"># positive semi-definite</span></span>
<span id="cb18-2"><span class="fu" style="color: #4758AB;">plot_quadratic_form</span>(x1<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">*</span> x1 <span class="sc" style="color: #5E5E5E;">*</span> x2 <span class="sc" style="color: #5E5E5E;">+</span> x2<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">~</span> x1 <span class="sc" style="color: #5E5E5E;">&amp;</span> x2)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/08.quadratic_form_files/figure-html/unnamed-chunk-6-2.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><span class="co" style="color: #5E5E5E;"># negative definite</span></span>
<span id="cb19-2"><span class="fu" style="color: #4758AB;">plot_quadratic_form</span>(<span class="sc" style="color: #5E5E5E;">-</span>x1<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">-</span> x2<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">~</span> x1 <span class="sc" style="color: #5E5E5E;">&amp;</span> x2)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/08.quadratic_form_files/figure-html/unnamed-chunk-6-3.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><span class="co" style="color: #5E5E5E;"># negative semi-definite</span></span>
<span id="cb20-2"><span class="fu" style="color: #4758AB;">plot_quadratic_form</span>(<span class="sc" style="color: #5E5E5E;">-</span>x1<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">*</span> x1 <span class="sc" style="color: #5E5E5E;">*</span> x2 <span class="sc" style="color: #5E5E5E;">-</span> x2<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">~</span> x1 <span class="sc" style="color: #5E5E5E;">&amp;</span> x2)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/08.quadratic_form_files/figure-html/unnamed-chunk-6-4.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><span class="co" style="color: #5E5E5E;"># indefinite matrix</span></span>
<span id="cb21-2"><span class="fu" style="color: #4758AB;">plot_quadratic_form</span>(x1<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">-</span> x2<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">~</span> x1 <span class="sc" style="color: #5E5E5E;">&amp;</span> x2)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/08.quadratic_form_files/figure-html/unnamed-chunk-6-5.png" class="img-fluid" width="672"></p>
</div>
</div>
<ul>
<li>all the <img src="https://latex.codecogs.com/png.latex?a_%7Bii%7D"> s has the same sign for the following cases
<ul>
<li>positive definite: except for f(x1,x2)= f(0,0) =0, all f(x1,x2) are positive.</li>
<li>semi-positive definite: f(x1,x2)= f(0,0),… , f(10,10) =0, the other all f(x1,x2) are negative.</li>
<li>negative definite: except for f(x1,x2)= f(0,0) =0, all f(x1,x2) are positive.</li>
<li>semi-negative definite: f(x1,x2)= f(0,0),… , f(10,10) =0, the other all f(x1,x2) are negative.</li>
</ul></li>
<li>indefinite: the graph has the saddle point</li>
</ul>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><span class="co" style="color: #5E5E5E;"># c(3,3,3,7)</span></span>
<span id="cb22-2"><span class="fu" style="color: #4758AB;">plot_quadratic_form</span>(<span class="dv" style="color: #AD0000;">3</span> <span class="sc" style="color: #5E5E5E;">*</span> x1<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">6</span> <span class="sc" style="color: #5E5E5E;">*</span> x1 <span class="sc" style="color: #5E5E5E;">*</span> x2 <span class="sc" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">7</span> <span class="sc" style="color: #5E5E5E;">*</span> x2<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">~</span> x1 <span class="sc" style="color: #5E5E5E;">&amp;</span> x2)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/08.quadratic_form_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<section id="how-to-tell-positive-definite-matrix-from-positive-semi-definite-matrix" class="level4" data-number="2.1.1">
<h4 data-number="2.1.1" class="anchored" data-anchor-id="how-to-tell-positive-definite-matrix-from-positive-semi-definite-matrix"><span class="header-section-number">2.1.1</span> How to Tell Positive Definite Matrix from Positive Semi-Definite Matrix?</h4>
<p>Eigen value decomposition makes it possible to tell a Positive Definite Matrix from a Positive Semi-Definite Matrix.</p>
<div id="thm-spectral" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Spectral Theorem for Symmetric Matrices) </strong></span>A real symmetric matrix is positive definite if and only if all its eigenvalues are positive.</p>
</div>
<p>The result of eigen value decomposition are eigen values and eigen vectors. According to the types of the eigen values, we can determine what the matrix is:</p>
<ul>
<li>positive definite if the eigen values are all positive.</li>
<li>semi-positive definite if at least one of the eigen values is zero and the rest of them are all positive.</li>
<li>negative definite if the eigen values are all negative.</li>
<li>semi-negative definite if the eigen values are all negative and the rest of them are all negative.</li>
<li>indefinite if the eigen values has both signs +/-.</li>
</ul>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>If eigen values are all positive and the matrix is positive definite, the matrix is invertible.</p>
</div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1">A <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">7</span>), <span class="at" style="color: #657422;">nrow =</span> <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb23-2"><span class="fu" style="color: #4758AB;">eigen</span>(A)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>eigen() decomposition
$values
[1] 8.605551 1.394449

$vectors
          [,1]       [,2]
[1,] 0.4718579 -0.8816746
[2,] 0.8816746  0.4718579</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><span class="fu" style="color: #4758AB;">eigen</span>(PDM)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>eigen() decomposition
$values
[1] 1 1

$vectors
     [,1] [,2]
[1,]    0   -1
[2,]    1    0</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><span class="fu" style="color: #4758AB;">eigen</span>(PSDM)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>eigen() decomposition
$values
[1] 2 0

$vectors
          [,1]       [,2]
[1,] 0.7071068 -0.7071068
[2,] 0.7071068  0.7071068</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><span class="fu" style="color: #4758AB;">eigen</span>(NDM)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>eigen() decomposition
$values
[1] -1 -1

$vectors
     [,1] [,2]
[1,]    1    0
[2,]    0    1</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><span class="fu" style="color: #4758AB;">eigen</span>(NSDM)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>eigen() decomposition
$values
[1]  0 -2

$vectors
           [,1]      [,2]
[1,] -0.7071068 0.7071068
[2,]  0.7071068 0.7071068</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><span class="fu" style="color: #4758AB;">eigen</span>(IDM)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>eigen() decomposition
$values
[1]  1 -1

$vectors
     [,1] [,2]
[1,]   -1    0
[2,]    0   -1</code></pre>
</div>
</div>
</section>
</section>
<section id="positive-definite-matrix" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="positive-definite-matrix"><span class="header-section-number">2.2</span> Positive Definite Matrix</h3>
<p>A symmetric matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is positive definite if and only if the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"> is positive for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>To see why this is true, consider the eigenvalue decomposition of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, which can be written as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BQ%7D%20%5CLambda%20%5Cmathbf%7BQ%7D%5ET">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D"> is an orthogonal matrix and <img src="https://latex.codecogs.com/png.latex?%5CLambda"> is a diagonal matrix containing the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. Then, for any nonzero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbf%20x%5ET%20%5Cmathbf%20A%20%5Cmathbf%20x&amp;=%5Cmathbf%20x%5ET%20%5Cmathbf%20Q%20%5Cmathbf%20%5CLambda%20%5Cmathbf%20Q%5ET%20%5Cmathbf%20x%20%5Cquad%20(%5Cbecause%20%5Ctext%7Bdiagonalization%20of%20%7D%20%5Cmathbf%20A)%5C%5C%0A&amp;=(%5Cmathbf%20x%5ET%20%5Cmathbf%20Q)%5Cmathbf%20%5CLambda%20(%20%5Cmathbf%20Q%5ET%5Cmathbf%20x)%5C%5C%0A&amp;=%5Csum_%7Bi=1%7D%5E%7Bn%7D%20%5Clambda_iy_i%5E2%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?y_i%20=%20(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BQ%7D)_i"> is the <img src="https://latex.codecogs.com/png.latex?i"> th coordinate (i.e., a scalar value that represents the position of a point or a vector relative to a chosen basis) of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?n"> is the dimension of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>Note that since <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D"> is orthogonal, we have <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D%5ET%20%5Cmathbf%7BQ%7D%20=%20%5Cmathbf%7BI%7D">, so <img src="https://latex.codecogs.com/png.latex?y_i%20=%20%5Cmathbf%7Bq%7D_i%5ET%20%5Cmathbf%7Bx%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bq%7D_i"> is the <img src="https://latex.codecogs.com/png.latex?i"> th column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D">. Therefore, the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)%20=%20%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"> can be written in terms of the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and the coordinates of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> with respect to the eigenvectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Diagonalization
</div>
</div>
<div class="callout-body-container callout-body">
<p>Diagonalization is a process of finding a diagonal matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20D"> and an invertible matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20P"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BP%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%5Cmathbf%7BP%7D%20=%20%5Cmathbf%7BD%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is a square matrix. In other words, diagonalization is a way of representing a matrix as a diagonal matrix, which is a matrix with non-zero values only on its main diagonal.</p>
</div>
</div>
<p>Since <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is positive definite, we have <img src="https://latex.codecogs.com/png.latex?%5Clambda_i%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?i">, and so <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5En%20%5Clambda_i%20y_i%5E2%20%3E%200"> for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">. Therefore, the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"> is positive for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, which implies that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is positive definite.</p>
<p>In other words, the positive definiteness of a symmetric matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is equivalent to the positivity of the associated quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"> for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>Therefore, a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is said to be positive definite if all of its eigenvalues are positive or equivalently, a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is positive definite if left-multiplying and right-multiplying it by the same vector, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x"> always gives a positive number if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x%5ET%20%5Cmathbf%20A%20%5Cmathbf%20x"></p>
</section>
<section id="bilinear-form" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="bilinear-form"><span class="header-section-number">2.3</span> Bilinear Form</h3>
<p>A quadratic form can be expressed as a bilinear form. In other words, a quadratic form can be written in terms of a bilinear form by defining a new matrix that is the sum of the matrix representing the quadratic form and its transpose.</p>
<div id="def-bilinear" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (Bilinear Form) </strong></span>Suppose we have a quadratic form defined as <img src="https://latex.codecogs.com/png.latex?q(%5Cmathbf%7Bx%7D)%20=%20%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a symmetric matrix. Then, we can define a bilinear form as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ab(%5Cmathbf%7Bx%7D,%20%5Cmathbf%7By%7D)%20=%20%5Cfrac%7B1%7D%7B2%7D(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7By%7D%20+%20%5Cmathbf%7By%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D)%0A"></p>
</div>
<p>Note that the factor of <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D"> is introduced to avoid double-counting. It can be shown that the two forms are equivalent, in the sense that for any <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, <img src="https://latex.codecogs.com/png.latex?q(%5Cmathbf%7Bx%7D)%20=%20b(%5Cmathbf%7Bx%7D,%20%5Cmathbf%7Bx%7D)">.</p>
<p>In other words, every quadratic form can be expressed as a bilinear form, and every symmetric bilinear form can be expressed as a quadratic form.</p>
</section>
</section>
<section id="examples" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="examples"><span class="header-section-number">3</span> Examples</h2>
</section>
<section id="apllications" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="apllications"><span class="header-section-number">4</span> Apllications</h2>
<section id="sum-of-squares" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="sum-of-squares"><span class="header-section-number">4.1</span> Sum of Squares</h3>
<p>The sum of squares of a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Bx_1,%20x_2,%20%5Cldots,%20x_n%5D%5ET"> can be represented as a quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%5Cmathbf%7Bx%7D">. To see this, consider the sum of squares:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bi=1%7D%5E%7Bn%7D%20x_i%5E2%20=%20x_1%5E2%20+%20x_2%5E2%20+%20%5Cdots%20+x_n%5E2%0A"></p>
<p>Now, we can write this in vector form as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20x%5ET%20%5Cmathbf%20I%20%5Cmathbf%20x%20=%20%20%5Cmathbf%20x%5ET%20%5Cmathbf%20x%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20&amp;%20%5Cdots%20&amp;%20x_n%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5C%5C%20%5Cvdots%20%5C%5C%20x_n%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>Therefore, the sum of squares can be represented as a quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%5Cmathbf%7Bx%7D">.</p>
</section>
<section id="define-multivariate-distributions" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="define-multivariate-distributions"><span class="header-section-number">4.2</span> Define Multivariate Distributions</h3>
<section id="bivariate-distributions" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="bivariate-distributions"><span class="header-section-number">4.2.1</span> Bivariate Distributions</h4>
<p>Quadratic form is used in the power part of the pdf of a bivariate normal distribution: <img src="https://latex.codecogs.com/png.latex?%0Af(x_1,x_2)=%5Cfrac%7B1%7D%7B2%5Cpi%5Csigma_%7B%5Cmathbf%7BX_1%7D%7D%5Csigma_%7B%5Cmathbf%7BX_2%7D%7D%5Csqrt%7B1-%5Crho%5E2%7D%7D%5Coperatorname%7Bexp%7D%5Cleft%5B%20-%5Cfrac%7B1%7D%7B2(1-%5Crho%5E2)%7D%5Cleft(%5Cfrac%7B(x_1-%5Cmu_%7BX_1%7D)%5E2%7D%7B%5Csigma_%7BX_1%7D%7D-%5Cfrac%7B2%5Crho(x_1-%5Cmu_%7BX_1%7D)(x_1-%5Cmu_%7BX_1%7D)(x_2-%5Cmu_%7BX_2%7D)%7D%7B%5Csigma_%7BX_1%7D%5Csigma_%7BX_2%7D%7D+%5Cfrac%7B(x_2-%5Cmu_%7BX_2%7D)%5E2%7D%7B%5Csigma_%7BX_2%7D%7D%5Cright)%20%5Cright%5D%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7BX_1%7D,%20%5Cmu_%7BX_2%7D"> are the means of <img src="https://latex.codecogs.com/png.latex?X,%20Y"> respectively, <img src="https://latex.codecogs.com/png.latex?%5Csigma_%7BX_1%7D,%20%5Csigma_%7BX_2%7D"> are the standard deviations of <img src="https://latex.codecogs.com/png.latex?X_1,X_2"> respectively, and <img src="https://latex.codecogs.com/png.latex?%5Crho"> is the correlation coefficient <img src="https://latex.codecogs.com/png.latex?X_1,X_2">.</p>
<p>The pdf is equivalent with the following matrix form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(%5Cmathbf%7Bx%7D)=%5Cfrac%7B1%7D%7B(2%5Cpi)%5E%7B%5Cfrac%7Bn%7D%7B2%7D%7D%7C%5CSigma%7C%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D%7D%5Coperatorname%7Bexp%7D%5Cleft%5B%20-%5Cfrac%7B1%7D%7B2%7D(%5Cmathbf%7Bx%7D-%5Cmathbf%7B%5Cmu%7D)%5E%7BT%7D%5Cmathbf%7B%5CSigma%7D%5E%7B-1%7D(%5Cmathbf%7Bx%7D-%5Cmathbf%7B%5Cmu%7D)%5Cright%5D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7Bx%7D-%5Cmathbf%7B%5Cmu%7D)%5E%7BT%7D%5Cmathbf%7B%5CSigma%7D%5E%7B-1%7D(%5Cmathbf%7Bx%7D-%5Cmathbf%7B%5Cmu%7D)"> is a quadratic form, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is a vector in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cmu%7D"> is a mean vector in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CSigma%7D"> is a covariance matrix.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bx%7D=%0A%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D%20%5Cquad%0A%5Cmathbf%7B%5Cmu%7D=%5Cbegin%7Bbmatrix%7D%5Cmu_%7Bx_1%7D%20%5C%5C%20%5Cmu_%7Bx_2%7D%5Cend%7Bbmatrix%7D%20%5Cquad%0A%5Cmathbf%7B%5CSigma%7D=%5Cbegin%7Bbmatrix%7D%0A%5Csigma_%7BX_1%7D%5E2%20&amp;%20%5Crho%5Csigma_%7BX_1%7D%5Csigma_%7BX_2%7D%5C%5C%0A%5Crho%5Csigma_%7BX_1%7D%5Csigma_%7BX_2%7D%20&amp;%20%5Csigma_%7BX_2%7D%0A%5Cend%7Bbmatrix%7D%0A">.</p>
<p>The quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Csigma%7D"> informs us of how data are distributed with means <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cmu%7D"> and the covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CSigma%7D">.</p>
<p><strong>Exmaple1</strong> <img src="https://latex.codecogs.com/png.latex?2x_1%5E2+4x_2%5E2"> can be represented as a quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7Dx_1%20&amp;%20x_2%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D2&amp;0%20%5C%5C0&amp;4%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D">. The matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> part is <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D2&amp;0%20%5C%5C0&amp;4%5Cend%7Bbmatrix%7D"> and its inverse is <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D2%5E%7B-1%7D&amp;0%20%5C%5C0&amp;4%5E%7B-1%7D%5Cend%7Bbmatrix%7D">. Here, since the off diagonal entries are zeros, <img src="https://latex.codecogs.com/png.latex?X_1"> and <img src="https://latex.codecogs.com/png.latex?X_2"> are independent. Then, the pdf is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(x_1,x_2)=%5Cfrac%7B1%7D%7B2%5Cpi%5Csigma_%7B%5Cmathbf%7BX_1%7D%7D%5Csigma_%7B%5Cmathbf%7BX_2%7D%7D%7D%5Coperatorname%7Bexp%7D%5Cleft%5B%20-%5Cfrac%7B1%7D%7B2%7D%5Cleft(%5Cfrac%7B(x_1-%5Cmu_%7BX_1%7D)%5E2%7D%7B%5Csigma_%7BX_1%7D%7D+%5Cfrac%7B(x_2-%5Cmu_%7BX_2%7D)%5E2%7D%7B%5Csigma_%7BX_2%7D%7D%5Cright)%20%5Cright%5D%0A"></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The inverse of a diagonal matrix is the inverse of each entry</p>
</div>
</div>
<p><strong>Exmaple2</strong> <img src="https://latex.codecogs.com/png.latex?9x_1%5E2+6x_1x_2+4x_2%5E2"></p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1">mu <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb35-2">mu1 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb35-3">mu2 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb35-4">sigma1 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">3</span></span>
<span id="cb35-5">sigma2 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb35-6">rho <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fl" style="color: #AD0000;">0.5</span></span>
<span id="cb35-7">sigma <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(sigma1<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span>, rho <span class="sc" style="color: #5E5E5E;">*</span> sigma1 <span class="sc" style="color: #5E5E5E;">*</span> sigma2, rho <span class="sc" style="color: #5E5E5E;">*</span> sigma1 <span class="sc" style="color: #5E5E5E;">*</span> sigma2, sigma2<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span>), <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb35-8"></span>
<span id="cb35-9"><span class="fu" style="color: #4758AB;">dmvnorm</span>(<span class="at" style="color: #657422;">x =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>), <span class="at" style="color: #657422;">mean =</span> mu, <span class="at" style="color: #657422;">sigma =</span> sigma) <span class="co" style="color: #5E5E5E;"># a pdf value at (1,1)</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.02592721</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><span class="co" style="color: #5E5E5E;"># 1/((2*pi)*sqrt(det(sigma)))*exp(-0.5*t(c(1,1)-mu) %*% solve(sigma) %*% (c(1,1)-mu))</span></span></code></pre></div>
</details>
</div>
<p>This value is the same as <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cfrac%7B1%7D%7B(2%5Cpi)%5E%7B%5Cfrac%7Bn%7D%7B2%7D%7D%7C%5CSigma%7C%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D%7D%5Coperatorname%7Bexp%7D%5Cleft%5B%20-%5Cfrac%7B1%7D%7B2%7D(%5Cmathbf%7Bx%7D-%5Cmathbf%7B%5Cmu%7D)%5E%7BT%7D%5Cmathbf%7B%5CSigma%7D%5E%7B-1%7D(%5Cmathbf%7Bx%7D-%5Cmathbf%7B%5Cmu%7D)%5Cright%5D="> 0.0259272</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><span class="fu" style="color: #4758AB;">plotFun</span>(<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">/</span> (<span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">*</span> pi <span class="sc" style="color: #5E5E5E;">*</span> sigma1 <span class="sc" style="color: #5E5E5E;">*</span> sigma2 <span class="sc" style="color: #5E5E5E;">*</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">-</span> rho<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span>)) <span class="sc" style="color: #5E5E5E;">*</span> <span class="fu" style="color: #4758AB;">exp</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">/</span> (<span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">*</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">-</span> rho<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span>)) <span class="sc" style="color: #5E5E5E;">*</span> ((x1 <span class="sc" style="color: #5E5E5E;">-</span> mu1)<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">/</span> sigma1<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">+</span> (x2 <span class="sc" style="color: #5E5E5E;">-</span> mu2)<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">*</span> rho <span class="sc" style="color: #5E5E5E;">*</span> (x1 <span class="sc" style="color: #5E5E5E;">-</span> mu1) <span class="sc" style="color: #5E5E5E;">*</span> (x2 <span class="sc" style="color: #5E5E5E;">-</span> mu2)) <span class="sc" style="color: #5E5E5E;">/</span> (sigma1 <span class="sc" style="color: #5E5E5E;">*</span> sigma2)) <span class="sc" style="color: #5E5E5E;">~</span> x1 <span class="sc" style="color: #5E5E5E;">&amp;</span> x2,</span>
<span id="cb38-2">    <span class="at" style="color: #657422;">mu1 =</span> <span class="dv" style="color: #AD0000;">1</span>,</span>
<span id="cb38-3">    <span class="at" style="color: #657422;">mu2 =</span> <span class="dv" style="color: #AD0000;">2</span>,</span>
<span id="cb38-4">    <span class="at" style="color: #657422;">sigma1 =</span> <span class="dv" style="color: #AD0000;">3</span>,</span>
<span id="cb38-5">    <span class="at" style="color: #657422;">sigma2 =</span> <span class="dv" style="color: #AD0000;">2</span>,</span>
<span id="cb38-6">    <span class="at" style="color: #657422;">rho =</span> <span class="fl" style="color: #AD0000;">0.5</span>,</span>
<span id="cb38-7">    <span class="at" style="color: #657422;">x1.lim =</span> <span class="fu" style="color: #4758AB;">range</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">5</span>),</span>
<span id="cb38-8">    <span class="at" style="color: #657422;">x2.lim =</span> <span class="fu" style="color: #4758AB;">range</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">5</span>),</span>
<span id="cb38-9">    <span class="at" style="color: #657422;">surface =</span> <span class="cn" style="color: #8f5902;">TRUE</span>,</span>
<span id="cb38-10">    <span class="at" style="color: #657422;">xlab =</span> <span class="fu" style="color: #4758AB;">expression</span>(x[<span class="dv" style="color: #AD0000;">1</span>]),</span>
<span id="cb38-11">    <span class="at" style="color: #657422;">ylab =</span> <span class="fu" style="color: #4758AB;">expression</span>(x[<span class="dv" style="color: #AD0000;">2</span>]),</span>
<span id="cb38-12">    <span class="at" style="color: #657422;">zlab =</span> <span class="fu" style="color: #4758AB;">expression</span>(<span class="fu" style="color: #4758AB;">f</span>(x[<span class="dv" style="color: #AD0000;">1</span>], x[<span class="dv" style="color: #AD0000;">2</span>]))</span>
<span id="cb38-13">)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/08.quadratic_form_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="multivariate-distributions" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="multivariate-distributions"><span class="header-section-number">4.2.2</span> Multivariate Distributions</h4>
</section>
</section>
<section id="optimization-problems" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="optimization-problems"><span class="header-section-number">4.3</span> Optimization Problems</h3>
<section id="quadratic-programming" class="level4" data-number="4.3.1">
<h4 data-number="4.3.1" class="anchored" data-anchor-id="quadratic-programming"><span class="header-section-number">4.3.1</span> Quadratic Programming</h4>
<p>Objective Functions, Constraints, or Penalty Terms</p>
</section>
</section>
<section id="study-of-quadratic-behavior-systems" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="study-of-quadratic-behavior-systems"><span class="header-section-number">4.4</span> Study of Quadratic Behavior Systems</h3>
<section id="harmonic-oscillators" class="level4" data-number="4.4.1">
<h4 data-number="4.4.1" class="anchored" data-anchor-id="harmonic-oscillators"><span class="header-section-number">4.4.1</span> Harmonic Oscillators</h4>
</section>
<section id="vibrating-systems." class="level4" data-number="4.4.2">
<h4 data-number="4.4.2" class="anchored" data-anchor-id="vibrating-systems."><span class="header-section-number">4.4.2</span> Vibrating Systems.</h4>
</section>
</section>
<section id="machine-learning" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="machine-learning"><span class="header-section-number">4.5</span> Machine Learning</h3>
<section id="cost-functions" class="level4" data-number="4.5.1">
<h4 data-number="4.5.1" class="anchored" data-anchor-id="cost-functions"><span class="header-section-number">4.5.1</span> Cost Functions</h4>
</section>
<section id="regularization-terms" class="level4" data-number="4.5.2">
<h4 data-number="4.5.2" class="anchored" data-anchor-id="regularization-terms"><span class="header-section-number">4.5.2</span> Regularization terms,</h4>
<p>Ridge Regression</p>
</section>
<section id="kernel-functions" class="level4" data-number="4.5.3">
<h4 data-number="4.5.3" class="anchored" data-anchor-id="kernel-functions"><span class="header-section-number">4.5.3</span> Kernel Functions</h4>
<p>SVM</p>
</section>
</section>
<section id="finance" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="finance"><span class="header-section-number">4.6</span> Finance</h3>
<p>Modeling risk and return in portfolio optimization and asset pricing models</p>
</section>
<section id="cryptography" class="level3" data-number="4.7">
<h3 data-number="4.7" class="anchored" data-anchor-id="cryptography"><span class="header-section-number">4.7</span> Cryptography</h3>
<p>quadratic sieve algorithm for integer factorization</p>
</section>
<section id="variability-of-vector-x" class="level3" data-number="4.8">
<h3 data-number="4.8" class="anchored" data-anchor-id="variability-of-vector-x"><span class="header-section-number">4.8</span> Variability of Vector, x</h3>
<p>The covariance matrix of a random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> can be represented as a quadratic form in terms of the vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> and the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20x%5ET%20%5Cmathbf%20C%20%5Cmathbf%20x%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> is the covariance matrix. This expression is a quadratic form because it involves a quadratic polynomial in the elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>In this representation, the diagonal elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> correspond to the variances of the individual components of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, and the off-diagonal elements correspond to the covariances between the components. The expression <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BC%7D%20%5Cmathbf%7Bx%7D"> measures the variability of the random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> in all possible directions, weighted by the covariances between the components.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> measures the covariance between each pair of components of the random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">. It is a matrix that summarizes the <strong>pairwise covariances</strong> between the components of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>On the other hand, the quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BC%7D%20%5Cmathbf%7Bx%7D"> measures the total variability of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, taking into account the covariances between <strong>all possible pairs of components</strong>.</p>
<p>It does this by weighting the contribution of each component to the overall variability by its covariance with every other component. So, while the covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> captures the pairwise covariances between the components of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, the quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BC%7D%20%5Cmathbf%7Bx%7D"> captures the total variability of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> in all directions.</p>
</div>
</div>
<p>Let’s take a simple example with a 2-dimensional random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D=%5Bx_1,%20x_2%5D%5ET">. We can think of this random vector as representing data points in a 2D space. The covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> will capture the covariances between <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2">. Let’s say that the covariance matrix is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20C%20=%5Cbegin%7Bbmatrix%7D%20%5Csigma_%7Bx_1%7D%20&amp;%20%5Coperatorname%7BCov%7D(x_1,x_2)%20%5C%5C%20%5Coperatorname%7BCov%7D(x_2,x_1)%20&amp;%20%5Csigma_%7Bx_2%7D%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Csigma_%7Bx_1%7D%5E2"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma_%7Bx_2%7D%5E2"> are the variances of <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2">, respectively, and <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BCov%7D(x_1,x_2)"> is their covariance.</p>
<p>Now, let’s consider the quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BC%7D%20%5Cmathbf%7Bx%7D">. This expression gives us a scalar value that measures the variability of the random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> in all possible directions, weighted by the covariances between the components. We can see this geometrically by plotting the data points in the 2D space and drawing an ellipse that captures the variability of the data. The shape of the ellipse is determined by the eigenvalues and eigenvectors of the covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D">.</p>
<p>To see this, let’s first rewrite the quadratic form as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20x%5ET%20%5Cmathbf%20C%20%5Cmathbf%20x%20=%20%5Csigma_%7Bx_1%7D%5E2x_1%5E2%20+2%5Coperatorname%7BCov%7D(x_1,x_2)x_1x_2+%5Csigma_%7Bx_2%7D%5E2%0A"></p>
<p>This is a quadratic equation in <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2"> and can be thought of as the equation of an ellipse centered at the origin by the determinant of the conic equation. The shape of the ellipse is determined by the coefficients of the quadratic terms, which are the variances and covariances in the covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D">.</p>
<p>Now, let’s find the eigenvectors and eigenvalues of the covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D">. The eigenvectors are the directions along which the data has the most variance, and the corresponding eigenvalues are the variances of the data along those directions.</p>
<p>Let’s assume that the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> are ordered such that <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20%5Cgeq%20%5Clambda_2">. Then, the eigenvectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_2"> satisfy:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20C%20%5Cmathbf%20v_1%20=%5Clambda_1%5Cmathbf%20v_1%20%5Ctext%7B%20%20%7D%0A%5Cmathbf%20C%20%5Cmathbf%20v_2%20=%5Clambda_2%5Cmathbf%20v_2%0A"></p>
<p>These equations can be rewritten as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%5Csigma_%7Bx_1%7D%5E2%20&amp;%20%5Ctext%7BCov%7D(x_1,x_2)%5C%5C%0A%20%20%5Ctext%7BCov%7D(x_1,x_2)%20&amp;%20%5Csigma_%7Bx_2%7D%5E2%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20v_%7B11%7D%5C%5C%0A%20%20v_%7B21%7D%0A%5Cend%7Bbmatrix%7D%0A=%20%5Clambda_1%0A%5Cbegin%7Bbmatrix%7D%0Av_%7B11%7D%5C%5C%0Av_%7B21%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation%7D%0A"></p>
<p>This equation can be expanded as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Csigma_%7Bx_1%7D%5E2%20v_%7B11%7D%20+%20%5Ctext%7BCov%7D(x_1,x_2)%20v_%7B21%7D%20=%20%5Clambda_1%20v_%7B11%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BCov%7D(x_1,x_2)%20v_%7B11%7D%20+%20%5Csigma_%7Bx_2%7D%5E2%20v_%7B21%7D%20=%20%5Clambda_1%20v_%7B21%7D"></p>
<p>Now, let’s multiply the first equation by <img src="https://latex.codecogs.com/png.latex?v_%7B11%7D"> and the second equation by <img src="https://latex.codecogs.com/png.latex?v_%7B21%7D">, and then subtract the second equation from the first:</p>
<p><img src="https://latex.codecogs.com/png.latex?(%5Csigma_%7Bx_1%7D%5E2%20-%20%5Clambda_1)v_%7B11%7Dv_%7B21%7D%20+%20%5Ctext%7BCov%7D(x_1,x_2)(v_%7B21%7D%5E2%20-%20v_%7B11%7D%5E2)%20=%200"></p>
<p>This can be rewritten as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bv_%7B21%7D%7D%7Bv_%7B11%7D%7D%20=%20%5Cfrac%7B%5Csigma_%7Bx_1%7D%5E2%20-%20%5Clambda_1%7D%7B%5Ctext%7BCov%7D(x_1,x_2)%7D%20-%20%5Cfrac%7Bv_%7B11%7D%7D%7Bv_%7B21%7D%7D"></p>
<p>Let <img src="https://latex.codecogs.com/png.latex?t%20=%20%5Cfrac%7Bv_%7B21%7D%7D%7Bv_%7B11%7D%7D">. Then, we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?t%5E2%20-%20%5Cleft(%5Cfrac%7B%5Csigma_%7Bx_1%7D%5E2%20+%20%5Csigma_%7Bx_2%7D%5E2%7D%7B%5Ctext%7BCov%7D(x_1,x_2)%7D%5Cright)t%20+%20%5Cfrac%7B%5Clambda_1%7D%7B%5Ctext%7BCov%7D(x_1,x_2)%7D%20=%200"></p>
<p>This is a quadratic equation in <img src="https://latex.codecogs.com/png.latex?t">, and its roots can be solved using the quadratic formula. The roots are:</p>
<p><img src="https://latex.codecogs.com/png.latex?t_1%20=%20%5Cfrac%7B%5Csigma_%7Bx_1%7D%5E2%20-%20%5Csigma_%7Bx_2%7D%5E2%20+%20%5Csqrt%7B(%5Csigma_%7Bx_1%7D%5E2%20-%20%5Csigma_%7Bx_2%7D%5E2)%5E2%20+%204%5Ctext%7BCov%7D(x_1,x_2)%5E2%7D%7D%7B2%5Ctext%7BCov%7D(x_1,x_2)%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?t_2%20=%20%5Cfrac%7B%5Csigma_%7Bx_1%7D%5E2%20-%20%5Csigma_%7Bx_2%7D%5E2%20-%20%5Csqrt%7B(%5Csigma_%7Bx_1%7D%5E2%20-%20%5Csigma_%7Bx_2%7D%5E2)%5E2%20+%204%5Ctext%7BCov%7D(x_1,x_2)%5E2%7D%7D%7B2%5Ctext%7BCov%7D(x_1,x_2)%7D"></p>
<p>Finally, the eigenvectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_2"> are given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bv%7D_1%20=%20%5Cbegin%7Bbmatrix%7D1%20%5C%5C%20t_1%20%5Cend%7Bbmatrix%7D%20%5Ctext%7B%20%20%7D%0A%5Cmathbf%7Bv%7D_2%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%20t_2%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>These eigenvectors define the principal components of the data, which are the orthogonal directions in the feature space along which the data varies the most.</p>
<p>Let’s apply this difference between <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20C=%20%5Coperatorname%7BCov(%5Cmathbf%20X)%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x%5ET%20%5Cmathbf%7BC%7D%20%5Cmathbf%20x"> or PCA to Iris dataset:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb39-2"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb39-3"><span class="im" style="color: #00769E;">from</span> sklearn.datasets <span class="im" style="color: #00769E;">import</span> load_iris</span>
<span id="cb39-4"><span class="im" style="color: #00769E;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;">import</span> StandardScaler</span>
<span id="cb39-5"></span>
<span id="cb39-6"><span class="co" style="color: #5E5E5E;"># Load the iris dataset</span></span>
<span id="cb39-7">iris <span class="op" style="color: #5E5E5E;">=</span> load_iris()</span>
<span id="cb39-8">X <span class="op" style="color: #5E5E5E;">=</span> iris.data</span>
<span id="cb39-9"></span>
<span id="cb39-10"><span class="co" style="color: #5E5E5E;"># Standardize the data</span></span>
<span id="cb39-11">scaler <span class="op" style="color: #5E5E5E;">=</span> StandardScaler()</span>
<span id="cb39-12">X_std <span class="op" style="color: #5E5E5E;">=</span> scaler.fit_transform(X)</span>
<span id="cb39-13"></span>
<span id="cb39-14"><span class="co" style="color: #5E5E5E;"># Calculate the covariance matrix</span></span>
<span id="cb39-15">cov_matrix <span class="op" style="color: #5E5E5E;">=</span> np.cov(X_std.T)</span>
<span id="cb39-16"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'C=Cov(X) =</span><span class="ch" style="color: #20794D;">\n</span><span class="sc" style="color: #5E5E5E;">{</span>cov_matrix<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb39-17"><span class="co" style="color: #5E5E5E;"># Calculate the eigenvalues and eigenvectors of the covariance matrix</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>C=Cov(X) =
[[ 1.00671141 -0.11835884  0.87760447  0.82343066]
 [-0.11835884  1.00671141 -0.43131554 -0.36858315]
 [ 0.87760447 -0.43131554  1.00671141  0.96932762]
 [ 0.82343066 -0.36858315  0.96932762  1.00671141]]</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">eigenvalues, eigenvectors <span class="op" style="color: #5E5E5E;">=</span> np.linalg.eig(cov_matrix)</span>
<span id="cb41-2"></span>
<span id="cb41-3"><span class="co" style="color: #5E5E5E;"># Sort the eigenvalues in descending order</span></span>
<span id="cb41-4">sorted_indexes <span class="op" style="color: #5E5E5E;">=</span> eigenvalues.argsort()[::<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb41-5">sorted_eigenvalues <span class="op" style="color: #5E5E5E;">=</span> eigenvalues[sorted_indexes]</span>
<span id="cb41-6">sorted_eigenvectors <span class="op" style="color: #5E5E5E;">=</span> eigenvectors[:, sorted_indexes]</span>
<span id="cb41-7"></span>
<span id="cb41-8"><span class="co" style="color: #5E5E5E;"># Project the data onto the principal components</span></span>
<span id="cb41-9">transformed_data <span class="op" style="color: #5E5E5E;">=</span> X_std.dot(sorted_eigenvectors)</span>
<span id="cb41-10"></span>
<span id="cb41-11"><span class="co" style="color: #5E5E5E;"># Visualize the PCA</span></span>
<span id="cb41-12">fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots()</span>
<span id="cb41-13">ax.scatter(transformed_data[:, <span class="dv" style="color: #AD0000;">0</span>], transformed_data[:, <span class="dv" style="color: #AD0000;">1</span>], c<span class="op" style="color: #5E5E5E;">=</span>iris.target)</span>
<span id="cb41-14">ax.set_xlabel(<span class="st" style="color: #20794D;">'PC1'</span>)</span>
<span id="cb41-15">ax.set_ylabel(<span class="st" style="color: #20794D;">'PC2'</span>)</span>
<span id="cb41-16">ax.set_title(<span class="st" style="color: #20794D;">'PCA, Part of the Total Variance of Iris Data Explained by 2 PCs'</span>)</span>
<span id="cb41-17">plt.show()</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/08.quadratic_form_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>This representation is useful in many statistical and machine learning applications, where the covariance matrix provides information about the variability and dependencies between different features or variables. For example, in principal component analysis (PCA), the covariance matrix is used to identify the directions of maximum variability in a dataset, which can be used to reduce the dimensionality of the data while retaining as much information as possible.</p>
</section>
<section id="pca" class="level3" data-number="4.9">
<h3 data-number="4.9" class="anchored" data-anchor-id="pca"><span class="header-section-number">4.9</span> PCA</h3>
<p>The principal components of a dataset can be obtained by finding the eigenvectors of the covariance matrix. In other words, we can express the covariance matrix as a quadratic form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20C%20=%20%5Cmathbf%20x%5ET%20%5Cmathbf%20A%20%5Cmathbf%20x%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is a column vector of centered data, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a symmetric positive semi-definite matrix (the covariance matrix). Diagonalizing <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> gives us the eigenvalues and eigenvectors, which are used to transform the original data into a new coordinate system, where the first axis (the first principal component) corresponds to the direction of greatest variance, the second axis (the second principal component) corresponds to the direction of second greatest variance, and so on. This new coordinate system is called the principal component space.</p>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/08.quadratic_form.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Matrix Transformation (4) - Biinear Form</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.bilinear_form.html</link>
  <description><![CDATA[ 



<section id="binear-form" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="binear-form"><span class="header-section-number">1</span> Binear Form</h2>
<p>A bilinear form of a matrix is a function that extends the linear form and takes two vectors as inputs and produces a scalar as output. It is linear in both of its arguments, meaning that it satisfies the following properties:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AB(%5Cmathbf%20u+%5Cmathbf%20v)&amp;=B(%5Cmathbf%20u+%5Cmathbf%20w)+B(%5Cmathbf%20v+%5Cmathbf%20w)%5C%5C%0AB(%5Cmathbf%20u,%5Calpha%20%5Cmathbf%20v)&amp;=%5Calpha%20B(%5Cmathbf%20u,%5Cmathbf%20v)%5C%5C%0AB(%5Calpha%5Cmathbf%20u,%5Cmathbf%20v)&amp;=%5Calpha%20B(%5Cmathbf%20u,%5Cmathbf%20v)%0A%5Cend%7Baligned%7D%0A"></p>
<p>for all vectors <img src="https://latex.codecogs.com/png.latex?u">, <img src="https://latex.codecogs.com/png.latex?v">, <img src="https://latex.codecogs.com/png.latex?w"> and scalars <img src="https://latex.codecogs.com/png.latex?%5Calpha">.</p>
<p>A bilinear form can be represented by a matrix <img src="https://latex.codecogs.com/png.latex?B"> such that <img src="https://latex.codecogs.com/png.latex?B_%7Bi,j%7D"> is the coefficient of the product <img src="https://latex.codecogs.com/png.latex?u_i%20v_j"> in the expansion of <img src="https://latex.codecogs.com/png.latex?B(u,v)">. The bilinear form can then be written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AB(%5Cmathbf%20u,%5Cmathbf%20v)=%5Cmathbf%20u%5ET%20B%20%5Cmathbf%20v%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20u"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v"> are column vectors and <img src="https://latex.codecogs.com/png.latex?B"> is a matrix.</p>
<p>For example, consider the bilinear form <img src="https://latex.codecogs.com/png.latex?B(%5Cmathbf%20u,%5Cmathbf%20v)%20=%20u_1%20v_1%20+%20u_2%20v_2">. This bilinear form can be represented by the matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AB=%5Cbegin%7Bbmatrix%7D1&amp;0%5C%5C0&amp;1%5Cend%7Bbmatrix%7D%0A"></p>
<p>and written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AB(%5Cmathbf%20u,%5Cmathbf%20v)=%5Cbegin%7Bbmatrix%7Du_1&amp;%20u_2%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D1&amp;0%5C%5C0&amp;1%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dv_1%5C%5Cv_2%5Cend%7Bbmatrix%7D=u_1v_1+u_2v_2%0A"></p>
<p>This bilinear form computes the dot product of <img src="https://latex.codecogs.com/png.latex?u"> and <img src="https://latex.codecogs.com/png.latex?v">, which measures the similarity between the two vectors. Bilinear forms are commonly used in applications such as optimization, geometry, and physics, where they capture the interaction between two quantities or variables.</p>
<p>The covariance matrix can be represented as a bilinear form using matrix multiplication. Let’s say we have a random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Bx_1,%20x_2,%20%5Cldots,%20x_n%5D%5ET"> with mean vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cmu%7D%20=%20%5B%5Cmu_1,%20%5Cmu_2,%20%5Cldots,%20%5Cmu_n%5D%5ET"> and covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CSigma%7D">. Then, we can represent the covariance matrix as a bilinear form in the following way:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5CSigma&amp;=%5Coperatorname%7BE%7D%5B(%5Cmathbf%20x-%5Cmathbf%20%5Cmu)(%5Cmathbf%20x-%5Cmathbf%20%5Cmu)%5ET%5D%5C%5C%0A&amp;=%5Cfrac%7B1%7D%7Bn-1%7D%5Csum_%7Bi=1%7D%5E%7Bn%7D(x_i-%5Cbar%7Bx%7D)(x_i-%5Cbar%7Bx%7D)%5ET%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BE%7D"> denotes the expectation operator. We can expand this expression as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5CSigma&amp;=%5Coperatorname%7BE%7D%5B%5Cmathbf%20x%5Cmathbf%20x%5ET%5D-%5Cmathbf%20%5Cmu%5Cmathbf%20%5Cmu%5ET%0A%5Cend%7Baligned%7D%0A"></p>
<p>Now, we can represent the covariance matrix as a bilinear form using matrix multiplication as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5CSigma&amp;=%5Coperatorname%7BE%7D%5B(%5Cmathbf%20x-%5Cmathbf%20%5Cmu)(%5Cmathbf%20x-%5Cmathbf%20%5Cmu)%5ET%5D%5C%5C%0A&amp;=%5Csum_%7Bi=1%7D%5E%7Bn%7D%5Csum_%7Bj=1%7D%5E%7Bn%7D(%5Cmathbf%20x_i-%5Cmathbf%5Cmu_i)(%5Cmathbf%20x_i-%5Cmathbf%20%5Cmu_j)%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bx%7D-%5Cmathbf%7B%5Cmu%7D%5D"> is the deviation of the random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> from its mean vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cmu%7D">.</p>
<p>covariance matrix, and correlation matrix</p>
<p>One of the most famous examples is the use of bilinear forms in convolutional neural networks (CNNs), which are a type of deep learning model used for image and video recognition tasks.</p>
<p>In a CNN, a bilinear form is used to compute the similarity between a filter and a local region of an input image. This similarity measure is used to determine how much the filter “matches” the local region of the image, and is used to produce an output feature map.</p>
<p>More specifically, the bilinear form used in a CNN takes the form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Az_%7Bi,j%7D%20=%20%5Csum_%7Bm=1%7D%5E%7BM%7D%5Csum_%7Bn=1%7D%5E%7BN%7D%20w_%7Bm,n%7Dx_%7Bi+m-1,j+n-1%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?z_%7Bi,j%7D"> is the output feature map at location <img src="https://latex.codecogs.com/png.latex?(i,j)">, <img src="https://latex.codecogs.com/png.latex?x_%7Bi+m-1,j+n-1%7D"> is the input image pixel at location <img src="https://latex.codecogs.com/png.latex?(i+m-1,j+n-1)">, and <img src="https://latex.codecogs.com/png.latex?w_%7Bm,n%7D"> is the weight of the filter at position <img src="https://latex.codecogs.com/png.latex?(m,n)">. This computation is performed for each location <img src="https://latex.codecogs.com/png.latex?(i,j)"> in the output feature map.</p>
<p>The bilinear form used in CNNs is a type of convolution operation, and is used to learn features such as edges, corners, and other patterns in the input image. CNNs with bilinear forms have achieved state-of-the-art performance on many image recognition tasks, including object detection, face recognition, and scene classification.</p>
<p>Bilinear forms also have applications in other areas of machine learning, such as natural language processing (NLP). In NLP, bilinear forms can be used to compute the similarity between two word embeddings, which are vector representations of words. This similarity measure can be used for tasks such as sentiment analysis, text classification, and machine translation.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AB(%5Cmathbf%7Bu%7D,%5Cmathbf%7Bv%7D)=%5Cmathbf%7Bu%7D%5ET%20%5Cmathbf%7BW%7D%5Cmathbf%7Bv%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> are word embeddings, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BW%7D"> is a weight matrix, and <img src="https://latex.codecogs.com/png.latex?B(%5Cmathbf%7Bu%7D,%5Cmathbf%7Bv%7D)"> represents the bilinear form used to compute the similarity between the two embeddings.</p>
<p>Overall, bilinear forms are a powerful tool for learning features from complex data such as images and text, and have many applications in deep learning and machine learning.</p>


</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.bilinear_form.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Matrix Transformation (3) - Linear Form</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.linear_form.html</link>
  <description><![CDATA[ 



<section id="linear-form" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="linear-form"><span class="header-section-number">1</span> Linear Form</h2>
<p>A linear form is a linear function that maps a vector space to its underlying field. Let <img src="https://latex.codecogs.com/png.latex?V"> be a vector space over a field <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BF%7D">, and let <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(V,%5Cmathbb%7BF%7D)"> denote the set of all linear functions from <img src="https://latex.codecogs.com/png.latex?V"> to <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BF%7D">. A linear form on <img src="https://latex.codecogs.com/png.latex?V"> is an element of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(V,%5Cmathbb%7BF%7D)">.</p>
<p>A linear form <img src="https://latex.codecogs.com/png.latex?%5Cvarphi"> can be represented by a row vector of dimension <img src="https://latex.codecogs.com/png.latex?1%5Ctimes%20n">, where <img src="https://latex.codecogs.com/png.latex?n"> is the dimension of <img src="https://latex.codecogs.com/png.latex?V">. Let <img src="https://latex.codecogs.com/png.latex?%7B%5Cmathbf%7Be%7D_1,%20%5Cmathbf%7Be%7D_2,%20%5Cdots,%20%5Cmathbf%7Be%7D_n%7D"> be a basis for <img src="https://latex.codecogs.com/png.latex?V">, and let <img src="https://latex.codecogs.com/png.latex?%7B%5Calpha_1,%20%5Calpha_2,%20%5Cdots,%20%5Calpha_n%7D"> be the corresponding dual basis for <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(V,%5Cmathbb%7BF%7D)">, such that <img src="https://latex.codecogs.com/png.latex?%5Calpha_i(%5Cmathbf%7Be%7Dj)%20=%20%5Cdelta%7Bij%7D"> (the Kronecker delta). Then, any linear form <img src="https://latex.codecogs.com/png.latex?%5Cvarphi%5Cin%5Cmathcal%7BL%7D(V,%5Cmathbb%7BF%7D)"> can be written as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cvarphi(x)=%5Csum_%7Bi=1%7D%5E%7Bn%7Da_ix_i=%5Cmathbf%20a%20%5Cmathbf%20x%5ET=%5Cmathbf%20x%20%5Cmathbf%20a%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5Cin%20V"> is a column vector of dimension <img src="https://latex.codecogs.com/png.latex?n%5Ctimes%201">, <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Ba%7D%5D"> is the row vector representing <img src="https://latex.codecogs.com/png.latex?%5Cvarphi">, and <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bx%7D%5D"> is the column vector representing <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?V%20=%20%5Cmathbb%7BR%7D%5E2"> be the vector space of 2-dimensional column vectors, and let <img src="https://latex.codecogs.com/png.latex?%5Cvarphi%5Cin%5Cmathcal%7BL%7D(V,%5Cmathbb%7BR%7D)"> be the linear form defined by <img src="https://latex.codecogs.com/png.latex?%5Cvarphi(%5Cbegin%7Bbmatrix%7Dx%5Cy%5Cend%7Bbmatrix%7D)%20=%203x%20-%202y">. Then, we can represent <img src="https://latex.codecogs.com/png.latex?%5Cvarphi"> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5B%5Cmathbf%20a%5D=%5Cbegin%7Bbmatrix%7D%203%20&amp;%20-2%5Cend%7Bbmatrix%7D%20%5B%5Cmathbf%20x%5D=%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D%20%5Cvarphi(x)=%5Csum_%7Bi=1%7D%5E%7Bn%7D%5Cmathbf%20a%5Cmathbf%20x%5ET=3x_1-2x_2%0A"></p>
<p>which shows that <img src="https://latex.codecogs.com/png.latex?%5Cvarphi"> is a linear form on <img src="https://latex.codecogs.com/png.latex?V">.</p>
<p>consider a linear regression model that predicts the price of a house based on its size and location. The model can be represented by the linear form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cvarphi(x)=%5Cmathbf%20w%5Cmathbf%20x%5ET=%5Csum_%7Bi=1%7D%5E%7Bn%7Dw_ix_i=w_0+w_1x_1+w_2x_2%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cvarphi(%5Cmathbf%7Bx%7D)"> is the predicted price, <img src="https://latex.codecogs.com/png.latex?x_1"> is the size of the house, <img src="https://latex.codecogs.com/png.latex?x_2"> is a measure of the location (such as the distance from the city center), and <img src="https://latex.codecogs.com/png.latex?w_0">, <img src="https://latex.codecogs.com/png.latex?w_1">, and <img src="https://latex.codecogs.com/png.latex?w_2"> are the model parameters that control the intercept and the weights of the features. This linear form can be written in matrix form as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cvarphi(x)=%5Cmathbf%20x%5Cmathbf%20w=%5Cmathbf%20w%20%5Cmathbf%20x%5ET%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bw%7D%5D"> is a row vector of the model parameters and <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bx%7D%5D"> is a row vector of the features.</p>
<p>Linear forms can also be used in deep learning and machine learning models that involve linear transformations, such as fully connected layers in neural networks or linear classifiers. For example, consider a simple linear classifier that classifies images of digits into one of 10 classes. The classifier can be represented by the linear form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cvarphi(x)=%5Cmathbf%20x%5Cmathbf%20w%20+%20b%20=%5Cmathbf%20w%20%5Cmathbf%20x%5ET%20+b%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cvarphi(%5Cmathbf%7Bx%7D)"> is the predicted class score, <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bx%7D%5D"> is a row vector of the pixel values of the image, <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bw%7D%5D"> is a row vector of the weights of the classifier, and <img src="https://latex.codecogs.com/png.latex?b"> is the bias term. This linear form can be used to classify the image by selecting the class with the highest score.</p>
<p>In both of these examples, linear forms are used to represent linear relationships between variables or features, and the model parameters are learned through training on a set of labeled examples.</p>


</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.linear_form.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Matrix Calculus (1) - Matrix to Vector Derivatives</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/derivative_matrix_vector.html</link>
  <description><![CDATA[ 



<section id="matrix-to-vector-derivatives" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="matrix-to-vector-derivatives"><span class="header-section-number">1</span> Matrix to Vector Derivatives</h2>
<p>Matrix-to-vector derivatives refer to the derivatives of a matrix function with respect to a vector argument. Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)"> be a matrix-valued function of a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En">. The matrix-to-vector derivative is denoted as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20x_1%7D%20&amp;%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20x_2%7D%20&amp;%20%5Ccdots%20&amp;%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20x_n%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>Here, the matrix-to-vector derivative is a matrix whose <img src="https://latex.codecogs.com/png.latex?i">th column is the partial derivative of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bf%7D"> with respect to the <img src="https://latex.codecogs.com/png.latex?i">th component of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> be matrices in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%20n%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%201%7D">, respectively. Consider the function <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%20=%20%5Cmathbf%7BAx%7D">, which is a matrix-vector product. The matrix-to-vector derivative of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is given by: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D%20=%20%5Cmathbf%7BA%7D%0A"> Here, the derivative is a matrix whose rows are the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<section id="differentiation-of-quadratic-form" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="differentiation-of-quadratic-form"><span class="header-section-number">1.1</span> Differentiation of Quadratic Form</h3>
<p>This is because the output of this differentiation is a vector (with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">), rather than a scalar.</p>
<p>The differentiation of a quadratic form is the process of finding the gradient of a quadratic form with respect to its input vector.</p>
<p>Given a quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is an <img src="https://latex.codecogs.com/png.latex?n">-dimensional column vector, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En"> is a vector, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> symmetric matrix, the derivative of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%7B%5Cmathbf%20x%7D%20f(%5Cmathbf%20x)%20=%20(A%20+%20A%5ET)%5Cmathbf%20x%20+%20b%0A"></p>
<p>In this expression, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET"> is the transpose of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A+%5Cmathbf%20A%5ET"> is written instead of <img src="https://latex.codecogs.com/png.latex?2%5Cmathbf%20A"> when calculating the gradient of a quadratic form. It is because in general, the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> might not be symmetric, so <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5Cneq%20%5Cmathbf%20A%5ET">. However, for any matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, we have <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A+%5Cmathbf%20A%5ET%20=%20(%5Cmathbf%20A+%5Cmathbf%20A%5ET)%5ET">, which is a symmetric matrix. Therefore, by writing the gradient as <img src="https://latex.codecogs.com/png.latex?%5Cnabla_x%20f(x)%20=%20(%5Cmathbf%20A+%5Cmathbf%20A%5ET)x">, we ensure that the gradient is always a symmetric matrix, even if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is not symmetric. This is useful in many applications where symmetric matrices are preferred. But if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is constrained to be a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?2%5Cmathbf%20A"> can be written.</p>
</section>
<section id="when-mathbfa-is-symmetric" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="when-mathbfa-is-symmetric"><span class="header-section-number">1.2</span> When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is Symmetric</h3>
<p>As an example, consider the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=x_1%5E2+2x_1x_2+3x_2%5E2">, which can be written in the form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D">, where:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20x=%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%20x_2%20%5Cend%7Bbmatrix%7D,%20%5Cmathbf%20A=%5Cbegin%7Bbmatrix%7D%201%20&amp;%201%20%5C%5C%201%20&amp;%203%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>The derivative of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is then:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D=(%5Cmathbf%7BA%7D+%5Cmathbf%7BA%7D%5ET)%5Cmathbf%7Bx%7D=%5Cbegin%7Bbmatrix%7D2%20&amp;%202%5C%5C2%20&amp;%206%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D=%5Cbegin%7Bbmatrix%7D2x_1+2x_2%5C%5C2x_1+6x_2%5Cend%7Bbmatrix%7D%0A"></p>
<p>This represents the gradient vector of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> at any point <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
</section>
<section id="when-mathbfa-is-not-symmetric" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="when-mathbfa-is-not-symmetric"><span class="header-section-number">1.3</span> When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is Not Symmetric</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%203%20&amp;%204%20%5Cend%7Bbmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D">.</p>
<p>Then, we have <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%203%20&amp;%204%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20+%202x_2%20%5C%5C%203x_1%20+%204x_2%20%5Cend%7Bbmatrix%7D%20=%20x_1%5E2%20+%205x_1x_2%20+%204x_2%5E2">.</p>
<p>To find the gradient of this quadratic form, we can take the partial derivatives of <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2"> with respect to each variable:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20x_1%7D%20(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D)%20=%202x_1%20+%205x_2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20x_2%7D%20(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D)%20=%205x_1%20+%208x_2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D=(%5Cmathbf%7BA%7D+%5Cmathbf%7BA%7D%5ET)%5Cmathbf%7Bx%7D=%5Cbegin%7Bbmatrix%7D1+1%20&amp;%202+3%5C%5C3+2%20&amp;%204+4%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D=%5Cbegin%7Bbmatrix%7D2%20&amp;%205%5C%5C5%20&amp;%208%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%202x_1%20+%205x_2%20%5C%5C%205x_1%20+%208x_2%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>So the gradient of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cnabla_x%20f(x)%20=%20%5Cbegin%7Bbmatrix%7D%202x_1%20+%205x_2%20%5C%5C%205x_1%20+%208x_2%20%5Cend%7Bbmatrix%7D">.</p>
</section>
<section id="ordinary-least-square" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="ordinary-least-square"><span class="header-section-number">1.4</span> Ordinary Least Square</h3>
<p>For the <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%201"> vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D">, the <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20k"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">, the <img src="https://latex.codecogs.com/png.latex?k%20%5Ctimes%201"> vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cbeta%7D">, when <img src="https://latex.codecogs.com/png.latex?L=(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)">, what is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Cmathbf%7B%5Cbeta%7D%7D">?</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AL%20&amp;=%20(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%20%5C%5C%0A&amp;=%20%5Cmathbf%7By%7D%5ET%5Cmathbf%7By%7D%20-%20%5Cmathbf%7By%7D%5ET%5Cmathbf%7BA%7D%5Cmathbf%7B%5Cbeta%7D%20-%20%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%20%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D%20%5C%5C%0A&amp;=%20%5Cmathbf%7By%7D%5ET%5Cmathbf%7By%7D%20-%202%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%20%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>Now, we can take the derivative of <img src="https://latex.codecogs.com/png.latex?L"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cbeta%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Cmathbf%7B%5Cbeta%7D%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Cmathbf%7B%5Cbeta%7D%7D%20(%5Cmathbf%7By%7D%5ET%5Cmathbf%7By%7D%20-%202%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%20%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%20%5C%5C%0A&amp;=%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Cmathbf%7B%5Cbeta%7D%7D%20(-%202%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%20%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%20%5C%5C%0A&amp;=%20-%202%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%202%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D"> is the solution to the optimization problem by taking its derivative with respect to and setting it equal to zero.</p>
<p>Starting with the expression for <img src="https://latex.codecogs.com/png.latex?L">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL=(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%0A"></p>
<p>Expanding the quadratic term gives:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL=%5Cmathbf%7By%7D%5ET%5Cmathbf%7By%7D-%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D-%5Cmathbf%7By%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D+%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D%0A"></p>
<p>Taking the derivative of <img src="https://latex.codecogs.com/png.latex?L"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cbeta%7D"> and setting it to zero gives:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Cmathbf%7B%5Cbeta%7D%7D%20=%20-2%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%202%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D%20=%200%0A"></p>
<p>Solving for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cbeta%7D"> gives:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D=%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7B%5Cbeta%7D=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%0A"> Multiplying both sides of the equation by <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)"> gives:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5Cmathbf%7B%5Cbeta%7D=%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%0A"></p>
<p>Therefore, we have verified that <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Cbeta%7D=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%0A"> is the solution to the optimization problem of OLS (Ordinary Least Square).</p>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/derivative_matrix_vector.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Matrix Calculus (1) - Matrix to Vector Derivatives</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/derivative_vector_matrix.html</link>
  <description><![CDATA[ 



<section id="matrix-to-vector-derivatives" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="matrix-to-vector-derivatives"><span class="header-section-number">1</span> Matrix to Vector Derivatives</h2>
<p>The matrix-to-vector derivative is a derivative where a function <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7BX%7D)"> maps an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> to a <img src="https://latex.codecogs.com/png.latex?p">-dimensional vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D">, and we want to find the derivative of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">. It is denoted by <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20%5Cmathbf%7BX%7D%7D">.</p>
<p>Formally, let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D%20=%20f(%5Cmathbf%7BX%7D)%20%5Cin%20%5Cmathbb%7BR%7D%5Ep"> be a function that maps an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix to a <img src="https://latex.codecogs.com/png.latex?p">-dimensional vector. Then, the matrix-to-vector derivative is defined as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B11%7D%7D%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B12%7D%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B1n%7D%7D%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B21%7D%7D%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B22%7D%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B2n%7D%7D%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7Bm1%7D%7D%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7Bm2%7D%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7Bmn%7D%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>where each element of the matrix is the derivative of the corresponding element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D"> with respect to the corresponding element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7BX%7D)%20=%20%5Cmathbf%7BA%7D%5Cmathbf%7BX%7D+%5Cmathbf%7Bb%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bp%20%5Ctimes%20m%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D%20%5Cin%20%5Cmathbb%7BR%7D%5Ep">. Then, the matrix-to-vector derivative of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D%20=%20f(%5Cmathbf%7BX%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> is: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20%5Cmathbf%7BX%7D%7D%20=%20%5Cmathbf%7BA%7D%0A"></p>
<section id="differentiation-of-quadratic-form" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="differentiation-of-quadratic-form"><span class="header-section-number">1.1</span> Differentiation of Quadratic Form</h3>
<p>This is because the output of this differentiation is a vector (with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">), rather than a scalar.</p>
<p>The differentiation of a quadratic form is the process of finding the gradient of a quadratic form with respect to its input vector.</p>
<p>Given a quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is an <img src="https://latex.codecogs.com/png.latex?n">-dimensional column vector, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En"> is a vector, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> symmetric matrix, the derivative of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%7B%5Cmathbf%20x%7D%20f(%5Cmathbf%20x)%20=%20(A%20+%20A%5ET)%5Cmathbf%20x%20+%20b%0A"></p>
<p>In this expression, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET"> is the transpose of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A+%5Cmathbf%20A%5ET"> is written instead of <img src="https://latex.codecogs.com/png.latex?2%5Cmathbf%20A"> when calculating the gradient of a quadratic form. It is because in general, the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> might not be symmetric, so <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5Cneq%20%5Cmathbf%20A%5ET">. However, for any matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, we have <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A+%5Cmathbf%20A%5ET%20=%20(%5Cmathbf%20A+%5Cmathbf%20A%5ET)%5ET">, which is a symmetric matrix. Therefore, by writing the gradient as <img src="https://latex.codecogs.com/png.latex?%5Cnabla_x%20f(x)%20=%20(%5Cmathbf%20A+%5Cmathbf%20A%5ET)x">, we ensure that the gradient is always a symmetric matrix, even if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is not symmetric. This is useful in many applications where symmetric matrices are preferred. But if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is constrained to be a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?2%5Cmathbf%20A"> can be written.</p>
</section>
<section id="when-mathbfa-is-symmetric" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="when-mathbfa-is-symmetric"><span class="header-section-number">1.2</span> When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is Symmetric</h3>
<p>As an example, consider the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=x_1%5E2+2x_1x_2+3x_2%5E2">, which can be written in the form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D">, where:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20x=%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%20x_2%20%5Cend%7Bbmatrix%7D,%20%5Cmathbf%20A=%5Cbegin%7Bbmatrix%7D%201%20&amp;%201%20%5C%5C%201%20&amp;%203%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>The derivative of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is then:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D=(%5Cmathbf%7BA%7D+%5Cmathbf%7BA%7D%5ET)%5Cmathbf%7Bx%7D=%5Cbegin%7Bbmatrix%7D2%20&amp;%202%5C%5C2%20&amp;%206%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D=%5Cbegin%7Bbmatrix%7D2x_1+2x_2%5C%5C2x_1+6x_2%5Cend%7Bbmatrix%7D%0A"></p>
<p>This represents the gradient vector of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> at any point <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
</section>
<section id="when-mathbfa-is-not-symmetric" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="when-mathbfa-is-not-symmetric"><span class="header-section-number">1.3</span> When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is Not Symmetric</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%203%20&amp;%204%20%5Cend%7Bbmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D">.</p>
<p>Then, we have <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%203%20&amp;%204%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20+%202x_2%20%5C%5C%203x_1%20+%204x_2%20%5Cend%7Bbmatrix%7D%20=%20x_1%5E2%20+%205x_1x_2%20+%204x_2%5E2">.</p>
<p>To find the gradient of this quadratic form, we can take the partial derivatives of <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2"> with respect to each variable:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20x_1%7D%20(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D)%20=%202x_1%20+%205x_2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20x_2%7D%20(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D)%20=%205x_1%20+%208x_2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D=(%5Cmathbf%7BA%7D+%5Cmathbf%7BA%7D%5ET)%5Cmathbf%7Bx%7D=%5Cbegin%7Bbmatrix%7D1+1%20&amp;%202+3%5C%5C3+2%20&amp;%204+4%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D=%5Cbegin%7Bbmatrix%7D2%20&amp;%205%5C%5C5%20&amp;%208%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%202x_1%20+%205x_2%20%5C%5C%205x_1%20+%208x_2%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>So the gradient of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cnabla_x%20f(x)%20=%20%5Cbegin%7Bbmatrix%7D%202x_1%20+%205x_2%20%5C%5C%205x_1%20+%208x_2%20%5Cend%7Bbmatrix%7D">.</p>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/derivative_vector_matrix.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
</item>
</channel>
</rss>
