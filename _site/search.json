[
  {
    "objectID": "about.html#enthusiastic-data-scientist",
    "href": "about.html#enthusiastic-data-scientist",
    "title": "Kwangmin Kim",
    "section": "Enthusiastic Data Scientist",
    "text": "Enthusiastic Data Scientist\n\nInterests\nData Modeling, Statistics, Machine Learning, Deep Learning, Optimization"
  },
  {
    "objectID": "docs/blog/index.html",
    "href": "docs/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "P-value\n\n\nStatistical Hypothesis Test\n\n\n\n\nStatistics\n\n\n\n\nP value is one of the most commonly used statistcal index to show confidence of a hypothesis testing result of your experiment.\n\n\n\n\n\n\nDec 8, 2022\n\n\nKwangmin Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/blog/posts/2022-12-08-P-value/index.html",
    "href": "docs/blog/posts/2022-12-08-P-value/index.html",
    "title": "P-value",
    "section": "",
    "text": "![](coding.png){.preview-image fig-align=“center” fig-alt=“just for testing”}"
  },
  {
    "objectID": "docs/CV/index.html",
    "href": "docs/CV/index.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "an application for a position in industry, non-profit, and public sector\nskills, work experience, educuation\nwithin 2 pages"
  },
  {
    "objectID": "docs/CV/index.html#cv",
    "href": "docs/CV/index.html#cv",
    "title": "Curriculum Vitae",
    "section": "CV",
    "text": "CV\n\napplying for positions in academia, fellowships and grants\nacademic accomplishments\nLength depends upon experience and includes a complete list of publications, posters, and presentations\nbegins with education and can include name of advisor and dissertation title or summary (see examples). Also used for merit/tenure review and sabbatical leave"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "(Working on creating my website…)\n\nShort Introduction\nI am passionate and self-taught in statistics, machine learning, deep learning, and programming using open-source tools such as Python, R, SQL, and Airflow. It has been 7years to deal with data research from data modeling to data visualization through modeling.\n\n\nExperience\nMy educational background is a BS in biochemistry from South Korea, BA in mathematics in the U.S., and MS in biostatistics in the U.S. Since I have a strong background in biology, my career started in the medical realm of analytics. My work experience exposed me to an environment where I had to communicate with the laymen to understand data science to them.\n\n\nReason for Blogging in Data Science\n(Pursuing Goal in Data Science)\nSince I deal with data, SW developers, and non-professional coworkers, I have found myself needing more knowledge in mathematics, statistics, and IT to acquire skills in giving an easy, clear, and concise explanation for better communication. That’s why I decided to start blogging to have opportunities to convince myself of new skills in machine learning, not only with fundamental statistics."
  },
  {
    "objectID": "docs/projects/back_fitting.html",
    "href": "docs/projects/back_fitting.html",
    "title": "Back Fitting Algorithm",
    "section": "",
    "text": "flowchart LR\n\nhelper.R --> visualize.R"
  },
  {
    "objectID": "docs/projects/high_dimension.html",
    "href": "docs/projects/high_dimension.html",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "",
    "text": "library(glmnet)\n\n필요한 패키지를 로딩중입니다: Matrix\n\n\nLoaded glmnet 4.1-6\n\nlibrary(tidyverse)\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::expand() masks Matrix::expand()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tidyr::pack()   masks Matrix::pack()\n✖ tidyr::unpack() masks Matrix::unpack()\n\nset.seed(20221213) \n\n\nn <- 1000    # sample size\np <- 10000     # the number of predictors\nsignificant_p <- p*0.25  # the number of predictors associated with a response variable\n\nx <- matrix(rnorm(n*p), nrow=n, ncol=p)\ny <- apply(x[,1:significant_p], 1, sum) + rnorm(n)\n\n# Split data into train (2/3) and test (1/3) sets\ntrain_rows <- sample(1:n, .66*n)\nx.train <- x[train_rows, ]\nx.test <- x[-train_rows, ]\n\ny.train <- y[train_rows]\ny.test <- y[-train_rows]\n\nlist.of.fits <- list()\nfor (i in 0:10) {\n  fit.name <- paste0(\"alpha\", i/10)\n  \n  list.of.fits[[fit.name]] <-\n    cv.glmnet(x.train, y.train, type.measure=\"mse\", alpha=i/10, \n      family=\"gaussian\")\n}\n\nresults <- data.frame()\nfor (i in 0:10) {\n  fit.name <- paste0(\"alpha\", i/10)\n  \n  predicted <- \n    predict(list.of.fits[[fit.name]], \n      s=list.of.fits[[fit.name]]$lambda.1se, newx=x.test)\n  \n  mse <- mean((y.test - predicted)^2)\n  \n  temp <- data.frame(alpha=i/10, mse=mse, fit.name=fit.name)\n  results <- rbind(results, temp)\n}\n\nresults\n\n   alpha      mse fit.name\n1    0.0 2529.146   alpha0\n2    0.1 2529.146 alpha0.1\n3    0.2 2529.146 alpha0.2\n4    0.3 2529.146 alpha0.3\n5    0.4 2529.146 alpha0.4\n6    0.5 2529.146 alpha0.5\n7    0.6 2529.146 alpha0.6\n8    0.7 2529.146 alpha0.7\n9    0.8 2529.146 alpha0.8\n10   0.9 2529.146 alpha0.9\n11   1.0 2529.146   alpha1"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-QMD-formatting/qmd-formatting.html",
    "href": "docs/blog/posts/2022-12-10-QMD-formatting/qmd-formatting.html",
    "title": "FDA",
    "section": "",
    "text": "dsf"
  }
]