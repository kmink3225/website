[
  {
    "objectID": "about.html#enthusiastic-data-scientist",
    "href": "about.html#enthusiastic-data-scientist",
    "title": "Kwangmin Kim",
    "section": "Enthusiastic Data Scientist",
    "text": "Enthusiastic Data Scientist\n\nInterests\nData Modeling, Statistics, Machine Learning, Deep Learning, Optimization"
  },
  {
    "objectID": "docs/blog/index.html",
    "href": "docs/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Blog Content List\n\n\nGuide Map of Blog Contents\n\n\n\n\nAll List\n\n\n\n\nAs the number of blog topics increased, it became difficult to organize the contents. In order to increase content accessibility and to look at the relevance of blogs, links to the content list by topic are listed.\n\n\n\n\n\n\nJan 1, 3000\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Statistics\n\n\nGuide Map of Blogs in Statistics Section\n\n\n\n\nStatistics\n\n\n\n\nJust enumerating a list of the contents in the the statistics section\n\n\n\n\n\n\nMay 1, 2100\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Machine Learning\n\n\nGuide Map of Blogs in Machine Learning (ML) Section\n\n\n\n\nML\n\n\n\n\nJust enumerating a list of the contents in the ML section\n\n\n\n\n\n\nApr 1, 2100\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Mathematics\n\n\nGuide Map of Blogs in Mathematics Section\n\n\n\n\nMathematics\n\n\n\n\nRather than studying pure mathematics, I focus on studying and organizing mathematical concepts by filling out this mathematics blog section with some mathematics stuff for deep learning\n\n\n\n\n\n\nMar 1, 2100\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Deep Learning\n\n\nGuide Map of Blogs in Deep Learning Section\n\n\n\n\nDL\n\n\n\n\nJust enumerating a list of the contents in the Deep Learning (DL) section\n\n\n\n\n\n\nJan 1, 2100\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Engineering\n\n\nEngineering for Python\n\n\n\n\nEngineering\n\n\n\n\nEngineering for Data Science\n\n\n\n\n\n\nJan 1, 2100\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Language\n\n\nContants of Blogs in Language Section\n\n\n\n\nLanguage\n\n\n\n\nJust enumerating a list of the contents in the Language section\n\n\n\n\n\n\nJan 1, 2100\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Patent\n\n\nGuide Map of Blogs\n\n\n\n\nPatent\n\n\n\n\nJust enumerating a list of the contents in the the statistics section\n\n\n\n\n\n\nJan 1, 2100\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Validation\n\n\nVerification & Validation on Software\n\n\n\n\nSurveilance\n\n\n\n\nAny business that directly or indirectly affects human health or life must comply with regulations regarding inspection, testing, verification and validation. It is necessary to systematically manage and document risks by arranging regulatory policy data for the medical and IT industry. These materials are rigorous and conservative, so there are various documents for each case, but the underlying principles have the same root. This blog section summarises and organizes documents with fundamental explanations of regulation for each area. 사람의 건강이나 생명에 직 간접적으로 영향을 미치는 어떠한 비즈니스는 검사, 테스트, 검증 및 인증에 관한 규정을 준수해야한다. 의료분야와 IT 분야에 대한 규정 방침 자료를 정리하여 체계적인 위험 관리를 해야한다. 이러한 자료들은 엄격하고 보수적이어서 각 사례마다 다양한 문서들이 존재하지만 그 근본 원리는 같다. 이 블로그에서는 각 영역마다 근본이 되는 문서들을 요약 및 정리한다.\n\n\n\n\n\n\nJan 1, 2090\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensorflow - Data Input Pipeline\n\n\ntf.data, optimize pipeline performance, analyze pipeline performance\n\n\n\n\nLanguage\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 24, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nMinimizer & Maximizer\n\n\nLocal & Global Minimizer and Maximizer, Critical points\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 23, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Types\n\n\ntemplate\n\n\n\n\ntemplate\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 23, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLDA (2) - CD4+ Cell Depletion\n\n\nOverview\n\n\n\n\nStatistics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 23, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLDA (1) - Introduction\n\n\nOverview\n\n\n\n\nStatistics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 23, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nStorage and Database\n\n\nWeek3\n\n\n\n\nEngineering\n\n\n\n\nAWS\n\n\n\n\n\n\nMar 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nDifferentiation - Higher Order Derivative\n\n\nHigher Order Derivative\n\n\n\n\nMathematics\n\n\n\n\nTo solve optimization problems, it is required to know about derivatives because derivatives are mostly used 최적화 문제를 풀기위해 미분이 항상 사용되기 떄문에 미분에 대해서 알 필요가 있다.\n\n\n\n\n\n\nMar 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategorical Data Analysis\n\n\ntemplate\n\n\n\n\ntemplate\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 17, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nTaylor’s Series\n\n\nTaylor’s Series and Second Derivative Test\n\n\n\n\nMathematics\n\n\n\n\n\n\n\n\n\n\n\nMar 16, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\epsilon - \\delta\\) Method\n\n\nThe Precise definition of a Limit\n\n\n\n\nMathematics\n\n\n\n\nPre-requisite for convergence in probability and convergence in distribution.\n\n\n\n\n\n\nMar 14, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nCNN\n\n\n\n\n\n\n\nDL\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputing and Networking\n\n\nWeek2\n\n\n\n\nEngineering\n\n\n\n\nAWS\n\n\n\n\n\n\nMar 9, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMomment Generating Function\n\n\nMoment\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransformation of Random Variables\n\n\nTransformation of Functions\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransofrmations of Functions\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nFeb 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifferentiation - Chain Rule & Partial Derivative\n\n\nDerivative of Multivariable Composite Function\n\n\n\n\nMathematics\n\n\n\n\nTo solve optimization problems, it is required to know about derivatives because derivatives are mostly used 최적화 문제를 풀기위해 미분이 항상 사용되기 떄문에 미분에 대해서 알 필요가 있다.\n\n\n\n\n\n\nFeb 10, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNaive Bayes Classification\n\n\nBayes’ Rule, Supervised Learning, Classification, Regression\n\n\n\n\ntemplate\n\n\n\n\ntemplate\n\n\n\n\n\n\nFeb 6, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBayes’ Rule\n\n\nBayesean Statistics, Frequentist Statistics, Deductive Method, Inductive Method, Proof by Contratiction, Hypothetical Deductive Method, Total Probability Rule, Naive Bayes\n\n\n\n\nStatistics\n\n\n\n\nProbability for statistics, machine learning and deep learning. Studying conditional probability is fundamental to stochastic processes, reinforcement learning, and naive Bayes classification, so it’s important to understand the concept.\n\n\n\n\n\n\nFeb 5, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConditional Probability\n\n\nConditional Probability\n\n\n\n\nStatistics\n\n\n\n\nProbability for statistics, machine learning and deep learning. Studying conditional probability is fundamental to stochastic processes, reinforcement learning, and naive Bayes classification, so it’s important to understand the concept.\n\n\n\n\n\n\nFeb 5, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability\n\n\nSet Thoery, Theory Errors, Calculus, Real Analysis, Measure Thoery\n\n\n\n\nStatistics\n\n\n\n\nProbability for statistics, machine learning and deep learning.\n\n\n\n\n\n\nFeb 5, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nDifferentiation - Univariabe Function\n\n\nUnivariable Differentiation\n\n\n\n\nMathematics\n\n\n\n\nTo solve optimization problems, it is required to know about derivatives because derivatives are mostly used 최적화 문제를 풀기위해 미분이 항상 사용되기 떄문에 미분에 대해서 알 필요가 있다.\n\n\n\n\n\n\nFeb 4, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (9) Priority Queue\n\n\nPython List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nFeb 3, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytorch Introduction\n\n\noverview, object creation, indexing, concatenating, casting, shape, transpose, arithematic operations, matrix multiplication, mean, max, argmax, dimension manipulation, automatic differenctiation\n\n\n\n\nML\n\n\n\n\nLearn how to manipulate Pytorch, one of the most commonly used Python frameworks to implement machine learning algorithms using Python. 파이썬을 이용하여 머신러닝 알고리즘을 구현하기 위해 가장 대표적으로 쓰이는 파이썬 package중 하나인 Tensor flow조작법에 대해 알아본다.\n\n\n\n\n\n\nFeb 3, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensor Introduction\n\n\noverview, object creation, indexing, concatenating, casting, shape, transpose, arithematic operations, matrix multiplication, mean, max, argmax, dimension manipulation, automatic differenctiation\n\n\n\n\nML\n\n\n\n\nLearn how to manipulate Tensor flow, one of the most commonly used Python frameworks to implement machine learning algorithms using Python. 파이썬을 이용하여 머신러닝 알고리즘을 구현하기 위해 가장 대표적으로 쓰이는 파이썬 package중 하나인 Tensor flow조작법에 대해 알아본다.\n\n\n\n\n\n\nFeb 3, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComposite Function\n\n\nDraft\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nJan 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nJan 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction - Multivariable Scalar Function\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nJan 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nFunction - Multivariable Vector Function\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nJan 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction - Univariable Scalar Function\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nJan 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction - Univariable Vector Function\n\n\nOne to Many\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nJan 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nData Structure (8) Binary Search Tree\n\n\nPython List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANCOVA\n\n\nANOVA, one-way ANOVA, Two-way ANOVA, ANCOVA, repeated measures ANOVA, MANOVA, MANCOVA\n\n\n\n\nStatistics\n\n\n\n\nThe analysis of variance (ANOVA) is one of the most widely used statistical techniques. When we conduct a comparison testing of multiple groups such as A, B, and C on each with numeric data, the statistical test for a significant difference among the groups is called analysis of variance, or ANOVA.\n\n\n\n\n\n\nJan 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepeated Meausres ANOVA\n\n\nANOVA, one-way ANOVA, Two-way ANOVA, ANCOVA, repeated measures ANOVA, MANOVA, MANCOVA\n\n\n\n\nStatistics\n\n\n\n\n.\n\n\n\n\n\n\nJan 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (7) Deque\n\n\nPython List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 26, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (10) Graph\n\n\nPython List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 20, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (6) Queue\n\n\nPython List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 19, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (5) Stack\n\n\nPython List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 19, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (2) Array\n\n\nArray\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (3) Linked List\n\n\nLinked List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nData Structure (1) Overview\n\n\nOverview\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 17, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (4) Python List\n\n\nPython List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 17, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormality Check\n\n\nShapiro-Wilk Test, Anderson–Darling test, Kolmogorov–Smirnov test, and Lilliefors test, qqplot\n\n\n\n\nStatistics\n\n\n\n\nnormality check\n\n\n\n\n\n\nJan 16, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA\n\n\nANOVA, one-way ANOVA, Two-way ANOVA, ANCOVA, repeated measures ANOVA, MANOVA\n\n\n\n\nStatistics\n\n\n\n\nThe analysis of variance (ANOVA) is one of the most widely used statistical techniques. When we conduct a comparison testing of multiple groups such as A, B, and C on each with numeric data, the statistical test for a significant difference among the groups is called analysis of variance, or ANOVA.\n\n\n\n\n\n\nJan 7, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMANOVA\n\n\nANOVA, one-way ANOVA, Two-way ANOVA, ANCOVA, repeated measures ANOVA, MANOVA\n\n\n\n\nStatistics\n\n\n\n\nThe analysis of variance (ANOVA) is one of the most widely used statistical techniques. When we conduct a comparison testing of multiple groups such as A, B, and C on each with numeric data, the statistical test for a significant difference among the groups is called analysis of variance, or ANOVA.\n\n\n\n\n\n\nJan 7, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFDA Software Validation Guidance Presentation\n\n\nSource: General Principles of Software Validation\n\n\n\n\nSurveilance\n\n\n\n\nThe purpose of this article is to help understand the summary of the ‘General Principles of the ’Software Validation; Final Guidance for Industry and FDA Staff’ document issued on 2002-01-11. This article provides short sentences with many diagrams for intuitive understanding.\n\n\n\n\n\n\nDec 28, 2022\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\np-values\n\n\nStatistical Hypothesis Test\n\n\n\n\nStatistics\n\n\n\n\np-value is one of the most commonly used statistcal index to show significance level of a hypothesis testing result of your experiment.\n\n\n\n\n\n\nDec 15, 2022\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nFDA Software Validation Guidance Summary\n\n\nDcoument: General Principles of Software Validation\n\n\n\n\nSurveilance\n\n\n\n\nThe purpose of this blog is to get a rough concept of the FDA approval process by making a summary of the ‘General Principles of the ’Software Validation; Final Guidance for Industry and FDA Staff’ document issued on 2002-01-11. So far, the document seems to be still valid taking into account that its guidance for the FDA approval are broad, general, and comprehensive, and that many recent FDA documents supplement it.\n\n\n\n\n\n\nDec 15, 2022\n\n\nKwangmin Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/blog/posts/2022-12-08-P-value/index.html",
    "href": "docs/blog/posts/2022-12-08-P-value/index.html",
    "title": "p-values",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n실험 결과가 우연히 생성된 것보다 더 극단적인 경우를 통계적으로 유의하다고 한다. (결과가 귀무 가설 하의 분포와 다른 분포에서 나올 수 있다고 생각해 보십시오.)\n\n\n\n’p-값’의 p는 ’확률’을 나타냅니다. p-값은 실험에서 관찰된 결과가 귀무 가설 하에서 발생할 수 있는 극단적인 결과를 얻을 확률의 합계입니다. 즉, p-값은 실험 결과가 우연히 얻어질 확률입니다.\n\n\n\n우연한 결과가 통계적으로 유의미하다고 하기 위해 실험의 실제 결과를 넘어서야 하는 극단적이거나 드문 결과의 확률 임계값입니다.\n\n\n\n귀무가설이 참인데 실수로 귀무가설을 기각하는 오류\n\n\n\n대립가설이 참인데 실수로 귀무가설을 기각하지 못하는 오류\n\n\n\n\n\np-값은 테스트 결과의 유의성을 측정할 때 효율적이고 효과적인 통계 지표입니다. 회귀 분석을 수행했다고 가정해 봅시다. 그런 다음 회귀 모델의 결과로 베타 계수와 표준 오차를 얻을 수 있습니다.\n\nNumber of Cases of How You Interpret Regresssion Result\n\n\n\nhigh Standard Error\nlow Standard Error\n\n\n\n\nhigh \\(\\beta\\)\nUnclear Interpretation\nOK\n\n\nlow \\(\\beta\\)\nOK\nUnclear Interpretation\n\n\n\n위의 표는 회귀 모델의 결과를 해석할 수 있는 경우의 수를 보여줍니다. 각 계수 \\(\\beta\\) 에 대해 4개의 경우가 있습니다.\n\nhigh \\(\\beta\\) and high Standard Error mean that 해당 변수가 강한 영향을 미치나 그 영향이 변동될 수 있음을 의미하므로 회귀 모델에서 도출된 \\(\\beta\\) 계수는 유의하지 않을 가능성이 높습니다. 그 효과가 통계적으로 유의미한지 확신할 수 없습니다.\nhigh \\(\\beta\\) and low Standard Error mean that the corresponding variable has a strong effect, and its variation is small, so the \\(\\beta\\) coefficient resulted from your regression model is likely to be significant.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, its effect has a high variation. So, we can clearly interprete the variable with the \\(\\beta\\) as a variable that is not significantly associated with your response variable.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, but its effect has a low variation. So, it is difficult to conclude that the variable is significant.\n\nThe p-value could be used to provide a clearer interpretation of the unclear situation (i.e. (high \\(\\beta\\), high Standard Error), (low \\(\\beta\\), high Standard Error) ) by looking at the ratio of the estimated value of a parameter(= \\(\\beta\\)) to its standard error on the distribution under the null hypothesis. By general convention, the cut-off of p-value indicating statistical signficance is 0.05.\n\n\n\nDespite the goodness of p-value, it is controversial to make a decision based solely on the p-value. As mentioned above, p-value is the probability that the result of your experiment is due to chance. In addition, looking into \\(\\frac{\\beta}{\\frac{s.e}{\\sqrt{n}}}\\), the p-value gets smaller as the sample size becomes larger and larger. It should be avoided that something is proved just because a low p-value is calucated.\nEven if a result is statistically significant, that does not necessarily mean it has real significance. A small difference that has no practical meaning can be statistically significant if the sample size is large enough. It is because large samples ensure that meaningless effects can become big enough to possibly exclude chance due to simple math.\nThe American Statistical Association (ASA) has released a statement of six principles for researchers and journal editors on p-values:\nSource: ASA Statement on Statistical Significance and p-values\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nProper inference requires full reporting and transparency.\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n\n\n\n\n\nPersonally, I make use of p-values as a tool in data science to just check whether a model result or a set of variables that appears interesting and useful is in the range of normal variability by chance in the exploratory data analysis(EDA) or data mining step.\nIf you want to get a statistical significance level through p-values, other methodologies could help increase the accuracy of real significance such as permuted p-values, q-values, and penalization on multiple comparison tests\n\n\n\n\n\nIt is said to be statistically significant if a result of your experiment is more extreme than one that is produced by chance. (Try thinking that your result could have come from a different distribution from the one under the null hypothesis.)\n\n\n\np of ‘p-value’ stands for ‘probability’. The p-value is the summation of the probabilities of obtaining results as extreme as the observed results from your experiments could occur under the null hypothesis. In other words, p-value is the probability that the result of your experiment is obtained by chance.\n\n\n\nThe probability threshold of the extreme or rarer results that chance results must be beyond actual results of your experiments in order to be said to be statistically significant.\n\n\n\nconcluding \\(H_o\\) or the null hypothesis is true by mistake.\n\n\n\nconcluding \\(H_a\\) or the alternative hypothesis is true by mistake.\n\n\n\n\n\np-value is an efficient and effective statistical index when to measure the significance of your test result. Let’s make an assumption that you have conducted a regression analysis. Then, you can get beta coefficients and their standard errors as results of your regression model.\n\nNumber of Cases of How You Interpret Regresssion Result\n\n\n\nhigh Standard Error\nlow Standard Error\n\n\n\n\nhigh \\(\\beta\\)\nUnclear Interpretation\nOK\n\n\nlow \\(\\beta\\)\nOK\nUnclear Interpretation\n\n\n\nThe above table shows the number of cases you can interprete the results of your regression model. There are 4 cases for each coefficient \\(\\beta\\).\n\nhigh \\(\\beta\\) and high Standard Error mean that the corresponding variable has a strong effect but its effect may be fluctuated, so the \\(\\beta\\) coefficient resulted from your regression model is likely to be not significant. We are not sure that its effect is statistically significant.\nhigh \\(\\beta\\) and low Standard Error mean that the corresponding variable has a strong effect, and its variation is small, so the \\(\\beta\\) coefficient resulted from your regression model is likely to be significant.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, its effect has a high variation. So, we can clearly interprete the variable with the \\(\\beta\\) as a variable that is not significantly associated with your response variable.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, but its effect has a low variation. So, it is difficult to conclude that the variable is significant.\n\nThe p-value could be used to provide a clearer interpretation of the unclear situation (i.e. (high \\(\\beta\\), high Standard Error), (low \\(\\beta\\), high Standard Error) ) by looking at the ratio of the estimated value of a parameter(= \\(\\beta\\)) to its standard error on the distribution under the null hypothesis. By general convention, the cut-off of p-value indicating statistical signficance is 0.05.\n\n\n\nDespite the goodness of p-value, it is controversial to make a decision based solely on the p-value. As mentioned above, p-value is the probability that the result of your experiment is due to chance. In addition, looking into \\(\\frac{\\beta}{\\frac{s.e}{\\sqrt{n}}}\\), the p-value gets smaller as the sample size becomes larger and larger. It should be avoided that something is proved just because a low p-value is calucated.\nEven if a result is statistically significant, that does not necessarily mean it has real significance. A small difference that has no practical meaning can be statistically significant if the sample size is large enough. It is because large samples ensure that meaningless effects can become big enough to possibly exclude chance due to simple math.\nThe American Statistical Association (ASA) has released a statement of six principles for researchers and journal editors on p-values:\nSource: ASA Statement on Statistical Significance and p-values\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nProper inference requires full reporting and transparency.\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n\n\n\n\n\nPersonally, I make use of p-values as a tool in data science to just check whether a model result or a set of variables that appears interesting and useful is in the range of normal variability by chance in the exploratory data analysis(EDA) or data mining step.\nIf you want to get a statistical significance level through p-values, other methodologies could help increase the accuracy of real significance such as permuted p-values, q-values, and penalization on multiple comparison tests"
  },
  {
    "objectID": "docs/blog/posts/2022-12-08-P-value/index.html#p-value-good-vs-bad",
    "href": "docs/blog/posts/2022-12-08-P-value/index.html#p-value-good-vs-bad",
    "title": "p-values",
    "section": "p-value: Good vs Bad?",
    "text": "p-value: Good vs Bad?\n\nGoodness\np-value is an efficient and effective statistical index when to measure the significance of your test result. Let’s make an assumption that you have conducted a regression analysis. Then, you can get beta coefficients and their standard errors as results of your regression model.\n\nNumber of Cases of How You Interpret Regresssion Result\n\n\n\nhigh Standard Error\nlow Standard Error\n\n\n\n\nhigh \\(\\beta\\)\nUnclear Interpretation\nOK\n\n\nlow \\(\\beta\\)\nOK\nUnclear Interpretation\n\n\n\nThe above table shows the number of cases you can interprete the results of your regression model. There are 4 cases for each coefficient \\(\\beta\\).\n\nhigh \\(\\beta\\) and high Standard Error mean that the corresponding variable has a strong effect but its effect may be fluctuated, so the \\(\\beta\\) coefficient resulted from your regression model is likely to be not significant. We are not sure that its effect is statistically significant.\nhigh \\(\\beta\\) and low Standard Error mean that the corresponding variable has a strong effect, and its variation is small, so the \\(\\beta\\) coefficient resulted from your regression model is likely to be significant.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, its effect has a high variation. So, we can clearly interprete the variable with the \\(\\beta\\) as a variable that is not significantly associated with your response variable.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, but its effect has a low variation. So, it is difficult to conclude that the variable is significant.\n\nThe p-value could be used to provide a clearer interpretation of the unclear situation (i.e. (high \\(\\beta\\), high Standard Error), (low \\(\\beta\\), high Standard Error) ) by looking at the ratio of the estimated value of a parameter(= \\(\\beta\\)) to its standard error on the distribution under the null hypothesis. By general convention, the cut-off of p-value indicating statistical signficance is 0.05.\n\n\nBadness\nDespite the goodness of p-value, it is controversial to make a decision based solely on the p-value. As mentioned above, p-value is the probability that the result of your experiment is due to chance. In addition, looking into \\(\\frac{\\beta}{\\frac{s.e}{\\sqrt{n}}}\\), the p-value gets smaller as the sample size becomes larger and larger. It should be avoided that something is proved just because a low p-value is calucated.\nEven if a result is statistically significant, that does not necessarily mean it has real significance. A small difference that has no practical meaning can be statistically significant if the sample size is large enough. It is because large samples ensure that meaningless effects can become big enough to possibly exclude chance due to simple math.\nThe American Statistical Association (ASA) has released a statement of six principles for researchers and journal editors on p-values:\nSource: ASA Statement on Statistical Significance and p-values\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nProper inference requires full reporting and transparency.\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis."
  },
  {
    "objectID": "docs/blog/posts/2022-12-08-P-value/index.html#how-to-use-p-vlaues",
    "href": "docs/blog/posts/2022-12-08-P-value/index.html#how-to-use-p-vlaues",
    "title": "p-values",
    "section": "How to use p-vlaues?",
    "text": "How to use p-vlaues?\nPersonally, I make use of p-values as a tool in data science to just check whether a model result or a set of variables that appears interesting and useful is in the range of normal variability by chance in the exploratory data analysis(EDA) or data mining step.\nIf you want to get a statistical significance level through p-values, other methodologies could help increase the accuracy of real significance such as permuted p-values, q-values, and penalization on multiple comparison tests"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/index.html",
    "href": "docs/blog/posts/2022-12-10-FDA/index.html",
    "title": "FDA Software Validation Guidance Summary",
    "section": "",
    "text": "I am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto.\nThe FDA validation guidance document is a bit difficult to understand because its explanations provides abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\n\n\n2022-12-28, download this article as PDF\n2022-12-28, summary with diagrams\n\n\n\n\nFDA: General Principles of Software Validation\n\n\n\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\n\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/index.html#purpose",
    "href": "docs/blog/posts/2022-12-10-FDA/index.html#purpose",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.1 Purpose",
    "text": "2.1 Purpose\nThe purpose is to make a sketch of general validation principle of the validation of medical device software or software used to design or develop."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/index.html#scope",
    "href": "docs/blog/posts/2022-12-10-FDA/index.html#scope",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.2 Scope",
    "text": "2.2 Scope\nThe scope of this guidance is broad. The important activities for the software validation include at least:\n\nplanning,\nverfication,\ntesting,\ntraceability, and\nconfiguration management.\n\nAll of the activities above should be\n\nintegrated\nbe able to describe software life cycle management and\nbe able to describe software risk management.\n\nThe software validation and verification activities should be focused into the entire software life cycle. (It does not necessarily mean that the activies must follow any technical models.)\nThe guidance is applicable to any software related to a regulated medical device and anyone who is employed in a bio or medical industry.\n\n2.2.1 The Least Burdensome Approach\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\n2.2.2 Regulatory Requirements for Software Validation\n\nSoftware validation: a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997. (See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\nSpecific requirements for validation of device software are found in 21 CFR §820.30(g). Other design controls, such as planning, input, verification, and reviews, are required for medical device software. (See 21 CFR §820.30.)\ncomputer systems used to create, modify, and maintain electronic records and to manage electronic signatures are also subject to the validation requirements. (See 21 CFR §11.10(a).)\n\n\n2.2.2.1 Objective\nThe objective of software validation is to ensure:\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\n2.2.2.2 What to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\n\n2.2.3 Quality System Regulation vs Pre-market Submissions\nThis document does not address any specific requirements but general ones. Specific issues should be addressed to\n\nthe Office of Device Evaluation (ODE),\nCenter for Devices and Radiological Health (CDRH)\nthe Office of Blood Research and Review,\nCenter for Biologics Evaluation and Research (CBER). See the references in Appendix A for applicable FDA guidance documents for pre-market submissions."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/index.html#context-for-software-validation",
    "href": "docs/blog/posts/2022-12-10-FDA/index.html#context-for-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.3 Context for Software Validation",
    "text": "2.3 Context for Software Validation\n\nValidation elements that FDA expects to do for the Quality System regulation, using the principles and tasks are listed in Sections 4 and 5.\nAdditional specific information is available from many of the references listed in Appendix A\n\n\n2.3.1 Definition and Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology between the medical device Quality System regulation and the software industry:\n\nrequirements,\nspecification,\nverification, and\nvalidation.\n\n\n\n2.3.1.1 Requirements and Specifications\nThe Quality System regulation states\n\nthat design input requirements must be documented and\nthat specified requirements must be verified\n\nBut, the regulation does not further clarify the distinction between the terms “requirement” and “specification.”\n\nRequirement\n\ncan be any need or expectation for a system or for its software.\nreflects the stated or implied needs of the customer: requirements may be\n\nmarket-based,\ncontractual,\nstatutory, or\nan organization’s internal requirements.\n\nvarious examples of requirements\n\ndesign, functional, implementation, interface, performance, or physical requirements\n\nSoftware requirements derived from the system requirements for those aspects of system functionality\nSoftware requirements are typically stated in functional terms and are defined, refined, and updated as a development project progresses.\nSuccess in accurately and completely documenting software requirements is a crucial factor in successful validation of the resulting software.\n\nSpecification\n\ndefined as “a document that states requirements.” (See 21 CFR §820.3(y).)\nIt may refer to or include drawings, patterns, or other relevant documents\nIt usually indicates the means and the criteria whereby conformity with the requirement can be checked.\nVarious examples of written specifications\n\nsystem requirements specification,\nsoftware requirements specification,\nsoftware design specification,\nsoftware test specification,\nsoftware integration specification, etc.\n\nAll of these documents are design outputs for which various forms of verification are necessary.\n\n\n\n\n2.3.1.2 Verifiaction and Validation\nThe Quality System regulation is harmonized with ISO 8402:1994, which treats “verification” and “validation” as separate and distinct terms.\n\nSoftware verification\n\nIt provides objective evidence that the design outputs of a particular phase of the software development life cycle meet all of the specified requirements for that phase.\nIt looks for\n\nconsistency,\ncompleteness, and\ncorrectness of the software and its supporting documentation\n\nSoftware testing\n\nverification activities intended to confirm that software development output meets its input requirements.\n\nTypes of verification activities include\n\nvarious static and dynamic analyses,\ncode and document inspections,\nwalkthroughs, and other techniques.\n\n\nSoftware Validation\n\nConfirmation by examination and provision of the following objective evidence:\nEvidence 1: software specifications conform to user needs and intended uses, and\nEvidnece 2: the particular requirements implemented through software can be consistently fulfilled.\nEvidnece 3: all software requirements have been implemented correctly and completely and are traceable to system requirements.\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device.\n\nDifficulty in Software verification and validation\n\na developer cannot test forever, and\nit is difficult to know how much evidence is enough.\nIn large measure, software validation is a matter of developing a “level of confidence” that the device meets all requirements and user expectations for the software automated functions and features of the device.\nConsiderations for an acceptable level of confidence\n\nmeasures such as defects found in specifications documents,\nestimates of defects remaining,\ntesting coverage, and other techniques are all used to develop before shipping the product.\nHowever, a level of confidence varies depending upon the safety risk (hazard) posed by the automated functions of the device. (Info on safety risk is found in Section 4 and in the international standards ISO/IEC 14971-1 and IEC 60601-1-4 referenced in Appendix A).\n\n\n\n\n\n2.3.1.3 IQ/OQ/PQ\nIQ/OQ/PQ are the terminology related to user site software validation\n\nInstallation qualification (IQ)\nOperational qualification (OQ)\nPerformance qualification (PQ).\n\nDefinitions of these terms may be found in FDA’s Guideline on General Principles of Process Validation, dated May 11, 1987, and in FDA’s Glossary of Computerized System and Software Development Terminology, dated August 1995. Both FDA personnel and device manufacturers need to be aware of these differences in terminology as they ask for and provide information regarding software validation.\n\n\n\n2.3.2 Software Development as Part of System Design\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nthe user’s needs\nintended uses from which the product is developed.\n\nA primary goal of software validation is to then demonstrate that all completed software products comply with all documented software and system requirements.\n\n\n2.3.3 Software Is Different from Hardware\nSoftware engineering needs an even greater level of managerial scrutiny and control than does hardware engineering.\n\n\n2.3.4 Benefits of Software Validation\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nSoftware validation can also reduce long term costs by making it easier and less costly to reliably modify software and revalidate software changes.\n\n\n\n2.3.5 Design Review\nDesign reviews are documented, comprehensive, and systematic examinations of a design to evaluate\n\nthe adequacy of the design requirements,\nthe capability of the design to meet these requirements, and\nto identify problems.\n\nDesign review is a primary tool for managing and evaluating development projects.\n\nIt is strongly recommended that it should be formal design because it is more structured than the informal one.\nIt includes participation from others outside the development team.\nIt may review reference or include results from other formal and informal reviews.\nDesign reviews should include\n\nexamination of development plans,\nrequirements specifications,\ndesign specifications,\ntesting plans and procedures,\nall other documents and activities associated with the project,\nverification results from each stage of the defined life cycle, and\nvalidation results for the overall device.\n\nThe Quality System regulation requires that at least one formal design review be conducted during the device design process. However, it is recommended that multiple design reviews be conducted\n\n(e.g., at the end of each software life cycle activity, in preparation for proceeding to the next activity).\n\nFormal design reviews documented should include:\n\nthe appropriate tasks and expected results, outputs, or products been established for each software life cycle activity\ncorrectness, completeness, consistency, and accuracy\nsatisfaction for the standards, practices, and conventions of that activity\nestablishment of a proper basis for initiating tasks for the next software life cycle activity"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/index.html#principles-of-software-validation",
    "href": "docs/blog/posts/2022-12-10-FDA/index.html#principles-of-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.4 Principles of Software Validation",
    "text": "2.4 Principles of Software Validation\n\n2.4.1 Requirements\nA documented software requirements specification provides a baseline for both validation and verification. The software validation process must include an established software requirements specification (Ref: 21 CFR 820.3(z) and (aa) and 820.30(f) and (g)).\n\n\n2.4.2 Defect Prevention\nIn order to establish that confidence, software developers should use a mixture of methods and techniques to prevent software errors and to detect software errors that do occur.\n\n\n2.4.3 Time and Effort\nPreparation for software validation should begin early, i.e., during design and development planning and design input. The final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\n2.4.4 Software Life Cycle\n\nSoftware validation takes place within the environment of an established software life cycle.\nThe software life cycle contains software engineering tasks and documentation necessary to support the software validation effort.\nspecific verification and validation tasks need to be appropriate for the intended use of the software\n\n\n\n2.4.5 Plans\n\nThe software validation process is defined and controlled through the use of a plan.\nThe software validation plan defines “what” is to be accomplished through the software validation effort.\nSoftware validation plans specify areas such as\n\nscope,\napproach,\nresources,\nschedules and the types and extent of activities,\ntasks, and\nwork items.\n\n\n\n\n2.4.6 Procedures\nThe software validation process is executed through the use of procedures. These procedures establish “how” to conduct the software validation effort. The procedures should identify the specific actions or sequence of actions that must be taken to complete individual validation activities, tasks, and work items.\n\n\n2.4.7 Software Validation After a Change\n\nDue to the complexity of software, a small local change may have a significant global system impact.\nIf a change exists in the software, the whole validation status of the software needs to be re-established.\nneed to determine the extent and impact of that change on the entire software system.\nthe software developer should then conduct an appropriate level of software regression testing to show that unchanged but vulnerable portions of the system have not been adversely affected.\n\n\n\n2.4.8 Validation Coverage\n\nValidation coverage should be based on the software’s complexity and safety risk.\nThe selection of validation activities, tasks, and work items should be commensurate with the complexity of the software design and the risk associated with the use of the software for the specified intended use.\n\n\n\n2.4.9 Independence of Review\n\nValidation activities should be based on the basic quality assurance precept of “independence of review.”\nSelf-validation is extremely difficult.\nWhen possible, an independent evaluation is always better (like a contracted third-party independent verification and validation)\nAnother approach is to assign internal staff members that are not involved in a particular design or its implementation, but who have sufficient knowledge to evaluate the project and conduct the verification and validation activities.\n\n\n\n2.4.10 Flexibility and Responsibility\nThe device manufacturer has flexibility in choosing how to apply these validation principles, but retains ultimate responsibility for demonstrating that the software has been validated. FDA regulated medical device applications include software that:\n\nIs a component, part, or accessory of a medical device;\n\ncomponents: e.g., application software, operating systems, compilers, debuggers, configuration management tools, and many more\n\nIs itself a medical device; or\nIs used in manufacturing, design and development, or other parts of the quality system.\nNo matter how complex and disperse the software is, the manufacturer is in charge of responsibility for software validation."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/index.html#activities-and-tasks",
    "href": "docs/blog/posts/2022-12-10-FDA/index.html#activities-and-tasks",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.5 Activities and Tasks",
    "text": "2.5 Activities and Tasks\nSoftware validation is accomplished through a series of activities and tasks that are planned and executed at various stages of the software development life cycle. These tasks may be\n\none time occurrences\niterated many times\n\n\n2.5.1 Software Life Cycle Activities\n\nSoftware developers should establish a software life cycle model that is appropriate for their product and organization.\nThe selected software life cycle model should cover the software from its birth to its retirement.\nActivities in a typical software life cycle model:\n\nQuality Planning\nSystem Requirements Definition\nDetailed Software Requirements Specification\nSoftware Design Specification\nConstruction or Coding\nTesting\nInstallation\nOperation and Support\nMaintenance\nRetirement\n\nVerification, testing, and other tasks that support software validation occur during each of these activities.\nSeveral software life cycle models defined in FDA’s Glossary of Computerized System and Software Development\n\nTerminology dated August 1995:\n\nwaterfall\nspiral\nrapid prototyping\nincremental development, etc.\n\n\n\n2.5.2 Typical Tasks Supporting Validation\nthe software developer should at least consider each of the risk-related tasks and should define and document which tasks are or are not appropriate for their specific application.\n\n2.5.2.1 Quality Planning\nDesign and development planning should culminate in a plan that identifies\n\nnecessary tasks,\nprocedures for anomaly reporting and resolution,\nnecessary resources, and\nmanagement review requirements including formal design reviews.\n\nThe plan should include:\n\nThe specific tasks for each life cycle activity;\nEnumeration of important quality factors (e.g., reliability, maintainability, and usability);\nMethods and procedures for each task;\nTask acceptance criteria;\nCriteria for defining and documenting outputs in terms that will allow evaluation of their conformance to input requirements;\nInputs for each task;\nOutputs from each task;\nRoles, resources, and responsibilities for each task;\nRisks and assumptions; and\nDocumentation of user needs.\n\nThe plan should identify\n\nthe personnel,\nthe facility and equipment resources for each task, and\nthe role that risk (hazard) management will play.\n\nA configuration management plan should be developed that will guide and control multiple parallel development activities and ensure proper communications and documentation.\nControls are necessary to ensure positive and correct correspondence among all approved versions of the specifications documents, source code, object code, and test suites that comprise a software system. The controls also should ensure accurate identification of, and access to, the currently approved versions.\nProcedures should be created for reporting and resolving software anomalies found through validation or other activities.\nManagement should identify the reports and specify the contents, format, and responsible organizational elements for each report. Procedures also are necessary for the review and approval of software development results, including the responsible organizational elements for such reviews and approvals.\nTypical Tasks – Quality Planning\n\nRisk (Hazard) Management Plan\nConfiguration Management Plan\nSoftware Quality Assurance Plan\n\nSoftware Verification and Validation Plan\n\nVerification and Validation Tasks, and Acceptance Criteria\nSchedule and Resource Allocation (for software verification and validation activities)\nReporting Requirements\n\nFormal Design Review Requirements\nOther Technical Review Requirements\n\nProblem Reporting and Resolution Procedures\nOther Support Activities\n\n\n\n2.5.2.2 Requirements\nRequirements development includes the\n\nidentification,\nanalysis, and\ndocumentation of information about the device and its intended use.\n\nAreas of special importance include allocation of system functions to\n\nhardware/software,\noperating conditions,\nuser characteristics,\npotential hazards, and\nanticipated tasks.\n\nIn addition, the requirements should state clearly the intended use of the software. It is not possible to validate software without predetermined and documented software requirements. Typical software requirements specify the following:\n\nAll software system inputs;\nAll software system outputs;\nAll functions that the software system will perform;\nAll performance requirements that the software will meet, (e.g., data throughput, reliability, and timing);\nThe definition of all external and user interfaces, as well as any internal software-to-system interfaces;\nHow users will interact with the system;\nWhat constitutes an error and how errors should be handled;\nRequired response times;\nThe intended operating environment for the software, if this is a design constraint (e.g., hardware platform, operating system);\nAll ranges, limits, defaults, and specific values that the software will accept; and\nAll safety related requirements, specifications, features, or functions that will be implemented in software.\n\nSoftware requirement specifications should identify clearly the potential hazards that can result from a software failure in the system as well as any safety requirements to be implemented in software.\nThe consequences of software failure should be evaluated, along with means of mitigating such failures (e.g., hardware mitigation, defensive programming, etc.).\nThe Quality System regulation requires a mechanism for addressing incomplete, ambiguous, or conflicting requirements. (See 21 CFR 820.30(c).) Each requirement (e.g., hardware, software, user, operator interface, and safety) identified in the software requirements specification should be evaluated for accuracy, completeness, consistency, testability, correctness, and clarity.\nFor example, software requirements should be evaluated to verify that:\n\nThere are no internal inconsistencies among requirements;\nAll of the performance requirements for the system have been spelled out;\nFault tolerance, safety, and security requirements are complete and correct;\nAllocation of software functions is accurate and complete;\nSoftware requirements are appropriate for the system hazards; and\nAll requirements are expressed in terms that are measurable or objectively verifiable.\n\nA software requirements traceability analysis should be conducted to trace software requirements to (and from) system requirements and to risk analysis results. In addition to any other analyses and documentation used to verify software requirements, a formal design review is recommended to confirm that requirements are fully specified and appropriate before extensive software design efforts begin. Requirements can be approved and released incrementally, but care should be taken that interactions and interfaces among software (and hardware) requirements are properly reviewed, analyzed, and controlled.\nTypical Tasks – Requirements\n\nPreliminary Risk Analysis\nTraceability Analysis\n\nSoftware Requirements to System Requirements (and vice versa)\nSoftware Requirements to Risk Analysis\n\nDescription of User Characteristics\nListing of Characteristics and Limitations of Primary and Secondary Memory\nSoftware Requirements Evaluation\nSoftware User Interface Requirements Analysis\nSystem Test Plan Generation\nAcceptance Test Plan Generation\nAmbiguity Review or Analysis\n\n\n\n2.5.2.3 Design\nIn the design process, the software requirements specification is translated into a logical and physical representation of the software to be implemented. The software design specification is a description of what the software should do and how it should do it. The design specification may contain both a high level summary of the design and detailed design information. Human factors engineering should be woven into\n\nthe entire design and development process,\nthe device design requirements,\nanalyses, and\ntests.\n\nDevice safety and usability issues should be considered when developing\n\nflowcharts,\nstate diagrams,\nprototyping tools, and\ntest plans.\n\nAlso, task and function analyses, risk analyses, prototype tests and reviews, and full usability tests should be performed. Participants from the user population should be included when applying these methodologies.\nThe software design specification should include:\n\nSoftware requirements specification, including predetermined criteria for acceptance of the software;\nSoftware risk analysis;\nDevelopment procedures and coding guidelines (or other programming procedures);\nSystems documentation (e.g., a narrative or a context diagram) that describes the systems context in which the program is intended to function, including the relationship of hardware, software, and the physical environment;\nHardware to be used;\nParameters to be measured or recorded;\nLogical structure (including control logic) and logical processing steps (e.g., algorithms);\nData structures and data flow diagrams;\nDefinitions of variables (control and data) and description of where they are used;\nError, alarm, and warning messages;\nSupporting software (e.g., operating systems, drivers, other application software);\nCommunication links (links among internal modules of the software, links with the supporting software, links with the hardware, and links with the user);\nSecurity measures (both physical and logical security); and\nAny additional constraints not identified in the above elements.\n\nThe first four of the elements noted above usually are separate pre-existing documents that are included by reference in the software design specification. Software requirements specification was discussed in the preceding section, as was software risk analysis.\nSoftware design evaluations criteria:\n\ncomplete,\ncorrect,\nconsistent,\nunambiguous,\nfeasible,\nmaintainable,\nanalyses of control flow,\ndata flow,\ncomplexity,\ntiming,\nsizing,\nmemory allocation,\ncriticality analysis, and many other aspects of the design\n\nAppropriate consideration of software architecture (e.g., modular structure) during design can reduce the magnitude of future validation efforts when software changes are needed.\nA traceability analysis should be conducted to verify that the software design implements all of the software requirements. As a technique for identifying where requirements are not sufficient, the traceability analysis should also verify that all aspects of the design are traceable to software requirements.\nAn analysis of communication links should be conducted to evaluate the proposed design with respect to hardware, user, and related software requirements. At the end of the software design activity, a Formal Design Review should be conducted to verify that the design is correct, consistent, complete, accurate, and testable, before moving to implement the design.\nSeveral versions of both the software requirement specification and the software design specification should be maintained. All approved versions should be archived and controlled in accordance with established configuration management procedures.\nTypical Tasks – Design\n\nUpdated Software Risk Analysis\nTraceability Analysis - Design Specification to Software Requirements (and vice versa)\nSoftware Design Evaluation\nDesign Communication Link Analysis\nModule Test Plan Generation\nIntegration Test Plan Generation\nTest Design Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.4 Construction or Coding\nSoftware may be constructed either by coding. Coding is the software activity where the detailed design specification is implemented as source code. It is the last stage in decomposition of the software requirements where module specifications are translated into a programming language.\nCoding usually involves the use of a high-level programming language, but may also entail the use of assembly language (or microcode) for time-critical operations.\nA source code traceability analysis is an important tool to verify that all code is linked to established specifications and established test procedures. A source code traceability analysis should be conducted and documented to verify that:\n\nEach element of the software design specification has been implemented in code;\nModules and functions implemented in code can be traced back to an element in the software design specification and to the risk analysis;\nTests for modules and functions can be traced back to an element in the software design specification and to the risk analysis; and\nTests for modules and functions can be traced to source code for the same modules and functions.\n\nTypical Tasks – Construction or Coding\n\nTraceability Analyses\n\nSource Code to Design Specification (and vice versa)\nTest Cases to Source Code and to Design Specification\n\nSource Code and Source Code Documentation Evaluation\nSource Code Interface Analysis\nTest Procedure and Test Case Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.5 Testing by the Software Developer\nSoftware testing entails running software products under known conditions with defined inputs and documented outcomes that can be compared to their predefined expectations. It is a time consuming, difficult, and imperfect activity.\nAs such, it requires early planning in order to be effective and efficient. Test plans and test cases should be created as early in the software development process as feasible.\nThey should identify\n\nthe schedules,\nenvironments,\nresources (personnel, tools, etc.),\nmethodologies,\ncases (inputs, procedures, outputs, expected results),\ndocumentation, and\nreporting criteria.\n\nDescriptions of categories of software and software testing effort appear in the literature\n\nNIST Special Publication 500-235, Structured Testing: A Testing Methodology Using the Cyclomatic Complexity Metric;\nNUREG/CR-6293, Verification and Validation Guidelines for High Integrity Systems; and\nIEEE Computer Society Press, Handbook of Software Reliability Engineering.\n\nTesting of all program functionality does not mean all of the program has been tested. Testing of all of a program’s code does not mean all necessary functionality is present in the program. Testing of all program functionality and all program code does not mean the program is 100% correct! Software testing that finds no errors should not be interpreted to mean that errors do not exist in the software product; it may mean the testing was superficial.\nAn essential element of a software test case is the expected result. It is the key detail that permits objective evaluation of the actual test result. This necessary testing information is obtained from the corresponding, predefined definition or specification.\nA software testing process should be based on principles that foster effective examinations of a software product. Applicable software testing tenets include:\n\nThe expected test outcome is predefined;\nA good test case has a high probability of exposing an error;\nA successful test is one that finds an error;\nThere is independence from coding;\nBoth application (user) and software (programming) expertise are employed;\nTesters use different tools from coders;\nExamining only the usual case is insufficient;\nTest documentation permits its reuse and an independent confirmation of the pass/fail status of a test outcome during subsequent review.\n\nCode-based testing is also known as structural testing or “white-box” testing. It identifies test cases based on knowledge obtained from the source code, detailed design specification, and other development documents. Structural testing can identify “dead” code that is never executed when the program is run. Structural testing is accomplished primarily with unit (module) level testing, but can be extended to other levels of software testing.\nThe level of structural testing can be evaluated using metrics that are designed to show what percentage of the software structure has been evaluated during structural testing. These metrics are typically referred to as “coverage” and are a measure of completeness with respect to test selection criteria. The amount of structural coverage should be commensurate with the level of risk posed by the software. Use of the term “coverage” usually means 100% coverage. Common structural coverage metrics include:\n\nStatement Coverage – This criteria requires sufficient test cases for each program statement to be executed at least once; however, its achievement is insufficient to provide confidence in a software product’s behavior.\nDecision (Branch) Coverage – This criteria requires sufficient test cases for each program decision or branch to be executed so that each possible outcome occurs at least once. It is considered to be a minimum level of coverage for most software products, but decision coverage alone is insufficient for high-integrity applications.\nCondition Coverage – This criteria requires sufficient test cases for each condition in a program decision to take on all possible outcomes at least once. It differs from branch coverage only when multiple conditions must be evaluated to reach a decision.\nMulti-Condition Coverage – This criteria requires sufficient test cases to exercise all possible combinations of conditions in a program decision.\nLoop Coverage – This criteria requires sufficient test cases for all program loops to be executed for zero, one, two, and many iterations covering initialization, typical running and termination (boundary) conditions.\nPath Coverage – This criteria requires sufficient test cases for each feasible path, basis path, etc., from start to exit of a defined program segment, to be executed at least once. Because of the very large number of possible paths through a software program, path coverage is generally not achievable. The amount of path coverage is normally established based on the risk or criticality of the software under test.\nData Flow Coverage – This criteria requires sufficient test cases for each feasible data flow to be executed at least once. A number of data flow testing strategies are available.\n\nThe following types of functional software testing involve generally increasing levels of effort:\n\nNormal Case – Testing with usual inputs is necessary. However, testing a software product only with expected, valid inputs does not thoroughly test that software product. By itself, normal case testing cannot provide sufficient confidence in the dependability of the software product.\nOutput Forcing – Choosing test inputs to ensure that selected (or all) software outputs are generated by testing.\nRobustness – Software testing should demonstrate that a software product behaves correctly when given unexpected, invalid inputs. Methods for identifying a sufficient set of such test cases include Equivalence Class Partitioning, Boundary Value Analysis, and Special Case Identification (Error Guessing). While important and necessary, these techniques do not ensure that all of the most appropriate challenges to a software product have been identified for testing.\nCombinations of Inputs – The functional testing methods identified above all emphasize individual or single test inputs. Most software products operate with multiple inputs under their conditions of use. Thorough software product testing should consider the combinations of inputs a software unit or system may encounter during operation. Error guessing can be extended to identify combinations of inputs, but it is an ad hoc technique. Cause-effect graphing is one functional software testing technique that systematically identifies combinations of inputs to a software product for inclusion in test cases.\n\nFunctional and structural software test case identification techniques provide specific inputs for testing, rather than random test inputs. One weakness of these techniques is the difficulty in linking structural and functional test completion criteria to a software product’s reliability.\nAdvanced software testing methods, such as statistical testing, can be employed to provide further assurance that a software product is dependable. Statistical testing uses randomly generated test data from defined distributions based on an operational profile (e.g., expected use, hazardous use, or malicious use of the software product). Large amounts of test data are generated and can be targeted to cover particular areas or concerns, providing an increased possibility of identifying individual and multiple rare operating conditions that were not anticipated by either the software product’s designers or its testers. Statistical testing also provides high structural coverage. It does require a stable software product. Thus, structural and functional testing are prerequisites for statistical testing of a software product.\nAnother aspect of software testing is the testing of software changes. Changes occur frequently during software development. These changes are the result of\n\ndebugging that finds an error and it is corrected,\nnew or changed requirements (“requirements creep”), and\nmodified designs as more effective or efficient implementations are found.\n\nOnce a software product has been baselined (approved), any change to that product should have its own “mini life cycle,” including testing. Testing of a changed software product requires additional effort. It should demonstrate\n\nthat the change was implemented correctly, and\nthat the change did not adversely impact other parts of the software product.\n\nRegression analysis is the determination of the impact of a change based on review of the relevant documentation in order to identify the necessary regression tests to be run. Regression testing is the rerunning of test cases that a program has previously executed correctly and comparing the current result to the previous result in order to detect unintended effects of a software change. Regression analysis and regression testing should also be employed when using integration methods to build a software product to ensure that newly integrated modules do not adversely impact the operation of previously integrated modules.\nIn order to provide a thorough and rigorous examination of a software product, development testing is typically organized into levels: unit, integration, and system levels of testing.\n\nUnit (module or component) level testing focuses on the early examination of sub-program functionality and ensures that functionality not visible at the system level is examined by testing. Unit testing ensures that quality software units are furnished for integration into the finished software product.\nIntegration level testing focuses on the transfer of data and control across a program’s internal and external interfaces. External interfaces are those with\n\nother software (including operating system software),\nsystem hardware, and\nthe users and can be described as communications links.\n\nSystem level testing demonstrates that all specified functionality exists and that the software product is trustworthy. This testing verifies the as-built program’s functionality and performance with respect to the requirements for the software product as exhibited on the specified operating platform(s). System level software testing addresses functional concerns and the following elements of a device’s software that are related to the intended use(s):\n\nPerformance issues (e.g., response times, reliability measurements);\nResponses to stress conditions, e.g., behavior under maximum load, continuous use;\nOperation of internal and external security features;\nEffectiveness of recovery procedures, including disaster recovery;\nUsability; (Usability vs Utility??)\nCompatibility with other software products;\nBehavior in each of the defined hardware configurations; and\nAccuracy of documentation.\n\n\nControl measures (e.g., a traceability analysis) should be used to ensure that the intended coverage is achieved.\nSystem level testing also exhibits the software product’s behavior in the intended operating environment. The location of such testing is dependent upon the software developer’s ability to produce the target operating environment(s). Depending upon the circumstances, simulation and/or testing at (potential) customer locations may be utilized.\nTest plans should identify the controls needed to ensure\n\nthat the intended coverage is achieved and\nthat proper documentation is prepared when planned system level testing is conducted at sites not directly controlled by the software developer.\n\nTest procedures, test data, and test results\n\nshould be documented in a manner permitting objective pass/fail decisions to be reached.\nshould also be suitable for review and objective decision making subsequent to running the test,\nshould be suitable for use in any subsequent regression testing.\n\nErrors detected during testing should be\n\nlogged,\nclassified,\nreviewed, and\nresolved prior to release of the software.\n\nSoftware error data that is collected and analyzed during a development life cycle may be used to determine the suitability of the software product for release for commercial distribution. Test reports should comply with the requirements of the corresponding test plans.\nSoftware testing tools are frequently used to ensure consistency, thoroughness, and efficiency in the testing of such software products and to fulfill the requirements of the planned testing activities.\nAppropriate documentation providing evidence of the validation of these software tools for their intended use should be maintained (see section 6 of this guidance).\nTypical Tasks – Testing by the Software Developer\n\nTest Planning\nStructural Test Case Identification\nFunctional Test Case Identification\nTraceability Analysis - Testing\nUnit (Module) Tests to Detailed Design\nIntegration Tests to High Level Design\nSystem Tests to Software Requirements\nUnit (Module) Test Execution\nIntegration Test Execution\nFunctional Test Execution\nSystem Test Execution\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.6 User Site Testing\nTesting at the user site is an essential part of software validation. The Quality System regulation requires\n\ninstallation and\ninspection procedures (including testing where appropriate) as well as\ndocumentation of inspection and\ntesting to demonstrate proper installation. (See 21 CFR §820.170.)\n\nLikewise, manufacturing equipment must meet specified requirements, and automated systems must be validated for their intended use. (See 21 CFR §820.70(g) and 21 CFR §820.70(i) respectively.)\nTerminology regarding user site testing can be confusing. Terms such as\n\nbeta test,\nsite validation,\nuser acceptance test,\ninstallation verification, and\ninstallation testing have all been used to describe user site testing.\n\nFor the purposes of this guidance, the term “user site testing” encompasses all of these and any other testing that takes place outside of the developer’s controlled environment.\nThis testing should take place at a user’s site with the actual hardware and software that will be part of the installed system configuration. The testing is accomplished through either actual or simulated use of the software being tested within the context in which it is intended to function.\nTest planners should check with the FDA Center(s) with the corresponding product jurisdiction to determine whether there are any additional regulatory requirements for user site testing.\nUser site testing should follow a pre-defined written plan with\n\na formal summary of testing and\na record of formal acceptance.\n\nThe following documented evidence should be retained:\n\nall testing procedures,\ntest input data, and\ntest results\n\nThere should be evidence that hardware and software are installed and configured as specified. Measures should ensure that all system components are exercised during the testing and that the versions of these components are those specified. The testing plan should specify testing throughout the full range of operating conditions and should specify continuation for a sufficient time to allow the system to encounter a wide spectrum of conditions and events in an effort to detect any latent faults that are not apparent during more normal activities.\nSome of the evaluations of the system’s ability that have been performed earlier by the software developer at the developer’s site should be repeated at the site of actual use. These may include tests for:\n\na high volume of data,\nheavy loads or stresses,\nsecurity,\nfault testing (avoidance, detection, tolerance, and recovery),\nerror messages, and\nimplementation of safety requirements.\n\nThere should be an evaluation of the ability of the users of the system to understand and correctly interface with it.\nOperators should be able to perform the intended functions and respond in an appropriate and timely manner to all alarms, warnings, and error messages.\nRecords should be maintained of both proper system performance and any system failures that are encountered.\nThe revision of the system to compensate for faults detected during this user site testing should follow the same procedures and controls as for any other software change.\nThe developers of the software may or may not be involved in the user site testing.\n\nIf the developers are involved, they may seamlessly carry over to the user’s site the last portions of design-level systems testing.\nIf the developers are not involved, it is all the more important that the user have persons who understand the importance of careful test planning, the definition of expected test results, and the recording of all test outputs.\n\nTypical Tasks – User Site Testing\n\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.7 Maintenance and Software Changes\n\n2.5.2.7.1 Hardware vs Software\nHardware maintenance typically includes\n\npreventive hardware maintenance actions,\ncomponent replacement, and\ncorrective changes.\n\nSoftware maintenance includes\n\ncorrective,\nperfective, and\nadaptive maintenance\nbut does not include preventive maintenance actions or software component replacement.\n\n\n\n2.5.2.7.2 Maintenance Types\n\nCorrective maintenance: Changes made to correct errors and faults in the software.\nPerfective maintenance: Changes made to the software to improve the performance, maintainability, or other attributes of the software system .\nAdaptive maintenance: Changes to make the software system usable in a changed environment.\n\nSufficient regression analysis and testing should be conducted to demonstrate that portions of the software not involved in the change were not adversely impacted. When changes are made to a software system,\n\neither during initial development or\nduring post release maintenance,\n\nThis is in addition to testing that evaluates the correctness of the implemented change(s). The specific validation effort necessary for each software change is determined by\n\nthe type of change,\nthe development products affected, and the\nimpact of those products on the operation of the software.\n\n\n\n2.5.2.7.3 Factors of Limitting Validation Effort Needed When a Change Is Made\n\ncareful and complete documentation of the design structure and\ncareful and complete documentation of interrelationships of various modules,\ninterfaces, etc.\nFor example,\n\ntest documentation,\ntest cases, and\nresults of previous verification and validation testing All of them need to be archived if they are to be available for performing subsequent regression testing.\n\n\nThe following additional maintenance tasks should be addressed:\n\nSoftware Validation Plan Revision - For software that was previously validated, the existing software validation plan should be revised to support the validation of the revised software. If no previous software validation plan exists, such a plan should be established to support the validation of the revised software.\nAnomaly Evaluation – Software organizations frequently maintain documentation, such as software problem reports that describe software anomalies discovered and the specific corrective action taken to fix each anomaly.\n\nToo often, however, mistakes are repeated because software developers do not take the next step to determine the root causes of problems and make the process and procedural changes needed to avoid recurrence of the problem.\nSoftware anomalies should be evaluated in terms of their severity and their effects on system operation and safety,\nbut they should also be treated as symptoms of process deficiencies in the quality system.\nA root cause analysis of anomalies can identify specific quality system deficiencies.\nWhere trends are identified (e.g., recurrence of similar software anomalies), appropriate corrective and preventive actions must be implemented and documented to avoid further recurrence of similar quality problems. (See 21 CFR 820.100.)\n\nProblem Identification and Resolution Tracking - All problems discovered during maintenance of the software should be documented. The resolution of each problem should be tracked to ensure it is fixed, for historical reference, and for trending.\nProposed Change Assessment - All proposed modifications, enhancements, or additions should be assessed to determine the effect each change would have on the system. This information should determine the extent to which verification and/or validation tasks need to be iterated.\nTask Iteration - For approved software changes, all necessary verification and validation tasks should be performed to ensure that planned changes are implemented correctly, all documentation is complete and up to date, and no unacceptable changes have occurred in software performance.\nDocumentation Updating – Documentation should be carefully reviewed to determine which documents have been impacted by a change. All approved documents (e.g., specifications, test procedures, user manuals, etc.) that have been affected should be updated in accordance with configuration management procedures. Specifications should be updated before any maintenance and software changes are made."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "href": "docs/blog/posts/2022-12-10-FDA/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.6 Validation of Automated Process Equipment and Quality System Software",
    "text": "2.6 Validation of Automated Process Equipment and Quality System Software\nThe Quality System regulation requires that “when computers or automated data processing systems are used as part of production or the quality system, the [device] manufacturer shall validate computer software for its intended use according to an established protocol.” (See 21 CFR §820.70(i)). This has been a regulatory requirement of FDA’s medical device Good Manufacturing Practice (GMP) regulations since 1978.\nComputer systems that implement part of a device manufacturer’s production processes or quality system (or that are used to create and maintain records required by any other FDA regulation) are subject to the Electronic Records; Electronic Signatures regulation. (See 21 CFR Part 11.) This regulation establishes additional security, data integrity, and validation requirements when records are created or maintained electronically. These additional Part 11 requirements should be carefully considered and included in system requirements and software requirements for any automated record keeping systems. System validation and software validation should demonstrate that all Part 11 requirements have been met.\nComputers and automated equipment are used extensively throughout all aspects of\n\nmedical device design,\nlaboratory testing and analysis,\nproduct inspection and acceptance,\nproduction and process control,\nenvironmental controls,\npackaging,\nlabeling,\ntraceability,\ndocument control,\ncomplaint management, and many other aspects of the quality system.\n\nIncreasingly, automated plant floor operations can involve extensive use of embedded systems in:\n\nprogrammable logic controllers;\ndigital function controllers;\nstatistical process control;\nsupervisory control and data acquisition;\nrobotics;\nhuman-machine interfaces;\ninput/output devices; and\ncomputer operating systems.\n\nAll software tools used for software design are subject to the requirement for software validation, but the validation approach used for each application can vary widely.\nValidation is typically supported by:\n\nverifications of the outputs from each stage of that software development life cycle; and\nchecking for proper operation of the finished software in the device manufacturer’s intended use environment.\n\n\n2.6.1 How Much Validation Evidence Is Needed?\nThe level of validation effort should be commensurate with\n\nthe risk posed by the automated operation,\nthe complexity of the process software,\nthe degree to which the device manufacturer is dependent upon that automated process to produce a safe and effective device\n\nDocumented requirements and risk analysis of the automated process help to define the scope of the evidence needed to show that the software is validated for its intended use. Without a plan, extensive testing may be needed for:\n\na plant-wide electronic record and electronic signature system;\nan automated controller for a sterilization cycle; or\nautomated test equipment used for inspection and acceptance of finished circuit boards in a lifesustaining / life-supporting device.\n\nHigh risk applications should not be running in the same operating environment with non-validated software functions, even if those software functions are not used. Risk mitigation techniques such as memory partitioning or other approaches to resource protection may need to be considered when high risk applications and lower risk applications are to be used in the same operating environment.\nWhen software is upgraded or any changes are made to the software, the device manufacturer should consider how those changes may impact the “used portions” of the software and must reconfirm the validation of those portions of the software that are used. (See 21 CFR §820.70(i).)\n\n\n2.6.2 Defined User Equipment\nA very important key to software validation is a documented user requirements specification that defines:\n\nthe “intended use” of the software or automated equipment; and\nthe extent to which the device manufacturer is dependent upon that software or equipment for production of a quality medical device.\n\nThe device manufacturer (user) needs to define the expected operating environment including any required hardware and software configurations, software versions, utilities, etc. The user also needs to:\n\ndocument requirements for system performance, quality, error handling, startup, shutdown, security, etc.;\nidentify any safety related functions or features, such as sensors, alarms, interlocks, logical processing steps, or command sequences; and\ndefine objective criteria for determining acceptable performance.\n\nThe validation must be conducted in accordance with a documented protocol, and the validation results must also be documented. (See 21 CFR §820.70(i).) Test cases should be documented that will exercise the system to challenge its performance against the pre-determined criteria, especially for its most critical parameters.\nTest cases should address\n\nerror and alarm conditions,\nstartup, shutdown,\nall applicable user functions and operator controls,\npotential operator errors,\nmaximum and minimum ranges of allowed values, and\nstress conditions applicable to the intended use of the equipment.\n\nThe test cases should be executed and the results should be recorded and evaluated to determine whether the results support a conclusion that the software is validated for its intended use.\nA device manufacturer may conduct a validation using their own personnel or may depend on a third party such as the equipment/software vendor or a consultant. In any case, the device manufacturer retains the ultimate responsibility for ensuring that the production and quality system software:\n\nis validated according to a written procedure for the particular intended use; and\nwill perform as intended in the chosen application.\n\nThe device manufacturer should have documentation including:\n\ndefined user requirements;\nvalidation protocol used;\nacceptance criteria;\ntest cases and results; and\na validation summary that objectively confirms that the software is validated for its intended use.\n\n\n\n2.6.3 Validation of Off-The-Shelf Software and Automated Equipment\nMost of the automated equipment and systems used by device manufacturers are supplied by thirdparty vendors and are purchased off-the-shelf (OTS). The device manufacturer is responsible for ensuring that the product development methodologies used by the OTS software developer are appropriate and sufficient for the device manufacturer’s intended use of that OTS software.\nWhere possible and depending upon the device risk involved, the device manufacturer should consider auditing the vendor’s design and development methodologies used in the construction of the OTS software and should assess the development and validation documentation generated for the OTS software. Such audits can be conducted by the device manufacturer or by a qualified third party.\nThe audit should demonstrate that the vendor’s procedures for and results of the verification and validation activities performed the OTS software are appropriate and sufficient for the safety and effectiveness requirements of the medical device to be produced using that software."
  },
  {
    "objectID": "docs/CV/index.html",
    "href": "docs/CV/index.html",
    "title": "CV",
    "section": "",
    "text": "Curriculum Vitae\n\nResume(2 Pages) and CV(more than 2 pages)\n\n\n\n[Resume] English Version\n[Resume] Korean Version\n[CV] English Version\n[CV] Korean Version\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/CV/index.html#cv",
    "href": "docs/CV/index.html#cv",
    "title": "Curriculum Vitae",
    "section": "CV",
    "text": "CV\n\napplying for positions in academia, fellowships and grants\nacademic accomplishments\nLength depends upon experience and includes a complete list of publications, posters, and presentations\nbegins with education and can include name of advisor and dissertation title or summary (see examples). Also used for merit/tenure review and sabbatical leave"
  },
  {
    "objectID": "docs/projects/high_dimension.html#data-preparation",
    "href": "docs/projects/high_dimension.html#data-preparation",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nPackage Loading and Option Settings\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\n#library(mixOmics)\nset.seed(20221213) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\nData Simulation\n\n\n\n\n\nLoad Data\n\n\nShow the code\n#all_data=read_csv(\"C:/Users/kmkim/Desktop/my_project/website/docs/projects/data/llfs_simulated_data.rda\",\n#                  progress = show_progress(),\n#                  show_col_types = FALSE)%>%\n\n all_data=read_rds(\"C:/Users/kmkim/Desktop/my_project/website/docs/projects/data/llfs_simulated_data.rds\")%>%\n     dplyr::select(-1,-probabilities)\n all_data=all_data%>%\n     mutate(outcome=factor(outcome,levels=c(0,1)),\n            sex=factor(sex,levels=c(0,1)),\n            country=factor(country,levels=c(0,1)),\n            treatment=factor(treatment,levels=c(0,1)),\n            genotype=factor(genotype,levels=c(3,0,1,2,4,5))\n            )\n \n dim(all_data)\n\n\n[1]  1000 10006"
  },
  {
    "objectID": "docs/projects/high_dimension.html#data-description",
    "href": "docs/projects/high_dimension.html#data-description",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Data Description",
    "text": "Data Description\nThis data include\nMin.   :10.40  , 1st Qu.:15.43  , Median :19.20  , Mean   :20.09  , 3rd Qu.:22.80  , Max.   :33.90  , Min.   :4.000  , 1st Qu.:4.000  , Median :6.000  , Mean   :6.188  , 3rd Qu.:8.000  , Max.   :8.000  , Min.   : 71.1  , 1st Qu.:120.8  , Median :196.3  , Mean   :230.7  , 3rd Qu.:326.0  , Max.   :472.0  , Min.   : 52.0  , 1st Qu.: 96.5  , Median :123.0  , Mean   :146.7  , 3rd Qu.:180.0  , Max.   :335.0  , Min.   :2.760  , 1st Qu.:3.080  , Median :3.695  , Mean   :3.597  , 3rd Qu.:3.920  , Max.   :4.930  , Min.   :1.513  , 1st Qu.:2.581  , Median :3.325  , Mean   :3.217  , 3rd Qu.:3.610  , Max.   :5.424  , Min.   :14.50  , 1st Qu.:16.89  , Median :17.71  , Mean   :17.85  , 3rd Qu.:18.90  , Max.   :22.90  , Min.   :0.0000  , 1st Qu.:0.0000  , Median :0.0000  , Mean   :0.4375  , 3rd Qu.:1.0000  , Max.   :1.0000  , Min.   :0.0000  , 1st Qu.:0.0000  , Median :0.0000  , Mean   :0.4062  , 3rd Qu.:1.0000  , Max.   :1.0000  , Min.   :3.000  , 1st Qu.:3.000  , Median :4.000  , Mean   :3.688  , 3rd Qu.:4.000  , Max.   :5.000  , Min.   :1.000  , 1st Qu.:2.000  , Median :2.000  , Mean   :2.812  , 3rd Qu.:4.000  , Max.   :8.000  \nthis inline code works but it seems to rmd files look more organized and tidier."
  },
  {
    "objectID": "docs/projects/high_dimension.html#architecture-of-analysis-pipeline",
    "href": "docs/projects/high_dimension.html#architecture-of-analysis-pipeline",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Architecture of Analysis Pipeline",
    "text": "Architecture of Analysis Pipeline"
  },
  {
    "objectID": "docs/projects/high_dimension.html#methods",
    "href": "docs/projects/high_dimension.html#methods",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Methods",
    "text": "Methods\n\n\n   alpha      mse fit.name\n1    0.0 2529.146   alpha0\n2    0.1 2529.146 alpha0.1\n3    0.2 2529.146 alpha0.2\n4    0.3 2529.146 alpha0.3\n5    0.4 2529.146 alpha0.4\n6    0.5 2529.146 alpha0.5\n7    0.6 2529.146 alpha0.6\n8    0.7 2529.146 alpha0.7\n9    0.8 2529.146 alpha0.8\n10   0.9 2529.146 alpha0.9\n11   1.0 2529.146   alpha1"
  },
  {
    "objectID": "docs/projects/high_dimension.html#results",
    "href": "docs/projects/high_dimension.html#results",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "docs/projects/high_dimension.html#conclusion",
    "href": "docs/projects/high_dimension.html#conclusion",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "docs/projects/high_dimension.html#bibliography",
    "href": "docs/projects/high_dimension.html#bibliography",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Bibliography",
    "text": "Bibliography"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\nBrief Introduction\n저는 R과 Python같은 open-source tool을 사용하여 통계, machine learning 및 deep learning을 독학하는 열정 가득한 data scientist입니다. 약 7년여 동안 data modeling, 통계적 모델링, machine learning 모델링 및 시각화를 통하여 data 관련 업무 경험을 쌓았습니다.\n\n\nExperience\n저는 한국에서 학부과정으로 생화학과 미국에서 학부 과정으로 수학과 석사 과정으로 생물통계학을 전공했습니다. 저의 생화학 전공을 살려 Bio와 의료분야에서 커리어를 시작했고 그 과정에서 많은 data science 비전문가들과 협업하면서 통계 및 data science에 대하여 그들과 소통하는 법을 익혔습니다.\n\n\nReason for Blogging\nData science에 대하여 비전문가들과 SW 개발자들과 협업을 하면서 그들과 효율적이고 효과적으로 소통하는것이 중요하다는 것을 깨달았습니다. 그 효과적인 소통이 수학, 통계 및 IT에 대한 지식으로부터 온다고 생각해서 새로운 기술에 대하여 세부적이고 체계적인 지식을 쌓기위해 블로깅을 시작했습니다.\n\n\n\n\nBrief Introduction\nI am passionate and self-taught in statistics, machine learning, deep learning, and programming using open-source tools such as Python, R, and SQL. It has been 7years since I dealth with data research from data modeling to data visualization through modeling.\n\n\nExperience\nMy educational background is a Bachelor of Science in biochemistry in South Korea, Bachelor of Arts in mathematics in the USA, and Master of Sceince in biostatistics in the USA. I started my career in the medical area of analytics because I have a strong background in biology. My work experience exposed me to an environment where I had to interact with non-experts to make them understand data science.\n\n\nReason for Blogging\n(Pursuing Goal in Data Science)\nAfter dealing with data and collaborating with SW developers and non-professional colleagues, I realized that the importance of communicating with them efficiently and effectively. This requires more knowledge in mathematics, statistics, and IT to master the art of explaining easily, clearly, and concisely. So, I started blogging to have an opportunity to have a detailed and systematic understanding of new techaniques of machine learning, as well as fundamental statistics."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/sw_validation.html",
    "href": "docs/blog/posts/2022-12-10-FDA/sw_validation.html",
    "title": "FDA Validation Document Summary",
    "section": "",
    "text": "FDA: General Principles of Software Validation\n\n\n\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\n\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#purpose",
    "href": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#purpose",
    "title": "FDA Validation Document Summary",
    "section": "Purpose",
    "text": "Purpose\nThe purpose is to make a sketch of general validation principle of the validation of medical device software or software used to design or develop."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#scope",
    "href": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#scope",
    "title": "FDA Validation Document Summary",
    "section": "Scope",
    "text": "Scope\nThe scope of this guidance is broad. The important activities for the software validation include at least:\n\nplanning,\nverfication,\ntesting,\ntraceability, and\nconfiguration management.\n\nAll of the activities above should be\n\nintegrated\nbe able to describe software life cycle management and\nbe able to describe software risk management.\n\nThe software validation and verification activities should be focused into the entire software life cycle. (It does not necessarily mean that the activies must follow any technical models.)\nThe guidance is applicable to any software related to a regulated medical device and anyone who is employed in a bio or medical industry.\n\nThe Least Burdensome Approach\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\nRegulatory Requirements for Software Validation\n\nSoftware validation: a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997. (See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\nSpecific requirements for validation of device software are found in 21 CFR §820.30(g). Other design controls, such as planning, input, verification, and reviews, are required for medical device software. (See 21 CFR §820.30.)\ncomputer systems used to create, modify, and maintain electronic records and to manage electronic signatures are also subject to the validation requirements. (See 21 CFR §11.10(a).)\n\n\nObjective\nThe objective of software validation is to ensure:\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\nWhat to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\n\nQuality System Regulation vs Pre-market Submissions\nThis document does not address any specific requirements but general ones. Specific issues should be addressed to\n\nthe Office of Device Evaluation (ODE),\nCenter for Devices and Radiological Health (CDRH)\nthe Office of Blood Research and Review,\nCenter for Biologics Evaluation and Research (CBER). See the references in Appendix A for applicable FDA guidance documents for pre-market submissions."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#context-for-software-validation",
    "href": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#context-for-software-validation",
    "title": "FDA Validation Document Summary",
    "section": "Context for Software Validation",
    "text": "Context for Software Validation\n\nValidation elements that FDA expects to do for the Quality System regulation, using the principles and tasks are listed in Sections 4 and 5.\nAdditional specific information is available from many of the references listed in Appendix A\n\n\nDefinition and Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology between the medical device Quality System regulation and the software industry:\n\nrequirements,\nspecification,\nverification, and\nvalidation.\n\n\n\nRequirements and Specifications\nThe Quality System regulation states\n\nthat design input requirements must be documented and\nthat specified requirements must be verified\n\nBut, the regulation does not further clarify the distinction between the terms “requirement” and “specification.”\n\nRequirement\n\ncan be any need or expectation for a system or for its software.\nreflects the stated or implied needs of the customer: requirements may be\n\nmarket-based,\ncontractual,\nstatutory, or\nan organization’s internal requirements.\n\nvarious examples of requirements\n\ndesign, functional, implementation, interface, performance, or physical requirements\n\nSoftware requirements derived from the system requirements for those aspects of system functionality\nSoftware requirements are typically stated in functional terms and are defined, refined, and updated as a development project progresses.\nSuccess in accurately and completely documenting software requirements is a crucial factor in successful validation of the resulting software.\n\nSpecification\n\ndefined as “a document that states requirements.” (See 21 CFR §820.3(y).)\nIt may refer to or include drawings, patterns, or other relevant documents\nIt usually indicates the means and the criteria whereby conformity with the requirement can be checked.\nVarious examples of written specifications\n\nsystem requirements specification,\nsoftware requirements specification,\nsoftware design specification,\nsoftware test specification,\nsoftware integration specification, etc.\n\nAll of these documents are design outputs for which various forms of verification are necessary.\n\n\n\n\nVerifiaction and Validation\nThe Quality System regulation is harmonized with ISO 8402:1994, which treats “verification” and “validation” as separate and distinct terms.\n\nSoftware verification\n\nIt provides objective evidence that the design outputs of a particular phase of the software development life cycle meet all of the specified requirements for that phase.\nIt looks for\n\nconsistency,\ncompleteness, and\ncorrectness of the software and its supporting documentation\n\nSoftware testing\n\nverification activities intended to confirm that software development output meets its input requirements.\n\nTypes of verification activities include\n\nvarious static and dynamic analyses,\ncode and document inspections,\nwalkthroughs, and other techniques.\n\n\nSoftware Validation\n\nConfirmation by examination and provision of the following objective evidence:\nEvidence 1: software specifications conform to user needs and intended uses, and\nEvidnece 2: the particular requirements implemented through software can be consistently fulfilled.\nEvidnece 3: all software requirements have been implemented correctly and completely and are traceable to system requirements.\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device.\n\nDifficulty in Software verification and validation\n\na developer cannot test forever, and\nit is difficult to know how much evidence is enough.\nIn large measure, software validation is a matter of developing a “level of confidence” that the device meets all requirements and user expectations for the software automated functions and features of the device.\nConsiderations for an acceptable level of confidence\n\nmeasures such as defects found in specifications documents,\nestimates of defects remaining,\ntesting coverage, and other techniques are all used to develop before shipping the product.\nHowever, a level of confidence varies depending upon the safety risk (hazard) posed by the automated functions of the device. (Info on safety risk is found in Section 4 and in the international standards ISO/IEC 14971-1 and IEC 60601-1-4 referenced in Appendix A).\n\n\n\n\n\nIQ/OQ/PQ\nIQ/OQ/PQ are the terminology related to user site software validation\n\nInstallation qualification (IQ)\nOperational qualification (OQ)\nPerformance qualification (PQ).\n\nDefinitions of these terms may be found in FDA’s Guideline on General Principles of Process Validation, dated May 11, 1987, and in FDA’s Glossary of Computerized System and Software Development Terminology, dated August 1995. Both FDA personnel and device manufacturers need to be aware of these differences in terminology as they ask for and provide information regarding software validation.\n\n\n\nSoftware Development as Part of System Design\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nthe user’s needs\nintended uses from which the product is developed.\n\nA primary goal of software validation is to then demonstrate that all completed software products comply with all documented software and system requirements.\n\n\nSoftware Is Different from Hardware\nSoftware engineering needs an even greater level of managerial scrutiny and control than does hardware engineering.\n\n\nBenefits of Software Validation\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nSoftware validation can also reduce long term costs by making it easier and less costly to reliably modify software and revalidate software changes.\n\n\n\nDesign Review\nDesign reviews are documented, comprehensive, and systematic examinations of a design to evaluate\n\nthe adequacy of the design requirements,\nthe capability of the design to meet these requirements, and\nto identify problems.\n\nDesign review is a primary tool for managing and evaluating development projects.\n\nIt is strongly recommended that it should be formal design because it is more structured than the informal one.\nIt includes participation from others outside the development team.\nIt may review reference or include results from other formal and informal reviews.\nDesign reviews should include\n\nexamination of development plans,\nrequirements specifications,\ndesign specifications,\ntesting plans and procedures,\nall other documents and activities associated with the project,\nverification results from each stage of the defined life cycle, and\nvalidation results for the overall device.\n\nThe Quality System regulation requires that at least one formal design review be conducted during the device design process. However, it is recommended that multiple design reviews be conducted\n\n(e.g., at the end of each software life cycle activity, in preparation for proceeding to the next activity).\n\nFormal design reviews documented should include:\n\nthe appropriate tasks and expected results, outputs, or products been established for each software life cycle activity\ncorrectness, completeness, consistency, and accuracy\nsatisfaction for the standards, practices, and conventions of that activity\nestablishment of a proper basis for initiating tasks for the next software life cycle activity"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#principles-of-software-validation",
    "href": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#principles-of-software-validation",
    "title": "FDA Validation Document Summary",
    "section": "Principles of Software Validation",
    "text": "Principles of Software Validation\n\nRequirements\nA documented software requirements specification provides a baseline for both validation and verification. The software validation process must include an established software requirements specification (Ref: 21 CFR 820.3(z) and (aa) and 820.30(f) and (g)).\n\n\nDefect Prevention\nIn order to establish that confidence, software developers should use a mixture of methods and techniques to prevent software errors and to detect software errors that do occur.\n\n\nTime and Effort\nPreparation for software validation should begin early, i.e., during design and development planning and design input. The final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\nSoftware Life Cycle\n\nSoftware validation takes place within the environment of an established software life cycle.\nThe software life cycle contains software engineering tasks and documentation necessary to support the software validation effort.\nspecific verification and validation tasks need to be appropriate for the intended use of the software\n\n\n\nPlans\n\nThe software validation process is defined and controlled through the use of a plan.\nThe software validation plan defines “what” is to be accomplished through the software validation effort.\nSoftware validation plans specify areas such as\n\nscope,\napproach,\nresources,\nschedules and the types and extent of activities,\ntasks, and\nwork items.\n\n\n\n\nProcedures\nThe software validation process is executed through the use of procedures. These procedures establish “how” to conduct the software validation effort. The procedures should identify the specific actions or sequence of actions that must be taken to complete individual validation activities, tasks, and work items.\n\n\nSoftware Validation After a Change\n\nDue to the complexity of software, a small local change may have a significant global system impact.\nIf a change exists in the software, the whole validation status of the software needs to be re-established.\nneed to determine the extent and impact of that change on the entire software system.\nthe software developer should then conduct an appropriate level of software regression testing to show that unchanged but vulnerable portions of the system have not been adversely affected.\n\n\n\nValidation Coverage\n\nValidation coverage should be based on the software’s complexity and safety risk.\nThe selection of validation activities, tasks, and work items should be commensurate with the complexity of the software design and the risk associated with the use of the software for the specified intended use.\n\n\n\nIndependence of Review\n\nValidation activities should be based on the basic quality assurance precept of “independence of review.”\nSelf-validation is extremely difficult.\nWhen possible, an independent evaluation is always better (like a contracted third-party independent verification and validation)\nAnother approach is to assign internal staff members that are not involved in a particular design or its implementation, but who have sufficient knowledge to evaluate the project and conduct the verification and validation activities.\n\n\n\nFlexibility and Responsibility\nThe device manufacturer has flexibility in choosing how to apply these validation principles, but retains ultimate responsibility for demonstrating that the software has been validated. FDA regulated medical device applications include software that:\n\nIs a component, part, or accessory of a medical device;\n\ncomponents: e.g., application software, operating systems, compilers, debuggers, configuration management tools, and many more\n\nIs itself a medical device; or\nIs used in manufacturing, design and development, or other parts of the quality system.\nNo matter how complex and disperse the software is, the manufacturer is in charge of responsibility for software validation."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#activities-and-tasks",
    "href": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#activities-and-tasks",
    "title": "FDA Validation Document Summary",
    "section": "Activities and Tasks",
    "text": "Activities and Tasks\nSoftware validation is accomplished through a series of activities and tasks that are planned and executed at various stages of the software development life cycle. These tasks may be\n\none time occurrences\niterated many times\n\n\nSoftware Life Cycle Activities\n\nSoftware developers should establish a software life cycle model that is appropriate for their product and organization.\nThe selected software life cycle model should cover the software from its birth to its retirement.\nActivities in a typical software life cycle model:\n\nQuality Planning\nSystem Requirements Definition\nDetailed Software Requirements Specification\nSoftware Design Specification\nConstruction or Coding\nTesting\nInstallation\nOperation and Support\nMaintenance\nRetirement\n\nVerification, testing, and other tasks that support software validation occur during each of these activities.\nSeveral software life cycle models defined in FDA’s Glossary of Computerized System and Software Development\n\nTerminology dated August 1995:\n\nwaterfall\nspiral\nrapid prototyping\nincremental development, etc.\n\n\n\nTypical Tasks Supporting Validation\nthe software developer should at least consider each of the risk-related tasks and should define and document which tasks are or are not appropriate for their specific application.\n\nQuality Planning\nDesign and development planning should culminate in a plan that identifies\n\nnecessary tasks,\nprocedures for anomaly reporting and resolution,\nnecessary resources, and\nmanagement review requirements including formal design reviews.\n\nThe plan should include:\n\nThe specific tasks for each life cycle activity;\nEnumeration of important quality factors (e.g., reliability, maintainability, and usability);\nMethods and procedures for each task;\nTask acceptance criteria;\nCriteria for defining and documenting outputs in terms that will allow evaluation of their conformance to input requirements;\nInputs for each task;\nOutputs from each task;\nRoles, resources, and responsibilities for each task;\nRisks and assumptions; and\nDocumentation of user needs.\n\nThe plan should identify\n\nthe personnel,\nthe facility and equipment resources for each task, and\nthe role that risk (hazard) management will play.\n\nA configuration management plan should be developed that will guide and control multiple parallel development activities and ensure proper communications and documentation.\nControls are necessary to ensure positive and correct correspondence among all approved versions of the specifications documents, source code, object code, and test suites that comprise a software system. The controls also should ensure accurate identification of, and access to, the currently approved versions.\nProcedures should be created for reporting and resolving software anomalies found through validation or other activities.\nManagement should identify the reports and specify the contents, format, and responsible organizational elements for each report. Procedures also are necessary for the review and approval of software development results, including the responsible organizational elements for such reviews and approvals.\nTypical Tasks – Quality Planning\n\nRisk (Hazard) Management Plan\nConfiguration Management Plan\nSoftware Quality Assurance Plan\n\nSoftware Verification and Validation Plan\n\nVerification and Validation Tasks, and Acceptance Criteria\nSchedule and Resource Allocation (for software verification and validation activities)\nReporting Requirements\n\nFormal Design Review Requirements\nOther Technical Review Requirements\n\nProblem Reporting and Resolution Procedures\nOther Support Activities\n\n\n\nRequirements\nRequirements development includes the\n\nidentification,\nanalysis, and\ndocumentation of information about the device and its intended use.\n\nAreas of special importance include allocation of system functions to\n\nhardware/software,\noperating conditions,\nuser characteristics,\npotential hazards, and\nanticipated tasks.\n\nIn addition, the requirements should state clearly the intended use of the software. It is not possible to validate software without predetermined and documented software requirements. Typical software requirements specify the following:\n\nAll software system inputs;\nAll software system outputs;\nAll functions that the software system will perform;\nAll performance requirements that the software will meet, (e.g., data throughput, reliability, and timing);\nThe definition of all external and user interfaces, as well as any internal software-to-system interfaces;\nHow users will interact with the system;\nWhat constitutes an error and how errors should be handled;\nRequired response times;\nThe intended operating environment for the software, if this is a design constraint (e.g., hardware platform, operating system);\nAll ranges, limits, defaults, and specific values that the software will accept; and\nAll safety related requirements, specifications, features, or functions that will be implemented in software.\n\nSoftware requirement specifications should identify clearly the potential hazards that can result from a software failure in the system as well as any safety requirements to be implemented in software.\nThe consequences of software failure should be evaluated, along with means of mitigating such failures (e.g., hardware mitigation, defensive programming, etc.).\nThe Quality System regulation requires a mechanism for addressing incomplete, ambiguous, or conflicting requirements. (See 21 CFR 820.30(c).) Each requirement (e.g., hardware, software, user, operator interface, and safety) identified in the software requirements specification should be evaluated for accuracy, completeness, consistency, testability, correctness, and clarity.\nFor example, software requirements should be evaluated to verify that:\n\nThere are no internal inconsistencies among requirements;\nAll of the performance requirements for the system have been spelled out;\nFault tolerance, safety, and security requirements are complete and correct;\nAllocation of software functions is accurate and complete;\nSoftware requirements are appropriate for the system hazards; and\nAll requirements are expressed in terms that are measurable or objectively verifiable.\n\nA software requirements traceability analysis should be conducted to trace software requirements to (and from) system requirements and to risk analysis results. In addition to any other analyses and documentation used to verify software requirements, a formal design review is recommended to confirm that requirements are fully specified and appropriate before extensive software design efforts begin. Requirements can be approved and released incrementally, but care should be taken that interactions and interfaces among software (and hardware) requirements are properly reviewed, analyzed, and controlled.\nTypical Tasks – Requirements\n\nPreliminary Risk Analysis\nTraceability Analysis\n\nSoftware Requirements to System Requirements (and vice versa)\nSoftware Requirements to Risk Analysis\n\nDescription of User Characteristics\nListing of Characteristics and Limitations of Primary and Secondary Memory\nSoftware Requirements Evaluation\nSoftware User Interface Requirements Analysis\nSystem Test Plan Generation\nAcceptance Test Plan Generation\nAmbiguity Review or Analysis\n\n\n\nDesign\nIn the design process, the software requirements specification is translated into a logical and physical representation of the software to be implemented. The software design specification is a description of what the software should do and how it should do it. The design specification may contain both a high level summary of the design and detailed design information. Human factors engineering should be woven into\n\nthe entire design and development process,\nthe device design requirements,\nanalyses, and\ntests.\n\nDevice safety and usability issues should be considered when developing\n\nflowcharts,\nstate diagrams,\nprototyping tools, and\ntest plans.\n\nAlso, task and function analyses, risk analyses, prototype tests and reviews, and full usability tests should be performed. Participants from the user population should be included when applying these methodologies.\nThe software design specification should include:\n\nSoftware requirements specification, including predetermined criteria for acceptance of the software;\nSoftware risk analysis;\nDevelopment procedures and coding guidelines (or other programming procedures);\nSystems documentation (e.g., a narrative or a context diagram) that describes the systems context in which the program is intended to function, including the relationship of hardware, software, and the physical environment;\nHardware to be used;\nParameters to be measured or recorded;\nLogical structure (including control logic) and logical processing steps (e.g., algorithms);\nData structures and data flow diagrams;\nDefinitions of variables (control and data) and description of where they are used;\nError, alarm, and warning messages;\nSupporting software (e.g., operating systems, drivers, other application software);\nCommunication links (links among internal modules of the software, links with the supporting software, links with the hardware, and links with the user);\nSecurity measures (both physical and logical security); and\nAny additional constraints not identified in the above elements.\n\nThe first four of the elements noted above usually are separate pre-existing documents that are included by reference in the software design specification. Software requirements specification was discussed in the preceding section, as was software risk analysis.\nSoftware design evaluations criteria:\n\ncomplete,\ncorrect,\nconsistent,\nunambiguous,\nfeasible,\nmaintainable,\nanalyses of control flow,\ndata flow,\ncomplexity,\ntiming,\nsizing,\nmemory allocation,\ncriticality analysis, and many other aspects of the design\n\nAppropriate consideration of software architecture (e.g., modular structure) during design can reduce the magnitude of future validation efforts when software changes are needed.\nA traceability analysis should be conducted to verify that the software design implements all of the software requirements. As a technique for identifying where requirements are not sufficient, the traceability analysis should also verify that all aspects of the design are traceable to software requirements.\nAn analysis of communication links should be conducted to evaluate the proposed design with respect to hardware, user, and related software requirements. At the end of the software design activity, a Formal Design Review should be conducted to verify that the design is correct, consistent, complete, accurate, and testable, before moving to implement the design.\nSeveral versions of both the software requirement specification and the software design specification should be maintained. All approved versions should be archived and controlled in accordance with established configuration management procedures.\nTypical Tasks – Design\n\nUpdated Software Risk Analysis\nTraceability Analysis - Design Specification to Software Requirements (and vice versa)\nSoftware Design Evaluation\nDesign Communication Link Analysis\nModule Test Plan Generation\nIntegration Test Plan Generation\nTest Design Generation (module, integration, system, and acceptance)\n\n\n\nConstruction or Coding\nSoftware may be constructed either by coding. Coding is the software activity where the detailed design specification is implemented as source code. It is the last stage in decomposition of the software requirements where module specifications are translated into a programming language.\nCoding usually involves the use of a high-level programming language, but may also entail the use of assembly language (or microcode) for time-critical operations.\nA source code traceability analysis is an important tool to verify that all code is linked to established specifications and established test procedures. A source code traceability analysis should be conducted and documented to verify that:\n\nEach element of the software design specification has been implemented in code;\nModules and functions implemented in code can be traced back to an element in the software design specification and to the risk analysis;\nTests for modules and functions can be traced back to an element in the software design specification and to the risk analysis; and\nTests for modules and functions can be traced to source code for the same modules and functions.\n\nTypical Tasks – Construction or Coding\n\nTraceability Analyses\n\nSource Code to Design Specification (and vice versa)\nTest Cases to Source Code and to Design Specification\n\nSource Code and Source Code Documentation Evaluation\nSource Code Interface Analysis\nTest Procedure and Test Case Generation (module, integration, system, and acceptance)\n\n\n\nTesting by the Software Developer\nSoftware testing entails running software products under known conditions with defined inputs and documented outcomes that can be compared to their predefined expectations. It is a time consuming, difficult, and imperfect activity.\nAs such, it requires early planning in order to be effective and efficient. Test plans and test cases should be created as early in the software development process as feasible.\nThey should identify\n\nthe schedules,\nenvironments,\nresources (personnel, tools, etc.),\nmethodologies,\ncases (inputs, procedures, outputs, expected results),\ndocumentation, and\nreporting criteria.\n\nDescriptions of categories of software and software testing effort appear in the literature\n\nNIST Special Publication 500-235, Structured Testing: A Testing Methodology Using the Cyclomatic Complexity Metric;\nNUREG/CR-6293, Verification and Validation Guidelines for High Integrity Systems; and\nIEEE Computer Society Press, Handbook of Software Reliability Engineering.\n\nTesting of all program functionality does not mean all of the program has been tested. Testing of all of a program’s code does not mean all necessary functionality is present in the program. Testing of all program functionality and all program code does not mean the program is 100% correct! Software testing that finds no errors should not be interpreted to mean that errors do not exist in the software product; it may mean the testing was superficial.\nAn essential element of a software test case is the expected result. It is the key detail that permits objective evaluation of the actual test result. This necessary testing information is obtained from the corresponding, predefined definition or specification.\nA software testing process should be based on principles that foster effective examinations of a software product. Applicable software testing tenets include:\n\nThe expected test outcome is predefined;\nA good test case has a high probability of exposing an error;\nA successful test is one that finds an error;\nThere is independence from coding;\nBoth application (user) and software (programming) expertise are employed;\nTesters use different tools from coders;\nExamining only the usual case is insufficient;\nTest documentation permits its reuse and an independent confirmation of the pass/fail status of a test outcome during subsequent review.\n\nCode-based testing is also known as structural testing or “white-box” testing. It identifies test cases based on knowledge obtained from the source code, detailed design specification, and other development documents. Structural testing can identify “dead” code that is never executed when the program is run. Structural testing is accomplished primarily with unit (module) level testing, but can be extended to other levels of software testing.\nThe level of structural testing can be evaluated using metrics that are designed to show what percentage of the software structure has been evaluated during structural testing. These metrics are typically referred to as “coverage” and are a measure of completeness with respect to test selection criteria. The amount of structural coverage should be commensurate with the level of risk posed by the software. Use of the term “coverage” usually means 100% coverage. Common structural coverage metrics include:\n\nStatement Coverage – This criteria requires sufficient test cases for each program statement to be executed at least once; however, its achievement is insufficient to provide confidence in a software product’s behavior.\nDecision (Branch) Coverage – This criteria requires sufficient test cases for each program decision or branch to be executed so that each possible outcome occurs at least once. It is considered to be a minimum level of coverage for most software products, but decision coverage alone is insufficient for high-integrity applications.\nCondition Coverage – This criteria requires sufficient test cases for each condition in a program decision to take on all possible outcomes at least once. It differs from branch coverage only when multiple conditions must be evaluated to reach a decision.\nMulti-Condition Coverage – This criteria requires sufficient test cases to exercise all possible combinations of conditions in a program decision.\nLoop Coverage – This criteria requires sufficient test cases for all program loops to be executed for zero, one, two, and many iterations covering initialization, typical running and termination (boundary) conditions.\nPath Coverage – This criteria requires sufficient test cases for each feasible path, basis path, etc., from start to exit of a defined program segment, to be executed at least once. Because of the very large number of possible paths through a software program, path coverage is generally not achievable. The amount of path coverage is normally established based on the risk or criticality of the software under test.\nData Flow Coverage – This criteria requires sufficient test cases for each feasible data flow to be executed at least once. A number of data flow testing strategies are available.\n\nThe following types of functional software testing involve generally increasing levels of effort:\n\nNormal Case – Testing with usual inputs is necessary. However, testing a software product only with expected, valid inputs does not thoroughly test that software product. By itself, normal case testing cannot provide sufficient confidence in the dependability of the software product.\nOutput Forcing – Choosing test inputs to ensure that selected (or all) software outputs are generated by testing.\nRobustness – Software testing should demonstrate that a software product behaves correctly when given unexpected, invalid inputs. Methods for identifying a sufficient set of such test cases include Equivalence Class Partitioning, Boundary Value Analysis, and Special Case Identification (Error Guessing). While important and necessary, these techniques do not ensure that all of the most appropriate challenges to a software product have been identified for testing.\nCombinations of Inputs – The functional testing methods identified above all emphasize individual or single test inputs. Most software products operate with multiple inputs under their conditions of use. Thorough software product testing should consider the combinations of inputs a software unit or system may encounter during operation. Error guessing can be extended to identify combinations of inputs, but it is an ad hoc technique. Cause-effect graphing is one functional software testing technique that systematically identifies combinations of inputs to a software product for inclusion in test cases.\n\nFunctional and structural software test case identification techniques provide specific inputs for testing, rather than random test inputs. One weakness of these techniques is the difficulty in linking structural and functional test completion criteria to a software product’s reliability.\nAdvanced software testing methods, such as statistical testing, can be employed to provide further assurance that a software product is dependable. Statistical testing uses randomly generated test data from defined distributions based on an operational profile (e.g., expected use, hazardous use, or malicious use of the software product). Large amounts of test data are generated and can be targeted to cover particular areas or concerns, providing an increased possibility of identifying individual and multiple rare operating conditions that were not anticipated by either the software product’s designers or its testers. Statistical testing also provides high structural coverage. It does require a stable software product. Thus, structural and functional testing are prerequisites for statistical testing of a software product.\nAnother aspect of software testing is the testing of software changes. Changes occur frequently during software development. These changes are the result of\n\ndebugging that finds an error and it is corrected,\nnew or changed requirements (“requirements creep”), and\nmodified designs as more effective or efficient implementations are found.\n\nOnce a software product has been baselined (approved), any change to that product should have its own “mini life cycle,” including testing. Testing of a changed software product requires additional effort. It should demonstrate\n\nthat the change was implemented correctly, and\nthat the change did not adversely impact other parts of the software product.\n\nRegression analysis is the determination of the impact of a change based on review of the relevant documentation in order to identify the necessary regression tests to be run. Regression testing is the rerunning of test cases that a program has previously executed correctly and comparing the current result to the previous result in order to detect unintended effects of a software change. Regression analysis and regression testing should also be employed when using integration methods to build a software product to ensure that newly integrated modules do not adversely impact the operation of previously integrated modules.\nIn order to provide a thorough and rigorous examination of a software product, development testing is typically organized into levels: unit, integration, and system levels of testing.\n\nUnit (module or component) level testing focuses on the early examination of sub-program functionality and ensures that functionality not visible at the system level is examined by testing. Unit testing ensures that quality software units are furnished for integration into the finished software product.\nIntegration level testing focuses on the transfer of data and control across a program’s internal and external interfaces. External interfaces are those with\n\nother software (including operating system software),\nsystem hardware, and\nthe users and can be described as communications links.\n\nSystem level testing demonstrates that all specified functionality exists and that the software product is trustworthy. This testing verifies the as-built program’s functionality and performance with respect to the requirements for the software product as exhibited on the specified operating platform(s). System level software testing addresses functional concerns and the following elements of a device’s software that are related to the intended use(s):\n\nPerformance issues (e.g., response times, reliability measurements);\nResponses to stress conditions, e.g., behavior under maximum load, continuous use;\nOperation of internal and external security features;\nEffectiveness of recovery procedures, including disaster recovery;\nUsability; (Usability vs Utility??)\nCompatibility with other software products;\nBehavior in each of the defined hardware configurations; and\nAccuracy of documentation.\n\n\nControl measures (e.g., a traceability analysis) should be used to ensure that the intended coverage is achieved.\nSystem level testing also exhibits the software product’s behavior in the intended operating environment. The location of such testing is dependent upon the software developer’s ability to produce the target operating environment(s). Depending upon the circumstances, simulation and/or testing at (potential) customer locations may be utilized.\nTest plans should identify the controls needed to ensure\n\nthat the intended coverage is achieved and\nthat proper documentation is prepared when planned system level testing is conducted at sites not directly controlled by the software developer.\n\nTest procedures, test data, and test results\n\nshould be documented in a manner permitting objective pass/fail decisions to be reached.\nshould also be suitable for review and objective decision making subsequent to running the test,\nshould be suitable for use in any subsequent regression testing.\n\nErrors detected during testing should be\n\nlogged,\nclassified,\nreviewed, and\nresolved prior to release of the software.\n\nSoftware error data that is collected and analyzed during a development life cycle may be used to determine the suitability of the software product for release for commercial distribution. Test reports should comply with the requirements of the corresponding test plans.\nSoftware testing tools are frequently used to ensure consistency, thoroughness, and efficiency in the testing of such software products and to fulfill the requirements of the planned testing activities.\nAppropriate documentation providing evidence of the validation of these software tools for their intended use should be maintained (see section 6 of this guidance).\nTypical Tasks – Testing by the Software Developer\n\nTest Planning\nStructural Test Case Identification\nFunctional Test Case Identification\nTraceability Analysis - Testing\nUnit (Module) Tests to Detailed Design\nIntegration Tests to High Level Design\nSystem Tests to Software Requirements\nUnit (Module) Test Execution\nIntegration Test Execution\nFunctional Test Execution\nSystem Test Execution\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\nUser Site Testing\nTesting at the user site is an essential part of software validation. The Quality System regulation requires\n\ninstallation and\ninspection procedures (including testing where appropriate) as well as\ndocumentation of inspection and\ntesting to demonstrate proper installation. (See 21 CFR §820.170.)\n\nLikewise, manufacturing equipment must meet specified requirements, and automated systems must be validated for their intended use. (See 21 CFR §820.70(g) and 21 CFR §820.70(i) respectively.)\nTerminology regarding user site testing can be confusing. Terms such as\n\nbeta test,\nsite validation,\nuser acceptance test,\ninstallation verification, and\ninstallation testing have all been used to describe user site testing.\n\nFor the purposes of this guidance, the term “user site testing” encompasses all of these and any other testing that takes place outside of the developer’s controlled environment.\nThis testing should take place at a user’s site with the actual hardware and software that will be part of the installed system configuration. The testing is accomplished through either actual or simulated use of the software being tested within the context in which it is intended to function.\nTest planners should check with the FDA Center(s) with the corresponding product jurisdiction to determine whether there are any additional regulatory requirements for user site testing.\nUser site testing should follow a pre-defined written plan with\n\na formal summary of testing and\na record of formal acceptance.\n\nThe following documented evidence should be retained:\n\nall testing procedures,\ntest input data, and\ntest results\n\nThere should be evidence that hardware and software are installed and configured as specified. Measures should ensure that all system components are exercised during the testing and that the versions of these components are those specified. The testing plan should specify testing throughout the full range of operating conditions and should specify continuation for a sufficient time to allow the system to encounter a wide spectrum of conditions and events in an effort to detect any latent faults that are not apparent during more normal activities.\nSome of the evaluations of the system’s ability that have been performed earlier by the software developer at the developer’s site should be repeated at the site of actual use. These may include tests for:\n\na high volume of data,\nheavy loads or stresses,\nsecurity,\nfault testing (avoidance, detection, tolerance, and recovery),\nerror messages, and\nimplementation of safety requirements.\n\nThere should be an evaluation of the ability of the users of the system to understand and correctly interface with it.\nOperators should be able to perform the intended functions and respond in an appropriate and timely manner to all alarms, warnings, and error messages.\nRecords should be maintained of both proper system performance and any system failures that are encountered.\nThe revision of the system to compensate for faults detected during this user site testing should follow the same procedures and controls as for any other software change.\nThe developers of the software may or may not be involved in the user site testing.\n\nIf the developers are involved, they may seamlessly carry over to the user’s site the last portions of design-level systems testing.\nIf the developers are not involved, it is all the more important that the user have persons who understand the importance of careful test planning, the definition of expected test results, and the recording of all test outputs.\n\nTypical Tasks – User Site Testing\n\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\nMaintenance and Software Changes\n\nHardware vs Software\nHardware maintenance typically includes\n\npreventive hardware maintenance actions,\ncomponent replacement, and\ncorrective changes.\n\nSoftware maintenance includes\n\ncorrective,\nperfective, and\nadaptive maintenance\nbut does not include preventive maintenance actions or software component replacement.\n\n\n\nMaintenance Types\n\nCorrective maintenance: Changes made to correct errors and faults in the software.\nPerfective maintenance: Changes made to the software to improve the performance, maintainability, or other attributes of the software system .\nAdaptive maintenance: Changes to make the software system usable in a changed environment.\n\nSufficient regression analysis and testing should be conducted to demonstrate that portions of the software not involved in the change were not adversely impacted. When changes are made to a software system,\n\neither during initial development or\nduring post release maintenance,\n\nThis is in addition to testing that evaluates the correctness of the implemented change(s). The specific validation effort necessary for each software change is determined by\n\nthe type of change,\nthe development products affected, and the\nimpact of those products on the operation of the software.\n\n\n\nFactors of Limitting Validation Effort Needed When a Change Is Made\n\ncareful and complete documentation of the design structure and\ncareful and complete documentation of interrelationships of various modules,\ninterfaces, etc.\nFor example,\n\ntest documentation,\ntest cases, and\nresults of previous verification and validation testing All of them need to be archived if they are to be available for performing subsequent regression testing.\n\n\nThe following additional maintenance tasks should be addressed:\n\nSoftware Validation Plan Revision - For software that was previously validated, the existing software validation plan should be revised to support the validation of the revised software. If no previous software validation plan exists, such a plan should be established to support the validation of the revised software.\nAnomaly Evaluation – Software organizations frequently maintain documentation, such as software problem reports that describe software anomalies discovered and the specific corrective action taken to fix each anomaly.\n\nToo often, however, mistakes are repeated because software developers do not take the next step to determine the root causes of problems and make the process and procedural changes needed to avoid recurrence of the problem.\nSoftware anomalies should be evaluated in terms of their severity and their effects on system operation and safety,\nbut they should also be treated as symptoms of process deficiencies in the quality system.\nA root cause analysis of anomalies can identify specific quality system deficiencies.\nWhere trends are identified (e.g., recurrence of similar software anomalies), appropriate corrective and preventive actions must be implemented and documented to avoid further recurrence of similar quality problems. (See 21 CFR 820.100.)\n\nProblem Identification and Resolution Tracking - All problems discovered during maintenance of the software should be documented. The resolution of each problem should be tracked to ensure it is fixed, for historical reference, and for trending.\nProposed Change Assessment - All proposed modifications, enhancements, or additions should be assessed to determine the effect each change would have on the system. This information should determine the extent to which verification and/or validation tasks need to be iterated.\nTask Iteration - For approved software changes, all necessary verification and validation tasks should be performed to ensure that planned changes are implemented correctly, all documentation is complete and up to date, and no unacceptable changes have occurred in software performance.\nDocumentation Updating – Documentation should be carefully reviewed to determine which documents have been impacted by a change. All approved documents (e.g., specifications, test procedures, user manuals, etc.) that have been affected should be updated in accordance with configuration management procedures. Specifications should be updated before any maintenance and software changes are made."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#validation-of-automated-process-equipment-and-quality-system-software",
    "href": "docs/blog/posts/2022-12-10-FDA/sw_validation.html#validation-of-automated-process-equipment-and-quality-system-software",
    "title": "FDA Validation Document Summary",
    "section": "Validation of Automated Process Equipment and Quality System Software",
    "text": "Validation of Automated Process Equipment and Quality System Software\nThe Quality System regulation requires that “when computers or automated data processing systems are used as part of production or the quality system, the [device] manufacturer shall validate computer software for its intended use according to an established protocol.” (See 21 CFR §820.70(i)). This has been a regulatory requirement of FDA’s medical device Good Manufacturing Practice (GMP) regulations since 1978.\nComputer systems that implement part of a device manufacturer’s production processes or quality system (or that are used to create and maintain records required by any other FDA regulation) are subject to the Electronic Records; Electronic Signatures regulation. (See 21 CFR Part 11.) This regulation establishes additional security, data integrity, and validation requirements when records are created or maintained electronically. These additional Part 11 requirements should be carefully considered and included in system requirements and software requirements for any automated record keeping systems. System validation and software validation should demonstrate that all Part 11 requirements have been met.\nComputers and automated equipment are used extensively throughout all aspects of\n\nmedical device design,\nlaboratory testing and analysis,\nproduct inspection and acceptance,\nproduction and process control,\nenvironmental controls,\npackaging,\nlabeling,\ntraceability,\ndocument control,\ncomplaint management, and many other aspects of the quality system.\n\nIncreasingly, automated plant floor operations can involve extensive use of embedded systems in:\n\nprogrammable logic controllers;\ndigital function controllers;\nstatistical process control;\nsupervisory control and data acquisition;\nrobotics;\nhuman-machine interfaces;\ninput/output devices; and\ncomputer operating systems.\n\nAll software tools used for software design are subject to the requirement for software validation, but the validation approach used for each application can vary widely.\nValidation is typically supported by:\n\nverifications of the outputs from each stage of that software development life cycle; and\nchecking for proper operation of the finished software in the device manufacturer’s intended use environment.\n\n\nHow Much Validation Evidence Is Needed?\nThe level of validation effort should be commensurate with\n\nthe risk posed by the automated operation,\nthe complexity of the process software,\nthe degree to which the device manufacturer is dependent upon that automated process to produce a safe and effective device\n\nDocumented requirements and risk analysis of the automated process help to define the scope of the evidence needed to show that the software is validated for its intended use. Without a plan, extensive testing may be needed for:\n\na plant-wide electronic record and electronic signature system;\nan automated controller for a sterilization cycle; or\nautomated test equipment used for inspection and acceptance of finished circuit boards in a lifesustaining / life-supporting device.\n\nHigh risk applications should not be running in the same operating environment with non-validated software functions, even if those software functions are not used. Risk mitigation techniques such as memory partitioning or other approaches to resource protection may need to be considered when high risk applications and lower risk applications are to be used in the same operating environment.\nWhen software is upgraded or any changes are made to the software, the device manufacturer should consider how those changes may impact the “used portions” of the software and must reconfirm the validation of those portions of the software that are used. (See 21 CFR §820.70(i).)\n\n\nDefined User Equipment\nA very important key to software validation is a documented user requirements specification that defines:\n\nthe “intended use” of the software or automated equipment; and\nthe extent to which the device manufacturer is dependent upon that software or equipment for production of a quality medical device.\n\nThe device manufacturer (user) needs to define the expected operating environment including any required hardware and software configurations, software versions, utilities, etc. The user also needs to:\n\ndocument requirements for system performance, quality, error handling, startup, shutdown, security, etc.;\nidentify any safety related functions or features, such as sensors, alarms, interlocks, logical processing steps, or command sequences; and\ndefine objective criteria for determining acceptable performance.\n\nThe validation must be conducted in accordance with a documented protocol, and the validation results must also be documented. (See 21 CFR §820.70(i).) Test cases should be documented that will exercise the system to challenge its performance against the pre-determined criteria, especially for its most critical parameters.\nTest cases should address\n\nerror and alarm conditions,\nstartup, shutdown,\nall applicable user functions and operator controls,\npotential operator errors,\nmaximum and minimum ranges of allowed values, and\nstress conditions applicable to the intended use of the equipment.\n\nThe test cases should be executed and the results should be recorded and evaluated to determine whether the results support a conclusion that the software is validated for its intended use.\nA device manufacturer may conduct a validation using their own personnel or may depend on a third party such as the equipment/software vendor or a consultant. In any case, the device manufacturer retains the ultimate responsibility for ensuring that the production and quality system software:\n\nis validated according to a written procedure for the particular intended use; and\nwill perform as intended in the chosen application.\n\nThe device manufacturer should have documentation including:\n\ndefined user requirements;\nvalidation protocol used;\nacceptance criteria;\ntest cases and results; and\na validation summary that objectively confirms that the software is validated for its intended use.\n\n\n\nValidation of Off-The-Shelf Software and Automated Equipment\nMost of the automated equipment and systems used by device manufacturers are supplied by thirdparty vendors and are purchased off-the-shelf (OTS). The device manufacturer is responsible for ensuring that the product development methodologies used by the OTS software developer are appropriate and sufficient for the device manufacturer’s intended use of that OTS software.\nWhere possible and depending upon the device risk involved, the device manufacturer should consider auditing the vendor’s design and development methodologies used in the construction of the OTS software and should assess the development and validation documentation generated for the OTS software. Such audits can be conducted by the device manufacturer or by a qualified third party.\nThe audit should demonstrate that the vendor’s procedures for and results of the verification and validation activities performed the OTS software are appropriate and sufficient for the safety and effectiveness requirements of the medical device to be produced using that software."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "",
    "text": "Notice\nLast Update\nIntroduction\n\nDefinition of SW Validation\nSome Terminology\nRationale\nObjective of SW Validation\nWhat to validate\nMain Institutions\n\nQuality System Regulation\nVerification\nValidation\nBenefits and Difficulty in SW V&V\nSW Development as Part of System Design\n\nOverview\nDesign Reveiw\n\n\n\n\nValidation Pinciples\n\nOverview\nConditions\nPlanning\nAfter SW Change\nSW Lifecycle\n\nSW Lifecycle Tasks\n\nOverview\nQuality Planning\nConfiguration Management\nTask Requirements\nDesign Overview\n\nDesign Consideration\nDesign Specification\nDesign Activity and Task\n\n\n\n\n\nTesting Tasks\n\nOverview\nConsideration Before Testing Tasks\nCode Based Testing\nSolution to White Box Testing\nDevelopment Testing\nUser Site Testing\n\nOverview\nTesting\n\n\nMaintenance and SW Changes\nValidation of Quality System SW\n\nOverview\nFactors in Validation"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#definition-of-software-validation",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#definition-of-software-validation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Definition of Software Validation",
    "text": "Definition of Software Validation\nSoftware Validation is a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997.\n(See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\n\nSome Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology: requirements, specification, verification, and validation.\n\n\n\nObjective of SW validation is to ensure\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\nWhat to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#quality-system-regulation",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#quality-system-regulation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Quality System Regulation",
    "text": "Quality System Regulation\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\n\n\nflowchart TB\n    subgraph Quality_System_Regulation\n        direction LR\n        subgraph Requirement\n            direction TB\n            user_requirements\n        end\n        subgraph Specification\n           direction TB\n           document_user_requirements \n        end \n        subgraph Verification\n           direction TB\n           verify_spacified_requirements\n        end\n        subgraph Validation\n           direction TB\n           Confirmation_by_Examinations\n           Provision_of_objective_3evidences\n        end\n        Requirement--> Specification --> Verification --> Validation                    \n    end\n    subgraph First_Detail\n        direction TB\n        subgraph User_Requirement\n            direction TB\n            any_need_for_customer---\n            any_need_for_system---\n            any_need_for_software\n        end\n            subgraph Document_User_Requirement\n            direction TB\n            define_means_for_requirements---\n          define_criteria_for_requirements\n        end         \n        subgraph Verify_Spacified_Requirement\n            direction TB\n            Objective_Evidence--->|needs|Software_Testing\n        end\n        subgraph SW_Validation\n            direction TB\n            subgraph Confirmation_by_Examination\n            direction TB\n                subgraph Examination_List_of_SW_LifeCycle\n                    direction TB\n                    comprehensiveness_of_software_testing---\n                    inspection_verification_test---\n                    analysis_verification_test---\n                    other_varification_tests    \n                end \n            end             \n            subgraph Provision_of_Objective_3evidences\n                direction TB\n                Software_specifications_conformity---\n                Consistent_SW_Implementation---\n                Correctness_Completeness_Traceability\n            end\n        end\n        Requirement---User_Requirement\n        Specification---Document_User_Requirement\n        Verification---Verify_Spacified_Requirement\n        Confirmation_by_Examinations---Confirmation_by_Examination\n        Provision_of_objective_3evidences---Provision_of_Objective_3evidences             \n    end"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#verification",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#verification",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Verification",
    "text": "Verification\n\n\n\n\nflowchart LR\n    subgraph Objective_Evidence\n        direction LR\n        subgraph Design_Outputs_of_SW_life_cycle_for_Specified_Requirements\n            direction TB\n            Consistency---\n            Completeness---\n            Correctness---\n            Documentation\n        end       \n        subgraph Software_Testing\n            direction LR\n            subgraph Testing_Environments\n                direction TB\n                satisfaction_for_input_requirements\n                satisfaction_for_input_requirements---Simulated_Use_Environment\n                subgraph User_Site_Testing\n                    direction TB                            \n                    Installation_Qualification---\n                    Operational_Qualification---\n                    Performance_Qualification\n                end\n            end\n            satisfaction_for_input_requirements---User_Site_Testing\n            subgraph Testing_Activities\n                direction TB\n                static_analyses---\n                dynamic_analyses---\n                code_and_document_inspections---\n                walkthroughs\n            end \n        Testing_Environments-->Testing_Activities\n        end\n    Design_Outputs_of_SW_life_cycle_for_Specified_Requirements-->Software_Testing-->Testing_Activities\nend    \n\n\n\n\n\n\n\n\n\n\nInstallation_Qualification (IQ): documentation of correct installations according to requirements, specifications, vendor’s recommendations, and the FDA’s guidance for all hardware, software, equipment and systems.\nOperational_Qualification (OQ): establishment of confidence that the software shows constant performances according to specified requirements.\nPerformance_Qualification (PQ): confirmation of the performance in the intended use according to the specified requirements for functionality and safety throughout the SW life cycle."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation",
    "text": "Validation\n\n\n\n\nflowchart LR\n    subgraph Validation\n    direction LR\n        subgraph Confirmation_by_Examination\n            direction TB\n            subgraph Examination_List_at_each_stage_of_SW_Life_Cycle\n                direction TB\n                comprehensiveness_of_software_testing---\n                inspection_verification_test---\n                analysis_verification_test---\n                other_varification_tests    \n            end \n        end\n        subgraph Provision_of_objective_3evidences\n            direction TB\n            subgraph Software_specifications_conform_to\n                direction TB\n                user_needs \n                intended_uses\n            end\n            subgraph Consistent_SW_Implementation\n                direction TB\n                particular_requirements\n            end\n            subgraph Correctness_Completeness_Traceability\n                direction TB\n                correct_complete_implementation_by_all_SW_requirements---\n                traceable_to_system_requirements\n            end\n            Software_specifications_conform_to---\n            Consistent_SW_Implementation---\n            Correctness_Completeness_Traceability\n        end\n        Confirmation_by_Examination-->\n        Provision_of_objective_3evidences\n    end\n\n\n\n\n\n\n\n\n\n\n\n\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#benefits-and-difficulty-of-sw-vv",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#benefits-and-difficulty-of-sw-vv",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Benefits and Difficulty of SW V&V",
    "text": "Benefits and Difficulty of SW V&V\n\nBenefits of SW V&V\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nReduce long term costs by making V&V easier and less costly to reliably modify software and revalidate software changes.\n\n\n\nDifficulty in SW V&V\n\na developer cannot test forever, and\n\nit is difficult to know how much evidence is enough.\na matter of developing a level of confidence that the device meets all requirements\n\nConsiderations for an acceptable level of confidence\nmeasures and estimates such as defects found in specifications documents\ntesting coverage, and other techniques are all used before shipping the product.\na level of confidence varies depending upon the safety risk (hazard) of a SW or device"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#sw-development-as-part-of-system-design",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#sw-development-as-part-of-system-design",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Development as Part of System Design",
    "text": "SW Development as Part of System Design\n\nOverview\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        purpose_design_review---\n        design_review_types---\n        design_review_requirements---\n        design_review_outputs\n    end\n\n\n\n\n\n\n\n\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nuser’s needs\nintended uses from which the product is developed.\n\nA primary goal of SW validation is to demonstrate that all completed SW products comply with all documented requirements.\n\n\nDesign Review\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        subgraph Purpose_Design_Review\n            direction TB\n            documented_structured_comprehensive_systematic_examinations---\n            adequacy_of_design_requirements---\n            capability_of_design_for_requirements---\n            identification_of_problem   \n        end\n        subgraph Design_Reivew_Types\n            direction TB\n            subgraph Formal_Design_Review\n                direction TB\n                3rd_parties_outside_development_team\n            end\n            subgraph Informal_Design_Review\n                direction TB\n                within_development_team\n            end\n        Formal_Design_Review---Informal_Design_Review    \n        end\n        subgraph Design_Review_Requirements\n            direction TB\n               necessary_at_least_one_formal_design_review---\n               optinal_informal_design_review---\n               recommended_multiple_design_reviews\n        end\n        subgraph Formal_Design_Review_Outputs\n            direction TB\n            more_than_10_outputs\n        end\n        Purpose_Design_Review--> Design_Reivew_Types--> Design_Review_Requirements\n        Design_Review_Requirements-->Formal_Design_Review_Outputs\n    end\n\n\n\n\n\n\n\n\n\n\nDesign review is a primary tool for managing and evaluating development projects.\nAt least one formal design review must be conducted during the device design process.\nIt is recommended that multiple design reviews be conducted.\nProblems found at this point can\n\nbe resolved more easily,\nsave time and money, and\nreduce the likelihood of missing a critical issue."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#design-review",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#design-review",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Design Review",
    "text": "Design Review\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        subgraph Purpose_Design_Review\n            direction TB\n            documented_structured_comprehensive_systematic_examinations---\n            adequacy_of_design_requirements---\n            capability_of_design_for_requirements---\n            identification_of_problem   \n        end\n        subgraph Design_Reivew_Types\n            direction TB\n            subgraph Formal_Design_Review\n                direction TB\n                3rd_parties_outside_development_team\n            end\n            subgraph Informal_Design_Review\n                direction TB\n                within_development_team\n            end\n        Formal_Design_Review---Informal_Design_Review    \n        end\n        subgraph Design_Review_Requirements\n            direction TB\n               necessary_at_least_one_formal_design_review---\n               optinal_informal_design_review---\n               recommended_multiple_design_reviews\n        end\n        subgraph Formal_Design_Review_Outputs\n            direction TB\n            more_than_10_outputs\n        end\n        Purpose_Design_Review--> Design_Reivew_Types--> Design_Review_Requirements\n        Design_Review_Requirements-->Formal_Design_Review_Outputs\n    end\n\n\n\n\n\n\n\n\n\n\nDesign review is a primary tool for managing and evaluating development projects.\nAt least one formal design review must be conducted during the device design process.\nIt is recommended that multiple design reviews be conducted.\nProblems found at this point can\n\nbe resolved more easily,\nsave time and money, and\nreduce the likelihood of missing a critical issue."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation-principles",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation-principles",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation Principles",
    "text": "Validation Principles\n\nOverview\n\n\n\n\nflowchart LR\n  subgraph Validation_Principles\n        direction LR\n        subgraph Validation_Starting_Point\n            direction TB\n            during_design_planning---\n            during_development_planning---\n            all_results_should_be_supported_by_evidence_collected_from_planning_SW_lifecylce\n        end\n        subgraph Validation_Conditions\n            direction TB\n            Requirements---Estabilishment_Confidence---SW_Lifecycle\n        end\n\n        subgraph Validation_Planning\n            direction TB\n            Specify_Areas\n            subgraph Validation_Coverage\n                direction TB\n            end\n            subgraph Validation_Process_Establishment\n                direction TB\n            end\n        Specify_Areas---Validation_Coverage---Validation_Process_Establishment\n        end\n\n        subgraph After_Self_Validation\n            direction TB\n            subgraph Validation_After_SW_Change\n        direction TB\n        end\n\n        subgraph Independence_of_Review\n        direction TB\n\n        end\n        Validation_After_SW_Change---Independence_of_Review\n        end\n            Validation_Starting_Point-->Validation_Conditions-->Validation_Planning-->\nAfter_Self_Validation\n    end\n\n\n\n\n\n\n\n\n\nPreparation for software validation should begin as early as possible because the final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\n\nConditions\n\n\n\n\nflowchart LR\n\nsubgraph Validation_Conditions\n    direction LR\n    subgraph SW_Requirments\n        direction TB\n        subgraph Documented_SW_Requirments_Specification\n            direction TB\n            Baseline_Provision_for_V&V---\n            establishment_of_software_requirements_specification\n        end\n    end\n    subgraph Estabilishment_Confidence\n        direction TB\n            mixture_of_methods_techinques---\n            preventing_SW_errors---\n            detecting_SW_errors                 \n    end\n    subgraph SW_Lifecycle\n        direction TB\n        validation_must_be_conducted_within_established_environment_across_lifecycle---\n        lifecycle_contains_SW_engineering_tasks_and_documentation---\n        V&V_tasks_must_reflect_intended_use\n    end\nend\nSW_Requirments---Estabilishment_Confidence---SW_Lifecycle\n\n\n\n\n\n\n\n\n\n\nPlanning\n\n\n\n\nflowchart LR\n    subgraph Validation_Planning\n        direction LR\n        define_what_to_accomplish\n        subgraph Specify_Areas\n            direction TB\n            scope---\n            approach---\n            resources---\n            schedules_activities---\n            types_activitieis---\n            extent_of_activities---\n            tasks---\n            work_items\n        end\n            define_what_to_accomplish-->Specify_Areas\n        subgraph Validation_Coverage\n               direction TB\n            depending_on_SW_complexity_of_SW_design---\n            depending_on_safety_risk_for_specified_intended_use---\n            select_activities_tasks_work_items_for_complexity_safety_risk\n        end\n        subgraph Validation_Process_Establishment\n            direction TB\n            establish_how_to_conduct-->\n            identify_sequence_of_specific_actions-->\n            identify_specific_activitieis-->\n            identify_specific_tasks-->\n            identify_specific_work_items\n        end\n    Specify_Areas-->Validation_Coverage-->Validation_Process_Establishment\n    end\n\n\n\n\n\n\n\n\n\n\nAfter SW Change\n\n\n\n\nflowchart LR\n\nsubgraph After_Self_Validation\n    direction LR\n    subgraph Validation_After_SW_Change\n        direction TB\n        determine_extent_of_change_on_entire_SW_system-->\n        determine_impact_of_change_on_entire_SW_system-->\n        conduct_SW_regression_testing_on_unchanged_but_vulnerable_modules\n    end\n    subgraph Independence_of_Review\n        direction TB\n        follow_basic_quality_assurance_precept_of_independence_of_review---\n        avoid_self_validation---\n        should_conduct_contracted_3rd_party_independent_V&V---\n        or_conduct_blind_test_with_internal_staff\n    end\n    Validation_After_SW_Change---Independence_of_Review\nend\n    \n\n\n\n\n\n\n\n\n\n\nSW Lifecycle\n\n\n\n\nflowchart LR\nsubgraph SW_Lifecycle\n    direction TB\n    validation_must_be_conducted_within_the_established_environment_across_lifecycle---\n    lifecycle_contains_SW_engineering_tasks_and_documentation---\n    V&V_tasks_must_reflect_intended_use\nend\n\nsubgraph SW_Lifecycle_Activities\n    direction TB\n    subgraph should_establish_lifecycle_model\n        direction TB\n        subgraph SW_Lifecycle_Model_List_Defined_in_FDA\n            direction TB\n            waterfall---\n            spiral---\n            rapid_prototyping---\n            incremental_development---\n            etc\n        end     \n    end\n    subgraph should_cover_SW_birth_to_retirement\n        direction TB\n        subgraph Lifecycle_Activities\n            direction TB\n            Quality_Plan-->\n            System_Requirements_Definition-->\n            Detailed_Software_Requirements_Specification-->\n            Software_Design_Specification-->\n            Construction_or_Coding-->\n            Testing-->\n            Installation-->\n            Operation_and_Support-->\n            Maintenance-->\n            Retirement\n        end\n    end\n    should_establish_lifecycle_model-->should_cover_SW_birth_to_retirement\n    should_cover_SW_birth_to_retirement-->Lifecycle_Activities\nend\nsubgraph SW_Lifecycle_Tasks\n    direction TB\n    should_define_and_document_risk_related_tasks---\n    should_define_and_document_which_tasks_are_appropriate_in_vice_versa---\n    Quality_Planning---\n    Quality_Planning_Tasks---\n    Inclusion_Task_List_for_Plan---\n    Identification_Task_List_for_Plan---\n    Configuration_Management---\n    Control---\n    Management---\n    Procedures---\n    ensure_proper_communications_and_documentation---\n    Task_Requirements\nend\nSW_Lifecycle-->SW_Lifecycle_Activities-->SW_Lifecycle_Tasks"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#sw-lifecycle-tasks",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#sw-lifecycle-tasks",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Lifecycle Tasks",
    "text": "SW Lifecycle Tasks\n\nOverview\n\n\n\n\n \nflowchart TB\n\nsubgraph SW_Lifecycle_Tasks\n    direction LR\n    subgraph Define_and_Document_List\n        direction TB\n        risk_related_tasks---\n        whether_or_not_tasks_are_appropriate\n    end\n    \n    subgraph Quality_Planning\n        direction TB\n        subgraph Quality_Planning_Tasks\n            direction TB\n        \n        end\n        subgraph Inclusion_List_for_Plan\n            direction TB\n            \n        end\n        subgraph Identification_List_for_Plan\n            direction TB\n            \n        end\n    Quality_Planning_Tasks-->Inclusion_List_for_Plan-->Identification_List_for_Plan\n    end\n    \n    subgraph Configuration_Management\n        direction TB\n        subgraph Control\n            direction TB\n            \n        end\n        subgraph Management\n            direction TB\n        end\n        subgraph Procedures\n            direction TB\n        end\n        ensure_proper_communications_and_documentation\n        Control-->Management-->Procedures-->ensure_proper_communications_and_documentation \n    end\n    subgraph Task_Requirements\n        direction TB\n        identification---\n        analysis---\n        predetermined_documentation_about_device_its_intended_use---\n        Requirements_Specification_List---\n        Verfification_List_by_Evaluation---\n        Requirements_Tasks    \n    end\nDefine_and_Document_List-->Quality_Planning-->Configuration_Management-->Task_Requirements\nend     \n\n\n\n\n\n\n\n\n\n\nQuality Planning\n\n\n\n\nflowchart TB\nsubgraph Quality_Planning\n    direction LR\n    subgraph Quality_Planning_Tasks\n        direction TB\n        Risk_Hazard_Management_Plan---\n        Configuration_Management_Plan---\n        Software_Quality_Assurance_Plan---\n        Software_Verification_and_Validation_Plan---\n        Verification_and_Validation_Tasks---\n        Acceptance_Criteria---\n        Schedule_and_Resource_Allocation_for_V&V_activities---\n        Reporting_Requirements---\n        Formal_Design_Review_Requirements---\n        Other_Technical_Review_Requirements---\n        Problem_Reporting_and_Resolution_Procedures---\n        Other_Support_Activities\n    end\n    subgraph Inclusion_List_for_Plan\n        direction TB\n        specific_tasks_for_each_life_cycle_activity---\n        Enumeration_of_important_quality_factors--- \n        like_reliability_maintainability_usability---\n        Methods_and_procedures_for_each_task---\n        Task_acceptance_criteria---\n        Criteria_for_defining_and_documenting_outputs_for_input_requirements---\n        Inputs_for_each_task---\n        Outputs_from_each_task---\n        Roles_resources_and_responsibilities_for_each_task---\n        Risks_and_assumptions---\n        Documentation_of_user_needs    \n    end\n    subgraph Identification_List_for_Plan\n        direction TB\n        personnel---\n        facility_and_equipment_resources_for_each_task---\n        role_that_risk_hazard_management        \n    end\nQuality_Planning_Tasks-->Inclusion_List_for_Plan-->Identification_List_for_Plan\nend\n\n\n\n\n\n\n\n\n\n\nConfiguration Management\n\n\n\n\nflowchart LR\nsubgraph Configuration_Management\n    direction LR\n    subgraph Control\n        direction TB\n        control_multiple_parallel_development_activities---\n        ensure_positive_and_correct_correspondence_of---\n        specifications_documents---\n        source_code---\n        object_code---\n        test_suites---\n        ensure_accurate_identification_of_approved_versions---\n        ensure_access_to_approved_versions---\n        create_procedures_for_reporting---\n        create_procedures_for_resolving_SW_anomalies                            \n    end\n    subgraph Management\n        direction TB\n        identify_reports---\n        specify_contents---\n        specify_format---\n        specify_responsible_organizational_elements_for_each_report\n    end\n    subgraph Procedures\n        direction TB\n        necessary_for_review_of_SW_development_results---\n        necessary_for_approval_of_SW_development_results\n    end\n    ensure_proper_communications_and_documentation\n    Control-->Management-->Procedures-->ensure_proper_communications_and_documentation \nend\n\n\n\n\n\n\n\n\n\n\nTask Requirements\n\n\n\n\n\nflowchart TB\n    subgraph Task_Requirements\n        direction LR\n        subgraph group\n            direction TB\n            identification---\n            analysis---\n            predetermined_documentation_about_device_its_intended_use\n        end\n        \n        subgraph Requirements_Specification_List\n            direction TB\n            All_software_system_inputs---\n            All_software_system_outputs---\n            All_functions_that_software_system_will_perform---\n            All_performance_requirements_that_software_will_meet---\n            requirement_example_data_throughput_reliability_timing---\n            definition_of_all_external_and_user_interfaces---\n            any_internal_software_to_system_interfaces---\n            How_users_will_interact_with_system---\n            What_constitutes_error---\n            how_errors_should_be_handled---\n            Required_response_times---\n            Intended_operating_environment_for_software---\n            All_acceptable_ranges_limits_defaults_specific_values---\n            All_safety_related_requirements_that_will_be_implemented_in_SW---\n            All_safety_related_specifications_that_will_be_implemented_in_SW---\n            All_safety_related_features_that_will_be_implemented_in_SW---\n            All_safety_related_functions_that_will_be_implemented_in_SW---\n            clearly_identify_potential_hazards---\n            risk_evaluation_for_accuracy---\n            risk_evaluation_for_completeness---\n            risk_evaluation_for_consistency---\n            risk_evaluation_for_testability---\n            risk_evaluation_for_correctness---\n            risk_evaluation_for_clarity\n        end\n        subgraph Verfification_List_by_Evaluation\n            direction TB\n            no_internal_inconsistencies_among_requirements---\n            All_of_performance_requirements_for_system---\n            Complete_correct_Fault_tolerance_safety_security_requirements---\n            Accurate_Complete_Allocation_of_software_functions---\n            Appropriate_Software_requirements_for_system_hazards---\n            mesurable_requirements---\n            objectively_verifiable_requirements---\n            traceable_requirements\n        end\n        subgraph Requirements_Tasks\n            direction TB\n            Preliminary_Risk_Analysis---\n            Traceability_Analysis---\n            ex_Software_Requirements_to_System_Requirements_vice_versa---\n            ex_Software_Requirements_to_Risk_Analysis---\n            Description_of_User_Characteristics---\n            Listing_of_Characteristics_and_Limitations_of_Memory---\n            Software_Requirements_Evaluation---\n            Software_User_Interface_Requirements_Analysis---\n            System_Test_Plan_Generation---\n            Acceptance_Test_Plan_Generation---\n            Ambiguity_Review_or_Analysis\n        end\n    group-->Requirements_Specification_List \n    Verfification_List_by_Evaluation-->Requirements_Tasks\n    end\n\n\n\n\n\n\n\n\n\n\nDesign Overview\n\n\n\n\nflowchart TB\n    subgraph Deign_Task\n        direction LR\n    subgraph Design_Consideration_List\n        direction TB\n        subgraph Description\n                    direction TB\n                end\n        subgraph Human_Factors_Engineering\n          direction TB\n    \n        end\n        subgraph Safety_Usability_Issues_Conisderation\n            direction TB\n\n            end\n        Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n    subgraph Design_Specificiation\n        direction TB\n        subgraph Performing_List\n            direction TB\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n        end\n    Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design \n    end\n    subgraph Design_Activity_and_Task_List\n        direction TB\n        subgraph Final_Design_activity\n            direction TB\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n            end\n            subgraph Coding_Tasks\n                direction TB\n            end\n        traceability_analysis-->Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end\n    Design_Consideration_List---Design_Specificiation---Design_Activity_and_Task_List\n\n    end\n\n\n\n\n\n\n\n\n\nDesign Consideration\n\n\n\n\nflowchart TB\nsubgraph Design_Consideration_List\n    direction LR\n        subgraph Requirement_Specification\n            direction TB\n            logical_representation---\n            physical_representation\n        end\n    subgraph Description\n            direction TB\n            what_to_do---\n            how_to_do                   \n        end\n    subgraph Human_Factors_Engineering\n      direction TB\n            entire_design_and_development_process---\n            device_design_requirements---\n            analyses---\n            tests\n    end\n    subgraph Safety_Usability_Issues_Conisderation\n        direction TB\n                flowcharts--- \n                state_diagrams--- \n                prototyping_tools---\n                test_plans\n        end\n        Requirement_Specification---Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n\n\n\n\n\n\n\n\n\n\nDesign Specification\n\n\n\n\nflowchart TB\nsubgraph Design_Specificiation\n        direction LR\n        subgraph Conceptual_Specification\n            direction TB\n            requirements_specification---\n            predetermined_criteria---\n            Software_risk_analysis---\n            Development_procedures---\n            coding_guidelines\n        end\n        subgraph Performing_List\n            direction TB\n            task---\n            function_analyses---\n            risk_analyses---\n            prototype_tests_and_reviews---\n            full_usability_tests\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n            SW_requirements_specification---\n            predetermined_criteria_for_SW_acceptance---\n            SW_risk_analysis---\n            Development_procedure_list---\n            coding_guidance---\n            Systems_documentation---\n            Hardware_to_be_used---\n            Parameters_to_be_measured---\n            Logical_structure---\n            Control_logic---\n            logical_processing_steps_aka_algorithms---\n            Data_structures_diagram---\n            data_flow_diagrams---\n            Definitions_of_variables---\n            description_of_where_they_are_used---\n            Error_alarm_and_warning_messages---\n            Supporting_software---\n            internal_modules_Communication_links---\n            supporting_sw_links---\n            link_with_hardware---\n            link_with_user---\n            physical_Security_measures---\n            logical_security_measures\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n            complete--- \n            correct---\n            consistent--- \n            unambiguous--- \n            feasible---\n            maintainable---\n            analyses_of_control_flow---\n            data_flow--- \n            complexity--- \n            timing--- \n            sizing--- \n            memory_allocation---\n            module_architecture---\n            traceability_analysis_of_modules--- \n            criticality_analysis\n        end\n    Conceptual_Specification---Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design  \n    end\n\n\n\n\n\n\n\n\n\n\nDesign Activity and Task\n\n\n\n\n\nflowchart TB\nsubgraph Design_Activity_and_Task_List\n        direction LR\n        subgraph Final_Design_activity\n            direction TB\n            Formal_Design_Review_Before_Design_Implementation---\n            correct_consistent_complete_accurate_testable\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n            Updated_Software_Risk_Analysis---\n            Traceability_Analysis---\n            Software_Design_Evaluation---\n            Design_Communication_Link_Analysis---\n            Module_Test_Plan_Generation---\n            Integration_Test_Plan_Generation---\n            module_Test_Design_Generation---\n            integration_Test_Design_Generation---\n            system_Test_Design_Generation---\n            acceptance_Test_Design_Generation   \n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n                each_element_implementation---\n                each_module_implementation_to_element_and_risk_analysis---\n                each_functions_implemented_to_element_and_risk_analysis---\n                Tests_for_modules_to_element_and_risk_analysis--- \n                Tests_for_functions_to_element_and_risk_analysis---\n                Tests_for_modules_to_source_code---\n                Tests_for_functions_to_source_code\n            end\n            subgraph Coding_Tasks\n                direction TB\n                Traceability_Analyses---\n                Source_Code_to_Design_Specification_and_vice_versa---\n                Test_Cases_to_Source_Code_and_to_Design_Specification---\n                Source_Code_and_Source_Code_Documentation_Evaluation---\n                Source_Code_Interface_Analysis---\n                Test_Procedure_and_Test_Case_Generation \n            end\n        traceability_analysis-->Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#testing_task",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#testing_task",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Testing_Task",
    "text": "Testing_Task\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction TB\n            subgraph Test_Plans\n                direction TB\n            end\n            subgraph Conditions\n                direction TB\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n        subgraph Code_Based_Testing\n            direction TB\n            subgraph white_box_testing\n                direction TB\n            end\n            subgraph Evaluation_of_level_of_white_box_testing\n                direction TB\n            end\n            subgraph Coverage_Metrics_of_White_Box_Testing\n                direction TB\n            end\n        white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n        end\n        subgraph Alternatives_to_White_Box_Testing\n            direction TB\n            subgraph Types_of_Functional_Software_Testing_Increasing_Cost\n                direction TB\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n            end\n            subgraph Change_in_SW\n                direction TB    \n            end\n        Types_of_Functional_Software_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW\n        end\n        \n\n        subgraph Development_Testing\n            direction TB\n            subgraph unit_level_testing\n                direction TB    \n            end\n            subgraph integration_level_testing\n                direction TB\n            end\n            subgraph system_level_testing\n                direction TB\n            end\n            subgraph Error_Detected\n                direction TB        \n            end\n        unit_level_testing-->integration_level_testing-->system_level_testing-->Error_Detected\n        end\n\n        subgraph Testing_Tasks\n            direction TB\n        end\n        subgraph User_Site_Testing\n            direction TB\n            subgraph Quality_System_Rregulation\n                direction TB\n            end\n            subgraph Understand_Terminology\n                direction TB\n            end\n            subgraph Testing\n                direction TB\n            end\n            Quality_System_Rregulation---Understand_Terminology---Testing\n        end\nConsideration_Before_Testing_Tasks---Code_Based_Testing---Alternatives_to_White_Box_Testing\nDevelopment_Testing---Testing_Tasks---User_Site_Testing\n    end\n\n\n\n\n\n\n\n\n\n\nConsideration_Before_Testing_Tasks\n\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction LR\n            subgraph Test_Plans\n                direction TB\n                should_identify_control_measures_like_traceability_analysis---\n                ensure_that_intended_coverage_is_achieved---\n                ensure_that_proper_documentation_is_prepared---\n                conduct_tests_not_by_SW_developers_but_in_other_sites\n            end\n            subgraph Conditions\n                direction TB\n                use_defined_inputs---\n                documented_outcomes---\n                gonnabe_time_consuming_activity---\n                gonnabe_difficult_activity---\n                gonnabe_imperfect_activity---\n                testing_all_program_functionality---\n                does_not_mean_100_prcnt_correction_perfection---\n                make_detailed_objective_evaluation---\n                requires_sophisticated_definition_specificiation---\n                all_test_procedures_data_results_are_documented---\n                all_test_procedures_data_results_are_suitable_for_review---\n                all_test_procedures_data_results_are_suitable_for_objective_decision_making---\n                all_test_procedures_data_results_are_suitable_for_subsequent_regression_testing\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n                make_test_plans---\n                make_test_cases---\n                plan_schedules---\n                plan_environments---\n                plan_resources_of_personnel_tools---\n                plan_methodologies---\n                plan_inputs_procedures_outputs_expected_results---\n                plan_documentation---\n                plan_reporting_criteria\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n                expected_test_outcome_is_predefined---\n                good_test_case_has_high_probability_of_exposing_errors---\n                successful_test_is_one_that_finds_errors---\n                There_is_independence_from_coding---\n                Both_application_for_user_and_SW_for_programming_expertise_are_employed---\n                Testers_use_different_tools_from_coders---\n                Examining_only_the_usual_case_is_insufficient---\n                Test_documentation_permits_its_reuse---\n                Test_documentation_permits_independent_confirmation_---\n                of_pass/fail_test_outcome_during_subsequent_review\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n\nend\n\n\n\n\n\n\n\n\n\n\nCode_Based_Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n            subgraph Code_Based_Testing\n                direction LR\n                subgraph white_box_testing\n                    direction TB\n                    identify_dead_code_never_executed---\n                    conduct_unit_test---\n                    conduct_other_level_tests\n                end\n                subgraph Evaluation_of_level_of_white_box_testing\n                    direction TB\n                    use_coverage_metrics---\n                    metrics_of_completeness_of_test_selection_criteria---\n                    coverage_should_be_commensurate_with_level_of_SW_risk---\n                    coverage_means_100_prcnt_coverage\n                end\n                subgraph Coverage_Metrics_of_White_Box_Testing\n                    direction TB\n                    Statement_Coverage---\n                    Decision_or_Branch_Coverage---\n                    Condition_Coverage---\n                    Multi_Condition_Coverage\n                    Loop_Coverage---\n                    Path_Coverage---\n                    Data_Flow_Coverage\n                end\n            white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n            end\nend"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#testing_task-1",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#testing_task-1",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Testing_Task",
    "text": "Testing_Task\n\nSolution_to_White_Box_Testing\n\n\n\n\n\n \nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Alternatives_to_White_Box_Testing\n            direction LR\n            subgraph Types_of_Testing_Increasing_Cost\n                direction TB\n                    Normal_Case---\n                    Output_Forcing---\n                    Robustness---\n                    Combinations_of_Inputs\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n                difficulty_in_linking_---\n                tests_completion_criteria_to_SW_reliability\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n                statistical_testing---\n                provide_further_assurance_of_reliability---\n                generate_randomly_test_data_from_defined_distributions---\n                distribution_defined_by_expected_use---\n                distribution_defined_by_hazardous_use---\n                distribution_defined_by_malicious_use---\n                large_test_data_cover_particular_areas_or_concerns---\n                statistical_testing_provides_high_structural_coverage---\n                statistical_testing_requires_stable_system---\n                structural_and_functional_testing_are_prerequisites_for_statistical_testing\n            end\n            subgraph Change_in_SW\n                direction TB\n                conduct_regression_analysis_and_testing---\n                should_demonstrate_correct_implementation---\n                should_demonstrate_no_adverse_impact_on_other_modules   \n            end\n            subgraph Testing_Tasks\n                direction TB\n                Test_Planning---\n                Structural_Test_Case_Identification---\n                Functional_Test_Case_Identification---\n                Traceability_Analysis_Testing---\n                Unit_Tests_to_Detailed_Design---\n                Integration_Tests_to_High_Level_Design---\n                System_Tests_to_Software_Requirements---\n                Unit_Test_Execution---\n                Integration_Test_Execution---\n                Functional_Test_Execution---\n                System_Test_Execution---\n                Acceptance_Test_Execution---\n                Test_Results_Evaluation---\n                Error_Evaluation_Resolution---\n                Final_Test_Report\n            end\n        Types_of_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW---Testing_Tasks\n        end\nend\n\n\n\n\n\n\n\n\n\n\nDevelopment_Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Development_Testing\n            direction LR\n            subgraph unit_level_testing\n                direction TB    \n                focus_on_early_examination_of_sub_program_functionality---\n                ensure_functionality_invisible_at_system_level_examined---\n                ensure_quality_software_units_furnished_for_integration\n            end\n            subgraph integration_level_testing\n                direction TB\n                focuses_on_transfer_of_data---\n                focuses_on_control_across_program's_internal_and_external_interfaces\n            end\n            subgraph system_level_testing\n                direction TB\n                demonstrate_all_specified_functionality_exists---\n                demonstrate_SW_is_trustworthy---\n                verifies_as_built_program's_functionality_and_performance_on_requirements---\n                addresses_functional_concerns_and_intended_uses---\n                like_Performance_issues---\n                like_Responses_to_stress_conditions---\n                like_Operation_of_internal_and_external_security_features---\n                like_Effectiveness_of_recovery_procedures---\n                like_disaster_recovery---\n                like_Usability---\n                like_Compatibility_with_other_SW---\n                like_Behavior_in_each_of_the_defined_hardware_configurations---\n                like_Accuracy_of_documentation\n            end\n            subgraph Error_Detected\n                direction TB        \n                should_be_logged---\n                should_be_classified---\n                should_be_reviewed---\n                should_be_resolved_before_SW_release\n            end\n        unit_level_testing-->integration_level_testing-->system_level_testing-->Error_Detected\n        end\nend"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#user_site_testing",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#user_site_testing",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "User_Site_Testing",
    "text": "User_Site_Testing\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph User_Site_Testing\n            direction LR\n            subgraph Quality_System_Rregulation\n                direction TB\n                installation---\n                inspection_procedures---\n                testing_appropriateness---\n                documentation_of_inspection---\n                testing_to_demonstrate_proper_installation\n            end\n            subgraph Understand_Terminology\n                direction TB\n                beta_test---\n                site_validation---\n                user_acceptance_test---\n                installation_verification---\n                installation_testing\n            end\n            subgraph Testing\n                direction TB\n                subgraph Requirements\n                    direction TB\n                    either_actual_or_simulated_use---\n                    verification_of_intended_functionality---\n                    constant_contact_FDA_center\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n    \n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_System_Ability\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_User_Ability\n                        direction TB\n        \n                    end \n                    subgraph Evaluation_of_Operator_Ability\n                        direction TB\n        \n                    end\n                constant_contact_FDA_center-->Follow_Predefiened_Plan-->Documented_Evidence-->\n            Evaluation_of_System_Ability-->Evaluation_of_User_Ability-->Evaluation_of_Operator_Ability    \n                end \n                        \n            \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements-->User_Site_Testing_Task\n        end\n        Quality_System_Rregulation-->    Understand_Terminology-->Testing-->User_Site_Testing_Task\n        end\nend\n\n\n\n\n\n\n\n\n\n\nTesting\n\n\n\n\nflowchart TB\n            subgraph Testing\n                direction LR\n                subgraph Requirements\n                    direction LR\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n                        either_actual_or_simulated_use---\n                        verification_of_intended_functionality---\n                        constant_contact_FDA_center---\n                        formal_summary_of_testing---\n                        record_of_formal_acceptance\n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n                        testing_plan_of_full_range_of_operating_conditions---\n                        testing_plan_to_detect_any_latent_faults---\n                        all_testing_procedures---\n                        test_input_data---\n                        test_results---\n                        hardware_installation_and_configuration---\n                        software_installation_and_configuration---\n                        exercising_measure_of_all_system_components---\n                        versions_of_all_system_components           \n                    end\n                    subgraph Evaluation\n                        direction TB\n                      subgraph Evaluation_of_System_Ability\n                            direction TB\n                            high_volume_of_data---\n                            heavy_loads_or_stresses---\n                            security\n                            subgraph fault_testing\n                                direction TB\n                                avoidance---\n                                detection---\n                                tolerance---\n                                recovery\n                            end\n                        security---fault_testing---\n                        error_message---\n                        implementation_of_safety_requirements\n                        end\n                      subgraph Evaluation_of_User_Ability\n                            direction TB\n                            ability_to_understand_system---\n                            ability_to_interface_with_system\n                        end \n                        subgraph Evaluation_of_Operator_Ability\n                            direction TB\n                            ability_to_perform_intended_functions---\n                            ability_to_respond_in_alarms---\n                            ability_to_respond_in_warnings---\n                            ability_to_respond_in_error_messages\n                        end\n\n                    end\n            Follow_Predefiened_Plan-->Documented_Evidence-->\n            Evaluation_of_System_Ability-->Evaluation_of_User_Ability-->Evaluation_of_Operator_Ability    \n            end     \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements-->User_Site_Testing_Task\n        end"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#maintenance-and-software-changes",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#maintenance-and-software-changes",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Maintenance and Software Changes",
    "text": "Maintenance and Software Changes\n\n\n\n\nflowchart LR\n    subgraph Hardware_VS_Software\n        direction LR\n        subgraph HW_maintenance_Inclusion\n            direction TB\n            preventive_hardware_maintenance_actions--- \n            component_replacement---\n            corrective_changes\n        end\n        subgraph SW_maintenance_Inclusion\n            direction TB\n            corrective---\n            perfective---\n            adaptive_maintenance---\n            not_include_preventive_maintenance_actions---\n            not_include_software_component_replacement\n        end\n    end\n    subgraph Maintenance_Type\n        direction TB\n        Corrective_maintenance---\n        Perfective_maintenance---\n        Adaptive_maintenance---\n        Sufficient_regression_analysis---\n        Sufficient_regression_testing\n    end\n    subgraph Factors_of_Validation_for_SW_change\n        direction TB\n        type_of_change---\n        development_products_affected---\n        impact_of_those_products_on_operation\n    end\n    subgraph Factors_of_Limitting_Validation_Effort\n        direction TB\n        documentation_of_design_structure---\n        documentation_of_interrelationships_of_modules---\n        documentation_of_interrelationships_of_interfaces---\n        test_documentation---\n    test_cases---\n        results_of_previous_verification_and_validation_testing\n    end\n    subgraph Maintenance_tasks\n        direction TB\n        Software_Validation_Plan_Revision---\n        Anomaly_Evaluation---\n        Problem_Identification_and_Resolution_Tracking---\n        Proposed_Change_Assessment---\n        Task_Iteration---\n        Documentation_Updating\n    end\nHardware_VS_Software-->Maintenance_Type-->Factors_of_Validation_for_SW_change-->\nFactors_of_Limitting_Validation_Effort-->Maintenance_tasks"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation-of-quality-system-software",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation-of-quality-system-software",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation of Quality System Software",
    "text": "Validation of Quality System Software\n\nOverview\n\n\n\n\nflowchart LR\n    subgraph Use_of_Computers_and_automated_equipment\n        direction TB\n        medical_device_design---\n        laboratory_testing_and_analysis---\n        product_inspection_and_acceptance---\n        production_and_process_control---\n        environmental_controls---\n        packaging---\n        labeling---\n        traceability---\n        document_control---\n        complaint_management---\n        programmable_logic_controllers---\n        digital_function_controllers---\n        statistical_process_control---\n        supervisory_control_and_data_acquisition---\n        robotics---\n        human_machine_interfaces---\n        input_output_devices---\n        computer_operating_systems\n    end\n    subgraph Factors_in_Validation\n        direction TB\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System-->Validation_Supporting_Factors-->Factors_of_Validation_Evidence_Level-->\nFactors_of_Easing_Validation_Effort\n    end\n    subgraph Documented_User_Requirements\n        direction TB\n        intended_use_of_software_or_automated_equipment---\n      level_of_dependency_on_software_or_equipment\n    end\n    subgraph List_That_Must_Be_Defined_by_User\n        direction TB\n        \n    end\n    subgraph Documentation_List\n        direction TB\n        documented_protocol---\n        documented_validation_results\n        subgraph Documented_Test_Cases\n            direction TB\n        \n        end\n        documented_validation_results---Documented_Test_Cases\n    end\n\n    subgraph Manufaturer's_Responsbility\n        direction TB\n        \n    end\nUse_of_Computers_and_automated_equipment---Factors_in_Validation---Documented_User_Requirements---\nList_That_Must_Be_Defined_by_User---Documentation_List---Manufaturer's_Responsbility\n\n\n\n\n\n\n\n\n\n\nFactors in Validation\n\n\n\n\nflowchart LR\n    subgraph Factors_in_Validation\n        direction LR\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n                electronic_records_regulation---\n                electronic_signatures_regulation---\n                regulations_establishment---\n                security---\n                data_integrity---\n                validation_requirements \n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n            verifications_of_outputs_from_each_stage--- \n            verifications_of_outputs_throught_SW_life_cycle---\n            checking_for_proper_operation_in_intended_use_environment\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n            risk_posed_by_automated_operation---\n            complexity_of_process_software---\n            degree_of_dependence_on_automated_process\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System-->Validation_Supporting_Factors-->Factors_of_Validation_Evidence_Level-->\nFactors_of_Easing_Validation_Effort\n    end"
  },
  {
    "objectID": "docs/projects/high_dimension.html",
    "href": "docs/projects/high_dimension.html",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "",
    "text": "Alzheimer Disease (AD) is a most common form of dementia that affects millions of Americans. AD affects memory, thinking and behavior, but its progression is slow, spanning nearly two decades before the symptoms appear. Thus, it is imperative to understand the physiology at the pre-clinical stage. It is estimated that genetic factors contribute nearly 50% to AD. To better understand how genes contribute to the risk of AD by altering cellular milieu, we have examined the metabolome of high risk individuals, the APOE4 carriers. The metabolome represents the products that were generated from the genome and proteome. These biochemical products represent influences of both genetic and environmental factors. In this article, I prepared simulated data to reproduce and demonstrate a rough analysis pipeline due to security concerns.\n\n\n\nThe aim of this simulation study is to identify a set of metabolites that will enable to differentiate bio-markers that are associated with AD vs. non-AD."
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#notice",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#notice",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Notice",
    "text": "Notice\n\nI am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto. (it seems that Quarto system has some issues on mermaid diagrams.)\nThe FDA validation guidance document is a bit difficult to understand because its explanations provide abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\nLast Update\n\n2022-12-28, Summary of Document"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#introduction",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#introduction",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Introduction",
    "text": "Introduction\n\nDefinition of Software Validation\nSoftware Validation is a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997.\n(See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\n\n\nSome Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology: requirements, specification, verification, and validation.\n\n\n\nRationale\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\nObjective of SW validation is to ensure\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\nWhat to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\nMain Institutions\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#testing-task",
    "href": "docs/blog/posts/2022-12-10-FDA/FDA_SW_validation_diagram.html#testing-task",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Testing Task",
    "text": "Testing Task\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction TB\n            subgraph Test_Plans\n                direction TB\n            end\n            subgraph Conditions\n                direction TB\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n        subgraph Code_Based_Testing\n            direction TB\n            subgraph white_box_testing\n                direction TB\n            end\n            subgraph Evaluation_of_level_of_white_box_testing\n                direction TB\n            end\n            subgraph Coverage_Metrics_of_White_Box_Testing\n                direction TB\n            end\n        white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n        end\n        subgraph Alternatives_to_White_Box_Testing\n            direction TB\n            subgraph Types_of_Functional_Software_Testing_Increasing_Cost\n                direction TB\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n            end\n            subgraph Change_in_SW\n                direction TB    \n            end\n        Types_of_Functional_Software_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW\n        end\n        \n\n        subgraph Development_Testing\n            direction TB\n            subgraph unit_level_testing\n                direction TB    \n            end\n            subgraph integration_level_testing\n                direction TB\n            end\n            subgraph system_level_testing\n                direction TB\n            end\n            subgraph Error_Detected\n                direction TB        \n            end\n        unit_level_testing-->integration_level_testing-->system_level_testing-->Error_Detected\n        end\n\n        subgraph Testing_Tasks\n            direction TB\n        end\n        subgraph User_Site_Testing\n            direction TB\n            subgraph Quality_System_Rregulation\n                direction TB\n            end\n            subgraph Understand_Terminology\n                direction TB\n            end\n            subgraph Testing\n                direction TB\n            end\n            Quality_System_Rregulation---Understand_Terminology---Testing\n        end\nConsideration_Before_Testing_Tasks---Code_Based_Testing---Alternatives_to_White_Box_Testing\nDevelopment_Testing---Testing_Tasks---User_Site_Testing\n    end\n\n\n\n\n\n\n\n\n\n\nConsideration Before Testing Tasks\n\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction LR\n            subgraph Test_Plans\n                direction TB\n                should_identify_control_measures_like_traceability_analysis---\n                ensure_that_intended_coverage_is_achieved---\n                ensure_that_proper_documentation_is_prepared---\n                conduct_tests_not_by_SW_developers_but_in_other_sites\n            end\n            subgraph Conditions\n                direction TB\n                use_defined_inputs---\n                documented_outcomes---\n                gonnabe_time_consuming_activity---\n                gonnabe_difficult_activity---\n                gonnabe_imperfect_activity---\n                testing_all_program_functionality---\n                does_not_mean_100_prcnt_correction_perfection---\n                make_detailed_objective_evaluation---\n                requires_sophisticated_definition_specificiation---\n                all_test_procedures_data_results_are_documented---\n                all_test_procedures_data_results_are_suitable_for_review---\n                all_test_procedures_data_results_are_suitable_for_objective_decision_making---\n                all_test_procedures_data_results_are_suitable_for_subsequent_regression_testing\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n                make_test_plans---\n                make_test_cases---\n                plan_schedules---\n                plan_environments---\n                plan_resources_of_personnel_tools---\n                plan_methodologies---\n                plan_inputs_procedures_outputs_expected_results---\n                plan_documentation---\n                plan_reporting_criteria\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n                expected_test_outcome_is_predefined---\n                good_test_case_has_high_probability_of_exposing_errors---\n                successful_test_is_one_that_finds_errors---\n                There_is_independence_from_coding---\n                Both_application_for_user_and_SW_for_programming_expertise_are_employed---\n                Testers_use_different_tools_from_coders---\n                Examining_only_the_usual_case_is_insufficient---\n                Test_documentation_permits_its_reuse---\n                Test_documentation_permits_independent_confirmation_---\n                of_pass/fail_test_outcome_during_subsequent_review\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n\nend\n\n\n\n\n\n\n\n\n\n\nCode Based Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n            subgraph Code_Based_Testing\n                direction LR\n                subgraph white_box_testing\n                    direction TB\n                    identify_dead_code_never_executed---\n                    conduct_unit_test---\n                    conduct_other_level_tests\n                end\n                subgraph Evaluation_of_level_of_white_box_testing\n                    direction TB\n                    use_coverage_metrics---\n                    metrics_of_completeness_of_test_selection_criteria---\n                    coverage_should_be_commensurate_with_level_of_SW_risk---\n                    coverage_means_100_prcnt_coverage\n                end\n                subgraph Coverage_Metrics_of_White_Box_Testing\n                    direction TB\n                    Statement_Coverage---\n                    Decision_or_Branch_Coverage---\n                    Condition_Coverage---\n                    Multi_Condition_Coverage\n                    Loop_Coverage---\n                    Path_Coverage---\n                    Data_Flow_Coverage\n                end\n            white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n            end\nend\n\n\n\n\n\n\n\n\n\n\nSolution to White Box Testing\n\n\n\n\n\n \nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Alternatives_to_White_Box_Testing\n            direction LR\n            subgraph Types_of_Testing_Increasing_Cost\n                direction TB\n                    Normal_Case---\n                    Output_Forcing---\n                    Robustness---\n                    Combinations_of_Inputs\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n                difficulty_in_linking_---\n                tests_completion_criteria_to_SW_reliability\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n                statistical_testing---\n                provide_further_assurance_of_reliability---\n                generate_randomly_test_data_from_defined_distributions---\n                distribution_defined_by_expected_use---\n                distribution_defined_by_hazardous_use---\n                distribution_defined_by_malicious_use---\n                large_test_data_cover_particular_areas_or_concerns---\n                statistical_testing_provides_high_structural_coverage---\n                statistical_testing_requires_stable_system---\n                structural_and_functional_testing_are_prerequisites_for_statistical_testing\n            end\n            subgraph Change_in_SW\n                direction TB\n                conduct_regression_analysis_and_testing---\n                should_demonstrate_correct_implementation---\n                should_demonstrate_no_adverse_impact_on_other_modules   \n            end\n            subgraph Testing_Tasks\n                direction TB\n                Test_Planning---\n                Structural_Test_Case_Identification---\n                Functional_Test_Case_Identification---\n                Traceability_Analysis_Testing---\n                Unit_Tests_to_Detailed_Design---\n                Integration_Tests_to_High_Level_Design---\n                System_Tests_to_Software_Requirements---\n                Unit_Test_Execution---\n                Integration_Test_Execution---\n                Functional_Test_Execution---\n                System_Test_Execution---\n                Acceptance_Test_Execution---\n                Test_Results_Evaluation---\n                Error_Evaluation_Resolution---\n                Final_Test_Report\n            end\n        Types_of_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW---Testing_Tasks\n        end\nend\n\n\n\n\n\n\n\n\n\n\nDevelopment Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Development_Testing\n            direction LR\n            subgraph unit_level_testing\n                direction TB    \n                focus_on_early_examination_of_sub_program_functionality---\n                ensure_functionality_invisible_at_system_level_examined---\n                ensure_quality_software_units_furnished_for_integration\n            end\n            subgraph integration_level_testing\n                direction TB\n                focuses_on_transfer_of_data---\n                focuses_on_control_across_program's_internal_and_external_interfaces\n            end\n            subgraph system_level_testing\n                direction TB\n                demonstrate_all_specified_functionality_exists---\n                demonstrate_SW_is_trustworthy---\n                verifies_as_built_program's_functionality_and_performance_on_requirements---\n                addresses_functional_concerns_and_intended_uses---\n                like_Performance_issues---\n                like_Responses_to_stress_conditions---\n                like_Operation_of_internal_and_external_security_features---\n                like_Effectiveness_of_recovery_procedures---\n                like_disaster_recovery---\n                like_Usability---\n                like_Compatibility_with_other_SW---\n                like_Behavior_in_each_of_the_defined_hardware_configurations---\n                like_Accuracy_of_documentation\n            end\n            subgraph Error_Detected\n                direction TB        \n                should_be_logged---\n                should_be_classified---\n                should_be_reviewed---\n                should_be_resolved_before_SW_release\n            end\n        unit_level_testing-->integration_level_testing-->system_level_testing-->Error_Detected\n        end\nend\n\n\n\n\n\n\n\n\n\n\nUser Site Testing\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph User_Site_Testing\n            direction LR\n            subgraph Quality_System_Rregulation\n                direction TB\n                installation---\n                inspection_procedures---\n                testing_appropriateness---\n                documentation_of_inspection---\n                testing_to_demonstrate_proper_installation\n            end\n            subgraph Understand_Terminology\n                direction TB\n                beta_test---\n                site_validation---\n                user_acceptance_test---\n                installation_verification---\n                installation_testing\n            end\n            subgraph Testing\n                direction TB\n                subgraph Requirements\n                    direction TB\n                    either_actual_or_simulated_use---\n                    verification_of_intended_functionality---\n                    constant_contact_FDA_center\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n    \n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_System_Ability\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_User_Ability\n                        direction TB\n        \n                    end \n                    subgraph Evaluation_of_Operator_Ability\n                        direction TB\n        \n                    end\n                constant_contact_FDA_center-->Follow_Predefiened_Plan-->Documented_Evidence-->\n            Evaluation_of_System_Ability-->Evaluation_of_User_Ability-->Evaluation_of_Operator_Ability    \n                end \n                        \n            \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements-->User_Site_Testing_Task\n        end\n        Quality_System_Rregulation-->    Understand_Terminology-->Testing-->User_Site_Testing_Task\n        end\nend\n\n\n\n\n\n\n\n\n\n\nTesting\n\n\n\n\nflowchart TB\n            subgraph Testing\n                direction LR\n                subgraph Requirements\n                    direction LR\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n                        either_actual_or_simulated_use---\n                        verification_of_intended_functionality---\n                        constant_contact_FDA_center---\n                        formal_summary_of_testing---\n                        record_of_formal_acceptance\n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n                        testing_plan_of_full_range_of_operating_conditions---\n                        testing_plan_to_detect_any_latent_faults---\n                        all_testing_procedures---\n                        test_input_data---\n                        test_results---\n                        hardware_installation_and_configuration---\n                        software_installation_and_configuration---\n                        exercising_measure_of_all_system_components---\n                        versions_of_all_system_components           \n                    end\n                    subgraph Evaluation\n                        direction TB\n                      subgraph Evaluation_of_System_Ability\n                            direction TB\n                            high_volume_of_data---\n                            heavy_loads_or_stresses---\n                            security\n                            subgraph fault_testing\n                                direction TB\n                                avoidance---\n                                detection---\n                                tolerance---\n                                recovery\n                            end\n                        security---fault_testing---\n                        error_message---\n                        implementation_of_safety_requirements\n                        end\n                      subgraph Evaluation_of_User_Ability\n                            direction TB\n                            ability_to_understand_system---\n                            ability_to_interface_with_system\n                        end \n                        subgraph Evaluation_of_Operator_Ability\n                            direction TB\n                            ability_to_perform_intended_functions---\n                            ability_to_respond_in_alarms---\n                            ability_to_respond_in_warnings---\n                            ability_to_respond_in_error_messages\n                        end\n\n                    end\n            Follow_Predefiened_Plan-->Documented_Evidence-->\n            Evaluation_of_System_Ability-->Evaluation_of_User_Ability-->Evaluation_of_Operator_Ability    \n            end     \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements-->User_Site_Testing_Task\n        end"
  },
  {
    "objectID": "docs/blog/posts/2023-01-02 statistics/index.html",
    "href": "docs/blog/posts/2023-01-02 statistics/index.html",
    "title": "Statistics Blog Guide Map",
    "section": "",
    "text": "(Draft)"
  },
  {
    "objectID": "docs/blog/posts/2023-01-02 statistics/index.html#basic-statistics",
    "href": "docs/blog/posts/2023-01-02 statistics/index.html#basic-statistics",
    "title": "Statistics Blog Guide Map",
    "section": "Basic Statistics",
    "text": "Basic Statistics\n\n2022-12-28, p-values"
  },
  {
    "objectID": "docs/blog/posts/2023-01-02_statistics/index.html",
    "href": "docs/blog/posts/2023-01-02_statistics/index.html",
    "title": "Statistics Blog Guide Map",
    "section": "",
    "text": "(Draft)"
  },
  {
    "objectID": "docs/blog/posts/2023-01-02_statistics/index.html#basic-statistics",
    "href": "docs/blog/posts/2023-01-02_statistics/index.html#basic-statistics",
    "title": "Statistics Blog Guide Map",
    "section": "Basic Statistics",
    "text": "Basic Statistics\n\n1111-11-11, Hypothesis Testing\n2022-12-28, p-values\n1111-11-11, Permutation Test\n1111-11-11, Power\n1111-11-11, Multiple Testing\n1111-11-11, Sample Size Calculation\n1111-11-11, A/B Testing\n2023-01-07, ANOVA\n\n1111-11-11, ANCOVA\n1111-11-11, repeated measures ANOVA\n1111-11-11, MANOVA\n1111-11-11, MANCOVA"
  },
  {
    "objectID": "docs/projects/high_dimension/high_dimension.html",
    "href": "docs/projects/high_dimension/high_dimension.html",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "",
    "text": "Alzheimer Disease (AD) is a most common form of dementia that affects millions of Americans. AD affects memory, thinking and behavior, but its progression is slow, spanning nearly two decades before the symptoms appear. Thus, it is imperative to understand the physiology at the pre-clinical stage. It is estimated that genetic factors contribute nearly 50% to AD. To better understand how genes contribute to the risk of AD by altering cellular milieu, we have examined the metabolome of high risk individuals, the APOE4 carriers. The metabolome represents the products that were generated from the genome and proteome. These biochemical products represent influences of both genetic and environmental factors. In this article, I prepared simulated data to reproduce and demonstrate a rough analysis pipeline due to security concerns.\n\n\n\nThe aim of this simulation study is to identify a set of metabolites that will enable to differentiate bio-markers that are associated with AD vs. non-AD."
  },
  {
    "objectID": "docs/projects/high_dimension/high_dimension.html#data-preparation",
    "href": "docs/projects/high_dimension/high_dimension.html#data-preparation",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nPackage Loading and Option Settings\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\n#library(mixOmics)\nset.seed(20221213) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\nData Simulation\n\n\n\n\n\nLoad Data\n\n\nShow the code\n#all_data=read_csv(\"C:/Users/kmkim/Desktop/my_project/website/docs/projects/data/llfs_simulated_data.rda\",\n#                  progress = show_progress(),\n#                  show_col_types = FALSE)%>%\n\n all_data=read_rds(\"C:/Users/kmkim/Desktop/my_project/website/docs/projects/data/llfs_simulated_data.rds\")%>%\n     dplyr::select(-1,-probabilities)\n all_data=all_data%>%\n     mutate(outcome=factor(outcome,levels=c(0,1)),\n            sex=factor(sex,levels=c(0,1)),\n            country=factor(country,levels=c(0,1)),\n            treatment=factor(treatment,levels=c(0,1)),\n            genotype=factor(genotype,levels=c(3,0,1,2,4,5))\n            )\n \n dim(all_data)\n\n\n[1]  1000 10006"
  },
  {
    "objectID": "docs/projects/high_dimension/high_dimension.html#data-description",
    "href": "docs/projects/high_dimension/high_dimension.html#data-description",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Data Description",
    "text": "Data Description\nThis data include\nMin.   :10.40  , 1st Qu.:15.43  , Median :19.20  , Mean   :20.09  , 3rd Qu.:22.80  , Max.   :33.90  , Min.   :4.000  , 1st Qu.:4.000  , Median :6.000  , Mean   :6.188  , 3rd Qu.:8.000  , Max.   :8.000  , Min.   : 71.1  , 1st Qu.:120.8  , Median :196.3  , Mean   :230.7  , 3rd Qu.:326.0  , Max.   :472.0  , Min.   : 52.0  , 1st Qu.: 96.5  , Median :123.0  , Mean   :146.7  , 3rd Qu.:180.0  , Max.   :335.0  , Min.   :2.760  , 1st Qu.:3.080  , Median :3.695  , Mean   :3.597  , 3rd Qu.:3.920  , Max.   :4.930  , Min.   :1.513  , 1st Qu.:2.581  , Median :3.325  , Mean   :3.217  , 3rd Qu.:3.610  , Max.   :5.424  , Min.   :14.50  , 1st Qu.:16.89  , Median :17.71  , Mean   :17.85  , 3rd Qu.:18.90  , Max.   :22.90  , Min.   :0.0000  , 1st Qu.:0.0000  , Median :0.0000  , Mean   :0.4375  , 3rd Qu.:1.0000  , Max.   :1.0000  , Min.   :0.0000  , 1st Qu.:0.0000  , Median :0.0000  , Mean   :0.4062  , 3rd Qu.:1.0000  , Max.   :1.0000  , Min.   :3.000  , 1st Qu.:3.000  , Median :4.000  , Mean   :3.688  , 3rd Qu.:4.000  , Max.   :5.000  , Min.   :1.000  , 1st Qu.:2.000  , Median :2.000  , Mean   :2.812  , 3rd Qu.:4.000  , Max.   :8.000  \nthis inline code works but it seems to rmd files look more organized and tidier."
  },
  {
    "objectID": "docs/projects/high_dimension/high_dimension.html#architecture-of-analysis-pipeline",
    "href": "docs/projects/high_dimension/high_dimension.html#architecture-of-analysis-pipeline",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Architecture of Analysis Pipeline",
    "text": "Architecture of Analysis Pipeline"
  },
  {
    "objectID": "docs/projects/high_dimension/high_dimension.html#methods",
    "href": "docs/projects/high_dimension/high_dimension.html#methods",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Methods",
    "text": "Methods\n\n\n   alpha      mse fit.name\n1    0.0 2529.146   alpha0\n2    0.1 2529.146 alpha0.1\n3    0.2 2529.146 alpha0.2\n4    0.3 2529.146 alpha0.3\n5    0.4 2529.146 alpha0.4\n6    0.5 2529.146 alpha0.5\n7    0.6 2529.146 alpha0.6\n8    0.7 2529.146 alpha0.7\n9    0.8 2529.146 alpha0.8\n10   0.9 2529.146 alpha0.9\n11   1.0 2529.146   alpha1"
  },
  {
    "objectID": "docs/projects/high_dimension/high_dimension.html#results",
    "href": "docs/projects/high_dimension/high_dimension.html#results",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "docs/projects/high_dimension/high_dimension.html#conclusion",
    "href": "docs/projects/high_dimension/high_dimension.html#conclusion",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "docs/projects/high_dimension/high_dimension.html#bibliography",
    "href": "docs/projects/high_dimension/high_dimension.html#bibliography",
    "title": "Comparative Study of Dimension Reduction Methods",
    "section": "Bibliography",
    "text": "Bibliography"
  },
  {
    "objectID": "docs/projects/high_dimension/data_preparation.html",
    "href": "docs/projects/high_dimension/data_preparation.html",
    "title": "Data Preparation",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\nplease, read the English section first.\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(mixOmics)\nset.seed(20221213) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nShow the code\n# the number of samples\nsample_size <- 1000\n# the number of predictors\npredictor_size <- 10000\ngroup_size <- 10\n# the number of predictors truly associated with a response variable\nsignificant_predictors <- floor(predictor_size*sample((50:200)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors<-floor(significant_predictors*0.4) \nnegatively_associated_predictors<-significant_predictors-positively_associated_predictors \n\n## set correlated predictors within each group\n### randomly sampling proportions of 10 correlated predictor groups \n### to become their sum equal to 1\nproportion_list<-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%>%round(3) \nnames(proportion_list)<-paste0(\"group\",1:length(proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix <- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\nmeta_data<-\n    data.frame(group_name=c(names(proportion_list)),\n               proportion=proportion_list)%>%\n    mutate(group_n=(predictor_size*proportion_list)%>%round(0), # the within-group number of predictors\n           first_index=c(1,cumsum(group_n[-length(proportion_list)])+1), # the 1st index of predictors in each group\n           last_index=cumsum(group_n), # the last index of predictors in each group\n           group_correlation=sample((0:700)/1000,length(proportion_list),replace=TRUE), # correlation among the within-group predictors\n           group_effect=sample((-2:2)/10,length(proportion_list),replace=TRUE)); # effect of each group on an outcome variable\n\n\ndata<-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.01), \n             nrow = sample_size, ncol = predictor_size)\ncovariance_matrix<-matrix(rnorm(predictor_size*predictor_size,0.15,0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients <- rnorm(predictor_size,0,0.05)\n\nfor (i in 1:nrow(meta_data)) {\n    \n    group_range <- meta_data[i, \"first_index\"]:meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] <- meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) <- 1\n    data[, group_range] <- \n        mvrnorm(n = sample_size, \n                mu = rep(0,meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[meta_data[i, \"first_index\"]:meta_data[i, \"last_index\"]] <-\n        beta_coefficients[meta_data[i, \"first_index\"]:meta_data[i, \"last_index\"]]+\n        meta_data[i,\"group_effect\"]\n    predictor_names<-paste0(meta_data[i,\"group_name\"],\"_\",1:meta_data[i,\"group_n\"])\n    names(beta_coefficients)[meta_data[i, \"first_index\"]:meta_data[i, \"last_index\"]] <- predictor_names\n    names(data)[meta_data[i, \"first_index\"]:meta_data[i, \"last_index\"]]<-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n# logistic function to get a probability, intercept = 0, \n# to decrease prevalence, set p-0.2, negative probabilities into 0\nprobabilities <- ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%>%\n    ifelse(.>1,1,.)%>%abs()\nresponse <- rbinom(sample_size, 1, probabilities) \n\nage_distribution=rchisq(sample_size,df=9)\nsex_distribution=sample(c(0,1),sample_size,replace=TRUE,prob = c(0.45,0.55))\ncountry_distribution=sample(c(0:3),sample_size,replace=TRUE,prob = c(0.3,0.2,0.2,0.3))\ntreatment_distribution=sample(c(0:2),sample_size,replace=TRUE,prob = c(0.7,0.2,0.1))\ngenotype_distribution=sample(c(0:5),sample_size,replace=TRUE,\n                             prob = c(0.08,0.15,0.086,0.3,0.25,0.134)) #just random numbers\nphenotype_data<-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        probabilities=probabilities,\n        age=ifelse(probabilities<0.15,age_distribution-4,\n                   ifelse(probabilities<0.3,age_distribution-2,\n                          ifelse(probabilities>0.5,age_distribution+3,\n                                 ifelse(probabilities>0.7,age_distribution+6,age_distribution)))))%>%\n    mutate(age=sapply(age,\n                   function(x)(x-min(age))/(max(age)-min(age))*(105-65)+65)%>%round(0),\n           sex=sex_distribution,\n           country=country_distribution,\n           treatment=treatment_distribution,\n           treatment=ifelse(probabilities>0.7,1,\n                            ifelse(probabilities>0.8,2,treatment)),\n           genotype=genotype_distribution,\n           genotype=ifelse(probabilities<0.1,0,\n                           ifelse(probabilities<0.15,1,\n                                  ifelse(probabilities>0.7,3,\n                                         ifelse(probabilities>0.8,4,\n                                                ifelse(probabilities>0.9,5,genotype))))),\n           age=ifelse(outcome==0&genotype==0,age+10,\n                      ifelse(outcome==0&genotype==1,age+5,\n                             ifelse(genotype==4,age-3,\n                                    ifelse(genotype==5,age-2,age)))),\n           genotype=ifelse(outcome==1&genotype==3&age>83&age<92,4,genotype),\n           genotype=ifelse(outcome==1&genotype==4&age>87,5,genotype))\n\n\nall_data=inner_join(phenotype_data,data%>%mutate(id=1:n()),by=\"id\")\n\n#write_rds(all_data,\"./docs/projects/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nShow the code\ndatapath<-\"C:/Users/kmkim/Desktop/my_project/website/docs/projects/data/llfs_simulated_data.rds\"\n#datapath<-\"C:/Users/kkm/Desktop/projects/R/website/docs/projects/data/llfs_simulated_data.rds\"\n simulated_data=read_rds(datapath)%>%\n     dplyr::select(-1,-probabilities)\n all_data=simulated_data%>%\n mutate(\n      outcome=ifelse(outcome==0,\"negative\",\"positive\"),\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      country=ifelse(country==0,\"center_a\",ifelse(country==1,\"center_b\",ifelse(country==2,\"center_c\",\"center_d\"))),\n      country=factor(country,levels=c(\"center_a\",\"center_b\",\"center_c\",\"center_d\")),\n      treatment=ifelse(treatment==0,\"trmnt1\",ifelse(treatment==1,\"trmnt2\",\"trmnt3\")),\n      treatment=factor(treatment,levels=c(\"trmnt1\",\"trmnt2\",\"trmnt3\")),\n      genotype=ifelse(genotype==0,\"e2/e2\",\n      ifelse(genotype==1,\"e2/e3\",\n      ifelse(genotype==2,\"e2/e4\",\n      ifelse(genotype==3,\"e3/e3\",\n      ifelse(genotype==4,\"e3/e4\",\"e4/e4\"))))),\n      genotype=factor(genotype,levels=c(\"e2/e2\",\"e2/e3\",\"e2/e4\",\"e3/e3\",\"e3/e4\",\"e4/e4\"))\n      )\n names(all_data)[7:ncol(all_data)]<-paste0(\"meta\",1:10000)\n\n\n\n\n\n\n\n\n\nThis data include 1000 samples and 10006 variables:\n\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is negative.\nage: an age\nsex: a sex status (woman, man) and the reference group is man.\ncountry: a country where data were collected (center_d, center_b, center_a, center_c) and the reference group is center_a.\ntreatment: a treatment for the disease (trmnt2, trmnt1, trmnt3) and the reference group is trmnt1.\ngenotype: a genotype of APOE.\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3 (reference group)\ne3/e4\ne4/e4\n\n\nmeta1 ~ meta10000: a list of metabolites that were blood-sampled from the APOE carriers."
  },
  {
    "objectID": "docs/projects/high_dimension/data_preparation.html#data-preparation",
    "href": "docs/projects/high_dimension/data_preparation.html#data-preparation",
    "title": "Data Preparation",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nPackage Loading and Option Settings\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\n#library(mixOmics)\nset.seed(20221213) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\nData Simulation\n\n\n\n\n\nLoad Data\n\n\nShow the code\n#all_data=read_csv(\"C:/Users/kmkim/Desktop/my_project/website/docs/projects/data/llfs_simulated_data.rda\",\n#                  progress = show_progress(),\n#                  show_col_types = FALSE)%>%\n\n all_data=read_rds(\"C:/Users/kmkim/Desktop/my_project/website/docs/projects/data/llfs_simulated_data.rds\")%>%\n     dplyr::select(-1,-probabilities)\n all_data=all_data%>%\n     mutate(outcome=factor(outcome,levels=c(0,1)),\n            sex=factor(sex,levels=c(0,1)),\n            country=factor(country,levels=c(0,1)),\n            treatment=factor(treatment,levels=c(0,1)),\n            genotype=factor(genotype,levels=c(3,0,1,2,4,5))\n            )\n \n dim(all_data)\n\n\n[1]  1000 10006"
  },
  {
    "objectID": "docs/projects/high_dimension/data_preparation.html#data-description",
    "href": "docs/projects/high_dimension/data_preparation.html#data-description",
    "title": "Data Preparation",
    "section": "Data Description",
    "text": "Data Description\nThis data include\nMin.   :10.40  , 1st Qu.:15.43  , Median :19.20  , Mean   :20.09  , 3rd Qu.:22.80  , Max.   :33.90  , Min.   :4.000  , 1st Qu.:4.000  , Median :6.000  , Mean   :6.188  , 3rd Qu.:8.000  , Max.   :8.000  , Min.   : 71.1  , 1st Qu.:120.8  , Median :196.3  , Mean   :230.7  , 3rd Qu.:326.0  , Max.   :472.0  , Min.   : 52.0  , 1st Qu.: 96.5  , Median :123.0  , Mean   :146.7  , 3rd Qu.:180.0  , Max.   :335.0  , Min.   :2.760  , 1st Qu.:3.080  , Median :3.695  , Mean   :3.597  , 3rd Qu.:3.920  , Max.   :4.930  , Min.   :1.513  , 1st Qu.:2.581  , Median :3.325  , Mean   :3.217  , 3rd Qu.:3.610  , Max.   :5.424  , Min.   :14.50  , 1st Qu.:16.89  , Median :17.71  , Mean   :17.85  , 3rd Qu.:18.90  , Max.   :22.90  , Min.   :0.0000  , 1st Qu.:0.0000  , Median :0.0000  , Mean   :0.4375  , 3rd Qu.:1.0000  , Max.   :1.0000  , Min.   :0.0000  , 1st Qu.:0.0000  , Median :0.0000  , Mean   :0.4062  , 3rd Qu.:1.0000  , Max.   :1.0000  , Min.   :3.000  , 1st Qu.:3.000  , Median :4.000  , Mean   :3.688  , 3rd Qu.:4.000  , Max.   :5.000  , Min.   :1.000  , 1st Qu.:2.000  , Median :2.000  , Mean   :2.812  , 3rd Qu.:4.000  , Max.   :8.000  \nthis inline code works but it seems to rmd files look more organized and tidier."
  },
  {
    "objectID": "docs/projects/high_dimension/data_preparation.html#architecture-of-analysis-pipeline",
    "href": "docs/projects/high_dimension/data_preparation.html#architecture-of-analysis-pipeline",
    "title": "Data Preparation",
    "section": "Architecture of Analysis Pipeline",
    "text": "Architecture of Analysis Pipeline"
  },
  {
    "objectID": "docs/projects/high_dimension/data_preparation.html#methods",
    "href": "docs/projects/high_dimension/data_preparation.html#methods",
    "title": "Data Preparation",
    "section": "Methods",
    "text": "Methods\n\n\n   alpha      mse fit.name\n1    0.0 2529.146   alpha0\n2    0.1 2529.146 alpha0.1\n3    0.2 2529.146 alpha0.2\n4    0.3 2529.146 alpha0.3\n5    0.4 2529.146 alpha0.4\n6    0.5 2529.146 alpha0.5\n7    0.6 2529.146 alpha0.6\n8    0.7 2529.146 alpha0.7\n9    0.8 2529.146 alpha0.8\n10   0.9 2529.146 alpha0.9\n11   1.0 2529.146   alpha1"
  },
  {
    "objectID": "docs/projects/high_dimension/data_preparation.html#results",
    "href": "docs/projects/high_dimension/data_preparation.html#results",
    "title": "Data Preparation",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "docs/projects/high_dimension/data_preparation.html#conclusion",
    "href": "docs/projects/high_dimension/data_preparation.html#conclusion",
    "title": "Data Preparation",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "docs/projects/high_dimension/data_preparation.html#bibliography",
    "href": "docs/projects/high_dimension/data_preparation.html#bibliography",
    "title": "Data Preparation",
    "section": "Bibliography",
    "text": "Bibliography"
  },
  {
    "objectID": "docs/projects/high_dimension/description.html",
    "href": "docs/projects/high_dimension/description.html",
    "title": "Description",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n알츠하이머병(AD)은 수백만 명의 미국인에게 영향을 미치는 가장 흔한 형태의 치매이다. 알츠하이머병은 기억력, 사고력 및 행동에 영향을 주지만 증상이 나타나기까지 거의 20년에 걸쳐 진행이 된다. 따라서 전 임상 단계에서 생리학을 이해하는 것이 필수적이다. 유전적 요인이 AD에 거의 50% 기여하는 것으로 추정된다. 유전자가 세포 환경을 변경하여 알츠하이머병 위험에 어떻게 기여하는지 더 잘 이해하기 위해 AD와 연관이 있는 유전자인 APOE를 보유한 사람들의 대사체를 조사했다. 대사체는 게놈과 프로테옴에서 생성된 산물을 나타낸다. 이러한 생화학 부산물은 유전적 요인과 환경적 요인 모두의 영향을 받는다. 모집단은 장수마을에 사는 Caucasian (백인) 참여자들이다.\n\n\n\nLLFS(Long Life Family Study) 프로젝트의 목적은 전사체 및 단백질체 단계를 통해 게놈에서 대사체 단계에 이르는 다단계에서 통계 및 기계 학습을 사용하여 분석 파이프라인을 구축하고 알츠하이머병에 대한 중요한 바이오마커를 식별하기 위함이다.\n\n\n\n이 시뮬레이션 연구에서는 정규분포 하에서 대사 물질 데이터를 생성하여 대사 단계에서 가상의 데이터를 생성하는 데에만 집중할 것이다. (현재, 이산 변수에 대하여 통계적으로 시뮬레이션하는 방법론 연구가 더 필요함.) 이 시뮬레이션 연구의 목표는 알츠하이머병과 비알츠하이머병과 관련된 바이오마커를 구별할 수 있는 일련의 예측인자(또는 대사물질 또는 생화학물질)를 식별하는 것이다.\n\n\n\n\n보안 문제로 인해 이 프로젝트에 사용된 실제 데이터와 전체 분석 파이프라인을 표시하기 어렵다. 따라서 시뮬레이션을 통해 대략적인 분석 파이프라인을 재현하고 시연하기 위해 가짜 데이터를 생성한다.이 글에서는 보안 문제로 인해 대략적인 분석 파이프라인을 재현하고 시연하기 위한 시뮬레이션 데이터를 준비한다. 시뮬레이션 데이터는 실제 연구를 위해 표본으로 쓰이는 데이터의 특성을 전혀 반영하지 않는다.\n이 시뮬레이션 연구에서, 이산 변수 (age 포함)와 종속 변수를 통계적으로 연관시키지 못했기 때문에 이산 변수에 대한 분석 결과가 생물학적인 사실과 많이 다를 수 있다. (참조 문헌을 찾을 예정)\n\n\n\n\n\nOperating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 in the English section\nPython 3.11 in the Korean section\n\n\n\n\n\n읽기의 편의를 위해 이 프로젝트를 5개 섹션으로 나누었습니다.\n\nDescription (Current)\nData Preparation\nEDA (Exploratory Data Analysis)\nMethod1: statistical approach\nMethod2: ML Approach\nConclusion\n\n\n\n\n\n\nAlzheimer Disease (AD) is a most common form of dementia that affects millions of Americans. AD affects memory, thinking and behavior, but its progression is slow, spanning nearly two decades before the symptoms appear. Thus, it is imperative to understand the physiology at the pre-clinical stage. It is estimated that genetic factors contribute nearly 50% to AD. To better understand how genes contribute to the risk of AD by altering cellular milieu, we have examined the metabolome of individuals with the AD-related genotype, APOE. The metabolome represents the products that were generated from the genome and proteome. These biochemical products represent influences of both genetic and environmental factors. The population is Caucasian participants living in longevity village.\n\n\n\nThe objective of the Long Life Family Study (LLFS) project was to build an analysis pipeline of identifying significant biomarkers for AD using statistics and machine learning at the multi stages from the genomic to the metabolomic stage through the transcriptomic and proteomic stage.\n\n\n\nIn this simulation study, I will focus only on generating fake data at the metabolomic stage by generating data under multivariate normal distributions. The aim of this simulation study is to identify a set of predictors (or metabolites or bio-chemicals) that will enable to differentiate bio-markers that are associated with AD vs. non-AD.\n\n\n\n\nIn this article, due to security concerns, it is difficult to display the real data and the entire analysis pipeline used in this project. Therefore, I prepared simulated data to reproduce and demonstrate a rough analysis pipeline. The simulated data does not reflect the characteristics of the truely sampled data used in the LLFS at all.\nIn this simulation, since the discrete variables(including age) and the dependent variable could not be statistically associated, the analysis result for the discrete variables could be very different from the biological fact (will find references for simulating discrete variables).\n\n\n\n\n\nOperating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 in the English section\nPython 3.11 in the Korean section\n\n\n\n\n\nFor the convenience of reading, I divided this proejcts into 5 sections:\n\nDescription (Current)\nData Preparation\nEDA (Exploratory Data Analysis)\nMethod1: statistical approach\nMethod2: ML Approach\nConclusion"
  },
  {
    "objectID": "docs/projects/high_dimension/description.html#simulation",
    "href": "docs/projects/high_dimension/description.html#simulation",
    "title": "Description",
    "section": "Simulation",
    "text": "Simulation\n\n\n\n\nflowchart LR"
  },
  {
    "objectID": "template.html",
    "href": "template.html",
    "title": "template",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe radius of the circle is 10.\n\n\n\n1 Go to Project Content List\nProject Content List\n\n\n2 Go to Blog Content List\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/2023-01-07-anova/index.html",
    "href": "docs/blog/posts/2023-01-07-anova/index.html",
    "title": "ANOVA",
    "section": "",
    "text": "(draft)\n## CF"
  },
  {
    "objectID": "docs/blog/posts/2023-01-07-anova/index.html#application-to-example",
    "href": "docs/blog/posts/2023-01-07-anova/index.html#application-to-example",
    "title": "ANOVA",
    "section": "2 Application to Example",
    "text": "2 Application to Example\n\n2.1 Data Description\n\n2.1.1 Raw Data\n\n\n     sample Day Run    Ct\n1   patient   1   1 38.33\n2   patient   1   1 36.58\n3   patient   1   1 36.67\n4   patient   1   2 38.41\n5   patient   1   2 37.86\n6   patient   1   2 38.51\n7   patient   2   1 36.51\n8   patient   2   1 37.38\n9   patient   2   1 38.26\n10  patient   2   2 37.31\n11  patient   2   2 38.13\n12  patient   2   2 37.04\n13  patient   3   1 37.88\n14  patient   3   1 37.41\n15  patient   3   1 35.89\n16  patient   3   2 36.95\n17  patient   3   2 37.23\n18  patient   3   2 39.85\n19  patient   4   1 36.34\n20  patient   4   1 37.67\n21  patient   4   1 37.67\n22  patient   4   2 36.88\n23  patient   4   2 36.53\n24  patient   4   2 37.24\n25  patient   5   1 37.82\n26  patient   5   1 37.56\n27  patient   5   1 37.39\n28  patient   5   2 37.01\n29  patient   5   2 37.60\n30  patient   5   2 38.04\n31  patient   6   1 36.50\n32  patient   6   1 38.72\n33  patient   6   1 37.74\n34  patient   6   2 37.78\n35  patient   6   2 36.98\n36  patient   6   2 36.72\n37  patient   7   1 37.12\n38  patient   7   1 37.02\n39  patient   7   1 38.07\n40  patient   7   2 38.68\n41  patient   7   2 37.75\n42  patient   7   2 35.91\n43  patient   8   1 39.70\n44  patient   8   1 37.22\n45  patient   8   1 36.79\n46  patient   8   2 37.22\n47  patient   8   2 37.66\n48  patient   8   2 36.54\n49  patient   9   1 38.39\n50  patient   9   1 38.88\n51  patient   9   1 39.10\n52  patient   9   2 38.99\n53  patient   9   2 37.26\n54  patient   9   2 36.15\n55  patient  10   1 36.94\n56  patient  10   1 36.65\n57  patient  10   1 36.67\n58  patient  10   2 37.63\n59  patient  10   2 37.09\n60  patient  10   2 37.80\n61  patient  11   1 38.31\n62  patient  11   1 40.20\n63  patient  11   1 37.57\n64  patient  11   2 37.04\n65  patient  11   2 36.46\n66  patient  11   2 37.30\n67  patient  12   1 36.65\n68  patient  12   1 36.49\n69  patient  12   1 37.03\n70  patient  12   2 37.44\n71  patient  12   2 36.77\n72  patient  12   2 36.23\n73  patient  13   1 37.95\n74  patient  13   1 36.60\n75  patient  13   1 37.68\n76  patient  13   2 37.82\n77  patient  13   2 38.00\n78  patient  13   2 37.33\n79  patient  14   1 36.63\n80  patient  14   1 36.37\n81  patient  14   1 37.15\n82  patient  14   2 38.11\n83  patient  14   2 37.28\n84  patient  14   2 35.60\n85  patient  15   1 36.29\n86  patient  15   1 36.21\n87  patient  15   1 36.61\n88  patient  15   2 37.62\n89  patient  15   2 38.82\n90  patient  15   2 35.55\n91  patient  16   1 36.27\n92  patient  16   1 37.51\n93  patient  16   1 37.93\n94  patient  16   2 36.79\n95  patient  16   2 36.64\n96  patient  16   2 36.76\n97  patient  17   1 36.86\n98  patient  17   1 37.40\n99  patient  17   1 38.11\n100 patient  17   2 36.09\n101 patient  17   2 36.23\n102 patient  17   2 36.80\n103 patient  18   1 37.61\n104 patient  18   1 37.27\n105 patient  18   1 37.05\n106 patient  18   2 38.17\n107 patient  18   2 36.62\n108 patient  18   2 36.24\n109 patient  19   1 37.20\n110 patient  19   1 37.05\n111 patient  19   1 38.73\n112 patient  19   2 36.52\n113 patient  19   2 36.41\n114 patient  19   2 36.40\n115 patient  20   1 37.62\n116 patient  20   1 37.36\n117 patient  20   1    NA\n118 patient  20   2 38.41\n119 patient  20   2 37.26\n120 patient  20   2 37.28\n\n\n공유해주신 example data는 sample, Day, Run, Ct의 변수들을 포함하고 있습니다. 특히 sample은 여기서 같은 값을 갖고있기 때문에 의미가 없는 변수입니다. 공유해주신 정보에 따르면 아마도 Run은 오전과 오후를 나누는 변수인 것으로 생각 됩니다. 이 data만 보면 아마도 같은 샘플에 대해서 시약 제품이 시간에 따라 얼마나 안정적인 performance (Ct값)를 보여주는지 검사하는 실험으로 추측됩니다. 좀 더 분석하기 용이한 형태로 data structure를 바꾸겠습니다.\n\n\n2.1.2 Processed Data\n\n\n# A tibble: 120 × 5\n      id Day   noon    Run    Ct\n   <int> <fct> <fct> <int> <dbl>\n 1     1 1     AM        1  38.3\n 2     2 1     AM        2  36.6\n 3     3 1     AM        3  36.7\n 4     4 1     PM        1  38.4\n 5     5 1     PM        2  37.9\n 6     6 1     PM        3  38.5\n 7     7 2     AM        1  36.5\n 8     8 2     AM        2  37.4\n 9     9 2     AM        3  38.3\n10    10 2     PM        1  37.3\n# … with 110 more rows\n\n\n재가공된 data는 120개의 샘플과 5개의 변수를 갖고있습니다. 변수 목록은 다음과 같습니다.\n\nsample: 검체이름, 같은 검체 혹은 의미없음\n\nid: 열번호, 총 20일간 하루 2회 구동(AM, PM) 구동, 오전 오후 각 각 3번씩 구동 총 120 \\((=20 \\times 3 \\times 2)\\) 샘플\n\nDay: Day1~20\n\nnoon: AM= before noon, PM= after noon\nRun: 1회 구동당 3번 반복씩1, 2, 3\n\nCt: Ct values\n\nANOVA의 Assumption\n\nresponse variable should follow normal distribution.\n\nhomoscedasticity, equality of variance: 각 집단의 분포는 모두 동일한 분산을 가짐\nANOVA의 가정들을 반드시 충족하지 않아도 되지만 충족하면 Power 가 올라감\n\n\n\n\n2.2 EDA (Explorator Data Analysis)\n이 data는 아래 처럼 1의 결측치를 갖고 있습니다.\n\n\n\n\n\nid\nDay\nnoon\nRun\nCt\n\n\n\n\n117\n20\nAM\n3\nNA\n\n\n\n\n\nCt에 대한 Global Statistics는 다음과 같습니다.\n\n\n\n\n\ncount\nglobal_Ct_mean\nglobal_Ct_sd\nglobal_Ct_CV\n\n\n\n\n119\n37.322\n0.86754\n2.324 %\n\n\n\n\n\nDay groups의 Statistics은 다음과 같습니다.\n\n\n\n\n\nDay\ncount\nDay_group_Ct_mean\nDay_group_Ct_sd\nDay_group_Ct_CV\n\n\n\n\n1\n6\n37.727\n0.88247\n2.339 %\n\n\n2\n6\n37.438\n0.66240\n1.769 %\n\n\n3\n6\n37.535\n1.31417\n3.501 %\n\n\n4\n6\n37.055\n0.56712\n1.53 %\n\n\n5\n6\n37.570\n0.35508\n0.945 %\n\n\n6\n6\n37.407\n0.83077\n2.221 %\n\n\n7\n6\n37.425\n0.96426\n2.577 %\n\n\n8\n6\n37.522\n1.13537\n3.026 %\n\n\n9\n6\n38.128\n1.18219\n3.101 %\n\n\n10\n6\n37.130\n0.48551\n1.308 %\n\n\n11\n6\n37.813\n1.31872\n3.487 %\n\n\n12\n6\n36.768\n0.42447\n1.154 %\n\n\n13\n6\n37.563\n0.52955\n1.41 %\n\n\n14\n6\n36.857\n0.86064\n2.335 %\n\n\n15\n6\n36.850\n1.17852\n3.198 %\n\n\n16\n6\n36.983\n0.61442\n1.661 %\n\n\n17\n6\n36.915\n0.75224\n2.038 %\n\n\n18\n6\n37.160\n0.69062\n1.859 %\n\n\n19\n6\n37.052\n0.88971\n2.401 %\n\n\n20\n5\n37.586\n0.48247\n1.284 %\n\n\n\n\n\nAM/PM groups의 Statistics은 다음과 같습니다.\n\n\n\n\n\nnoon\ncount\nnoon_group_Ct_mean\nnoon_group_Ct_sd\nnoon_group_Ct_CV\n\n\n\n\nAM\n59\n37.400\n0.87839\n2.349 %\n\n\nPM\n60\n37.246\n0.85722\n2.302 %\n\n\n\n\n\nDays와 AM/PM 조합 groups의 Statistics은 다음과 같습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nnoon\ncount\ncombi_group_Ct_mean\ncombi_group_Ct_sd\ncombi_group_Ct_CV\n\n\n\n\n1\nAM\n3\n37.193\n0.98541\n2.649 %\n\n\n1\nPM\n3\n38.260\n0.35000\n0.915 %\n\n\n2\nAM\n3\n37.383\n0.87500\n2.341 %\n\n\n2\nPM\n3\n37.493\n0.56766\n1.514 %\n\n\n3\nAM\n3\n37.060\n1.04014\n2.807 %\n\n\n3\nPM\n3\n38.010\n1.59962\n4.208 %\n\n\n4\nAM\n3\n37.227\n0.76788\n2.063 %\n\n\n4\nPM\n3\n36.883\n0.35501\n0.963 %\n\n\n5\nAM\n3\n37.590\n0.21656\n0.576 %\n\n\n5\nPM\n3\n37.550\n0.51682\n1.376 %\n\n\n6\nAM\n3\n37.653\n1.11253\n2.955 %\n\n\n6\nPM\n3\n37.160\n0.55245\n1.487 %\n\n\n7\nAM\n3\n37.403\n0.57951\n1.549 %\n\n\n7\nPM\n3\n37.447\n1.40969\n3.765 %\n\n\n8\nAM\n3\n37.903\n1.57074\n4.144 %\n\n\n8\nPM\n3\n37.140\n0.56427\n1.519 %\n\n\n9\nAM\n3\n38.790\n0.36346\n0.937 %\n\n\n9\nPM\n3\n37.467\n1.43123\n3.82 %\n\n\n10\nAM\n3\n36.753\n0.16197\n0.441 %\n\n\n10\nPM\n3\n37.507\n0.37072\n0.988 %\n\n\n11\nAM\n3\n38.693\n1.35626\n3.505 %\n\n\n11\nPM\n3\n36.933\n0.43004\n1.164 %\n\n\n12\nAM\n3\n36.723\n0.27737\n0.755 %\n\n\n12\nPM\n3\n36.813\n0.60616\n1.647 %\n\n\n13\nAM\n3\n37.410\n0.71435\n1.91 %\n\n\n13\nPM\n3\n37.717\n0.34675\n0.919 %\n\n\n14\nAM\n3\n36.717\n0.39716\n1.082 %\n\n\n14\nPM\n3\n36.997\n1.27876\n3.456 %\n\n\n15\nAM\n3\n36.370\n0.21166\n0.582 %\n\n\n15\nPM\n3\n37.330\n1.65418\n4.431 %\n\n\n16\nAM\n3\n37.237\n0.86310\n2.318 %\n\n\n16\nPM\n3\n36.730\n0.07937\n0.216 %\n\n\n17\nAM\n3\n37.457\n0.62692\n1.674 %\n\n\n17\nPM\n3\n36.373\n0.37608\n1.034 %\n\n\n18\nAM\n3\n37.310\n0.28213\n0.756 %\n\n\n18\nPM\n3\n37.010\n1.02240\n2.762 %\n\n\n19\nAM\n3\n37.660\n0.92968\n2.469 %\n\n\n19\nPM\n3\n36.443\n0.06658\n0.183 %\n\n\n20\nAM\n2\n37.490\n0.18385\n0.49 %\n\n\n20\nPM\n3\n37.650\n0.65826\n1.748 %\n\n\n\n\n\n이제 ANOVA를 수행하기 위한 basic statistics는 모두 구했습니다. ANOVA를 수행하기 위해 집단 간 분산과 집단 내 분산을 계산하도록 하겠습니다.\n\n\n2.3 집단 간 분산\n앞에서 설명 드린바로 유추해보면 예시 data의 집단 간 분산의 범주형 변수는 Day로 설정하는 것이 합리적인 것으로 보입니다.\n\n\\(g=g\\) Day의 sample size = 20, 자유도 = 20-1 = 19 입니다.\n\\(n_g=g\\) group의 sample size, \\(\\overline{X}_g=g\\) 의 sample mean은 다음과 같습니다.\n\\(\\overline{X}\\) = global sample mean = 37.32202\n집단 간 분산: \\(\\frac{집단 간 제곱합}{자유도}=\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n2.3.1 SS_Day (집단간 분산 Day)\nDay sq = \\([(\\overline{X}_g-\\overline{X})^2n_g]\\) 집단(Day)간 분산 계산, 집단(Day)간 분산 = \\(\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\ncount\nDay_group_Ct_mean\nDay_group_Ct_sd\nDay_group_Ct_CV\nday_sq\nsum_day_ssq\ndf\nday_mean_ssq\nday_sd\n\n\n\n\n1\n6\n37.727\n0.88247\n2.339 %\n0.98245\n15.5\n19\n0.81581\n0.90322\n\n\n2\n6\n37.438\n0.66240\n1.769 %\n0.08118\n15.5\n19\n0.81581\n0.90322\n\n\n3\n6\n37.535\n1.31417\n3.501 %\n0.27217\n15.5\n19\n0.81581\n0.90322\n\n\n4\n6\n37.055\n0.56712\n1.53 %\n0.42779\n15.5\n19\n0.81581\n0.90322\n\n\n5\n6\n37.570\n0.35508\n0.945 %\n0.36897\n15.5\n19\n0.81581\n0.90322\n\n\n6\n6\n37.407\n0.83077\n2.221 %\n0.04299\n15.5\n19\n0.81581\n0.90322\n\n\n7\n6\n37.425\n0.96426\n2.577 %\n0.06363\n15.5\n19\n0.81581\n0.90322\n\n\n8\n6\n37.522\n1.13537\n3.026 %\n0.23916\n15.5\n19\n0.81581\n0.90322\n\n\n9\n6\n38.128\n1.18219\n3.101 %\n3.90088\n15.5\n19\n0.81581\n0.90322\n\n\n10\n6\n37.130\n0.48551\n1.308 %\n0.22122\n15.5\n19\n0.81581\n0.90322\n\n\n11\n6\n37.813\n1.31872\n3.487 %\n1.44835\n15.5\n19\n0.81581\n0.90322\n\n\n12\n6\n36.768\n0.42447\n1.154 %\n1.83939\n15.5\n19\n0.81581\n0.90322\n\n\n13\n6\n37.563\n0.52955\n1.41 %\n0.34940\n15.5\n19\n0.81581\n0.90322\n\n\n14\n6\n36.857\n0.86064\n2.335 %\n1.29930\n15.5\n19\n0.81581\n0.90322\n\n\n15\n6\n36.850\n1.17852\n3.198 %\n1.33680\n15.5\n19\n0.81581\n0.90322\n\n\n16\n6\n36.983\n0.61442\n1.661 %\n0.68824\n15.5\n19\n0.81581\n0.90322\n\n\n17\n6\n36.915\n0.75224\n2.038 %\n0.99398\n15.5\n19\n0.81581\n0.90322\n\n\n18\n6\n37.160\n0.69062\n1.859 %\n0.15750\n15.5\n19\n0.81581\n0.90322\n\n\n19\n6\n37.052\n0.88971\n2.401 %\n0.43854\n15.5\n19\n0.81581\n0.90322\n\n\n20\n5\n37.586\n0.48247\n1.284 %\n0.34844\n15.5\n19\n0.81581\n0.90322\n\n\n\n\n\nAnalysis-In program의 ANOVA결과값과 일치하는 것을 볼 수 있습니다. $SS_{day}= $ 15.50038 with \\(df=19\\).\n\n\n2.3.2 SS_noon (집단간 분산 noon)\nnoon sq = \\([(\\overline{X}_g-\\overline{X})^2n_g]\\) 집단(noon)간 분산 계산, 집단(noon)간 분산 = \\(\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnoon\ncount\nnoon_group_Ct_mean\nnoon_group_Ct_sd\nnoon_group_Ct_CV\nnoon_sq\nsum_noon_ssq\ndf\nnoon_mean_ssq\nnoon_sd\n\n\n\n\nAM\n59\n37.400\n0.87839\n2.349 %\n0.35569\n0.70545\n1\n0.70545\n0.83991\n\n\nPM\n60\n37.246\n0.85722\n2.302 %\n0.34976\n0.70545\n1\n0.70545\n0.83991\n\n\n\n\n\nAnalysis-In program의 결과에서 찾아 볼 수 없죠? 이 결과는 숨어 있습니다. 상호 작용에 대한 분산값을 구하고 나면 정체를 알 수 있습니다.\n\\(SS_{noon}\\) = 0.70545 with \\(df=1\\).\n\n\n2.3.3 SS_error (집단내 분산)\n\n집단 내 분산 (within-groups variability)\n\n\n\n# A tibble: 119 × 10\n   Day   noon    Run    Ct count combi_group_Ct_…¹ resid…²    df resid…³ mean_…⁴\n   <fct> <fct> <int> <dbl> <int>             <dbl>   <dbl> <dbl>   <dbl>   <dbl>\n 1 1     AM        1  38.3     3              37.2 1.29e+0    79    54.5   0.690\n 2 1     AM        2  36.6     3              37.2 3.76e-1    79    54.5   0.690\n 3 1     AM        3  36.7     3              37.2 2.74e-1    79    54.5   0.690\n 4 1     PM        1  38.4     3              38.3 2.25e-2    79    54.5   0.690\n 5 1     PM        2  37.9     3              38.3 1.60e-1    79    54.5   0.690\n 6 1     PM        3  38.5     3              38.3 6.25e-2    79    54.5   0.690\n 7 2     AM        1  36.5     3              37.4 7.63e-1    79    54.5   0.690\n 8 2     AM        2  37.4     3              37.4 1.11e-5    79    54.5   0.690\n 9 2     AM        3  38.3     3              37.4 7.69e-1    79    54.5   0.690\n10 2     PM        1  37.3     3              37.5 3.36e-2    79    54.5   0.690\n# … with 109 more rows, and abbreviated variable names ¹​combi_group_Ct_mean,\n#   ²​residual_sq, ³​residual_ssq, ⁴​mean_residual_ssq\n\n\n\\(SS_{error}\\) = 54.50093\nAnalysis-In program의 결과와 일치하는 것을 확인할 수 있습니다.\n\n\n2.3.4 SS_total\n\n\n# A tibble: 119 × 6\n      id Day   noon    Run    Ct total_ssq\n   <int> <fct> <fct> <int> <dbl>     <dbl>\n 1     1 1     AM        1  38.3      88.8\n 2     2 1     AM        2  36.6      88.8\n 3     3 1     AM        3  36.7      88.8\n 4     4 1     PM        1  38.4      88.8\n 5     5 1     PM        2  37.9      88.8\n 6     6 1     PM        3  38.5      88.8\n 7     7 2     AM        1  36.5      88.8\n 8     8 2     AM        2  37.4      88.8\n 9     9 2     AM        3  38.3      88.8\n10    10 2     PM        1  37.3      88.8\n# … with 109 more rows\n\n\n\\(SS_{total}\\) = 88.81032\nAnalysis-In program의 ANOVA 결과 table에 있는 SS들의 합과 일치하는 것을 확인할 수 있습니다.\n\n\n2.3.5 상호 작용 분산\n\n\n\n\\(SS_{interaction}=SS_{DayNoon}= SS_{total}-SS_{Day}-SS_{noon}-SS_{error}\\) = 88.81032-54.50093-0.70545-15.50038 = 18.10355\nAnalysis-In program의 ANOVA 결과 table과 일치하는 것을 확인할 수 있습니다.\n위의 결과들을 종합하면 아래와 같이 요약됩니다.\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   15.5   0.816    1.18   0.29\nnoon         1    0.7   0.748    1.08   0.30\nDay:noon    19   18.1   0.951    1.38   0.16\nResiduals   79   54.5   0.690               \n1 observation deleted due to missingness\n\n\n\nRepeatability SD = \\(\\sqrt{V_{error}}=\\sqrt{MS_{error}}\\) = 0.83059\nRepeatability CV = \\(\\frac{repeatability \\space SD}{global \\space mean \\space Ct}\\) = 0.02225\n\n(생략, 다른 통계량들은 ANOVA_A repeat2.xlsx 참조)\n위의 결과를 간단히 해석해 보면\n\n집단간 범주 변수인 Day는 p-value =0.29>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, 일별로 평균 Ct값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, 일별로 평균 Ct값이 다르지 않습니다.\n\n집단간 범주 변수인 noon은 p-value =0.30>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, 오전/오후별 평균 Ct값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, 오전/오후별 평균 Ct값이 다르지 않습니다.\n\nDay와 noon두 변수의 상호작용 변수는 p-value =0.16>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, Day별 Noon(오전/오후)별 평균 Ct값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, Day별 Noon(오전/오후)별 평균 Ct값이 다르지 않습니다.\n\n\n최종 결론, 제품의 Ct값이 Day별 오전/오후별 안정적인 performance를 보인다고 조심스럽게 결론을 내릴 수 있습니다.\n이제 까지는 질문에 대한 답이 되는 ANOVA의 원리 및 통계량의 재현 및 해석법에 대하여 알아봤습니다. 하지만 직관적으로 어떤 의미가 있을 까요? 원래는 시각화를 통해 데이터의 패턴을 짐작하고 통계 검정 결과를 예상하는데 우리는 반대로 가고 있네요 ㅎㅎ 시각화를 통해 ANOVA 결과가 얼마나 합리적인지 알아보겠습니다.\n\n\n\n2.4 Visualization\n\n2.4.1 One-way: Day\n\n\n\n\n\n\n\n\n자세히 보면 일별로 시간의 경과에 따라 전체적으로 평균 Ct값이 약간 하향하는 것으로 보입니다. 하지만 좀 더 세부적으로 관찰하면 1일~8일 평균 Ct의 경향이 constant한 패턴을 보입니다. 9일~13일 평균 Ct가 진동 하향하는 패턴을 보입니다. 14일~20일 평균 Ct가 상향하는 패턴을 보입니다.\n\n\nTables of means\nGrand mean\n       \n37.322 \n\n Day \n        1     2     3     4     5     6     7     8     9    10    11    12\n    37.73 37.44 37.53 37.05 37.57 37.41 37.42 37.52 38.13 37.13 37.81 36.77\nrep  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00\n       13    14    15    16    17    18    19    20\n    37.56 36.86 36.85 36.98 36.91 37.16 37.05 37.59\nrep  6.00  6.00  6.00  6.00  6.00  6.00  6.00  5.00\n\n\nTables of effects\n\n Day \n         1      2     3      4     5       6     7      8      9     10     11\n    0.4046 0.1163 0.213 -0.267 0.248 0.08465 0.103 0.1996 0.8063 -0.192 0.4913\nrep 6.0000 6.0000 6.000  6.000 6.000 6.00000 6.000 6.0000 6.0000  6.000 6.0000\n         12     13      14     15      16     17     18      19    20\n    -0.5537 0.2413 -0.4654 -0.472 -0.3387 -0.407 -0.162 -0.2704 0.264\nrep  6.0000 6.0000  6.0000  6.000  6.0000  6.000  6.000  6.0000 5.000\n\n\n위에 첫 번째표에서 Global Sample Ct Mean = 37.322 과 각 집단의 평균 Ct를 확인할 수 있습니다. 위에 두 번째표에서 Global Sample Ct Mean = 37.322 과 각 집단의 평균 Ct의 차이를 확인할 수 있습니다.\n\nDay 9에서 차이가 가장 큰 것으로 보아 9일째 실험에서 performance가 가장 낮은 것이 관측됐습니다.\n반대로, 12일에 performance 가장 좋은 것으로 관측됐습니다.\n\n9일과 12일에 Ct값에 영향을 미쳤던 요인이 있었는지 복기 하는것도 도움이 되겠군요.\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   15.5   0.816     1.1   0.36\nResiduals   99   73.3   0.741               \n1 observation deleted due to missingness\n\n\nOne-way ANOVA의 결과값입니다. Day별 평균 Ct의 차이는 거의 없는 것으로 보입니다. 따라서 Day 별 평균 Ct의 경향이 일관되지 않고 One-way ANOVA에서 역시 통계적으로 유의하지 않아 Day 변수는 평균 Ct에 영향을 미치지 않는 것 같습니다.\n\n\n2.4.2 One-way: AM/PM\n\n\n\n\n\n\n\n\n오후에 시간의 경과에 따라 전체적으로 평균 Ct값이 약간 하향하는 것으로 보입니다.\n\n\nTables of means\nGrand mean\n       \n37.322 \n\n noon \n      AM    PM\n    37.4 37.25\nrep 59.0 60.00\n\n\nTables of effects\n\n noon \n          AM       PM\n     0.07764 -0.07635\nrep 59.00000 60.00000\n\n\n위 첫 번째 표에서 AM/PM 간의 평균 Ct차이는 0.15 (농도가 약 0.5배) 차이가 나는 것을 확인할 수 있습니다. 생물학적으로 의미가 있는 수치일까요? 위 두 번째 표에서 Global Sample Mean 37.322와 오전/오후 별 약 0.07씩(농도가 약 0.25배) 차이가 납니다.\n\n\n             Df Sum Sq Mean Sq F value Pr(>F)\nnoon          1    0.7   0.705    0.94   0.34\nResiduals   117   88.1   0.753               \n1 observation deleted due to missingness\n\n\n오전 오후별 One way ANOVA를 실행한 결과가 오전/오부 평균 Ct값의 차이가 다르지 않다는 것을 시사하고 있습니다. 아무래도 위의 차이는 우연에 의해 발생한 현상인 것 같습니다.\n\n\n\n\n\n일별로 평균 Ct값이 차이를 모든 경우의 수에서 차이가 나는지 확인한 결과 신뢰구간 모두가 0을 포함하고 있으므로 Day 변수는 유의하다고 볼 수 없습니다.\n\n\n\n\n\n오전/오후별로 평균 Ct값이 차이를 모든 경우의 수에서 차이가 나는지 확인한 결과 신뢰구간 모두가 0을 포함하고 있으므로 오전/오후 변수는 유의하다고 볼 수 없습니다.\n여기 까지 각 변수별 평균 Ct로의 영향도를 통계적으로 시각적으로 관찰했습니다. 하지만 Day별 오전/오후별 영향도가 있는지 확인하겠습니다. (이미 위에서 통계적으로 없다고 검정됐습니다.)\n\n\n\n2.5 Two way Anova\n\n\nTables of means\nGrand mean\n       \n37.322 \n\n Day \n        1     2     3     4     5     6     7     8     9    10    11    12\n    37.73 37.44 37.53 37.05 37.57 37.41 37.42 37.52 38.13 37.13 37.81 36.77\nrep  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00\n       13    14    15    16    17    18    19    20\n    37.56 36.86 36.85 36.98 36.91 37.16 37.05 37.59\nrep  6.00  6.00  6.00  6.00  6.00  6.00  6.00  5.00\n\n noon \n      AM    PM\n    37.4 37.24\nrep 59.0 60.00\n\n Day:noon \n     noon\nDay   AM    PM   \n  1   37.19 38.26\n  rep  3.00  3.00\n  2   37.38 37.49\n  rep  3.00  3.00\n  3   37.06 38.01\n  rep  3.00  3.00\n  4   37.23 36.88\n  rep  3.00  3.00\n  5   37.59 37.55\n  rep  3.00  3.00\n  6   37.65 37.16\n  rep  3.00  3.00\n  7   37.40 37.45\n  rep  3.00  3.00\n  8   37.90 37.14\n  rep  3.00  3.00\n  9   38.79 37.47\n  rep  3.00  3.00\n  10  36.75 37.51\n  rep  3.00  3.00\n  11  38.69 36.93\n  rep  3.00  3.00\n  12  36.72 36.81\n  rep  3.00  3.00\n  13  37.41 37.72\n  rep  3.00  3.00\n  14  36.72 37.00\n  rep  3.00  3.00\n  15  36.37 37.33\n  rep  3.00  3.00\n  16  37.24 36.73\n  rep  3.00  3.00\n  17  37.46 36.37\n  rep  3.00  3.00\n  18  37.31 37.01\n  rep  3.00  3.00\n  19  37.66 36.44\n  rep  3.00  3.00\n  20  37.49 37.65\n  rep  2.00  3.00\n\n\nTables of effects\n\n Day \n         1      2     3      4     5       6     7      8      9     10     11\n    0.4046 0.1163 0.213 -0.267 0.248 0.08465 0.103 0.1996 0.8063 -0.192 0.4913\nrep 6.0000 6.0000 6.000  6.000 6.000 6.00000 6.000 6.0000 6.0000  6.000 6.0000\n         12     13      14     15      16     17     18      19    20\n    -0.5537 0.2413 -0.4654 -0.472 -0.3387 -0.407 -0.162 -0.2704 0.264\nrep  6.0000 6.0000  6.0000  6.000  6.0000  6.000  6.000  6.0000 5.000\n\n noon \n          AM       PM\n     0.07988 -0.07855\nrep 59.00000 60.00000\n\n Day:noon \n     noon\nDay   AM      PM     \n  1   -0.6127  0.6127\n  rep  3.0000  3.0000\n  2   -0.1343  0.1343\n  rep  3.0000  3.0000\n  3   -0.5543  0.5543\n  rep  3.0000  3.0000\n  4    0.0923 -0.0923\n  rep  3.0000  3.0000\n  5   -0.0593  0.0593\n  rep  3.0000  3.0000\n  6    0.1673 -0.1673\n  rep  3.0000  3.0000\n  7   -0.1010  0.1010\n  rep  3.0000  3.0000\n  8    0.3023 -0.3023\n  rep  3.0000  3.0000\n  9    0.5823 -0.5823\n  rep  3.0000  3.0000\n  10  -0.4560  0.4560\n  rep  3.0000  3.0000\n  11   0.8007 -0.8007\n  rep  3.0000  3.0000\n  12  -0.1243  0.1243\n  rep  3.0000  3.0000\n  13  -0.2327  0.2327\n  rep  3.0000  3.0000\n  14  -0.2193  0.2193\n  rep  3.0000  3.0000\n  15  -0.5593  0.5593\n  rep  3.0000  3.0000\n  16   0.1740 -0.1740\n  rep  3.0000  3.0000\n  17   0.4623 -0.4623\n  rep  3.0000  3.0000\n  18   0.0707 -0.0707\n  rep  3.0000  3.0000\n  19   0.5290 -0.5290\n  rep  3.0000  3.0000\n  20  -0.1912  0.1275\n  rep  2.0000  3.0000\n\n\none way ANOVA와 같이 해석\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   15.5   0.816    1.18   0.29\nnoon         1    0.7   0.748    1.08   0.30\nDay:noon    19   18.1   0.951    1.38   0.16\nResiduals   79   54.5   0.690               \n1 observation deleted due to missingness\n\n\n위 그림을 보듯이 두 변수의 영향도가 없음, ANOVA 역시 유의하지 않음\n\n\n\n\n\n\n Missing rows: 117 \n\n\n\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Ct ~ Day + noon + Day:noon, data = data)\n\n$Day\n            diff      lwr     upr   p adj\n2-1   -0.2883333 -2.04672 1.47005 1.00000\n3-1   -0.1916667 -1.95005 1.56672 1.00000\n4-1   -0.6716667 -2.43005 1.08672 0.99700\n5-1   -0.1566667 -1.91505 1.60172 1.00000\n6-1   -0.3200000 -2.07839 1.43839 1.00000\n7-1   -0.3016667 -2.06005 1.45672 1.00000\n8-1   -0.2050000 -1.96339 1.55339 1.00000\n9-1    0.4016667 -1.35672 2.16005 1.00000\n10-1  -0.5966667 -2.35505 1.16172 0.99934\n11-1   0.0866667 -1.67172 1.84505 1.00000\n12-1  -0.9583333 -2.71672 0.80005 0.89330\n13-1  -0.1633333 -1.92172 1.59505 1.00000\n14-1  -0.8700000 -2.62839 0.88839 0.95243\n15-1  -0.8766667 -2.63505 0.88172 0.94905\n16-1  -0.7433333 -2.50172 1.01505 0.99024\n17-1  -0.8116667 -2.57005 0.94672 0.97539\n18-1  -0.5666667 -2.32505 1.19172 0.99968\n19-1  -0.6750000 -2.43339 1.08339 0.99682\n20-1  -0.1406667 -1.98488 1.70355 1.00000\n3-2    0.0966667 -1.66172 1.85505 1.00000\n4-2   -0.3833333 -2.14172 1.37505 1.00000\n5-2    0.1316667 -1.62672 1.89005 1.00000\n6-2   -0.0316667 -1.79005 1.72672 1.00000\n7-2   -0.0133333 -1.77172 1.74505 1.00000\n8-2    0.0833333 -1.67505 1.84172 1.00000\n9-2    0.6900000 -1.06839 2.44839 0.99585\n10-2  -0.3083333 -2.06672 1.45005 1.00000\n11-2   0.3750000 -1.38339 2.13339 1.00000\n12-2  -0.6700000 -2.42839 1.08839 0.99709\n13-2   0.1250000 -1.63339 1.88339 1.00000\n14-2  -0.5816667 -2.34005 1.17672 0.99953\n15-2  -0.5883333 -2.34672 1.17005 0.99946\n16-2  -0.4550000 -2.21339 1.30339 0.99999\n17-2  -0.5233333 -2.28172 1.23505 0.99990\n18-2  -0.2783333 -2.03672 1.48005 1.00000\n19-2  -0.3866667 -2.14505 1.37172 1.00000\n20-2   0.1476667 -1.69655 1.99188 1.00000\n4-3   -0.4800000 -2.23839 1.27839 0.99997\n5-3    0.0350000 -1.72339 1.79339 1.00000\n6-3   -0.1283333 -1.88672 1.63005 1.00000\n7-3   -0.1100000 -1.86839 1.64839 1.00000\n8-3   -0.0133333 -1.77172 1.74505 1.00000\n9-3    0.5933333 -1.16505 2.35172 0.99939\n10-3  -0.4050000 -2.16339 1.35339 1.00000\n11-3   0.2783333 -1.48005 2.03672 1.00000\n12-3  -0.7666667 -2.52505 0.99172 0.98634\n13-3   0.0283333 -1.73005 1.78672 1.00000\n14-3  -0.6783333 -2.43672 1.08005 0.99662\n15-3  -0.6850000 -2.44339 1.07339 0.99620\n16-3  -0.5516667 -2.31005 1.20672 0.99978\n17-3  -0.6200000 -2.37839 1.13839 0.99891\n18-3  -0.3750000 -2.13339 1.38339 1.00000\n19-3  -0.4833333 -2.24172 1.27505 0.99997\n20-3   0.0510000 -1.79321 1.89521 1.00000\n5-4    0.5150000 -1.24339 2.27339 0.99992\n6-4    0.3516667 -1.40672 2.11005 1.00000\n7-4    0.3700000 -1.38839 2.12839 1.00000\n8-4    0.4666667 -1.29172 2.22505 0.99998\n9-4    1.0733333 -0.68505 2.83172 0.76824\n10-4   0.0750000 -1.68339 1.83339 1.00000\n11-4   0.7583333 -1.00005 2.51672 0.98785\n12-4  -0.2866667 -2.04505 1.47172 1.00000\n13-4   0.5083333 -1.25005 2.26672 0.99993\n14-4  -0.1983333 -1.95672 1.56005 1.00000\n15-4  -0.2050000 -1.96339 1.55339 1.00000\n16-4  -0.0716667 -1.83005 1.68672 1.00000\n17-4  -0.1400000 -1.89839 1.61839 1.00000\n18-4   0.1050000 -1.65339 1.86339 1.00000\n19-4  -0.0033333 -1.76172 1.75505 1.00000\n20-4   0.5310000 -1.31321 2.37521 0.99994\n6-5   -0.1633333 -1.92172 1.59505 1.00000\n7-5   -0.1450000 -1.90339 1.61339 1.00000\n8-5   -0.0483333 -1.80672 1.71005 1.00000\n9-5    0.5583333 -1.20005 2.31672 0.99974\n10-5  -0.4400000 -2.19839 1.31839 0.99999\n11-5   0.2433333 -1.51505 2.00172 1.00000\n12-5  -0.8016667 -2.56005 0.95672 0.97827\n13-5  -0.0066667 -1.76505 1.75172 1.00000\n14-5  -0.7133333 -2.47172 1.04505 0.99387\n15-5  -0.7200000 -2.47839 1.03839 0.99318\n16-5  -0.5866667 -2.34505 1.17172 0.99948\n17-5  -0.6550000 -2.41339 1.10339 0.99780\n18-5  -0.4100000 -2.16839 1.34839 1.00000\n19-5  -0.5183333 -2.27672 1.24005 0.99991\n20-5   0.0160000 -1.82821 1.86021 1.00000\n7-6    0.0183333 -1.74005 1.77672 1.00000\n8-6    0.1150000 -1.64339 1.87339 1.00000\n9-6    0.7216667 -1.03672 2.48005 0.99300\n10-6  -0.2766667 -2.03505 1.48172 1.00000\n11-6   0.4066667 -1.35172 2.16505 1.00000\n12-6  -0.6383333 -2.39672 1.12005 0.99841\n13-6   0.1566667 -1.60172 1.91505 1.00000\n14-6  -0.5500000 -2.30839 1.20839 0.99979\n15-6  -0.5566667 -2.31505 1.20172 0.99975\n16-6  -0.4233333 -2.18172 1.33505 1.00000\n17-6  -0.4916667 -2.25005 1.26672 0.99996\n18-6  -0.2466667 -2.00505 1.51172 1.00000\n19-6  -0.3550000 -2.11339 1.40339 1.00000\n20-6   0.1793333 -1.66488 2.02355 1.00000\n8-7    0.0966667 -1.66172 1.85505 1.00000\n9-7    0.7033333 -1.05505 2.46172 0.99480\n10-7  -0.2950000 -2.05339 1.46339 1.00000\n11-7   0.3883333 -1.37005 2.14672 1.00000\n12-7  -0.6566667 -2.41505 1.10172 0.99773\n13-7   0.1383333 -1.62005 1.89672 1.00000\n14-7  -0.5683333 -2.32672 1.19005 0.99966\n15-7  -0.5750000 -2.33339 1.18339 0.99960\n16-7  -0.4416667 -2.20005 1.31672 0.99999\n17-7  -0.5100000 -2.26839 1.24839 0.99993\n18-7  -0.2650000 -2.02339 1.49339 1.00000\n19-7  -0.3733333 -2.13172 1.38505 1.00000\n20-7   0.1610000 -1.68321 2.00521 1.00000\n9-8    0.6066667 -1.15172 2.36505 0.99918\n10-8  -0.3916667 -2.15005 1.36672 1.00000\n11-8   0.2916667 -1.46672 2.05005 1.00000\n12-8  -0.7533333 -2.51172 1.00505 0.98869\n13-8   0.0416667 -1.71672 1.80005 1.00000\n14-8  -0.6650000 -2.42339 1.09339 0.99735\n15-8  -0.6716667 -2.43005 1.08672 0.99700\n16-8  -0.5383333 -2.29672 1.22005 0.99984\n17-8  -0.6066667 -2.36505 1.15172 0.99918\n18-8  -0.3616667 -2.12005 1.39672 1.00000\n19-8  -0.4700000 -2.22839 1.28839 0.99998\n20-8   0.0643333 -1.77988 1.90855 1.00000\n10-9  -0.9983333 -2.75672 0.76005 0.85576\n11-9  -0.3150000 -2.07339 1.44339 1.00000\n12-9  -1.3600000 -3.11839 0.39839 0.35269\n13-9  -0.5650000 -2.32339 1.19339 0.99969\n14-9  -1.2716667 -3.03005 0.48672 0.47719\n15-9  -1.2783333 -3.03672 0.48005 0.46731\n16-9  -1.1450000 -2.90339 0.61339 0.66832\n17-9  -1.2133333 -2.97172 0.54505 0.56535\n18-9  -0.9683333 -2.72672 0.79005 0.88455\n19-9  -1.0766667 -2.83505 0.68172 0.76389\n20-9  -0.5423333 -2.38655 1.30188 0.99991\n11-10  0.6833333 -1.07505 2.44172 0.99631\n12-10 -0.3616667 -2.12005 1.39672 1.00000\n13-10  0.4333333 -1.32505 2.19172 0.99999\n14-10 -0.2733333 -2.03172 1.48505 1.00000\n15-10 -0.2800000 -2.03839 1.47839 1.00000\n16-10 -0.1466667 -1.90505 1.61172 1.00000\n17-10 -0.2150000 -1.97339 1.54339 1.00000\n18-10  0.0300000 -1.72839 1.78839 1.00000\n19-10 -0.0783333 -1.83672 1.68005 1.00000\n20-10  0.4560000 -1.38821 2.30021 0.99999\n12-11 -1.0450000 -2.80339 0.71339 0.80373\n13-11 -0.2500000 -2.00839 1.50839 1.00000\n14-11 -0.9566667 -2.71505 0.80172 0.89472\n15-11 -0.9633333 -2.72172 0.79505 0.88898\n16-11 -0.8300000 -2.58839 0.92839 0.96936\n17-11 -0.8983333 -2.65672 0.86005 0.93692\n18-11 -0.6533333 -2.41172 1.10505 0.99787\n19-11 -0.7616667 -2.52005 0.99672 0.98726\n20-11 -0.2273333 -2.07155 1.61688 1.00000\n13-12  0.7950000 -0.96339 2.55339 0.98004\n14-12  0.0883333 -1.67005 1.84672 1.00000\n15-12  0.0816667 -1.67672 1.84005 1.00000\n16-12  0.2150000 -1.54339 1.97339 1.00000\n17-12  0.1466667 -1.61172 1.90505 1.00000\n18-12  0.3916667 -1.36672 2.15005 1.00000\n19-12  0.2833333 -1.47505 2.04172 1.00000\n20-12  0.8176667 -1.02655 2.66188 0.98369\n14-13 -0.7066667 -2.46505 1.05172 0.99450\n15-13 -0.7133333 -2.47172 1.04505 0.99387\n16-13 -0.5800000 -2.33839 1.17839 0.99955\n17-13 -0.6483333 -2.40672 1.11005 0.99807\n18-13 -0.4033333 -2.16172 1.35505 1.00000\n19-13 -0.5116667 -2.27005 1.24672 0.99992\n20-13  0.0226667 -1.82155 1.86688 1.00000\n15-14 -0.0066667 -1.76505 1.75172 1.00000\n16-14  0.1266667 -1.63172 1.88505 1.00000\n17-14  0.0583333 -1.70005 1.81672 1.00000\n18-14  0.3033333 -1.45505 2.06172 1.00000\n19-14  0.1950000 -1.56339 1.95339 1.00000\n20-14  0.7293333 -1.11488 2.57355 0.99545\n16-15  0.1333333 -1.62505 1.89172 1.00000\n17-15  0.0650000 -1.69339 1.82339 1.00000\n18-15  0.3100000 -1.44839 2.06839 1.00000\n19-15  0.2016667 -1.55672 1.96005 1.00000\n20-15  0.7360000 -1.10821 2.58021 0.99493\n17-16 -0.0683333 -1.82672 1.69005 1.00000\n18-16  0.1766667 -1.58172 1.93505 1.00000\n19-16  0.0683333 -1.69005 1.82672 1.00000\n20-16  0.6026667 -1.24155 2.44688 0.99961\n18-17  0.2450000 -1.51339 2.00339 1.00000\n19-17  0.1366667 -1.62172 1.89505 1.00000\n20-17  0.6710000 -1.17321 2.51521 0.99837\n19-18 -0.1083333 -1.86672 1.65005 1.00000\n20-18  0.4260000 -1.41821 2.27021 1.00000\n20-19  0.5343333 -1.30988 2.37855 0.99993\n\n$noon\n          diff      lwr     upr   p adj\nPM-AM -0.15843 -0.46155 0.14469 0.30135\n\n$`Day:noon`\n                  diff      lwr     upr   p adj\n2:AM-1:AM    0.1900000 -2.55252 2.93252 1.00000\n3:AM-1:AM   -0.1333333 -2.87586 2.60919 1.00000\n4:AM-1:AM    0.0333333 -2.70919 2.77586 1.00000\n5:AM-1:AM    0.3966667 -2.34586 3.13919 1.00000\n6:AM-1:AM    0.4600000 -2.28252 3.20252 1.00000\n7:AM-1:AM    0.2100000 -2.53252 2.95252 1.00000\n8:AM-1:AM    0.7100000 -2.03252 3.45252 1.00000\n9:AM-1:AM    1.5966667 -1.14586 4.33919 0.92887\n10:AM-1:AM  -0.4400000 -3.18252 2.30252 1.00000\n11:AM-1:AM   1.5000000 -1.24252 4.24252 0.96592\n12:AM-1:AM  -0.4700000 -3.21252 2.27252 1.00000\n13:AM-1:AM   0.2166667 -2.52586 2.95919 1.00000\n14:AM-1:AM  -0.4766667 -3.21919 2.26586 1.00000\n15:AM-1:AM  -0.8233333 -3.56586 1.91919 1.00000\n16:AM-1:AM   0.0433333 -2.69919 2.78586 1.00000\n17:AM-1:AM   0.2633333 -2.47919 3.00586 1.00000\n18:AM-1:AM   0.1166667 -2.62586 2.85919 1.00000\n19:AM-1:AM   0.4666667 -2.27586 3.20919 1.00000\n20:AM-1:AM   0.2966667 -2.76957 3.36290 1.00000\n1:PM-1:AM    1.0666667 -1.67586 3.80919 0.99990\n2:PM-1:AM    0.3000000 -2.44252 3.04252 1.00000\n3:PM-1:AM    0.8166667 -1.92586 3.55919 1.00000\n4:PM-1:AM   -0.3100000 -3.05252 2.43252 1.00000\n5:PM-1:AM    0.3566667 -2.38586 3.09919 1.00000\n6:PM-1:AM   -0.0333333 -2.77586 2.70919 1.00000\n7:PM-1:AM    0.2533333 -2.48919 2.99586 1.00000\n8:PM-1:AM   -0.0533333 -2.79586 2.68919 1.00000\n9:PM-1:AM    0.2733333 -2.46919 3.01586 1.00000\n10:PM-1:AM   0.3133333 -2.42919 3.05586 1.00000\n11:PM-1:AM  -0.2600000 -3.00252 2.48252 1.00000\n12:PM-1:AM  -0.3800000 -3.12252 2.36252 1.00000\n13:PM-1:AM   0.5233333 -2.21919 3.26586 1.00000\n14:PM-1:AM  -0.1966667 -2.93919 2.54586 1.00000\n15:PM-1:AM   0.1366667 -2.60586 2.87919 1.00000\n16:PM-1:AM  -0.4633333 -3.20586 2.27919 1.00000\n17:PM-1:AM  -0.8200000 -3.56252 1.92252 1.00000\n18:PM-1:AM  -0.1833333 -2.92586 2.55919 1.00000\n19:PM-1:AM  -0.7500000 -3.49252 1.99252 1.00000\n20:PM-1:AM   0.4566667 -2.28586 3.19919 1.00000\n3:AM-2:AM   -0.3233333 -3.06586 2.41919 1.00000\n4:AM-2:AM   -0.1566667 -2.89919 2.58586 1.00000\n5:AM-2:AM    0.2066667 -2.53586 2.94919 1.00000\n6:AM-2:AM    0.2700000 -2.47252 3.01252 1.00000\n7:AM-2:AM    0.0200000 -2.72252 2.76252 1.00000\n8:AM-2:AM    0.5200000 -2.22252 3.26252 1.00000\n9:AM-2:AM    1.4066667 -1.33586 4.14919 0.98570\n10:AM-2:AM  -0.6300000 -3.37252 2.11252 1.00000\n11:AM-2:AM   1.3100000 -1.43252 4.05252 0.99521\n12:AM-2:AM  -0.6600000 -3.40252 2.08252 1.00000\n13:AM-2:AM   0.0266667 -2.71586 2.76919 1.00000\n14:AM-2:AM  -0.6666667 -3.40919 2.07586 1.00000\n15:AM-2:AM  -1.0133333 -3.75586 1.72919 0.99997\n16:AM-2:AM  -0.1466667 -2.88919 2.59586 1.00000\n17:AM-2:AM   0.0733333 -2.66919 2.81586 1.00000\n18:AM-2:AM  -0.0733333 -2.81586 2.66919 1.00000\n19:AM-2:AM   0.2766667 -2.46586 3.01919 1.00000\n20:AM-2:AM   0.1066667 -2.95957 3.17290 1.00000\n1:PM-2:AM    0.8766667 -1.86586 3.61919 1.00000\n2:PM-2:AM    0.1100000 -2.63252 2.85252 1.00000\n3:PM-2:AM    0.6266667 -2.11586 3.36919 1.00000\n4:PM-2:AM   -0.5000000 -3.24252 2.24252 1.00000\n5:PM-2:AM    0.1666667 -2.57586 2.90919 1.00000\n6:PM-2:AM   -0.2233333 -2.96586 2.51919 1.00000\n7:PM-2:AM    0.0633333 -2.67919 2.80586 1.00000\n8:PM-2:AM   -0.2433333 -2.98586 2.49919 1.00000\n9:PM-2:AM    0.0833333 -2.65919 2.82586 1.00000\n10:PM-2:AM   0.1233333 -2.61919 2.86586 1.00000\n11:PM-2:AM  -0.4500000 -3.19252 2.29252 1.00000\n12:PM-2:AM  -0.5700000 -3.31252 2.17252 1.00000\n13:PM-2:AM   0.3333333 -2.40919 3.07586 1.00000\n14:PM-2:AM  -0.3866667 -3.12919 2.35586 1.00000\n15:PM-2:AM  -0.0533333 -2.79586 2.68919 1.00000\n16:PM-2:AM  -0.6533333 -3.39586 2.08919 1.00000\n17:PM-2:AM  -1.0100000 -3.75252 1.73252 0.99997\n18:PM-2:AM  -0.3733333 -3.11586 2.36919 1.00000\n19:PM-2:AM  -0.9400000 -3.68252 1.80252 0.99999\n20:PM-2:AM   0.2666667 -2.47586 3.00919 1.00000\n4:AM-3:AM    0.1666667 -2.57586 2.90919 1.00000\n5:AM-3:AM    0.5300000 -2.21252 3.27252 1.00000\n6:AM-3:AM    0.5933333 -2.14919 3.33586 1.00000\n7:AM-3:AM    0.3433333 -2.39919 3.08586 1.00000\n8:AM-3:AM    0.8433333 -1.89919 3.58586 1.00000\n9:AM-3:AM    1.7300000 -1.01252 4.47252 0.84321\n10:AM-3:AM  -0.3066667 -3.04919 2.43586 1.00000\n11:AM-3:AM   1.6333333 -1.10919 4.37586 0.90946\n12:AM-3:AM  -0.3366667 -3.07919 2.40586 1.00000\n13:AM-3:AM   0.3500000 -2.39252 3.09252 1.00000\n14:AM-3:AM  -0.3433333 -3.08586 2.39919 1.00000\n15:AM-3:AM  -0.6900000 -3.43252 2.05252 1.00000\n16:AM-3:AM   0.1766667 -2.56586 2.91919 1.00000\n17:AM-3:AM   0.3966667 -2.34586 3.13919 1.00000\n18:AM-3:AM   0.2500000 -2.49252 2.99252 1.00000\n19:AM-3:AM   0.6000000 -2.14252 3.34252 1.00000\n20:AM-3:AM   0.4300000 -2.63623 3.49623 1.00000\n1:PM-3:AM    1.2000000 -1.54252 3.94252 0.99897\n2:PM-3:AM    0.4333333 -2.30919 3.17586 1.00000\n3:PM-3:AM    0.9500000 -1.79252 3.69252 0.99999\n4:PM-3:AM   -0.1766667 -2.91919 2.56586 1.00000\n5:PM-3:AM    0.4900000 -2.25252 3.23252 1.00000\n6:PM-3:AM    0.1000000 -2.64252 2.84252 1.00000\n7:PM-3:AM    0.3866667 -2.35586 3.12919 1.00000\n8:PM-3:AM    0.0800000 -2.66252 2.82252 1.00000\n9:PM-3:AM    0.4066667 -2.33586 3.14919 1.00000\n10:PM-3:AM   0.4466667 -2.29586 3.18919 1.00000\n11:PM-3:AM  -0.1266667 -2.86919 2.61586 1.00000\n12:PM-3:AM  -0.2466667 -2.98919 2.49586 1.00000\n13:PM-3:AM   0.6566667 -2.08586 3.39919 1.00000\n14:PM-3:AM  -0.0633333 -2.80586 2.67919 1.00000\n15:PM-3:AM   0.2700000 -2.47252 3.01252 1.00000\n16:PM-3:AM  -0.3300000 -3.07252 2.41252 1.00000\n17:PM-3:AM  -0.6866667 -3.42919 2.05586 1.00000\n18:PM-3:AM  -0.0500000 -2.79252 2.69252 1.00000\n19:PM-3:AM  -0.6166667 -3.35919 2.12586 1.00000\n20:PM-3:AM   0.5900000 -2.15252 3.33252 1.00000\n5:AM-4:AM    0.3633333 -2.37919 3.10586 1.00000\n6:AM-4:AM    0.4266667 -2.31586 3.16919 1.00000\n7:AM-4:AM    0.1766667 -2.56586 2.91919 1.00000\n8:AM-4:AM    0.6766667 -2.06586 3.41919 1.00000\n9:AM-4:AM    1.5633333 -1.17919 4.30586 0.94386\n10:AM-4:AM  -0.4733333 -3.21586 2.26919 1.00000\n11:AM-4:AM   1.4666667 -1.27586 4.20919 0.97452\n12:AM-4:AM  -0.5033333 -3.24586 2.23919 1.00000\n13:AM-4:AM   0.1833333 -2.55919 2.92586 1.00000\n14:AM-4:AM  -0.5100000 -3.25252 2.23252 1.00000\n15:AM-4:AM  -0.8566667 -3.59919 1.88586 1.00000\n16:AM-4:AM   0.0100000 -2.73252 2.75252 1.00000\n17:AM-4:AM   0.2300000 -2.51252 2.97252 1.00000\n18:AM-4:AM   0.0833333 -2.65919 2.82586 1.00000\n19:AM-4:AM   0.4333333 -2.30919 3.17586 1.00000\n20:AM-4:AM   0.2633333 -2.80290 3.32957 1.00000\n1:PM-4:AM    1.0333333 -1.70919 3.77586 0.99995\n2:PM-4:AM    0.2666667 -2.47586 3.00919 1.00000\n3:PM-4:AM    0.7833333 -1.95919 3.52586 1.00000\n4:PM-4:AM   -0.3433333 -3.08586 2.39919 1.00000\n5:PM-4:AM    0.3233333 -2.41919 3.06586 1.00000\n6:PM-4:AM   -0.0666667 -2.80919 2.67586 1.00000\n7:PM-4:AM    0.2200000 -2.52252 2.96252 1.00000\n8:PM-4:AM   -0.0866667 -2.82919 2.65586 1.00000\n9:PM-4:AM    0.2400000 -2.50252 2.98252 1.00000\n10:PM-4:AM   0.2800000 -2.46252 3.02252 1.00000\n11:PM-4:AM  -0.2933333 -3.03586 2.44919 1.00000\n12:PM-4:AM  -0.4133333 -3.15586 2.32919 1.00000\n13:PM-4:AM   0.4900000 -2.25252 3.23252 1.00000\n14:PM-4:AM  -0.2300000 -2.97252 2.51252 1.00000\n15:PM-4:AM   0.1033333 -2.63919 2.84586 1.00000\n16:PM-4:AM  -0.4966667 -3.23919 2.24586 1.00000\n17:PM-4:AM  -0.8533333 -3.59586 1.88919 1.00000\n18:PM-4:AM  -0.2166667 -2.95919 2.52586 1.00000\n19:PM-4:AM  -0.7833333 -3.52586 1.95919 1.00000\n20:PM-4:AM   0.4233333 -2.31919 3.16586 1.00000\n6:AM-5:AM    0.0633333 -2.67919 2.80586 1.00000\n7:AM-5:AM   -0.1866667 -2.92919 2.55586 1.00000\n8:AM-5:AM    0.3133333 -2.42919 3.05586 1.00000\n9:AM-5:AM    1.2000000 -1.54252 3.94252 0.99897\n10:AM-5:AM  -0.8366667 -3.57919 1.90586 1.00000\n11:AM-5:AM   1.1033333 -1.63919 3.84586 0.99980\n12:AM-5:AM  -0.8666667 -3.60919 1.87586 1.00000\n13:AM-5:AM  -0.1800000 -2.92252 2.56252 1.00000\n14:AM-5:AM  -0.8733333 -3.61586 1.86919 1.00000\n15:AM-5:AM  -1.2200000 -3.96252 1.52252 0.99860\n16:AM-5:AM  -0.3533333 -3.09586 2.38919 1.00000\n17:AM-5:AM  -0.1333333 -2.87586 2.60919 1.00000\n18:AM-5:AM  -0.2800000 -3.02252 2.46252 1.00000\n19:AM-5:AM   0.0700000 -2.67252 2.81252 1.00000\n20:AM-5:AM  -0.1000000 -3.16623 2.96623 1.00000\n1:PM-5:AM    0.6700000 -2.07252 3.41252 1.00000\n2:PM-5:AM   -0.0966667 -2.83919 2.64586 1.00000\n3:PM-5:AM    0.4200000 -2.32252 3.16252 1.00000\n4:PM-5:AM   -0.7066667 -3.44919 2.03586 1.00000\n5:PM-5:AM   -0.0400000 -2.78252 2.70252 1.00000\n6:PM-5:AM   -0.4300000 -3.17252 2.31252 1.00000\n7:PM-5:AM   -0.1433333 -2.88586 2.59919 1.00000\n8:PM-5:AM   -0.4500000 -3.19252 2.29252 1.00000\n9:PM-5:AM   -0.1233333 -2.86586 2.61919 1.00000\n10:PM-5:AM  -0.0833333 -2.82586 2.65919 1.00000\n11:PM-5:AM  -0.6566667 -3.39919 2.08586 1.00000\n12:PM-5:AM  -0.7766667 -3.51919 1.96586 1.00000\n13:PM-5:AM   0.1266667 -2.61586 2.86919 1.00000\n14:PM-5:AM  -0.5933333 -3.33586 2.14919 1.00000\n15:PM-5:AM  -0.2600000 -3.00252 2.48252 1.00000\n16:PM-5:AM  -0.8600000 -3.60252 1.88252 1.00000\n17:PM-5:AM  -1.2166667 -3.95919 1.52586 0.99867\n18:PM-5:AM  -0.5800000 -3.32252 2.16252 1.00000\n19:PM-5:AM  -1.1466667 -3.88919 1.59586 0.99957\n20:PM-5:AM   0.0600000 -2.68252 2.80252 1.00000\n7:AM-6:AM   -0.2500000 -2.99252 2.49252 1.00000\n8:AM-6:AM    0.2500000 -2.49252 2.99252 1.00000\n9:AM-6:AM    1.1366667 -1.60586 3.87919 0.99964\n10:AM-6:AM  -0.9000000 -3.64252 1.84252 1.00000\n11:AM-6:AM   1.0400000 -1.70252 3.78252 0.99994\n12:AM-6:AM  -0.9300000 -3.67252 1.81252 1.00000\n13:AM-6:AM  -0.2433333 -2.98586 2.49919 1.00000\n14:AM-6:AM  -0.9366667 -3.67919 1.80586 1.00000\n15:AM-6:AM  -1.2833333 -4.02586 1.45919 0.99660\n16:AM-6:AM  -0.4166667 -3.15919 2.32586 1.00000\n17:AM-6:AM  -0.1966667 -2.93919 2.54586 1.00000\n18:AM-6:AM  -0.3433333 -3.08586 2.39919 1.00000\n19:AM-6:AM   0.0066667 -2.73586 2.74919 1.00000\n20:AM-6:AM  -0.1633333 -3.22957 2.90290 1.00000\n1:PM-6:AM    0.6066667 -2.13586 3.34919 1.00000\n2:PM-6:AM   -0.1600000 -2.90252 2.58252 1.00000\n3:PM-6:AM    0.3566667 -2.38586 3.09919 1.00000\n4:PM-6:AM   -0.7700000 -3.51252 1.97252 1.00000\n5:PM-6:AM   -0.1033333 -2.84586 2.63919 1.00000\n6:PM-6:AM   -0.4933333 -3.23586 2.24919 1.00000\n7:PM-6:AM   -0.2066667 -2.94919 2.53586 1.00000\n8:PM-6:AM   -0.5133333 -3.25586 2.22919 1.00000\n9:PM-6:AM   -0.1866667 -2.92919 2.55586 1.00000\n10:PM-6:AM  -0.1466667 -2.88919 2.59586 1.00000\n11:PM-6:AM  -0.7200000 -3.46252 2.02252 1.00000\n12:PM-6:AM  -0.8400000 -3.58252 1.90252 1.00000\n13:PM-6:AM   0.0633333 -2.67919 2.80586 1.00000\n14:PM-6:AM  -0.6566667 -3.39919 2.08586 1.00000\n15:PM-6:AM  -0.3233333 -3.06586 2.41919 1.00000\n16:PM-6:AM  -0.9233333 -3.66586 1.81919 1.00000\n17:PM-6:AM  -1.2800000 -4.02252 1.46252 0.99674\n18:PM-6:AM  -0.6433333 -3.38586 2.09919 1.00000\n19:PM-6:AM  -1.2100000 -3.95252 1.53252 0.99880\n20:PM-6:AM  -0.0033333 -2.74586 2.73919 1.00000\n8:AM-7:AM    0.5000000 -2.24252 3.24252 1.00000\n9:AM-7:AM    1.3866667 -1.35586 4.12919 0.98840\n10:AM-7:AM  -0.6500000 -3.39252 2.09252 1.00000\n11:AM-7:AM   1.2900000 -1.45252 4.03252 0.99629\n12:AM-7:AM  -0.6800000 -3.42252 2.06252 1.00000\n13:AM-7:AM   0.0066667 -2.73586 2.74919 1.00000\n14:AM-7:AM  -0.6866667 -3.42919 2.05586 1.00000\n15:AM-7:AM  -1.0333333 -3.77586 1.70919 0.99995\n16:AM-7:AM  -0.1666667 -2.90919 2.57586 1.00000\n17:AM-7:AM   0.0533333 -2.68919 2.79586 1.00000\n18:AM-7:AM  -0.0933333 -2.83586 2.64919 1.00000\n19:AM-7:AM   0.2566667 -2.48586 2.99919 1.00000\n20:AM-7:AM   0.0866667 -2.97957 3.15290 1.00000\n1:PM-7:AM    0.8566667 -1.88586 3.59919 1.00000\n2:PM-7:AM    0.0900000 -2.65252 2.83252 1.00000\n3:PM-7:AM    0.6066667 -2.13586 3.34919 1.00000\n4:PM-7:AM   -0.5200000 -3.26252 2.22252 1.00000\n5:PM-7:AM    0.1466667 -2.59586 2.88919 1.00000\n6:PM-7:AM   -0.2433333 -2.98586 2.49919 1.00000\n7:PM-7:AM    0.0433333 -2.69919 2.78586 1.00000\n8:PM-7:AM   -0.2633333 -3.00586 2.47919 1.00000\n9:PM-7:AM    0.0633333 -2.67919 2.80586 1.00000\n10:PM-7:AM   0.1033333 -2.63919 2.84586 1.00000\n11:PM-7:AM  -0.4700000 -3.21252 2.27252 1.00000\n12:PM-7:AM  -0.5900000 -3.33252 2.15252 1.00000\n13:PM-7:AM   0.3133333 -2.42919 3.05586 1.00000\n14:PM-7:AM  -0.4066667 -3.14919 2.33586 1.00000\n15:PM-7:AM  -0.0733333 -2.81586 2.66919 1.00000\n16:PM-7:AM  -0.6733333 -3.41586 2.06919 1.00000\n17:PM-7:AM  -1.0300000 -3.77252 1.71252 0.99996\n18:PM-7:AM  -0.3933333 -3.13586 2.34919 1.00000\n19:PM-7:AM  -0.9600000 -3.70252 1.78252 0.99999\n20:PM-7:AM   0.2466667 -2.49586 2.98919 1.00000\n9:AM-8:AM    0.8866667 -1.85586 3.62919 1.00000\n10:AM-8:AM  -1.1500000 -3.89252 1.59252 0.99954\n11:AM-8:AM   0.7900000 -1.95252 3.53252 1.00000\n12:AM-8:AM  -1.1800000 -3.92252 1.56252 0.99925\n13:AM-8:AM  -0.4933333 -3.23586 2.24919 1.00000\n14:AM-8:AM  -1.1866667 -3.92919 1.55586 0.99916\n15:AM-8:AM  -1.5333333 -4.27586 1.20919 0.95531\n16:AM-8:AM  -0.6666667 -3.40919 2.07586 1.00000\n17:AM-8:AM  -0.4466667 -3.18919 2.29586 1.00000\n18:AM-8:AM  -0.5933333 -3.33586 2.14919 1.00000\n19:AM-8:AM  -0.2433333 -2.98586 2.49919 1.00000\n20:AM-8:AM  -0.4133333 -3.47957 2.65290 1.00000\n1:PM-8:AM    0.3566667 -2.38586 3.09919 1.00000\n2:PM-8:AM   -0.4100000 -3.15252 2.33252 1.00000\n3:PM-8:AM    0.1066667 -2.63586 2.84919 1.00000\n4:PM-8:AM   -1.0200000 -3.76252 1.72252 0.99996\n5:PM-8:AM   -0.3533333 -3.09586 2.38919 1.00000\n6:PM-8:AM   -0.7433333 -3.48586 1.99919 1.00000\n7:PM-8:AM   -0.4566667 -3.19919 2.28586 1.00000\n8:PM-8:AM   -0.7633333 -3.50586 1.97919 1.00000\n9:PM-8:AM   -0.4366667 -3.17919 2.30586 1.00000\n10:PM-8:AM  -0.3966667 -3.13919 2.34586 1.00000\n11:PM-8:AM  -0.9700000 -3.71252 1.77252 0.99999\n12:PM-8:AM  -1.0900000 -3.83252 1.65252 0.99985\n13:PM-8:AM  -0.1866667 -2.92919 2.55586 1.00000\n14:PM-8:AM  -0.9066667 -3.64919 1.83586 1.00000\n15:PM-8:AM  -0.5733333 -3.31586 2.16919 1.00000\n16:PM-8:AM  -1.1733333 -3.91586 1.56919 0.99933\n17:PM-8:AM  -1.5300000 -4.27252 1.21252 0.95647\n18:PM-8:AM  -0.8933333 -3.63586 1.84919 1.00000\n19:PM-8:AM  -1.4600000 -4.20252 1.28252 0.97602\n20:PM-8:AM  -0.2533333 -2.99586 2.48919 1.00000\n10:AM-9:AM  -2.0366667 -4.77919 0.70586 0.52759\n11:AM-9:AM  -0.0966667 -2.83919 2.64586 1.00000\n12:AM-9:AM  -2.0666667 -4.80919 0.67586 0.49403\n13:AM-9:AM  -1.3800000 -4.12252 1.36252 0.98920\n14:AM-9:AM  -2.0733333 -4.81586 0.66919 0.48663\n15:AM-9:AM  -2.4200000 -5.16252 0.32252 0.17781\n16:AM-9:AM  -1.5533333 -4.29586 1.18919 0.94789\n17:AM-9:AM  -1.3333333 -4.07586 1.40919 0.99364\n18:AM-9:AM  -1.4800000 -4.22252 1.26252 0.97131\n19:AM-9:AM  -1.1300000 -3.87252 1.61252 0.99968\n20:AM-9:AM  -1.3000000 -4.36623 1.76623 0.99943\n1:PM-9:AM   -0.5300000 -3.27252 2.21252 1.00000\n2:PM-9:AM   -1.2966667 -4.03919 1.44586 0.99595\n3:PM-9:AM   -0.7800000 -3.52252 1.96252 1.00000\n4:PM-9:AM   -1.9066667 -4.64919 0.83586 0.67322\n5:PM-9:AM   -1.2400000 -3.98252 1.50252 0.99812\n6:PM-9:AM   -1.6300000 -4.37252 1.11252 0.91136\n7:PM-9:AM   -1.3433333 -4.08586 1.39919 0.99284\n8:PM-9:AM   -1.6500000 -4.39252 1.09252 0.89961\n9:PM-9:AM   -1.3233333 -4.06586 1.41919 0.99436\n10:PM-9:AM  -1.2833333 -4.02586 1.45919 0.99660\n11:PM-9:AM  -1.8566667 -4.59919 0.88586 0.72631\n12:PM-9:AM  -1.9766667 -4.71919 0.76586 0.59536\n13:PM-9:AM  -1.0733333 -3.81586 1.66919 0.99989\n14:PM-9:AM  -1.7933333 -4.53586 0.94919 0.78850\n15:PM-9:AM  -1.4600000 -4.20252 1.28252 0.97602\n16:PM-9:AM  -2.0600000 -4.80252 0.68252 0.50145\n17:PM-9:AM  -2.4166667 -5.15919 0.32586 0.17988\n18:PM-9:AM  -1.7800000 -4.52252 0.96252 0.80070\n19:PM-9:AM  -2.3466667 -5.08919 0.39586 0.22767\n20:PM-9:AM  -1.1400000 -3.88252 1.60252 0.99962\n11:AM-10:AM  1.9400000 -0.80252 4.68252 0.63649\n12:AM-10:AM -0.0300000 -2.77252 2.71252 1.00000\n13:AM-10:AM  0.6566667 -2.08586 3.39919 1.00000\n14:AM-10:AM -0.0366667 -2.77919 2.70586 1.00000\n15:AM-10:AM -0.3833333 -3.12586 2.35919 1.00000\n16:AM-10:AM  0.4833333 -2.25919 3.22586 1.00000\n17:AM-10:AM  0.7033333 -2.03919 3.44586 1.00000\n18:AM-10:AM  0.5566667 -2.18586 3.29919 1.00000\n19:AM-10:AM  0.9066667 -1.83586 3.64919 1.00000\n20:AM-10:AM  0.7366667 -2.32957 3.80290 1.00000\n1:PM-10:AM   1.5066667 -1.23586 4.24919 0.96397\n2:PM-10:AM   0.7400000 -2.00252 3.48252 1.00000\n3:PM-10:AM   1.2566667 -1.48586 3.99919 0.99763\n4:PM-10:AM   0.1300000 -2.61252 2.87252 1.00000\n5:PM-10:AM   0.7966667 -1.94586 3.53919 1.00000\n6:PM-10:AM   0.4066667 -2.33586 3.14919 1.00000\n7:PM-10:AM   0.6933333 -2.04919 3.43586 1.00000\n8:PM-10:AM   0.3866667 -2.35586 3.12919 1.00000\n9:PM-10:AM   0.7133333 -2.02919 3.45586 1.00000\n10:PM-10:AM  0.7533333 -1.98919 3.49586 1.00000\n11:PM-10:AM  0.1800000 -2.56252 2.92252 1.00000\n12:PM-10:AM  0.0600000 -2.68252 2.80252 1.00000\n13:PM-10:AM  0.9633333 -1.77919 3.70586 0.99999\n14:PM-10:AM  0.2433333 -2.49919 2.98586 1.00000\n15:PM-10:AM  0.5766667 -2.16586 3.31919 1.00000\n16:PM-10:AM -0.0233333 -2.76586 2.71919 1.00000\n17:PM-10:AM -0.3800000 -3.12252 2.36252 1.00000\n18:PM-10:AM  0.2566667 -2.48586 2.99919 1.00000\n19:PM-10:AM -0.3100000 -3.05252 2.43252 1.00000\n20:PM-10:AM  0.8966667 -1.84586 3.63919 1.00000\n12:AM-11:AM -1.9700000 -4.71252 0.77252 0.60287\n13:AM-11:AM -1.2833333 -4.02586 1.45919 0.99660\n14:AM-11:AM -1.9766667 -4.71919 0.76586 0.59536\n15:AM-11:AM -2.3233333 -5.06586 0.41919 0.24541\n16:AM-11:AM -1.4566667 -4.19919 1.28586 0.97675\n17:AM-11:AM -1.2366667 -3.97919 1.50586 0.99821\n18:AM-11:AM -1.3833333 -4.12586 1.35919 0.98880\n19:AM-11:AM -1.0333333 -3.77586 1.70919 0.99995\n20:AM-11:AM -1.2033333 -4.26957 1.86290 0.99988\n1:PM-11:AM  -0.4333333 -3.17586 2.30919 1.00000\n2:PM-11:AM  -1.2000000 -3.94252 1.54252 0.99897\n3:PM-11:AM  -0.6833333 -3.42586 2.05919 1.00000\n4:PM-11:AM  -1.8100000 -4.55252 0.93252 0.77279\n5:PM-11:AM  -1.1433333 -3.88586 1.59919 0.99959\n6:PM-11:AM  -1.5333333 -4.27586 1.20919 0.95531\n7:PM-11:AM  -1.2466667 -3.98919 1.49586 0.99794\n8:PM-11:AM  -1.5533333 -4.29586 1.18919 0.94789\n9:PM-11:AM  -1.2266667 -3.96919 1.51586 0.99846\n10:PM-11:AM -1.1866667 -3.92919 1.55586 0.99916\n11:PM-11:AM -1.7600000 -4.50252 0.98252 0.81833\n12:PM-11:AM -1.8800000 -4.62252 0.86252 0.70190\n13:PM-11:AM -0.9766667 -3.71919 1.76586 0.99999\n14:PM-11:AM -1.6966667 -4.43919 1.04586 0.86852\n15:PM-11:AM -1.3633333 -4.10586 1.37919 0.99101\n16:PM-11:AM -1.9633333 -4.70586 0.77919 0.61038\n17:PM-11:AM -2.3200000 -5.06252 0.42252 0.24802\n18:PM-11:AM -1.6833333 -4.42586 1.05919 0.87792\n19:PM-11:AM -2.2500000 -4.99252 0.49252 0.30702\n20:PM-11:AM -1.0433333 -3.78586 1.69919 0.99994\n13:AM-12:AM  0.6866667 -2.05586 3.42919 1.00000\n14:AM-12:AM -0.0066667 -2.74919 2.73586 1.00000\n15:AM-12:AM -0.3533333 -3.09586 2.38919 1.00000\n16:AM-12:AM  0.5133333 -2.22919 3.25586 1.00000\n17:AM-12:AM  0.7333333 -2.00919 3.47586 1.00000\n18:AM-12:AM  0.5866667 -2.15586 3.32919 1.00000\n19:AM-12:AM  0.9366667 -1.80586 3.67919 1.00000\n20:AM-12:AM  0.7666667 -2.29957 3.83290 1.00000\n1:PM-12:AM   1.5366667 -1.20586 4.27919 0.95413\n2:PM-12:AM   0.7700000 -1.97252 3.51252 1.00000\n3:PM-12:AM   1.2866667 -1.45586 4.02919 0.99645\n4:PM-12:AM   0.1600000 -2.58252 2.90252 1.00000\n5:PM-12:AM   0.8266667 -1.91586 3.56919 1.00000\n6:PM-12:AM   0.4366667 -2.30586 3.17919 1.00000\n7:PM-12:AM   0.7233333 -2.01919 3.46586 1.00000\n8:PM-12:AM   0.4166667 -2.32586 3.15919 1.00000\n9:PM-12:AM   0.7433333 -1.99919 3.48586 1.00000\n10:PM-12:AM  0.7833333 -1.95919 3.52586 1.00000\n11:PM-12:AM  0.2100000 -2.53252 2.95252 1.00000\n12:PM-12:AM  0.0900000 -2.65252 2.83252 1.00000\n13:PM-12:AM  0.9933333 -1.74919 3.73586 0.99998\n14:PM-12:AM  0.2733333 -2.46919 3.01586 1.00000\n15:PM-12:AM  0.6066667 -2.13586 3.34919 1.00000\n16:PM-12:AM  0.0066667 -2.73586 2.74919 1.00000\n17:PM-12:AM -0.3500000 -3.09252 2.39252 1.00000\n18:PM-12:AM  0.2866667 -2.45586 3.02919 1.00000\n19:PM-12:AM -0.2800000 -3.02252 2.46252 1.00000\n20:PM-12:AM  0.9266667 -1.81586 3.66919 1.00000\n14:AM-13:AM -0.6933333 -3.43586 2.04919 1.00000\n15:AM-13:AM -1.0400000 -3.78252 1.70252 0.99994\n16:AM-13:AM -0.1733333 -2.91586 2.56919 1.00000\n17:AM-13:AM  0.0466667 -2.69586 2.78919 1.00000\n18:AM-13:AM -0.1000000 -2.84252 2.64252 1.00000\n19:AM-13:AM  0.2500000 -2.49252 2.99252 1.00000\n20:AM-13:AM  0.0800000 -2.98623 3.14623 1.00000\n1:PM-13:AM   0.8500000 -1.89252 3.59252 1.00000\n2:PM-13:AM   0.0833333 -2.65919 2.82586 1.00000\n3:PM-13:AM   0.6000000 -2.14252 3.34252 1.00000\n4:PM-13:AM  -0.5266667 -3.26919 2.21586 1.00000\n5:PM-13:AM   0.1400000 -2.60252 2.88252 1.00000\n6:PM-13:AM  -0.2500000 -2.99252 2.49252 1.00000\n7:PM-13:AM   0.0366667 -2.70586 2.77919 1.00000\n8:PM-13:AM  -0.2700000 -3.01252 2.47252 1.00000\n9:PM-13:AM   0.0566667 -2.68586 2.79919 1.00000\n10:PM-13:AM  0.0966667 -2.64586 2.83919 1.00000\n11:PM-13:AM -0.4766667 -3.21919 2.26586 1.00000\n12:PM-13:AM -0.5966667 -3.33919 2.14586 1.00000\n13:PM-13:AM  0.3066667 -2.43586 3.04919 1.00000\n14:PM-13:AM -0.4133333 -3.15586 2.32919 1.00000\n15:PM-13:AM -0.0800000 -2.82252 2.66252 1.00000\n16:PM-13:AM -0.6800000 -3.42252 2.06252 1.00000\n17:PM-13:AM -1.0366667 -3.77919 1.70586 0.99995\n18:PM-13:AM -0.4000000 -3.14252 2.34252 1.00000\n19:PM-13:AM -0.9666667 -3.70919 1.77586 0.99999\n20:PM-13:AM  0.2400000 -2.50252 2.98252 1.00000\n15:AM-14:AM -0.3466667 -3.08919 2.39586 1.00000\n16:AM-14:AM  0.5200000 -2.22252 3.26252 1.00000\n17:AM-14:AM  0.7400000 -2.00252 3.48252 1.00000\n18:AM-14:AM  0.5933333 -2.14919 3.33586 1.00000\n19:AM-14:AM  0.9433333 -1.79919 3.68586 0.99999\n20:AM-14:AM  0.7733333 -2.29290 3.83957 1.00000\n1:PM-14:AM   1.5433333 -1.19919 4.28586 0.95170\n2:PM-14:AM   0.7766667 -1.96586 3.51919 1.00000\n3:PM-14:AM   1.2933333 -1.44919 4.03586 0.99612\n4:PM-14:AM   0.1666667 -2.57586 2.90919 1.00000\n5:PM-14:AM   0.8333333 -1.90919 3.57586 1.00000\n6:PM-14:AM   0.4433333 -2.29919 3.18586 1.00000\n7:PM-14:AM   0.7300000 -2.01252 3.47252 1.00000\n8:PM-14:AM   0.4233333 -2.31919 3.16586 1.00000\n9:PM-14:AM   0.7500000 -1.99252 3.49252 1.00000\n10:PM-14:AM  0.7900000 -1.95252 3.53252 1.00000\n11:PM-14:AM  0.2166667 -2.52586 2.95919 1.00000\n12:PM-14:AM  0.0966667 -2.64586 2.83919 1.00000\n13:PM-14:AM  1.0000000 -1.74252 3.74252 0.99998\n14:PM-14:AM  0.2800000 -2.46252 3.02252 1.00000\n15:PM-14:AM  0.6133333 -2.12919 3.35586 1.00000\n16:PM-14:AM  0.0133333 -2.72919 2.75586 1.00000\n17:PM-14:AM -0.3433333 -3.08586 2.39919 1.00000\n18:PM-14:AM  0.2933333 -2.44919 3.03586 1.00000\n19:PM-14:AM -0.2733333 -3.01586 2.46919 1.00000\n20:PM-14:AM  0.9333333 -1.80919 3.67586 1.00000\n16:AM-15:AM  0.8666667 -1.87586 3.60919 1.00000\n17:AM-15:AM  1.0866667 -1.65586 3.82919 0.99986\n18:AM-15:AM  0.9400000 -1.80252 3.68252 0.99999\n19:AM-15:AM  1.2900000 -1.45252 4.03252 0.99629\n20:AM-15:AM  1.1200000 -1.94623 4.18623 0.99998\n1:PM-15:AM   1.8900000 -0.85252 4.63252 0.69123\n2:PM-15:AM   1.1233333 -1.61919 3.86586 0.99972\n3:PM-15:AM   1.6400000 -1.10252 4.38252 0.90560\n4:PM-15:AM   0.5133333 -2.22919 3.25586 1.00000\n5:PM-15:AM   1.1800000 -1.56252 3.92252 0.99925\n6:PM-15:AM   0.7900000 -1.95252 3.53252 1.00000\n7:PM-15:AM   1.0766667 -1.66586 3.81919 0.99988\n8:PM-15:AM   0.7700000 -1.97252 3.51252 1.00000\n9:PM-15:AM   1.0966667 -1.64586 3.83919 0.99983\n10:PM-15:AM  1.1366667 -1.60586 3.87919 0.99964\n11:PM-15:AM  0.5633333 -2.17919 3.30586 1.00000\n12:PM-15:AM  0.4433333 -2.29919 3.18586 1.00000\n13:PM-15:AM  1.3466667 -1.39586 4.08919 0.99256\n14:PM-15:AM  0.6266667 -2.11586 3.36919 1.00000\n15:PM-15:AM  0.9600000 -1.78252 3.70252 0.99999\n16:PM-15:AM  0.3600000 -2.38252 3.10252 1.00000\n17:PM-15:AM  0.0033333 -2.73919 2.74586 1.00000\n18:PM-15:AM  0.6400000 -2.10252 3.38252 1.00000\n19:PM-15:AM  0.0733333 -2.66919 2.81586 1.00000\n20:PM-15:AM  1.2800000 -1.46252 4.02252 0.99674\n17:AM-16:AM  0.2200000 -2.52252 2.96252 1.00000\n18:AM-16:AM  0.0733333 -2.66919 2.81586 1.00000\n19:AM-16:AM  0.4233333 -2.31919 3.16586 1.00000\n20:AM-16:AM  0.2533333 -2.81290 3.31957 1.00000\n1:PM-16:AM   1.0233333 -1.71919 3.76586 0.99996\n2:PM-16:AM   0.2566667 -2.48586 2.99919 1.00000\n3:PM-16:AM   0.7733333 -1.96919 3.51586 1.00000\n4:PM-16:AM  -0.3533333 -3.09586 2.38919 1.00000\n5:PM-16:AM   0.3133333 -2.42919 3.05586 1.00000\n6:PM-16:AM  -0.0766667 -2.81919 2.66586 1.00000\n7:PM-16:AM   0.2100000 -2.53252 2.95252 1.00000\n8:PM-16:AM  -0.0966667 -2.83919 2.64586 1.00000\n9:PM-16:AM   0.2300000 -2.51252 2.97252 1.00000\n10:PM-16:AM  0.2700000 -2.47252 3.01252 1.00000\n11:PM-16:AM -0.3033333 -3.04586 2.43919 1.00000\n12:PM-16:AM -0.4233333 -3.16586 2.31919 1.00000\n13:PM-16:AM  0.4800000 -2.26252 3.22252 1.00000\n14:PM-16:AM -0.2400000 -2.98252 2.50252 1.00000\n15:PM-16:AM  0.0933333 -2.64919 2.83586 1.00000\n16:PM-16:AM -0.5066667 -3.24919 2.23586 1.00000\n17:PM-16:AM -0.8633333 -3.60586 1.87919 1.00000\n18:PM-16:AM -0.2266667 -2.96919 2.51586 1.00000\n19:PM-16:AM -0.7933333 -3.53586 1.94919 1.00000\n20:PM-16:AM  0.4133333 -2.32919 3.15586 1.00000\n18:AM-17:AM -0.1466667 -2.88919 2.59586 1.00000\n19:AM-17:AM  0.2033333 -2.53919 2.94586 1.00000\n20:AM-17:AM  0.0333333 -3.03290 3.09957 1.00000\n1:PM-17:AM   0.8033333 -1.93919 3.54586 1.00000\n2:PM-17:AM   0.0366667 -2.70586 2.77919 1.00000\n3:PM-17:AM   0.5533333 -2.18919 3.29586 1.00000\n4:PM-17:AM  -0.5733333 -3.31586 2.16919 1.00000\n5:PM-17:AM   0.0933333 -2.64919 2.83586 1.00000\n6:PM-17:AM  -0.2966667 -3.03919 2.44586 1.00000\n7:PM-17:AM  -0.0100000 -2.75252 2.73252 1.00000\n8:PM-17:AM  -0.3166667 -3.05919 2.42586 1.00000\n9:PM-17:AM   0.0100000 -2.73252 2.75252 1.00000\n10:PM-17:AM  0.0500000 -2.69252 2.79252 1.00000\n11:PM-17:AM -0.5233333 -3.26586 2.21919 1.00000\n12:PM-17:AM -0.6433333 -3.38586 2.09919 1.00000\n13:PM-17:AM  0.2600000 -2.48252 3.00252 1.00000\n14:PM-17:AM -0.4600000 -3.20252 2.28252 1.00000\n15:PM-17:AM -0.1266667 -2.86919 2.61586 1.00000\n16:PM-17:AM -0.7266667 -3.46919 2.01586 1.00000\n17:PM-17:AM -1.0833333 -3.82586 1.65919 0.99987\n18:PM-17:AM -0.4466667 -3.18919 2.29586 1.00000\n19:PM-17:AM -1.0133333 -3.75586 1.72919 0.99997\n20:PM-17:AM  0.1933333 -2.54919 2.93586 1.00000\n19:AM-18:AM  0.3500000 -2.39252 3.09252 1.00000\n20:AM-18:AM  0.1800000 -2.88623 3.24623 1.00000\n1:PM-18:AM   0.9500000 -1.79252 3.69252 0.99999\n2:PM-18:AM   0.1833333 -2.55919 2.92586 1.00000\n3:PM-18:AM   0.7000000 -2.04252 3.44252 1.00000\n4:PM-18:AM  -0.4266667 -3.16919 2.31586 1.00000\n5:PM-18:AM   0.2400000 -2.50252 2.98252 1.00000\n6:PM-18:AM  -0.1500000 -2.89252 2.59252 1.00000\n7:PM-18:AM   0.1366667 -2.60586 2.87919 1.00000\n8:PM-18:AM  -0.1700000 -2.91252 2.57252 1.00000\n9:PM-18:AM   0.1566667 -2.58586 2.89919 1.00000\n10:PM-18:AM  0.1966667 -2.54586 2.93919 1.00000\n11:PM-18:AM -0.3766667 -3.11919 2.36586 1.00000\n12:PM-18:AM -0.4966667 -3.23919 2.24586 1.00000\n13:PM-18:AM  0.4066667 -2.33586 3.14919 1.00000\n14:PM-18:AM -0.3133333 -3.05586 2.42919 1.00000\n15:PM-18:AM  0.0200000 -2.72252 2.76252 1.00000\n16:PM-18:AM -0.5800000 -3.32252 2.16252 1.00000\n17:PM-18:AM -0.9366667 -3.67919 1.80586 1.00000\n18:PM-18:AM -0.3000000 -3.04252 2.44252 1.00000\n19:PM-18:AM -0.8666667 -3.60919 1.87586 1.00000\n20:PM-18:AM  0.3400000 -2.40252 3.08252 1.00000\n20:AM-19:AM -0.1700000 -3.23623 2.89623 1.00000\n1:PM-19:AM   0.6000000 -2.14252 3.34252 1.00000\n2:PM-19:AM  -0.1666667 -2.90919 2.57586 1.00000\n3:PM-19:AM   0.3500000 -2.39252 3.09252 1.00000\n4:PM-19:AM  -0.7766667 -3.51919 1.96586 1.00000\n5:PM-19:AM  -0.1100000 -2.85252 2.63252 1.00000\n6:PM-19:AM  -0.5000000 -3.24252 2.24252 1.00000\n7:PM-19:AM  -0.2133333 -2.95586 2.52919 1.00000\n8:PM-19:AM  -0.5200000 -3.26252 2.22252 1.00000\n9:PM-19:AM  -0.1933333 -2.93586 2.54919 1.00000\n10:PM-19:AM -0.1533333 -2.89586 2.58919 1.00000\n11:PM-19:AM -0.7266667 -3.46919 2.01586 1.00000\n12:PM-19:AM -0.8466667 -3.58919 1.89586 1.00000\n13:PM-19:AM  0.0566667 -2.68586 2.79919 1.00000\n14:PM-19:AM -0.6633333 -3.40586 2.07919 1.00000\n15:PM-19:AM -0.3300000 -3.07252 2.41252 1.00000\n16:PM-19:AM -0.9300000 -3.67252 1.81252 1.00000\n17:PM-19:AM -1.2866667 -4.02919 1.45586 0.99645\n18:PM-19:AM -0.6500000 -3.39252 2.09252 1.00000\n19:PM-19:AM -1.2166667 -3.95919 1.52586 0.99867\n20:PM-19:AM -0.0100000 -2.75252 2.73252 1.00000\n1:PM-20:AM   0.7700000 -2.29623 3.83623 1.00000\n2:PM-20:AM   0.0033333 -3.06290 3.06957 1.00000\n3:PM-20:AM   0.5200000 -2.54623 3.58623 1.00000\n4:PM-20:AM  -0.6066667 -3.67290 2.45957 1.00000\n5:PM-20:AM   0.0600000 -3.00623 3.12623 1.00000\n6:PM-20:AM  -0.3300000 -3.39623 2.73623 1.00000\n7:PM-20:AM  -0.0433333 -3.10957 3.02290 1.00000\n8:PM-20:AM  -0.3500000 -3.41623 2.71623 1.00000\n9:PM-20:AM  -0.0233333 -3.08957 3.04290 1.00000\n10:PM-20:AM  0.0166667 -3.04957 3.08290 1.00000\n11:PM-20:AM -0.5566667 -3.62290 2.50957 1.00000\n12:PM-20:AM -0.6766667 -3.74290 2.38957 1.00000\n13:PM-20:AM  0.2266667 -2.83957 3.29290 1.00000\n14:PM-20:AM -0.4933333 -3.55957 2.57290 1.00000\n15:PM-20:AM -0.1600000 -3.22623 2.90623 1.00000\n16:PM-20:AM -0.7600000 -3.82623 2.30623 1.00000\n17:PM-20:AM -1.1166667 -4.18290 1.94957 0.99998\n18:PM-20:AM -0.4800000 -3.54623 2.58623 1.00000\n19:PM-20:AM -1.0466667 -4.11290 2.01957 1.00000\n20:PM-20:AM  0.1600000 -2.90623 3.22623 1.00000\n2:PM-1:PM   -0.7666667 -3.50919 1.97586 1.00000\n3:PM-1:PM   -0.2500000 -2.99252 2.49252 1.00000\n4:PM-1:PM   -1.3766667 -4.11919 1.36586 0.98958\n5:PM-1:PM   -0.7100000 -3.45252 2.03252 1.00000\n6:PM-1:PM   -1.1000000 -3.84252 1.64252 0.99982\n7:PM-1:PM   -0.8133333 -3.55586 1.92919 1.00000\n8:PM-1:PM   -1.1200000 -3.86252 1.62252 0.99973\n9:PM-1:PM   -0.7933333 -3.53586 1.94919 1.00000\n10:PM-1:PM  -0.7533333 -3.49586 1.98919 1.00000\n11:PM-1:PM  -1.3266667 -4.06919 1.41586 0.99413\n12:PM-1:PM  -1.4466667 -4.18919 1.29586 0.97882\n13:PM-1:PM  -0.5433333 -3.28586 2.19919 1.00000\n14:PM-1:PM  -1.2633333 -4.00586 1.47919 0.99740\n15:PM-1:PM  -0.9300000 -3.67252 1.81252 1.00000\n16:PM-1:PM  -1.5300000 -4.27252 1.21252 0.95647\n17:PM-1:PM  -1.8866667 -4.62919 0.85586 0.69480\n18:PM-1:PM  -1.2500000 -3.99252 1.49252 0.99784\n19:PM-1:PM  -1.8166667 -4.55919 0.92586 0.76637\n20:PM-1:PM  -0.6100000 -3.35252 2.13252 1.00000\n3:PM-2:PM    0.5166667 -2.22586 3.25919 1.00000\n4:PM-2:PM   -0.6100000 -3.35252 2.13252 1.00000\n5:PM-2:PM    0.0566667 -2.68586 2.79919 1.00000\n6:PM-2:PM   -0.3333333 -3.07586 2.40919 1.00000\n7:PM-2:PM   -0.0466667 -2.78919 2.69586 1.00000\n8:PM-2:PM   -0.3533333 -3.09586 2.38919 1.00000\n9:PM-2:PM   -0.0266667 -2.76919 2.71586 1.00000\n10:PM-2:PM   0.0133333 -2.72919 2.75586 1.00000\n11:PM-2:PM  -0.5600000 -3.30252 2.18252 1.00000\n12:PM-2:PM  -0.6800000 -3.42252 2.06252 1.00000\n13:PM-2:PM   0.2233333 -2.51919 2.96586 1.00000\n14:PM-2:PM  -0.4966667 -3.23919 2.24586 1.00000\n15:PM-2:PM  -0.1633333 -2.90586 2.57919 1.00000\n16:PM-2:PM  -0.7633333 -3.50586 1.97919 1.00000\n17:PM-2:PM  -1.1200000 -3.86252 1.62252 0.99973\n18:PM-2:PM  -0.4833333 -3.22586 2.25919 1.00000\n19:PM-2:PM  -1.0500000 -3.79252 1.69252 0.99993\n20:PM-2:PM   0.1566667 -2.58586 2.89919 1.00000\n4:PM-3:PM   -1.1266667 -3.86919 1.61586 0.99970\n5:PM-3:PM   -0.4600000 -3.20252 2.28252 1.00000\n6:PM-3:PM   -0.8500000 -3.59252 1.89252 1.00000\n7:PM-3:PM   -0.5633333 -3.30586 2.17919 1.00000\n8:PM-3:PM   -0.8700000 -3.61252 1.87252 1.00000\n9:PM-3:PM   -0.5433333 -3.28586 2.19919 1.00000\n10:PM-3:PM  -0.5033333 -3.24586 2.23919 1.00000\n11:PM-3:PM  -1.0766667 -3.81919 1.66586 0.99988\n12:PM-3:PM  -1.1966667 -3.93919 1.54586 0.99902\n13:PM-3:PM  -0.2933333 -3.03586 2.44919 1.00000\n14:PM-3:PM  -1.0133333 -3.75586 1.72919 0.99997\n15:PM-3:PM  -0.6800000 -3.42252 2.06252 1.00000\n16:PM-3:PM  -1.2800000 -4.02252 1.46252 0.99674\n17:PM-3:PM  -1.6366667 -4.37919 1.10586 0.90755\n18:PM-3:PM  -1.0000000 -3.74252 1.74252 0.99998\n19:PM-3:PM  -1.5666667 -4.30919 1.17586 0.94247\n20:PM-3:PM  -0.3600000 -3.10252 2.38252 1.00000\n5:PM-4:PM    0.6666667 -2.07586 3.40919 1.00000\n6:PM-4:PM    0.2766667 -2.46586 3.01919 1.00000\n7:PM-4:PM    0.5633333 -2.17919 3.30586 1.00000\n8:PM-4:PM    0.2566667 -2.48586 2.99919 1.00000\n9:PM-4:PM    0.5833333 -2.15919 3.32586 1.00000\n10:PM-4:PM   0.6233333 -2.11919 3.36586 1.00000\n11:PM-4:PM   0.0500000 -2.69252 2.79252 1.00000\n12:PM-4:PM  -0.0700000 -2.81252 2.67252 1.00000\n13:PM-4:PM   0.8333333 -1.90919 3.57586 1.00000\n14:PM-4:PM   0.1133333 -2.62919 2.85586 1.00000\n15:PM-4:PM   0.4466667 -2.29586 3.18919 1.00000\n16:PM-4:PM  -0.1533333 -2.89586 2.58919 1.00000\n17:PM-4:PM  -0.5100000 -3.25252 2.23252 1.00000\n18:PM-4:PM   0.1266667 -2.61586 2.86919 1.00000\n19:PM-4:PM  -0.4400000 -3.18252 2.30252 1.00000\n20:PM-4:PM   0.7666667 -1.97586 3.50919 1.00000\n6:PM-5:PM   -0.3900000 -3.13252 2.35252 1.00000\n7:PM-5:PM   -0.1033333 -2.84586 2.63919 1.00000\n8:PM-5:PM   -0.4100000 -3.15252 2.33252 1.00000\n9:PM-5:PM   -0.0833333 -2.82586 2.65919 1.00000\n10:PM-5:PM  -0.0433333 -2.78586 2.69919 1.00000\n11:PM-5:PM  -0.6166667 -3.35919 2.12586 1.00000\n12:PM-5:PM  -0.7366667 -3.47919 2.00586 1.00000\n13:PM-5:PM   0.1666667 -2.57586 2.90919 1.00000\n14:PM-5:PM  -0.5533333 -3.29586 2.18919 1.00000\n15:PM-5:PM  -0.2200000 -2.96252 2.52252 1.00000\n16:PM-5:PM  -0.8200000 -3.56252 1.92252 1.00000\n17:PM-5:PM  -1.1766667 -3.91919 1.56586 0.99929\n18:PM-5:PM  -0.5400000 -3.28252 2.20252 1.00000\n19:PM-5:PM  -1.1066667 -3.84919 1.63586 0.99979\n20:PM-5:PM   0.1000000 -2.64252 2.84252 1.00000\n7:PM-6:PM    0.2866667 -2.45586 3.02919 1.00000\n8:PM-6:PM   -0.0200000 -2.76252 2.72252 1.00000\n9:PM-6:PM    0.3066667 -2.43586 3.04919 1.00000\n10:PM-6:PM   0.3466667 -2.39586 3.08919 1.00000\n11:PM-6:PM  -0.2266667 -2.96919 2.51586 1.00000\n12:PM-6:PM  -0.3466667 -3.08919 2.39586 1.00000\n13:PM-6:PM   0.5566667 -2.18586 3.29919 1.00000\n14:PM-6:PM  -0.1633333 -2.90586 2.57919 1.00000\n15:PM-6:PM   0.1700000 -2.57252 2.91252 1.00000\n16:PM-6:PM  -0.4300000 -3.17252 2.31252 1.00000\n17:PM-6:PM  -0.7866667 -3.52919 1.95586 1.00000\n18:PM-6:PM  -0.1500000 -2.89252 2.59252 1.00000\n19:PM-6:PM  -0.7166667 -3.45919 2.02586 1.00000\n20:PM-6:PM   0.4900000 -2.25252 3.23252 1.00000\n8:PM-7:PM   -0.3066667 -3.04919 2.43586 1.00000\n9:PM-7:PM    0.0200000 -2.72252 2.76252 1.00000\n10:PM-7:PM   0.0600000 -2.68252 2.80252 1.00000\n11:PM-7:PM  -0.5133333 -3.25586 2.22919 1.00000\n12:PM-7:PM  -0.6333333 -3.37586 2.10919 1.00000\n13:PM-7:PM   0.2700000 -2.47252 3.01252 1.00000\n14:PM-7:PM  -0.4500000 -3.19252 2.29252 1.00000\n15:PM-7:PM  -0.1166667 -2.85919 2.62586 1.00000\n16:PM-7:PM  -0.7166667 -3.45919 2.02586 1.00000\n17:PM-7:PM  -1.0733333 -3.81586 1.66919 0.99989\n18:PM-7:PM  -0.4366667 -3.17919 2.30586 1.00000\n19:PM-7:PM  -1.0033333 -3.74586 1.73919 0.99998\n20:PM-7:PM   0.2033333 -2.53919 2.94586 1.00000\n9:PM-8:PM    0.3266667 -2.41586 3.06919 1.00000\n10:PM-8:PM   0.3666667 -2.37586 3.10919 1.00000\n11:PM-8:PM  -0.2066667 -2.94919 2.53586 1.00000\n12:PM-8:PM  -0.3266667 -3.06919 2.41586 1.00000\n13:PM-8:PM   0.5766667 -2.16586 3.31919 1.00000\n14:PM-8:PM  -0.1433333 -2.88586 2.59919 1.00000\n15:PM-8:PM   0.1900000 -2.55252 2.93252 1.00000\n16:PM-8:PM  -0.4100000 -3.15252 2.33252 1.00000\n17:PM-8:PM  -0.7666667 -3.50919 1.97586 1.00000\n18:PM-8:PM  -0.1300000 -2.87252 2.61252 1.00000\n19:PM-8:PM  -0.6966667 -3.43919 2.04586 1.00000\n20:PM-8:PM   0.5100000 -2.23252 3.25252 1.00000\n10:PM-9:PM   0.0400000 -2.70252 2.78252 1.00000\n11:PM-9:PM  -0.5333333 -3.27586 2.20919 1.00000\n12:PM-9:PM  -0.6533333 -3.39586 2.08919 1.00000\n13:PM-9:PM   0.2500000 -2.49252 2.99252 1.00000\n14:PM-9:PM  -0.4700000 -3.21252 2.27252 1.00000\n15:PM-9:PM  -0.1366667 -2.87919 2.60586 1.00000\n16:PM-9:PM  -0.7366667 -3.47919 2.00586 1.00000\n17:PM-9:PM  -1.0933333 -3.83586 1.64919 0.99984\n18:PM-9:PM  -0.4566667 -3.19919 2.28586 1.00000\n19:PM-9:PM  -1.0233333 -3.76586 1.71919 0.99996\n20:PM-9:PM   0.1833333 -2.55919 2.92586 1.00000\n11:PM-10:PM -0.5733333 -3.31586 2.16919 1.00000\n12:PM-10:PM -0.6933333 -3.43586 2.04919 1.00000\n13:PM-10:PM  0.2100000 -2.53252 2.95252 1.00000\n14:PM-10:PM -0.5100000 -3.25252 2.23252 1.00000\n15:PM-10:PM -0.1766667 -2.91919 2.56586 1.00000\n16:PM-10:PM -0.7766667 -3.51919 1.96586 1.00000\n17:PM-10:PM -1.1333333 -3.87586 1.60919 0.99966\n18:PM-10:PM -0.4966667 -3.23919 2.24586 1.00000\n19:PM-10:PM -1.0633333 -3.80586 1.67919 0.99991\n20:PM-10:PM  0.1433333 -2.59919 2.88586 1.00000\n12:PM-11:PM -0.1200000 -2.86252 2.62252 1.00000\n13:PM-11:PM  0.7833333 -1.95919 3.52586 1.00000\n14:PM-11:PM  0.0633333 -2.67919 2.80586 1.00000\n15:PM-11:PM  0.3966667 -2.34586 3.13919 1.00000\n16:PM-11:PM -0.2033333 -2.94586 2.53919 1.00000\n17:PM-11:PM -0.5600000 -3.30252 2.18252 1.00000\n18:PM-11:PM  0.0766667 -2.66586 2.81919 1.00000\n19:PM-11:PM -0.4900000 -3.23252 2.25252 1.00000\n20:PM-11:PM  0.7166667 -2.02586 3.45919 1.00000\n13:PM-12:PM  0.9033333 -1.83919 3.64586 1.00000\n14:PM-12:PM  0.1833333 -2.55919 2.92586 1.00000\n15:PM-12:PM  0.5166667 -2.22586 3.25919 1.00000\n16:PM-12:PM -0.0833333 -2.82586 2.65919 1.00000\n17:PM-12:PM -0.4400000 -3.18252 2.30252 1.00000\n18:PM-12:PM  0.1966667 -2.54586 2.93919 1.00000\n19:PM-12:PM -0.3700000 -3.11252 2.37252 1.00000\n20:PM-12:PM  0.8366667 -1.90586 3.57919 1.00000\n14:PM-13:PM -0.7200000 -3.46252 2.02252 1.00000\n15:PM-13:PM -0.3866667 -3.12919 2.35586 1.00000\n16:PM-13:PM -0.9866667 -3.72919 1.75586 0.99998\n17:PM-13:PM -1.3433333 -4.08586 1.39919 0.99284\n18:PM-13:PM -0.7066667 -3.44919 2.03586 1.00000\n19:PM-13:PM -1.2733333 -4.01586 1.46919 0.99702\n20:PM-13:PM -0.0666667 -2.80919 2.67586 1.00000\n15:PM-14:PM  0.3333333 -2.40919 3.07586 1.00000\n16:PM-14:PM -0.2666667 -3.00919 2.47586 1.00000\n17:PM-14:PM -0.6233333 -3.36586 2.11919 1.00000\n18:PM-14:PM  0.0133333 -2.72919 2.75586 1.00000\n19:PM-14:PM -0.5533333 -3.29586 2.18919 1.00000\n20:PM-14:PM  0.6533333 -2.08919 3.39586 1.00000\n16:PM-15:PM -0.6000000 -3.34252 2.14252 1.00000\n17:PM-15:PM -0.9566667 -3.69919 1.78586 0.99999\n18:PM-15:PM -0.3200000 -3.06252 2.42252 1.00000\n19:PM-15:PM -0.8866667 -3.62919 1.85586 1.00000\n20:PM-15:PM  0.3200000 -2.42252 3.06252 1.00000\n17:PM-16:PM -0.3566667 -3.09919 2.38586 1.00000\n18:PM-16:PM  0.2800000 -2.46252 3.02252 1.00000\n19:PM-16:PM -0.2866667 -3.02919 2.45586 1.00000\n20:PM-16:PM  0.9200000 -1.82252 3.66252 1.00000\n18:PM-17:PM  0.6366667 -2.10586 3.37919 1.00000\n19:PM-17:PM  0.0700000 -2.67252 2.81252 1.00000\n20:PM-17:PM  1.2766667 -1.46586 4.01919 0.99689\n19:PM-18:PM -0.5666667 -3.30919 2.17586 1.00000\n20:PM-18:PM  0.6400000 -2.10252 3.38252 1.00000\n20:PM-19:PM  1.2066667 -1.53586 3.94919 0.99886\n\n\n  Tukey multiple comparisons of means\n    99% family-wise confidence level\n\nFit: aov(formula = Ct ~ Day + noon + Day:noon, data = data)\n\n$Day\n            diff      lwr     upr   p adj\n2-1   -0.2883333 -2.29705 1.72039 1.00000\n3-1   -0.1916667 -2.20039 1.81705 1.00000\n4-1   -0.6716667 -2.68039 1.33705 0.99700\n5-1   -0.1566667 -2.16539 1.85205 1.00000\n6-1   -0.3200000 -2.32872 1.68872 1.00000\n7-1   -0.3016667 -2.31039 1.70705 1.00000\n8-1   -0.2050000 -2.21372 1.80372 1.00000\n9-1    0.4016667 -1.60705 2.41039 1.00000\n10-1  -0.5966667 -2.60539 1.41205 0.99934\n11-1   0.0866667 -1.92205 2.09539 1.00000\n12-1  -0.9583333 -2.96705 1.05039 0.89330\n13-1  -0.1633333 -2.17205 1.84539 1.00000\n14-1  -0.8700000 -2.87872 1.13872 0.95243\n15-1  -0.8766667 -2.88539 1.13205 0.94905\n16-1  -0.7433333 -2.75205 1.26539 0.99024\n17-1  -0.8116667 -2.82039 1.19705 0.97539\n18-1  -0.5666667 -2.57539 1.44205 0.99968\n19-1  -0.6750000 -2.68372 1.33372 0.99682\n20-1  -0.1406667 -2.24743 1.96610 1.00000\n3-2    0.0966667 -1.91205 2.10539 1.00000\n4-2   -0.3833333 -2.39205 1.62539 1.00000\n5-2    0.1316667 -1.87705 2.14039 1.00000\n6-2   -0.0316667 -2.04039 1.97705 1.00000\n7-2   -0.0133333 -2.02205 1.99539 1.00000\n8-2    0.0833333 -1.92539 2.09205 1.00000\n9-2    0.6900000 -1.31872 2.69872 0.99585\n10-2  -0.3083333 -2.31705 1.70039 1.00000\n11-2   0.3750000 -1.63372 2.38372 1.00000\n12-2  -0.6700000 -2.67872 1.33872 0.99709\n13-2   0.1250000 -1.88372 2.13372 1.00000\n14-2  -0.5816667 -2.59039 1.42705 0.99953\n15-2  -0.5883333 -2.59705 1.42039 0.99946\n16-2  -0.4550000 -2.46372 1.55372 0.99999\n17-2  -0.5233333 -2.53205 1.48539 0.99990\n18-2  -0.2783333 -2.28705 1.73039 1.00000\n19-2  -0.3866667 -2.39539 1.62205 1.00000\n20-2   0.1476667 -1.95910 2.25443 1.00000\n4-3   -0.4800000 -2.48872 1.52872 0.99997\n5-3    0.0350000 -1.97372 2.04372 1.00000\n6-3   -0.1283333 -2.13705 1.88039 1.00000\n7-3   -0.1100000 -2.11872 1.89872 1.00000\n8-3   -0.0133333 -2.02205 1.99539 1.00000\n9-3    0.5933333 -1.41539 2.60205 0.99939\n10-3  -0.4050000 -2.41372 1.60372 1.00000\n11-3   0.2783333 -1.73039 2.28705 1.00000\n12-3  -0.7666667 -2.77539 1.24205 0.98634\n13-3   0.0283333 -1.98039 2.03705 1.00000\n14-3  -0.6783333 -2.68705 1.33039 0.99662\n15-3  -0.6850000 -2.69372 1.32372 0.99620\n16-3  -0.5516667 -2.56039 1.45705 0.99978\n17-3  -0.6200000 -2.62872 1.38872 0.99891\n18-3  -0.3750000 -2.38372 1.63372 1.00000\n19-3  -0.4833333 -2.49205 1.52539 0.99997\n20-3   0.0510000 -2.05576 2.15776 1.00000\n5-4    0.5150000 -1.49372 2.52372 0.99992\n6-4    0.3516667 -1.65705 2.36039 1.00000\n7-4    0.3700000 -1.63872 2.37872 1.00000\n8-4    0.4666667 -1.54205 2.47539 0.99998\n9-4    1.0733333 -0.93539 3.08205 0.76824\n10-4   0.0750000 -1.93372 2.08372 1.00000\n11-4   0.7583333 -1.25039 2.76705 0.98785\n12-4  -0.2866667 -2.29539 1.72205 1.00000\n13-4   0.5083333 -1.50039 2.51705 0.99993\n14-4  -0.1983333 -2.20705 1.81039 1.00000\n15-4  -0.2050000 -2.21372 1.80372 1.00000\n16-4  -0.0716667 -2.08039 1.93705 1.00000\n17-4  -0.1400000 -2.14872 1.86872 1.00000\n18-4   0.1050000 -1.90372 2.11372 1.00000\n19-4  -0.0033333 -2.01205 2.00539 1.00000\n20-4   0.5310000 -1.57576 2.63776 0.99994\n6-5   -0.1633333 -2.17205 1.84539 1.00000\n7-5   -0.1450000 -2.15372 1.86372 1.00000\n8-5   -0.0483333 -2.05705 1.96039 1.00000\n9-5    0.5583333 -1.45039 2.56705 0.99974\n10-5  -0.4400000 -2.44872 1.56872 0.99999\n11-5   0.2433333 -1.76539 2.25205 1.00000\n12-5  -0.8016667 -2.81039 1.20705 0.97827\n13-5  -0.0066667 -2.01539 2.00205 1.00000\n14-5  -0.7133333 -2.72205 1.29539 0.99387\n15-5  -0.7200000 -2.72872 1.28872 0.99318\n16-5  -0.5866667 -2.59539 1.42205 0.99948\n17-5  -0.6550000 -2.66372 1.35372 0.99780\n18-5  -0.4100000 -2.41872 1.59872 1.00000\n19-5  -0.5183333 -2.52705 1.49039 0.99991\n20-5   0.0160000 -2.09076 2.12276 1.00000\n7-6    0.0183333 -1.99039 2.02705 1.00000\n8-6    0.1150000 -1.89372 2.12372 1.00000\n9-6    0.7216667 -1.28705 2.73039 0.99300\n10-6  -0.2766667 -2.28539 1.73205 1.00000\n11-6   0.4066667 -1.60205 2.41539 1.00000\n12-6  -0.6383333 -2.64705 1.37039 0.99841\n13-6   0.1566667 -1.85205 2.16539 1.00000\n14-6  -0.5500000 -2.55872 1.45872 0.99979\n15-6  -0.5566667 -2.56539 1.45205 0.99975\n16-6  -0.4233333 -2.43205 1.58539 1.00000\n17-6  -0.4916667 -2.50039 1.51705 0.99996\n18-6  -0.2466667 -2.25539 1.76205 1.00000\n19-6  -0.3550000 -2.36372 1.65372 1.00000\n20-6   0.1793333 -1.92743 2.28610 1.00000\n8-7    0.0966667 -1.91205 2.10539 1.00000\n9-7    0.7033333 -1.30539 2.71205 0.99480\n10-7  -0.2950000 -2.30372 1.71372 1.00000\n11-7   0.3883333 -1.62039 2.39705 1.00000\n12-7  -0.6566667 -2.66539 1.35205 0.99773\n13-7   0.1383333 -1.87039 2.14705 1.00000\n14-7  -0.5683333 -2.57705 1.44039 0.99966\n15-7  -0.5750000 -2.58372 1.43372 0.99960\n16-7  -0.4416667 -2.45039 1.56705 0.99999\n17-7  -0.5100000 -2.51872 1.49872 0.99993\n18-7  -0.2650000 -2.27372 1.74372 1.00000\n19-7  -0.3733333 -2.38205 1.63539 1.00000\n20-7   0.1610000 -1.94576 2.26776 1.00000\n9-8    0.6066667 -1.40205 2.61539 0.99918\n10-8  -0.3916667 -2.40039 1.61705 1.00000\n11-8   0.2916667 -1.71705 2.30039 1.00000\n12-8  -0.7533333 -2.76205 1.25539 0.98869\n13-8   0.0416667 -1.96705 2.05039 1.00000\n14-8  -0.6650000 -2.67372 1.34372 0.99735\n15-8  -0.6716667 -2.68039 1.33705 0.99700\n16-8  -0.5383333 -2.54705 1.47039 0.99984\n17-8  -0.6066667 -2.61539 1.40205 0.99918\n18-8  -0.3616667 -2.37039 1.64705 1.00000\n19-8  -0.4700000 -2.47872 1.53872 0.99998\n20-8   0.0643333 -2.04243 2.17110 1.00000\n10-9  -0.9983333 -3.00705 1.01039 0.85576\n11-9  -0.3150000 -2.32372 1.69372 1.00000\n12-9  -1.3600000 -3.36872 0.64872 0.35269\n13-9  -0.5650000 -2.57372 1.44372 0.99969\n14-9  -1.2716667 -3.28039 0.73705 0.47719\n15-9  -1.2783333 -3.28705 0.73039 0.46731\n16-9  -1.1450000 -3.15372 0.86372 0.66832\n17-9  -1.2133333 -3.22205 0.79539 0.56535\n18-9  -0.9683333 -2.97705 1.04039 0.88455\n19-9  -1.0766667 -3.08539 0.93205 0.76389\n20-9  -0.5423333 -2.64910 1.56443 0.99991\n11-10  0.6833333 -1.32539 2.69205 0.99631\n12-10 -0.3616667 -2.37039 1.64705 1.00000\n13-10  0.4333333 -1.57539 2.44205 0.99999\n14-10 -0.2733333 -2.28205 1.73539 1.00000\n15-10 -0.2800000 -2.28872 1.72872 1.00000\n16-10 -0.1466667 -2.15539 1.86205 1.00000\n17-10 -0.2150000 -2.22372 1.79372 1.00000\n18-10  0.0300000 -1.97872 2.03872 1.00000\n19-10 -0.0783333 -2.08705 1.93039 1.00000\n20-10  0.4560000 -1.65076 2.56276 0.99999\n12-11 -1.0450000 -3.05372 0.96372 0.80373\n13-11 -0.2500000 -2.25872 1.75872 1.00000\n14-11 -0.9566667 -2.96539 1.05205 0.89472\n15-11 -0.9633333 -2.97205 1.04539 0.88898\n16-11 -0.8300000 -2.83872 1.17872 0.96936\n17-11 -0.8983333 -2.90705 1.11039 0.93692\n18-11 -0.6533333 -2.66205 1.35539 0.99787\n19-11 -0.7616667 -2.77039 1.24705 0.98726\n20-11 -0.2273333 -2.33410 1.87943 1.00000\n13-12  0.7950000 -1.21372 2.80372 0.98004\n14-12  0.0883333 -1.92039 2.09705 1.00000\n15-12  0.0816667 -1.92705 2.09039 1.00000\n16-12  0.2150000 -1.79372 2.22372 1.00000\n17-12  0.1466667 -1.86205 2.15539 1.00000\n18-12  0.3916667 -1.61705 2.40039 1.00000\n19-12  0.2833333 -1.72539 2.29205 1.00000\n20-12  0.8176667 -1.28910 2.92443 0.98369\n14-13 -0.7066667 -2.71539 1.30205 0.99450\n15-13 -0.7133333 -2.72205 1.29539 0.99387\n16-13 -0.5800000 -2.58872 1.42872 0.99955\n17-13 -0.6483333 -2.65705 1.36039 0.99807\n18-13 -0.4033333 -2.41205 1.60539 1.00000\n19-13 -0.5116667 -2.52039 1.49705 0.99992\n20-13  0.0226667 -2.08410 2.12943 1.00000\n15-14 -0.0066667 -2.01539 2.00205 1.00000\n16-14  0.1266667 -1.88205 2.13539 1.00000\n17-14  0.0583333 -1.95039 2.06705 1.00000\n18-14  0.3033333 -1.70539 2.31205 1.00000\n19-14  0.1950000 -1.81372 2.20372 1.00000\n20-14  0.7293333 -1.37743 2.83610 0.99545\n16-15  0.1333333 -1.87539 2.14205 1.00000\n17-15  0.0650000 -1.94372 2.07372 1.00000\n18-15  0.3100000 -1.69872 2.31872 1.00000\n19-15  0.2016667 -1.80705 2.21039 1.00000\n20-15  0.7360000 -1.37076 2.84276 0.99493\n17-16 -0.0683333 -2.07705 1.94039 1.00000\n18-16  0.1766667 -1.83205 2.18539 1.00000\n19-16  0.0683333 -1.94039 2.07705 1.00000\n20-16  0.6026667 -1.50410 2.70943 0.99961\n18-17  0.2450000 -1.76372 2.25372 1.00000\n19-17  0.1366667 -1.87205 2.14539 1.00000\n20-17  0.6710000 -1.43576 2.77776 0.99837\n19-18 -0.1083333 -2.11705 1.90039 1.00000\n20-18  0.4260000 -1.68076 2.53276 1.00000\n20-19  0.5343333 -1.57243 2.64110 0.99993\n\n\n유의한 패턴 없음 Tukey 검정 결과 (보통 유의할때 함) 테이블 도 유의한 수치가 없음"
  },
  {
    "objectID": "docs/blog/posts/2023-01-07-anova/index.html#terms",
    "href": "docs/blog/posts/2023-01-07-anova/index.html#terms",
    "title": "ANOVA",
    "section": "1 Terms",
    "text": "1 Terms\n\npairwise comparison: A hypothesis test (e.g., of means) between two groups among multiple groups.\nobmnibus set: A single hypothesis test of the overall variance among multiple group means.\ndecomposition of variance : Separation of components contributing to an individual value (e.g., from the overall average, from a treatment mean, and from a residual error).\nF-test\nF statistic: A standardized statistic that measures the extent to which differences among group means exceed what might be expected in a chance model.\nsum of squares: deviations from some average value\n\n\n1.1 Description\nANOVA는 3개 이상의 모집단 사이의 평균의 동일성을 검정하는 통계 분석 방법이다.\n\n일원 분산 분석 (One-way ANOVA)\n\n그룹을 구분하는 변수가 1개\nBetween-Groups one-way ANOVA(집단간 일원분산분석): 관측치를 grouping하는 범주형 변수가 1개이며 각 관측치는 범주형 변수에 의해 구분되는 그룹들 가운데 반드시 하나에만 할당되어야한다. 즉, 어떠한 경우에도 하나의 관측치 또는 샘플이 여러 groups에 동시에 들어가면 안된다. 이 때 이렇게 그룹을 나누는 범주형 변수를 집단간 요인이라고 한다.\nWithin-groups one-way ANOVA (집단 내 일원분산분석) or repeated measures ANOVA: 시간과 같은 하나의 범주형 변수로 샘플들을 측정한다. 시간의 경과에 따라 측정된 샘플들을 범주형 변수의 여러 기간에 걸쳐 모두 할당시킨다. 즉, 하나의 샘플이 여러 그룹에 다른 측정치로 관찰될 수 있다. 예를들어, sample A가 4주, 8주, 12주, 16주 그룹에 모두 측정 된다. 이때 기간변수는 집단 내 요인이라고 부른다.\n\n이원 분산 분석 (Two-way ANOVA)\n\n집단을 구분하는 변수가 2개이며 각 집단 간 요인과 집단 내 요인을 나타낸다.\n이원 분석 부터는 main effect와 interaction effect가 존재한다.\n범주형 변수 A와 범주형 변수 B의 Main effect 계산\n범주형 변수 A와 범주형 변수 B의 상호 작용 효과 or 교호 작용 효과 (Interaction effect) 계산\ngroup을 구분하는 독립변수가 2개 일때 모집단 간 평균의 동일성 검정\n\n2개의 주효과(main effect) 검정: 각 독립 변수에 의해 만들어지는 집단 간 평균의 차이에 대한 검정\n\n먼저, 두 독립변수가 종속변수에 개별적으로 영향을 미치는지 검정\n\n\n1개의 상호작용효과(interaction effect) 검정: 두 독립 변수의 조합에 의해 만들어지는 집단 간 평균의 차이에 대한 검정\n\n두 독립변수의 조합이 종속변수와 유의한 영향관계를 갖는지 검정\n\n만약에 유의하다면 2개의 독립변수가 합쳐져서 나온 파생효과이기 때문에 1개만 골라서 분석해서 해석 할 수 없음\n\n\n\n\n\n1.2 How to conduct ANOVA?\n\n분산 분석은 F검정(F test)을 통해 수행한다.\nF 검정은 집단 간 분산 (between-groups variability)과 집단 내 분산 (within-groups variability)의 ratio로 계산된 F값 (F value or F statistic)을 토대로 가설검정을 수행한다. 이때 F value or F statistic을 통계 검정을 위한 검정통계량 (test statistic) 라고 부른다.\nF 검정 결과가 통계적으로 유의하면 집단 간 평균의 차이가 존재한다. (즉, 독립 변수가 종속변수에 영향을 미침)\nF 분포 2개의 자유도에 의해 분포의 모양이 결정되며 대체로 오른쪽으로 긴 꼬리를 갖는다\n\n첫 번째 자유도: 집단 간(between-group)의 자유도\n두 번째 자유도: 집단 내(within-group)의 자유도\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(gplots)\nknitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)\noptions(digits = 5)\nset.seed(20230109)\n\n\n\n\n\n\n\n종속 변수의 변동성은 다음과 같이 설명되기 때문에 아래의 식을 만족한다.\n\\[SS_{total}=SS_A+SS_B+SS_{AB}+SS_{error}\\]\n\\(SS_{total}\\)은 쉽게 구할 수 있고 \\(SS_A\\), \\(SS_B\\), \\(SS_{error}\\)를 계산하여 빼준다.\nTwo Way Anova SS 계산 공식 링크\n\\(SS_{AB}\\) 즉,\n\\[ SS_{AB}=SS_{total}-SS_A-SS_B-SS_{error} \\]\n\n\n1.3 Meaning\nANOVA는 집단 간 분산과 집단 내 분산의 비교하는 방식으로, 좀 더 구체적으로는 집단 간 분산과 집단 내 분산의 비를 계산하여, 집단 간 분산이 클수록 그리고 집단 분산이 작을 수록 집단 평균이 다를 가능성이 증가한다는 알고리즘에 기초한다."
  },
  {
    "objectID": "docs/projects/high_dimension/eda.html",
    "href": "docs/projects/high_dimension/eda.html",
    "title": "EDA",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\nplease, read the English section first.\n\n\n\n\n\n\n\n\n\n\n\nmean, sd, normality check with p value\n\n\n\n\n\n\n\n\n\n\nCode\ngroup_variable=\"country\"\nsummary_variable=\"age\"\n\ncolor_function<-function(summary_data){\nreturn(\n    if(nrow(summary_data)==2){\n        c(\"darkblue\",\"darkred\")\n    }else if(nrow(summary_data)==3){\n        c(\"darkblue\",\"darkred\",\"yellow4\")\n    }else if(nrow(summary_data)==4){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\")\n    }else if(nrow(summary_data)==5){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\")\n    }else{\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\",\"darkgreen\")\n    }\n    )\n}\n\ngetNumericSummary=function(data=all_data,group_variable,summary_variable,set_color=color_function){\n    # table\n    temp<-data %>% \n    group_by(get(group_variable)) %>% \n    mutate(count=n())%>%\n    summarise_at(vars(summary_variable,count),\n                 list(mean=mean,\n                 sd=sd,\n                 min=min,\n                 Q1=~quantile(., probs = 0.25),\n                 median=median, \n                 Q3=~quantile(., probs = 0.75),\n                 max=max))%>%\n                 as.data.frame()%>%\n                 rename(\n                 n=count_mean)%>%\n                 dplyr::select(-contains('count'))%>%\n                 as.data.frame()\n    names(temp)<-c(\"group\",\n    sapply(names(temp)[-1],function(x)str_replace(x,paste0(summary_variable,\"_\"),\"\")))\n    temp<-temp%>%\n    mutate(\n        variable=group_variable,\n        summary=summary_variable,\n        mean=mean%>%round(2),\n        sd=sd%>%round(2),\n        min=min%>%round(2),\n        Q1=Q1%>%round(2),\n        Q4=Q3%>%round(2),\n        max=max%>%round(2),\n        IQR_min=Q1-(Q3-Q1)*1.5%>%round(2),\n    IQR_max=Q3+(Q3-Q1)*1.5%>%round(2),\n    proportion=paste0(round(n/nrow(all_data)*100,2),\"%\"))%>%\n    dplyr::select(variable,group,summary,n,proportion,mean,sd,min,IQR_min,Q1,median,Q3,IQR_max,max)\n\n    # plot\n    temp2=temp\n    names(temp2)[2]=group_variable\n    plot<-\n    data%>%\n    dplyr::select(group_variable,summary_variable)%>%\n    inner_join(.,temp2,by=group_variable)%>%\n    ggplot(aes(x=age,fill=get(group_variable),color=get(group_variable)))+\n    geom_histogram(aes(y=..density..),binwidth=1,alpha=0.5, position=\"identity\")+\n    geom_vline(aes(xintercept=mean,color=get(group_variable)), linetype=\"dashed\", size=1.5) + \n    geom_density(aes(y=..density..),alpha=0.3) +\n    scale_color_manual(values=set_color(temp2))+\n    scale_fill_manual(values=set_color(temp2))+\n    theme_bw()+\n    theme(legend.position = c(.95, .95),\n    legend.justification = c(\"right\", \"top\"),\n    legend.margin = margin(6, 6, 6, 6),\n    legend.text = element_text(size = 10))+\n    guides(fill=guide_legend(title=group_variable),\n    color=FALSE)+\n    geom_text(aes(label=round(mean,1),y=0,x=mean),\n                vjust=-1,col='yellow',size=5)+\n    ggtitle(paste0(\"Histogram & Density, \", summary_variable, \" Grouped by \", group_variable))+\n        labs(x=summary_variable, y = \"Density\")\n\n    result<-list(temp,plot)\n    return(result)\n}\n\nad_age_summary=getNumericSummary(data=all_data,group_variable=\"outcome\",summary_variable=\"age\")[[1]]\nsex_age_summary=getNumericSummary(data=all_data,group_variable=\"sex\",summary_variable=\"age\")[[1]]\ncountry_age_summary=getNumericSummary(data=all_data,group_variable=\"country\",summary_variable=\"age\")[[1]]\ntreatment_age_summary=getNumericSummary(data=all_data,group_variable=\"treatment\",summary_variable=\"age\")[[1]]\ngenotype_age_summary=getNumericSummary(data=all_data,group_variable=\"genotype\",summary_variable=\"age\")[[1]]\nage_summary=rbind(\n    ad_age_summary,\n    sex_age_summary,\n    country_age_summary,\n    treatment_age_summary,\n    genotype_age_summary)\n\n\nAge is known as a strong risk factor of AD or dementia. Human nerve system gets damaged as people are aged and the nerve fibrosis symptoms progress gradually (some reference, to be added after figuring out how to add bibliography in quarto). For this reason, we need to look into how the sample data are distributed in terms of age.\n\n\nCode\nage_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\ngroup\nsummary\nn\nproportion\nmean\nsd\nmin\nIQR_min\nQ1\nmedian\nQ3\nIQR_max\nmax\n\n\n\n\noutcome\nnegative\nage\n488\n48.8%\n79.05\n6.36\n65\n63.00\n75\n78.0\n83.0\n95.00\n110\n\n\noutcome\npositive\nage\n512\n51.2%\n81.79\n6.07\n66\n67.50\n78\n81.0\n85.0\n95.50\n104\n\n\nsex\nman\nage\n457\n45.7%\n80.03\n6.58\n66\n64.00\n76\n80.0\n84.0\n96.00\n102\n\n\nsex\nwoman\nage\n543\n54.3%\n80.81\n6.16\n65\n65.75\n77\n80.0\n84.5\n95.75\n110\n\n\ncountry\ncenter_a\nage\n305\n30.5%\n80.40\n6.50\n66\n62.50\n76\n80.0\n85.0\n98.50\n104\n\n\ncountry\ncenter_b\nage\n196\n19.6%\n80.82\n6.17\n67\n66.50\n77\n81.0\n84.0\n94.50\n110\n\n\ncountry\ncenter_c\nage\n193\n19.3%\n80.16\n6.32\n65\n64.00\n76\n80.0\n84.0\n96.00\n99\n\n\ncountry\ncenter_d\nage\n306\n30.6%\n80.46\n6.39\n66\n64.00\n76\n80.0\n84.0\n96.00\n104\n\n\ntreatment\ntrmnt1\nage\n360\n36%\n78.08\n6.25\n66\n62.00\n74\n77.0\n82.0\n94.00\n110\n\n\ntreatment\ntrmnt2\nage\n588\n58.8%\n82.06\n5.82\n66\n67.50\n78\n82.0\n85.0\n95.50\n104\n\n\ntreatment\ntrmnt3\nage\n52\n5.2%\n78.79\n7.52\n65\n56.50\n73\n78.5\n84.0\n100.50\n95\n\n\ngenotype\ne2/e2\nage\n42\n4.2%\n83.95\n5.18\n71\n73.50\n81\n84.0\n86.0\n93.50\n95\n\n\ngenotype\ne2/e3\nage\n148\n14.8%\n79.70\n6.01\n66\n65.50\n76\n79.0\n83.0\n93.50\n110\n\n\ngenotype\ne2/e4\nage\n51\n5.1%\n76.10\n5.46\n69\n64.75\n73\n75.0\n78.5\n86.75\n95\n\n\ngenotype\ne3/e3\nage\n462\n46.2%\n80.92\n5.95\n68\n68.00\n77\n80.0\n83.0\n92.00\n104\n\n\ngenotype\ne3/e4\nage\n214\n21.4%\n79.85\n6.64\n65\n57.50\n74\n84.0\n85.0\n101.50\n95\n\n\ngenotype\ne4/e4\nage\n83\n8.3%\n81.69\n7.81\n68\n54.00\n75\n83.0\n89.0\n110.00\n91\n\n\n\n\n\nThe table above shows the summary statistics of age grouped by the affected status, AD and non AD. The difference of age between the two groups are about 2.74, but their standard deviations are 6.07 and 6.36. Thus, it is hard to say their age in average differ in the affected status because the age variations of the two groups are overlapped. This research has two conflicting characteristics at the population level. First, the participants are the elderly whose age in average is 80.45, which indicates they are likely to develop dementia, an aging disease. Second, the data were collected from the longevity village where people live long and healthy lives and it is expected that where will be some protective factors against dementia. These two conflicting traits may have contributed to this unclear difference.\nThe glaring difference of age is also shown below. As you can see, the people with a negative status a bit younger than those with a positive one.\n\n\nCode\nplot<- ggarrange(\n    getNumericSummary(data=all_data,group_variable=\"outcome\",summary_variable=\"age\")[[2]],\n    getNumericSummary(data=all_data,group_variable=\"sex\",summary_variable=\"age\")[[2]],\n    getNumericSummary(data=all_data,group_variable=\"country\",summary_variable=\"age\")[[2]],\n    getNumericSummary(data=all_data,group_variable=\"treatment\",summary_variable=\"age\")[[2]],\n    getNumericSummary(data=all_data,group_variable=\"genotype\",summary_variable=\"age\")[[2]],\n    ncol=2, nrow=3,legend=\"bottom\")\nplot\n\n\n\n\n\n\n\n\n\n\nt-test\n\n#F-test to test for homogeneity in variances\n#| echo: false\n#| eval: false\nvar.test\n\nfunction (x, ...) \nUseMethod(\"var.test\")\n<bytecode: 0x000002929f983c50>\n<environment: namespace:stats>\n\n\n\n\n\nOne way Anova\n\n\n\nOne way Anova\n\n\n\n\n\n\n\n\n\n\n\n\n\nChisquare test\n\n\n\nchisquare test"
  },
  {
    "objectID": "docs/projects/high_dimension/ml_approach.html",
    "href": "docs/projects/high_dimension/ml_approach.html",
    "title": "ML Approach",
    "section": "",
    "text": "alpha      mse fit.name\n1    0.0 2217.331   alpha0\n2    0.1 2217.331 alpha0.1\n3    0.2 2217.331 alpha0.2\n4    0.3 2217.331 alpha0.3\n5    0.4 2217.331 alpha0.4\n6    0.5 2217.331 alpha0.5\n7    0.6 2217.331 alpha0.6\n8    0.7 2217.331 alpha0.7\n9    0.8 2217.331 alpha0.8\n10   0.9 2217.331 alpha0.9\n11   1.0 2217.331   alpha1"
  },
  {
    "objectID": "docs/projects/high_dimension/description.html#overview-2",
    "href": "docs/projects/high_dimension/description.html#overview-2",
    "title": "Description",
    "section": "Overview",
    "text": "Overview\n\nAs-Is\n\n\n\n\nflowchart TB\n    subgraph Skip_These_Steps_in_This_Article\n        direction LR\n        subgraph data_collection\n            direction LR\n            multi-centered_medical_centers---\n            set_inclusion\\exclusion_criteria---\n            patient_visit---\n            \n            blood_sampling---\n            store_samples_in_labs\n        end\n        subgraph mass_spectrometry\n            direction LR\n            conduct_mass_spectrometry---\n            digitalization---\n            store_in_database_system    \n        end\n        subgraph data_transfer\n            direction TB\n            subgraph data\n                direction LR\n                metabolome_data---\n                clinical_data---\n                demographic_data---\n                dietary_habit_data\n            end\n            data---csv_or_excel_format\n        end\n        subgraph data_quality_control\n            direction TB\n            identify_anomaly_data---\n            identify_missing_values\n            subgraph missing_value_analysis\n                direction LR\n                MCAR---\n                MAR---\n                MNAR\n            end\n            either_imputation_or_omission---\n            communication_with_labs---\n            set_data_inclusion/exclusion_criteria\n        end\n        identify_missing_values---missing_value_analysis---either_imputation_or_omission\n        subgraph data_preprocessing\n            direction LR\n            data_transformation\n        end\n    end\n    subgraph data_analytics\n        direction LR\n        exploratory_data_analysis---\n        statistical_analysis---\n        machine_learning\n    end\n    subgraph report_to_faculty_members\n        direction TB\n        share_with_medical_doctors---\n        share_with_epidemiologists---\n        share_with_neurologists---\n        share_with_biologists---\n        share_with_biostatisticians---\n        share_with_bioinformatician\n    end\n    subgraph write_manuscript\n        direction LR\n    end\n    \ndata_collection-->mass_spectrometry-->data_transfer-->data_quality_control-->data_preprocessing-->data_analytics-->report_to_faculty_members-->write_manuscript\n\n\n\n\n\n\n\n\n\n\nTo-Be\n\n\n\n\nflowchart TB\n    subgraph Simulation\n        direction TB\n        subgraph assign_frequency\n        direction LR\n        Assign_probabilities     \n        end\n        subgraph generate_multivariate_normal_distribution\n            direction LR\n            determine_sample_size ---\n            determine_number_of_columns---\n            create_covariate_correlation_matrix---\n            apply_some_noise_to_the_covariance_matrix\n        end\n        subgraph create_weight_matrix\n            direction LR\n            apply_some_noise_to_the_covariance_matrix\n        end\n        subgraph calculate_score_matrix\n            direction LR\n            matrix_multiplication_covariance_matrix_weight_matrix---\n            used_logistic_function\n        end\n        subgraph create_response_variable\n            direction LR\n            result_of_logistic_function\n        end\n        subgraph creation_some_discrete_variables\n            direction LR       \n        end\n        subgraph join\n            direction LR\n            all_the_matrix_and_vector_created       \n        end\n        assign_frequency-->generate_multivariate_normal_distribution-->create_weight_matrix-->\n        calculate_score_matrix-->create_response_variable-->creation_some_discrete_variables-->\n        join\n    end\n    subgraph data_analytics\n        direction LR\n        exploratory_data_analysis---\n        statistical_analysis---\n        machine_learning\n    end\n    subgraph conclusion\n        direction LR\n    end\n    Simulation-->data_analytics-->conclusion"
  },
  {
    "objectID": "docs/projects/high_dimension/description.html#overview",
    "href": "docs/projects/high_dimension/description.html#overview",
    "title": "Description",
    "section": "Overview",
    "text": "Overview\n\nAs-Is\n\n\n\n\nflowchart TB\n    subgraph Skip_These_Steps_in_This_Article\n        direction LR\n        subgraph data_collection\n            direction TB\n            multi-centered_medical_centers---\n            set_inclusion\\exclusion_criteria---\n            patient_visit---\n            blood_sampling---\n            store_samples_in_labs\n        end\n        subgraph mass_spectrometry\n            direction LR\n            conduct_mass_spectrometry---\n            digitalization---\n            store_in_database_system    \n        end\n        subgraph data_transfer\n            direction TB\n            subgraph data\n                direction LR\n                metabolome_data---\n                clinical_data---\n                demographic_data---\n                dietary_habit_data\n            end\n            data---csv_or_excel_format\n        end\n        subgraph data_quality_control\n            direction TB\n            identify_anomaly_data---\n            identify_missing_values\n            subgraph missing_value_analysis\n                direction LR\n                MCAR---\n                MAR---\n                MNAR\n            end\n            either_imputation_or_omission---\n            communication_with_labs---\n            set_data_inclusion/exclusion_criteria\n        end\n        identify_missing_values---missing_value_analysis---either_imputation_or_omission\n        subgraph data_preprocessing\n            direction LR\n            data_transformation\n        end\n    end\n    subgraph data_analytics\n        direction LR\n        exploratory_data_analysis---\n        statistical_analysis---\n        machine_learning\n    end\n    subgraph report_to_faculty_members\n        direction TB\n        share_with_medical_doctors---\n        share_with_epidemiologists---\n        share_with_neurologists---\n        share_with_biologists---\n        share_with_biostatisticians---\n        share_with_bioinformatician\n    end\n    subgraph write_manuscript\n        direction LR\n    end\n    \ndata_collection-->mass_spectrometry-->data_transfer-->data_quality_control-->data_preprocessing-->data_analytics-->report_to_faculty_members-->write_manuscript\n\n\n\n\n\n\n\n\n\n\nTo-Be\n\n\n\n\nflowchart TB\n    subgraph Simulation\n        direction TB\n        subgraph assign_frequency\n        direction LR\n        Assign_probabilities     \n        end\n        subgraph generate_multivariate_normal_distribution\n            direction TB\n            determine_sample_size ---\n            determine_number_of_columns---\n            create_covariate_correlation_matrix---\n            apply_noise_to_the_covariance_matrix\n        end\n        subgraph create_weight_matrix\n            direction LR\n            apply_some_noise_to_the_covariance_matrix\n        end\n        subgraph calculate_score_matrix\n            direction LR\n            matrix_multiplication_covariance_matrix_weight_matrix---\n            used_logistic_function\n        end\n        subgraph create_response_variable\n            direction LR\n            result_of_logistic_function\n        end\n        subgraph creation_some_discrete_variables\n            direction LR       \n        end\n        subgraph join\n            direction LR\n            all_the_matrix_and_vector_created       \n        end\n        assign_frequency-->generate_multivariate_normal_distribution-->create_weight_matrix-->\n        calculate_score_matrix-->create_response_variable-->creation_some_discrete_variables-->\n        join\n    end\n    subgraph data_analytics\n        direction LR\n        exploratory_data_analysis---\n        statistical_analysis---\n        machine_learning\n    end\n    subgraph conclusion\n        direction LR\n    end\n    Simulation-->data_analytics-->conclusion"
  },
  {
    "objectID": "docs/blog/posts/2023-01-16_normality/index.html",
    "href": "docs/blog/posts/2023-01-16_normality/index.html",
    "title": "Normality Check",
    "section": "",
    "text": "Wiki\n논문\n원래는 표본의 크기가 50이하인 작은 데이터 셋의 정규성 검정을 위해 고안됨. R 에서는 3~5천개 사이의 표본까지 다룰 수 있도록 조정됨\n\n정규 분포 전용 검정: 모든 검정 대비 최고의 검정력을 보임 (Power), 이상치가 있으면 p value가 너무 작아짐\n\\(H_0\\): 데이터가 정규분포를 따른다\n\\(H_a\\): 데이터가 정규분포를 따르지 않는다.\n검정 통계량 \\[\\mathbf W=\\frac{(\\sum_{i=1}^{n}a_ix_{(i)})^2}{\\sum_{i=1}^{n}(x_i-\\overline{x})^2}\\]\n\n\\(a_i\\) : 미리 정해진 숫자들, \\(x\\)의 개수에 의해 정해짐\n\\(x_{(i)}\\) 들은 순위 표본, 즉 i 번째로 큰 표본\n분자는 순서 통계량으로 계산한 정규분포의 분산, 분모는 데이터의 표본 분산 (표본 Sum of Squares)\n이미 이론적으로 세팅된 값과 표본 분산의 비율을 보는 것\n귀무 가설이 참이면 이론적으로 1 이 나와야 함\n\\(\\mathbf W \\in (0,1)\\), 상관계수의 제곱을 측정한 계량 값이라고 생각해도 된다.\n\n$W $ 값이 1에서 멀어질 수록 정규분포와는 다르게 분포되어 있음을 의미\n단점: 너무 민감함, 조그만 달라도 p value가 너무 작게 나와 귀무가설이 기각됨\n해결책 : 시각화 기법과 같이 사용해서 보여준다\n\nqqplot과 density 같이 사용"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html",
    "title": "Data Structure (1) Overview",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n딥러닝은 다양한 알고리즘의 조합으로 수행되기 때문에 다양한 알고리즘을 정확하게 작성하기 위해서는 다수의 다양한 자료(data)를 담기 위해 사용되는 자료 구조를 이해할 필요가 있다. 즉, 자료구조는 정확한 알고리즘을 구현하기 위해 다수의 자료(data)를 담기 위한 구조이다.\n\n딥러닝 유저들간에도 자료구조를 이해하는 것에 대한 의견이 분분하지만\n올바른 자료구조를 사용하는 것은 프로그램을 조직적으로 만들 수 있는 능력을 키울 수 있다.\n데이터의 수가 많아질수록 효율적인 자료구조가 필요하다.\n예시) 학생 수가 1,000,000명 이상인 학생 관리 프로그램\n\n매일 자료 조회가 1억번 이상 발생한다면 더 빠르게 동작하는 자료 구조를 사용해야 프로그램의 효율성을 올릴 수 있다.\n\n\n\n\n\n자료구조의 필요성에 대해서 이해할 필요가 있다.\n성능 비교: 자료구조/알고리즘의 성능 측정 방법에 대해 이해할 필요가 있다.\n\nA: 적당한 속도의 삽입 & 적당한 속도의 추출 (삽입: \\(O (log N)\\) / 추출: \\(O(log N)\\))\nB: 느린 삽입 & 빠른 추출 (삽입: \\(O (N)\\) / 추출: \\(O (1)\\))\nA vs B? 상황에 따라 A를 만들지 B를 만들지 선택해야 한다. 삽입 연산이 많으면 A를, 추출 연산이 많으면 B를 택해야 한다. (속도 비교: \\(O (N) < O (log N)< O (1)\\))\n하지만, 실무적으로 많은 개발자들이 A를 택한다. 왜냐면 log 복잡도는 상수 복잡도와 속도가 비슷하기 때문\n\n\n\n\n\n\n\n이처럼 상황에 맞게 알고리즘의 연산 속도를 결정해야 하므로 데이터를 효과적으로 저장하고, 처리하는 방법에 대해 바르게 이해할 필요가 있다.\n자료구조를 제대로 이해해야 불필요하게 메모리와 계산을 낭비하지 않는다.\nC언어를 기준으로 정수(int) 형식의 데이터가 100만 개가량이 존재한다고 가정하자.\n해당 프로그램을 이용하면, 내부적으로 하루에 데이터 조회가 1억 번 이상 발생한다.\n이때 원하는 데이터를 가장 빠르게 찾도록 해주는 자료구조는 무엇일까?\n\n트리(tree)와 같은 자료구조를 활용할 수 있다.\n\n\n\n\n\n\n선형 자료 구조(linear data structure) 선형 자료구조는 하나의 데이터 뒤에 다른 데이터가 하나 존재하는 자료구조이며 데이터가 일렬로 연속적으로(순차적으로) 연결되어 있다.\n\n배열(array)\n연결 리스트(linked list)\n스택(stack)\n큐(queue)\n\n비선형 자료 구조(non-linear data structure)\n비선형 자료구조는 하나의 데이터 뒤에 다른 데이터가 여러 개 올 수 있는 자료구조이며 데이터가 일직선상으로 연결되어 있지 않아도 된다.\n\n트리(tree)\n그래프(graph)\n\n\n\n\n\n\n효율적인 자료구조 설계를 위해 알고리즘 지식이 필요하다.\n효율적인 알고리즘을 작성하기 위해서 문제 상황에 맞는 적절한 자료구조가 사용되어야 한다.\n프로그램을 작성할 때 자료구조와 알고리즘 모두 고려해야 한다.\n\n\n\n\n\n시간 복잡도(time complexity): 알고리즘에 사용되는 연산 횟수를 측정 (시간 측정)\n공간 복잡도(space complexity): 알고리즘에 사용되는 메모리의 양을 측정 (공간 측정)\n공간을 많이 사용하는 대신 시간을 단축하는 방법이 흔히 사용된다.\n프로그램의 성능 측정 방법: Big-O 표기법\n\n복잡도를 표현할 때는 Big-O 표기법을 사용한다.\n\n특정한 알고리즘이 얼마나 효율적인지 수치적으로 표현할 수 있다.\n가장 빠르게 증가하는 항만을 고려하는 표기법이다.\n\n아래의 알고리즘은 \\(O(n)\\) 의 시간 복잡도를 가진다. 왜냐면, n에 따라 summary += i의 연산 횟수가 정해지기 때문이다.\n\n\n\n\nCode\nn = 10\nsummary = 0\nfor i in range(n):\n    summary += i\nprint(summary)\n\n\n45\n\n\n\n다음 알고리즘은 \\(O (n^2)\\) 의 시간 복잡도를 가진다. 2 중 for loop은 i와 j가 n에 따라 각 각 n 번씩 연산되기때문에 \\(n \\times n\\) 회 만큼 연산된다.\n\n\n\nCode\nn = 3\nfor i in range(1, n + 1):\n    for j in range(1, n + 1):\n        print(f\"{i} X {j} = {i * j}\")\n\n\n1 X 1 = 1\n1 X 2 = 2\n1 X 3 = 3\n2 X 1 = 2\n2 X 2 = 4\n2 X 3 = 6\n3 X 1 = 3\n3 X 2 = 6\n3 X 3 = 9\n\n\n\n일반적으로 연산 횟수가 10억 (\\(1.0 \\times 10^9\\))을 넘어가면 1초 이상의 시간이 소요된다.\n[예시] n이 1,000일 때를 고려해 보자.\n\n\\(O(n)\\): 약 1,000번의 연산\n\\(O(nlogn )\\): 약 10,000번의 연산 (약 \\(log10=10\\))\n\\(O(n^2)\\): 약 1,000,000번의 연산\n\\(O(n^3)\\): 약 1,000,000,000번의 연산\n\n그러므로, 알고리즘 짤 때 코딩 레벨로 연산 횟수를 계산해서 연산 시간을 어림잡아 추정할 수 있다.\n시간 복잡도 속도 비교\n By Cmglee - Own work, CC BY-SA 4.0\nBig-O 표기법으로 시간 복잡도를 표기할 때는 가장 영향력이 큰 항만을 표시한다.\n\n\\(O(3n^2 + n) = O(n^2)\\)\n현실 세계에서는 동작 시간이 1초 이내인 알고리즘을 설계할 필요가 있다.\n실무적으로 프로그램 동작 시간이 1초 이상이면 매우 느린 것으로 간주.\n\n공간 복잡도를 나타낼 때는 MB 단위로 표기한다.\nint a[1000]: 4KB int a[1000000]: 4MB int a[2000][2000]: 16MB\n자료구조를 적절히 활용하기\n\n자료구조의 종류로는 스택, 큐, 트리 등이 있다.\n프로그램을 작성할 때는 자료구조를 적절히 활용하여 시간 복잡도를 최소화하여야 한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료-구조의-필요성",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료-구조의-필요성",
    "title": "Data Structure, Overview",
    "section": "자료 구조의 필요성",
    "text": "자료 구조의 필요성\n\n데이터를 효과적으로 저장하고, 처리하는 방법에 대해 바르게 이해할 필요가 있다.\n자료구조를 제대로 이해하지 못하면 불필요하게 메모리와 계산을 낭비할 여지가 있다.\nC언어를 기준으로 정수(int) 형식의 데이터가 100만 개가량이 존재한다고 가정하자.\n해당 프로그램을 이용하면, 내부적으로 하루에 데이터 조회가 1억 번 이상 발생한다.\n이때 원하는 데이터를 가장 빠르게 찾도록 해주는 자료구조는 무엇일까?\n\n→ 트리(tree)와 같은 자료구조를 활용할 수 있다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료-구조의-종류",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료-구조의-종류",
    "title": "Data Structure, Overview",
    "section": "자료 구조의 종류",
    "text": "자료 구조의 종류\n\n선형 자료 구조(linear data structure)\n선형 자료구조는 하나의 데이터 뒤에 다른 데이터가 하나 존재하는 자료구조이며 데이터가 일렬로 연속적으로(순차적으로) 연결되어 있다.\n\n배열(array)\n연결 리스트(linked list)\n스택(stack)\n큐(queue)\n\n비선형 자료 구조(non-linear data structure)\n비선형 자료구조는 하나의 데이터 뒤에 다른 데이터가 여러 개 올 수 있는 자료구조이며 데이터가 일직선상으로 연결되어 있지 않아도 된다.\n\n트리(tree)\n그래프(graph)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료구조와-알고리즘",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#자료구조와-알고리즘",
    "title": "Data Structure, Overview",
    "section": "자료구조와 알고리즘",
    "text": "자료구조와 알고리즘\n\n효율적인 자료구조 설계를 위해 알고리즘 지식이 필요하다.\n효율적인 알고리즘을 작성하기 위해서 문제 상황에 맞는 적절한 자료구조가 사용되어야 한다.\n프로그램을 작성할 때 자료구조와 알고리즘 모두 고려해야 한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#프로그램의-성능-측정-방법",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html#프로그램의-성능-측정-방법",
    "title": "Data Structure, Overview",
    "section": "프로그램의 성능 측정 방법",
    "text": "프로그램의 성능 측정 방법\n\n시간 복잡도(time complexity): 알고리즘에 사용되는 연산 횟수를 측정한다.\n공간 복잡도(space complexity): 알고리즘에 사용되는 메모리의 양을 측정한다.\n공간을 많이 사용하는 대신 시간을 단축하는 방법이 흔히 사용된다.\n프로그램의 성능 측정 방법: Big-O 표기법\n\n복잡도를 표현할 때는 Big-O 표기법을 사용한다.\n\n특정한 알고리즘이 얼마나 효율적인지 수치적으로 표현할 수 있다.\n가장 빠르게 증가하는 항만을 고려하는 표기법이다.\n\n다음 알고리즘은 \\(O(n)\\) 의 시간 복잡도를 가진다.\n\n\n\nn = 10\nsummary = 0\nfor i in range(n):\n    summary += i\nprint(summary)\n\n45\n\n\n- 다음 알고리즘은 $O (n^2)$ 의 시간 복잡도를 가진다.\n\nn = 9\nfor i in range(1, n + 1):\n    for j in range(1, n + 1):\n        print(f\"{i} X {j} = {i * j}\")\n\n1 X 1 = 1\n1 X 2 = 2\n1 X 3 = 3\n1 X 4 = 4\n1 X 5 = 5\n1 X 6 = 6\n1 X 7 = 7\n1 X 8 = 8\n1 X 9 = 9\n2 X 1 = 2\n2 X 2 = 4\n2 X 3 = 6\n2 X 4 = 8\n2 X 5 = 10\n2 X 6 = 12\n2 X 7 = 14\n2 X 8 = 16\n2 X 9 = 18\n3 X 1 = 3\n3 X 2 = 6\n3 X 3 = 9\n3 X 4 = 12\n3 X 5 = 15\n3 X 6 = 18\n3 X 7 = 21\n3 X 8 = 24\n3 X 9 = 27\n4 X 1 = 4\n4 X 2 = 8\n4 X 3 = 12\n4 X 4 = 16\n4 X 5 = 20\n4 X 6 = 24\n4 X 7 = 28\n4 X 8 = 32\n4 X 9 = 36\n5 X 1 = 5\n5 X 2 = 10\n5 X 3 = 15\n5 X 4 = 20\n5 X 5 = 25\n5 X 6 = 30\n5 X 7 = 35\n5 X 8 = 40\n5 X 9 = 45\n6 X 1 = 6\n6 X 2 = 12\n6 X 3 = 18\n6 X 4 = 24\n6 X 5 = 30\n6 X 6 = 36\n6 X 7 = 42\n6 X 8 = 48\n6 X 9 = 54\n7 X 1 = 7\n7 X 2 = 14\n7 X 3 = 21\n7 X 4 = 28\n7 X 5 = 35\n7 X 6 = 42\n7 X 7 = 49\n7 X 8 = 56\n7 X 9 = 63\n8 X 1 = 8\n8 X 2 = 16\n8 X 3 = 24\n8 X 4 = 32\n8 X 5 = 40\n8 X 6 = 48\n8 X 7 = 56\n8 X 8 = 64\n8 X 9 = 72\n9 X 1 = 9\n9 X 2 = 18\n9 X 3 = 27\n9 X 4 = 36\n9 X 5 = 45\n9 X 6 = 54\n9 X 7 = 63\n9 X 8 = 72\n9 X 9 = 81\n\n\n- 일반적으로 연산 횟수가 10억을 넘어가면 1초 이상의 시간이 소요된다.\n    \n    [예시] n이 1,000일 때를 고려해 보자.\n    \n    - $O(n)$: 약 1,000번의 연산\n    - $O(nlogn )$: 약 10,000번의 연산\n    - $O(n^2)$: 약 1,000,000번의 연산\n    - $O(n^3)$: 약 1,000,000,000번의 연산\n        \n       \n        \n- 각 시간 복잡도를 비교해보자.\n    \n    \n    \n- Big-O 표기법으로 시간 복잡도를 표기할 때는 가장 영향력이 큰 항만을 표시한다.\n    - $O(3n^2 + n) = O(n^2)$\n    - 현실 세계에서는 동작 시간이 1초 이내인 알고리즘을 설계할 필요가 있다.\n- 공간을 나타낼 때는 MB 단위로 표기한다.\n    \n    int a[1000]: 4KB\n    \n    int a[1000000]: 4MB\n    \n    int a[2000][2000]: 16MB\n    \n\n자료구조를 적절히 활용하기\n\n자료구조의 종류로는 스택, 큐, 트리 등이 있다.\n프로그램을 작성할 때는 자료구조를 적절히 활용하여 시간 복잡도를 최소화하여야 한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html",
    "href": "docs/blog/posts/Engineering/guide_map/index.html",
    "title": "Content List, Engineering",
    "section": "",
    "text": "0000-00-00, Terminology"
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "",
    "text": "Notice\nLast Update\nIntroduction\n\nDefinition of SW Validation\nSome Terminology\nRationale\nObjective of SW Validation\nWhat to validate\nMain Institutions\n\nQuality System Regulation\nVerification\nValidation\nBenefits and Difficulty in SW V&V\nSW Development as Part of System Design\n\nOverview\nDesign Reveiw\n\n\n\n\nValidation Pinciples\n\nOverview\nConditions\nPlanning\nAfter SW Change\nSW Lifecycle\n\nSW Lifecycle Tasks\n\nOverview\nQuality Planning\nConfiguration Management\nTask Requirements\nDesign Overview\n\nDesign Consideration\nDesign Specification\nDesign Activity and Task\n\n\n\n\n\nTesting Tasks\n\nOverview\nConsideration Before Testing Tasks\nCode Based Testing\nSolution to White Box Testing\nDevelopment Testing\nUser Site Testing\n\nOverview\nTesting\n\n\nMaintenance and SW Changes\nValidation of Quality System SW\n\nOverview\nFactors in Validation"
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#notice",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#notice",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Notice",
    "text": "Notice\n\nI am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto. (it seems that Quarto system has some issues on mermaid diagrams.)\nThe FDA validation guidance document is a bit difficult to understand because its explanations provide abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\nLast Update\n\n2022-12-28, Summary of Document"
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#introduction",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#introduction",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Introduction",
    "text": "Introduction\n\nDefinition of Software Validation\nSoftware Validation is a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997.\n(See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\n\n\nSome Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology: requirements, specification, verification, and validation.\n\n\n\nRationale\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\nObjective of SW validation is to ensure\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\nWhat to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\nMain Institutions\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#quality-system-regulation",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#quality-system-regulation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Quality System Regulation",
    "text": "Quality System Regulation\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\n\n\nflowchart TB\n    subgraph Quality_System_Regulation\n        direction LR\n        subgraph Requirement\n            direction TB\n            user_requirements\n        end\n        subgraph Specification\n           direction TB\n           document_user_requirements \n        end \n        subgraph Verification\n           direction TB\n           verify_spacified_requirements\n        end\n        subgraph Validation\n           direction TB\n           Confirmation_by_Examinations\n           Provision_of_objective_3evidences\n        end\n        Requirement--> Specification --> Verification --> Validation                    \n    end\n    subgraph First_Detail\n        direction TB\n        subgraph User_Requirement\n            direction TB\n            any_need_for_customer---\n            any_need_for_system---\n            any_need_for_software\n        end\n            subgraph Document_User_Requirement\n            direction TB\n            define_means_for_requirements---\n          define_criteria_for_requirements\n        end         \n        subgraph Verify_Spacified_Requirement\n            direction TB\n            Objective_Evidence--->|needs|Software_Testing\n        end\n        subgraph SW_Validation\n            direction TB\n            subgraph Confirmation_by_Examination\n            direction TB\n                subgraph Examination_List_of_SW_LifeCycle\n                    direction TB\n                    comprehensiveness_of_software_testing---\n                    inspection_verification_test---\n                    analysis_verification_test---\n                    other_varification_tests    \n                end \n            end             \n            subgraph Provision_of_Objective_3evidences\n                direction TB\n                Software_specifications_conformity---\n                Consistent_SW_Implementation---\n                Correctness_Completeness_Traceability\n            end\n        end\n        Requirement---User_Requirement\n        Specification---Document_User_Requirement\n        Verification---Verify_Spacified_Requirement\n        Confirmation_by_Examinations---Confirmation_by_Examination\n        Provision_of_objective_3evidences---Provision_of_Objective_3evidences             \n    end"
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#verification",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#verification",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Verification",
    "text": "Verification\n\n\n\n\nflowchart LR\n    subgraph Objective_Evidence\n        direction LR\n        subgraph Design_Outputs_of_SW_life_cycle_for_Specified_Requirements\n            direction TB\n            Consistency---\n            Completeness---\n            Correctness---\n            Documentation\n        end       \n        subgraph Software_Testing\n            direction LR\n            subgraph Testing_Environments\n                direction TB\n                satisfaction_for_input_requirements\n                satisfaction_for_input_requirements---Simulated_Use_Environment\n                subgraph User_Site_Testing\n                    direction TB                            \n                    Installation_Qualification---\n                    Operational_Qualification---\n                    Performance_Qualification\n                end\n            end\n            satisfaction_for_input_requirements---User_Site_Testing\n            subgraph Testing_Activities\n                direction TB\n                static_analyses---\n                dynamic_analyses---\n                code_and_document_inspections---\n                walkthroughs\n            end \n        Testing_Environments-->Testing_Activities\n        end\n    Design_Outputs_of_SW_life_cycle_for_Specified_Requirements-->Software_Testing-->Testing_Activities\nend    \n\n\n\n\n\n\n\n\n\n\nInstallation_Qualification (IQ): documentation of correct installations according to requirements, specifications, vendor’s recommendations, and the FDA’s guidance for all hardware, software, equipment and systems.\nOperational_Qualification (OQ): establishment of confidence that the software shows constant performances according to specified requirements.\nPerformance_Qualification (PQ): confirmation of the performance in the intended use according to the specified requirements for functionality and safety throughout the SW life cycle."
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation",
    "text": "Validation\n\n\n\n\nflowchart LR\n    subgraph Validation\n    direction LR\n        subgraph Confirmation_by_Examination\n            direction TB\n            subgraph Examination_List_at_each_stage_of_SW_Life_Cycle\n                direction TB\n                comprehensiveness_of_software_testing---\n                inspection_verification_test---\n                analysis_verification_test---\n                other_varification_tests    \n            end \n        end\n        subgraph Provision_of_objective_3evidences\n            direction TB\n            subgraph Software_specifications_conform_to\n                direction TB\n                user_needs \n                intended_uses\n            end\n            subgraph Consistent_SW_Implementation\n                direction TB\n                particular_requirements\n            end\n            subgraph Correctness_Completeness_Traceability\n                direction TB\n                correct_complete_implementation_by_all_SW_requirements---\n                traceable_to_system_requirements\n            end\n            Software_specifications_conform_to---\n            Consistent_SW_Implementation---\n            Correctness_Completeness_Traceability\n        end\n        Confirmation_by_Examination-->\n        Provision_of_objective_3evidences\n    end\n\n\n\n\n\n\n\n\n\n\n\n\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device."
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#benefits-and-difficulty-of-sw-vv",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#benefits-and-difficulty-of-sw-vv",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Benefits and Difficulty of SW V&V",
    "text": "Benefits and Difficulty of SW V&V\n\nBenefits of SW V&V\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nReduce long term costs by making V&V easier and less costly to reliably modify software and revalidate software changes.\n\n\n\nDifficulty in SW V&V\n\na developer cannot test forever, and\n\nit is difficult to know how much evidence is enough.\na matter of developing a level of confidence that the device meets all requirements\n\nConsiderations for an acceptable level of confidence\nmeasures and estimates such as defects found in specifications documents\ntesting coverage, and other techniques are all used before shipping the product.\na level of confidence varies depending upon the safety risk (hazard) of a SW or device"
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#sw-development-as-part-of-system-design",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#sw-development-as-part-of-system-design",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Development as Part of System Design",
    "text": "SW Development as Part of System Design\n\nOverview\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        purpose_design_review---\n        design_review_types---\n        design_review_requirements---\n        design_review_outputs\n    end\n\n\n\n\n\n\n\n\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nuser’s needs\nintended uses from which the product is developed.\n\nA primary goal of SW validation is to demonstrate that all completed SW products comply with all documented requirements.\n\n\nDesign Review\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        subgraph Purpose_Design_Review\n            direction TB\n            documented_structured_comprehensive_systematic_examinations---\n            adequacy_of_design_requirements---\n            capability_of_design_for_requirements---\n            identification_of_problem   \n        end\n        subgraph Design_Reivew_Types\n            direction TB\n            subgraph Formal_Design_Review\n                direction TB\n                3rd_parties_outside_development_team\n            end\n            subgraph Informal_Design_Review\n                direction TB\n                within_development_team\n            end\n        Formal_Design_Review---Informal_Design_Review    \n        end\n        subgraph Design_Review_Requirements\n            direction TB\n               necessary_at_least_one_formal_design_review---\n               optinal_informal_design_review---\n               recommended_multiple_design_reviews\n        end\n        subgraph Formal_Design_Review_Outputs\n            direction TB\n            more_than_10_outputs\n        end\n        Purpose_Design_Review--> Design_Reivew_Types--> Design_Review_Requirements\n        Design_Review_Requirements-->Formal_Design_Review_Outputs\n    end\n\n\n\n\n\n\n\n\n\n\nDesign review is a primary tool for managing and evaluating development projects.\nAt least one formal design review must be conducted during the device design process.\nIt is recommended that multiple design reviews be conducted.\nProblems found at this point can\n\nbe resolved more easily,\nsave time and money, and\nreduce the likelihood of missing a critical issue."
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation-principles",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation-principles",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation Principles",
    "text": "Validation Principles\n\nOverview\n\n\n\n\nflowchart LR\n  subgraph Validation_Principles\n        direction LR\n        subgraph Validation_Starting_Point\n            direction TB\n            during_design_planning---\n            during_development_planning---\n            all_results_should_be_supported_by_evidence_collected_from_planning_SW_lifecylce\n        end\n        subgraph Validation_Conditions\n            direction TB\n            Requirements---Estabilishment_Confidence---SW_Lifecycle\n        end\n\n        subgraph Validation_Planning\n            direction TB\n            Specify_Areas\n            subgraph Validation_Coverage\n                direction TB\n            end\n            subgraph Validation_Process_Establishment\n                direction TB\n            end\n        Specify_Areas---Validation_Coverage---Validation_Process_Establishment\n        end\n\n        subgraph After_Self_Validation\n            direction TB\n            subgraph Validation_After_SW_Change\n        direction TB\n        end\n\n        subgraph Independence_of_Review\n        direction TB\n\n        end\n        Validation_After_SW_Change---Independence_of_Review\n        end\n            Validation_Starting_Point-->Validation_Conditions-->Validation_Planning-->\nAfter_Self_Validation\n    end\n\n\n\n\n\n\n\n\n\nPreparation for software validation should begin as early as possible because the final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\n\nConditions\n\n\n\n\nflowchart LR\n\nsubgraph Validation_Conditions\n    direction LR\n    subgraph SW_Requirments\n        direction TB\n        subgraph Documented_SW_Requirments_Specification\n            direction TB\n            Baseline_Provision_for_V&V---\n            establishment_of_software_requirements_specification\n        end\n    end\n    subgraph Estabilishment_Confidence\n        direction TB\n            mixture_of_methods_techinques---\n            preventing_SW_errors---\n            detecting_SW_errors                 \n    end\n    subgraph SW_Lifecycle\n        direction TB\n        validation_must_be_conducted_within_established_environment_across_lifecycle---\n        lifecycle_contains_SW_engineering_tasks_and_documentation---\n        V&V_tasks_must_reflect_intended_use\n    end\nend\nSW_Requirments---Estabilishment_Confidence---SW_Lifecycle\n\n\n\n\n\n\n\n\n\n\nPlanning\n\n\n\n\nflowchart LR\n    subgraph Validation_Planning\n        direction LR\n        define_what_to_accomplish\n        subgraph Specify_Areas\n            direction TB\n            scope---\n            approach---\n            resources---\n            schedules_activities---\n            types_activitieis---\n            extent_of_activities---\n            tasks---\n            work_items\n        end\n            define_what_to_accomplish-->Specify_Areas\n        subgraph Validation_Coverage\n               direction TB\n            depending_on_SW_complexity_of_SW_design---\n            depending_on_safety_risk_for_specified_intended_use---\n            select_activities_tasks_work_items_for_complexity_safety_risk\n        end\n        subgraph Validation_Process_Establishment\n            direction TB\n            establish_how_to_conduct-->\n            identify_sequence_of_specific_actions-->\n            identify_specific_activitieis-->\n            identify_specific_tasks-->\n            identify_specific_work_items\n        end\n    Specify_Areas-->Validation_Coverage-->Validation_Process_Establishment\n    end\n\n\n\n\n\n\n\n\n\n\nAfter SW Change\n\n\n\n\nflowchart LR\n\nsubgraph After_Self_Validation\n    direction LR\n    subgraph Validation_After_SW_Change\n        direction TB\n        determine_extent_of_change_on_entire_SW_system-->\n        determine_impact_of_change_on_entire_SW_system-->\n        conduct_SW_regression_testing_on_unchanged_but_vulnerable_modules\n    end\n    subgraph Independence_of_Review\n        direction TB\n        follow_basic_quality_assurance_precept_of_independence_of_review---\n        avoid_self_validation---\n        should_conduct_contracted_3rd_party_independent_V&V---\n        or_conduct_blind_test_with_internal_staff\n    end\n    Validation_After_SW_Change---Independence_of_Review\nend\n    \n\n\n\n\n\n\n\n\n\n\nSW Lifecycle\n\n\n\n\nflowchart LR\nsubgraph SW_Lifecycle\n    direction TB\n    validation_must_be_conducted_within_the_established_environment_across_lifecycle---\n    lifecycle_contains_SW_engineering_tasks_and_documentation---\n    V&V_tasks_must_reflect_intended_use\nend\n\nsubgraph SW_Lifecycle_Activities\n    direction TB\n    subgraph should_establish_lifecycle_model\n        direction TB\n        subgraph SW_Lifecycle_Model_List_Defined_in_FDA\n            direction TB\n            waterfall---\n            spiral---\n            rapid_prototyping---\n            incremental_development---\n            etc\n        end     \n    end\n    subgraph should_cover_SW_birth_to_retirement\n        direction TB\n        subgraph Lifecycle_Activities\n            direction TB\n            Quality_Plan-->\n            System_Requirements_Definition-->\n            Detailed_Software_Requirements_Specification-->\n            Software_Design_Specification-->\n            Construction_or_Coding-->\n            Testing-->\n            Installation-->\n            Operation_and_Support-->\n            Maintenance-->\n            Retirement\n        end\n    end\n    should_establish_lifecycle_model-->should_cover_SW_birth_to_retirement\n    should_cover_SW_birth_to_retirement-->Lifecycle_Activities\nend\nsubgraph SW_Lifecycle_Tasks\n    direction TB\n    should_define_and_document_risk_related_tasks---\n    should_define_and_document_which_tasks_are_appropriate_in_vice_versa---\n    Quality_Planning---\n    Quality_Planning_Tasks---\n    Inclusion_Task_List_for_Plan---\n    Identification_Task_List_for_Plan---\n    Configuration_Management---\n    Control---\n    Management---\n    Procedures---\n    ensure_proper_communications_and_documentation---\n    Task_Requirements\nend\nSW_Lifecycle-->SW_Lifecycle_Activities-->SW_Lifecycle_Tasks"
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#sw-lifecycle-tasks",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#sw-lifecycle-tasks",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Lifecycle Tasks",
    "text": "SW Lifecycle Tasks\n\nOverview\n\n\n\n\n \nflowchart TB\n\nsubgraph SW_Lifecycle_Tasks\n    direction LR\n    subgraph Define_and_Document_List\n        direction TB\n        risk_related_tasks---\n        whether_or_not_tasks_are_appropriate\n    end\n    \n    subgraph Quality_Planning\n        direction TB\n        subgraph Quality_Planning_Tasks\n            direction TB\n        \n        end\n        subgraph Inclusion_List_for_Plan\n            direction TB\n            \n        end\n        subgraph Identification_List_for_Plan\n            direction TB\n            \n        end\n    Quality_Planning_Tasks-->Inclusion_List_for_Plan-->Identification_List_for_Plan\n    end\n    \n    subgraph Configuration_Management\n        direction TB\n        subgraph Control\n            direction TB\n            \n        end\n        subgraph Management\n            direction TB\n        end\n        subgraph Procedures\n            direction TB\n        end\n        ensure_proper_communications_and_documentation\n        Control-->Management-->Procedures-->ensure_proper_communications_and_documentation \n    end\n    subgraph Task_Requirements\n        direction TB\n        identification---\n        analysis---\n        predetermined_documentation_about_device_its_intended_use---\n        Requirements_Specification_List---\n        Verfification_List_by_Evaluation---\n        Requirements_Tasks    \n    end\nDefine_and_Document_List-->Quality_Planning-->Configuration_Management-->Task_Requirements\nend     \n\n\n\n\n\n\n\n\n\n\nQuality Planning\n\n\n\n\nflowchart TB\nsubgraph Quality_Planning\n    direction LR\n    subgraph Quality_Planning_Tasks\n        direction TB\n        Risk_Hazard_Management_Plan---\n        Configuration_Management_Plan---\n        Software_Quality_Assurance_Plan---\n        Software_Verification_and_Validation_Plan---\n        Verification_and_Validation_Tasks---\n        Acceptance_Criteria---\n        Schedule_and_Resource_Allocation_for_V&V_activities---\n        Reporting_Requirements---\n        Formal_Design_Review_Requirements---\n        Other_Technical_Review_Requirements---\n        Problem_Reporting_and_Resolution_Procedures---\n        Other_Support_Activities\n    end\n    subgraph Inclusion_List_for_Plan\n        direction TB\n        specific_tasks_for_each_life_cycle_activity---\n        Enumeration_of_important_quality_factors--- \n        like_reliability_maintainability_usability---\n        Methods_and_procedures_for_each_task---\n        Task_acceptance_criteria---\n        Criteria_for_defining_and_documenting_outputs_for_input_requirements---\n        Inputs_for_each_task---\n        Outputs_from_each_task---\n        Roles_resources_and_responsibilities_for_each_task---\n        Risks_and_assumptions---\n        Documentation_of_user_needs    \n    end\n    subgraph Identification_List_for_Plan\n        direction TB\n        personnel---\n        facility_and_equipment_resources_for_each_task---\n        role_that_risk_hazard_management        \n    end\nQuality_Planning_Tasks-->Inclusion_List_for_Plan-->Identification_List_for_Plan\nend\n\n\n\n\n\n\n\n\n\n\nConfiguration Management\n\n\n\n\nflowchart LR\nsubgraph Configuration_Management\n    direction LR\n    subgraph Control\n        direction TB\n        control_multiple_parallel_development_activities---\n        ensure_positive_and_correct_correspondence_of---\n        specifications_documents---\n        source_code---\n        object_code---\n        test_suites---\n        ensure_accurate_identification_of_approved_versions---\n        ensure_access_to_approved_versions---\n        create_procedures_for_reporting---\n        create_procedures_for_resolving_SW_anomalies                            \n    end\n    subgraph Management\n        direction TB\n        identify_reports---\n        specify_contents---\n        specify_format---\n        specify_responsible_organizational_elements_for_each_report\n    end\n    subgraph Procedures\n        direction TB\n        necessary_for_review_of_SW_development_results---\n        necessary_for_approval_of_SW_development_results\n    end\n    ensure_proper_communications_and_documentation\n    Control-->Management-->Procedures-->ensure_proper_communications_and_documentation \nend\n\n\n\n\n\n\n\n\n\n\nTask Requirements\n\n\n\n\n\nflowchart TB\n    subgraph Task_Requirements\n        direction LR\n        subgraph group\n            direction TB\n            identification---\n            analysis---\n            predetermined_documentation_about_device_its_intended_use\n        end\n        \n        subgraph Requirements_Specification_List\n            direction TB\n            All_software_system_inputs---\n            All_software_system_outputs---\n            All_functions_that_software_system_will_perform---\n            All_performance_requirements_that_software_will_meet---\n            requirement_example_data_throughput_reliability_timing---\n            definition_of_all_external_and_user_interfaces---\n            any_internal_software_to_system_interfaces---\n            How_users_will_interact_with_system---\n            What_constitutes_error---\n            how_errors_should_be_handled---\n            Required_response_times---\n            Intended_operating_environment_for_software---\n            All_acceptable_ranges_limits_defaults_specific_values---\n            All_safety_related_requirements_that_will_be_implemented_in_SW---\n            All_safety_related_specifications_that_will_be_implemented_in_SW---\n            All_safety_related_features_that_will_be_implemented_in_SW---\n            All_safety_related_functions_that_will_be_implemented_in_SW---\n            clearly_identify_potential_hazards---\n            risk_evaluation_for_accuracy---\n            risk_evaluation_for_completeness---\n            risk_evaluation_for_consistency---\n            risk_evaluation_for_testability---\n            risk_evaluation_for_correctness---\n            risk_evaluation_for_clarity\n        end\n        subgraph Verfification_List_by_Evaluation\n            direction TB\n            no_internal_inconsistencies_among_requirements---\n            All_of_performance_requirements_for_system---\n            Complete_correct_Fault_tolerance_safety_security_requirements---\n            Accurate_Complete_Allocation_of_software_functions---\n            Appropriate_Software_requirements_for_system_hazards---\n            mesurable_requirements---\n            objectively_verifiable_requirements---\n            traceable_requirements\n        end\n        subgraph Requirements_Tasks\n            direction TB\n            Preliminary_Risk_Analysis---\n            Traceability_Analysis---\n            ex_Software_Requirements_to_System_Requirements_vice_versa---\n            ex_Software_Requirements_to_Risk_Analysis---\n            Description_of_User_Characteristics---\n            Listing_of_Characteristics_and_Limitations_of_Memory---\n            Software_Requirements_Evaluation---\n            Software_User_Interface_Requirements_Analysis---\n            System_Test_Plan_Generation---\n            Acceptance_Test_Plan_Generation---\n            Ambiguity_Review_or_Analysis\n        end\n    group-->Requirements_Specification_List \n    Verfification_List_by_Evaluation-->Requirements_Tasks\n    end\n\n\n\n\n\n\n\n\n\n\nDesign Overview\n\n\n\n\nflowchart TB\n    subgraph Deign_Task\n        direction LR\n    subgraph Design_Consideration_List\n        direction TB\n        subgraph Description\n                    direction TB\n                end\n        subgraph Human_Factors_Engineering\n          direction TB\n    \n        end\n        subgraph Safety_Usability_Issues_Conisderation\n            direction TB\n\n            end\n        Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n    subgraph Design_Specificiation\n        direction TB\n        subgraph Performing_List\n            direction TB\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n        end\n    Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design \n    end\n    subgraph Design_Activity_and_Task_List\n        direction TB\n        subgraph Final_Design_activity\n            direction TB\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n            end\n            subgraph Coding_Tasks\n                direction TB\n            end\n        traceability_analysis-->Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end\n    Design_Consideration_List---Design_Specificiation---Design_Activity_and_Task_List\n\n    end\n\n\n\n\n\n\n\n\n\nDesign Consideration\n\n\n\n\nflowchart TB\nsubgraph Design_Consideration_List\n    direction LR\n        subgraph Requirement_Specification\n            direction TB\n            logical_representation---\n            physical_representation\n        end\n    subgraph Description\n            direction TB\n            what_to_do---\n            how_to_do                   \n        end\n    subgraph Human_Factors_Engineering\n      direction TB\n            entire_design_and_development_process---\n            device_design_requirements---\n            analyses---\n            tests\n    end\n    subgraph Safety_Usability_Issues_Conisderation\n        direction TB\n                flowcharts--- \n                state_diagrams--- \n                prototyping_tools---\n                test_plans\n        end\n        Requirement_Specification---Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n\n\n\n\n\n\n\n\n\n\nDesign Specification\n\n\n\n\nflowchart TB\nsubgraph Design_Specificiation\n        direction LR\n        subgraph Conceptual_Specification\n            direction TB\n            requirements_specification---\n            predetermined_criteria---\n            Software_risk_analysis---\n            Development_procedures---\n            coding_guidelines\n        end\n        subgraph Performing_List\n            direction TB\n            task---\n            function_analyses---\n            risk_analyses---\n            prototype_tests_and_reviews---\n            full_usability_tests\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n            SW_requirements_specification---\n            predetermined_criteria_for_SW_acceptance---\n            SW_risk_analysis---\n            Development_procedure_list---\n            coding_guidance---\n            Systems_documentation---\n            Hardware_to_be_used---\n            Parameters_to_be_measured---\n            Logical_structure---\n            Control_logic---\n            logical_processing_steps_aka_algorithms---\n            Data_structures_diagram---\n            data_flow_diagrams---\n            Definitions_of_variables---\n            description_of_where_they_are_used---\n            Error_alarm_and_warning_messages---\n            Supporting_software---\n            internal_modules_Communication_links---\n            supporting_sw_links---\n            link_with_hardware---\n            link_with_user---\n            physical_Security_measures---\n            logical_security_measures\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n            complete--- \n            correct---\n            consistent--- \n            unambiguous--- \n            feasible---\n            maintainable---\n            analyses_of_control_flow---\n            data_flow--- \n            complexity--- \n            timing--- \n            sizing--- \n            memory_allocation---\n            module_architecture---\n            traceability_analysis_of_modules--- \n            criticality_analysis\n        end\n    Conceptual_Specification---Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design  \n    end\n\n\n\n\n\n\n\n\n\n\nDesign Activity and Task\n\n\n\n\n\nflowchart TB\nsubgraph Design_Activity_and_Task_List\n        direction LR\n        subgraph Final_Design_activity\n            direction TB\n            Formal_Design_Review_Before_Design_Implementation---\n            correct_consistent_complete_accurate_testable\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n            Updated_Software_Risk_Analysis---\n            Traceability_Analysis---\n            Software_Design_Evaluation---\n            Design_Communication_Link_Analysis---\n            Module_Test_Plan_Generation---\n            Integration_Test_Plan_Generation---\n            module_Test_Design_Generation---\n            integration_Test_Design_Generation---\n            system_Test_Design_Generation---\n            acceptance_Test_Design_Generation   \n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n                each_element_implementation---\n                each_module_implementation_to_element_and_risk_analysis---\n                each_functions_implemented_to_element_and_risk_analysis---\n                Tests_for_modules_to_element_and_risk_analysis--- \n                Tests_for_functions_to_element_and_risk_analysis---\n                Tests_for_modules_to_source_code---\n                Tests_for_functions_to_source_code\n            end\n            subgraph Coding_Tasks\n                direction TB\n                Traceability_Analyses---\n                Source_Code_to_Design_Specification_and_vice_versa---\n                Test_Cases_to_Source_Code_and_to_Design_Specification---\n                Source_Code_and_Source_Code_Documentation_Evaluation---\n                Source_Code_Interface_Analysis---\n                Test_Procedure_and_Test_Case_Generation \n            end\n        traceability_analysis-->Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end"
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#testing-task",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#testing-task",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Testing Task",
    "text": "Testing Task\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction TB\n            subgraph Test_Plans\n                direction TB\n            end\n            subgraph Conditions\n                direction TB\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n        subgraph Code_Based_Testing\n            direction TB\n            subgraph white_box_testing\n                direction TB\n            end\n            subgraph Evaluation_of_level_of_white_box_testing\n                direction TB\n            end\n            subgraph Coverage_Metrics_of_White_Box_Testing\n                direction TB\n            end\n        white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n        end\n        subgraph Alternatives_to_White_Box_Testing\n            direction TB\n            subgraph Types_of_Functional_Software_Testing_Increasing_Cost\n                direction TB\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n            end\n            subgraph Change_in_SW\n                direction TB    \n            end\n        Types_of_Functional_Software_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW\n        end\n        \n\n        subgraph Development_Testing\n            direction TB\n            subgraph unit_level_testing\n                direction TB    \n            end\n            subgraph integration_level_testing\n                direction TB\n            end\n            subgraph system_level_testing\n                direction TB\n            end\n            subgraph Error_Detected\n                direction TB        \n            end\n        unit_level_testing-->integration_level_testing-->system_level_testing-->Error_Detected\n        end\n\n        subgraph Testing_Tasks\n            direction TB\n        end\n        subgraph User_Site_Testing\n            direction TB\n            subgraph Quality_System_Rregulation\n                direction TB\n            end\n            subgraph Understand_Terminology\n                direction TB\n            end\n            subgraph Testing\n                direction TB\n            end\n            Quality_System_Rregulation---Understand_Terminology---Testing\n        end\nConsideration_Before_Testing_Tasks---Code_Based_Testing---Alternatives_to_White_Box_Testing\nDevelopment_Testing---Testing_Tasks---User_Site_Testing\n    end\n\n\n\n\n\n\n\n\n\n\nConsideration Before Testing Tasks\n\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction LR\n            subgraph Test_Plans\n                direction TB\n                should_identify_control_measures_like_traceability_analysis---\n                ensure_that_intended_coverage_is_achieved---\n                ensure_that_proper_documentation_is_prepared---\n                conduct_tests_not_by_SW_developers_but_in_other_sites\n            end\n            subgraph Conditions\n                direction TB\n                use_defined_inputs---\n                documented_outcomes---\n                gonnabe_time_consuming_activity---\n                gonnabe_difficult_activity---\n                gonnabe_imperfect_activity---\n                testing_all_program_functionality---\n                does_not_mean_100_prcnt_correction_perfection---\n                make_detailed_objective_evaluation---\n                requires_sophisticated_definition_specificiation---\n                all_test_procedures_data_results_are_documented---\n                all_test_procedures_data_results_are_suitable_for_review---\n                all_test_procedures_data_results_are_suitable_for_objective_decision_making---\n                all_test_procedures_data_results_are_suitable_for_subsequent_regression_testing\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n                make_test_plans---\n                make_test_cases---\n                plan_schedules---\n                plan_environments---\n                plan_resources_of_personnel_tools---\n                plan_methodologies---\n                plan_inputs_procedures_outputs_expected_results---\n                plan_documentation---\n                plan_reporting_criteria\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n                expected_test_outcome_is_predefined---\n                good_test_case_has_high_probability_of_exposing_errors---\n                successful_test_is_one_that_finds_errors---\n                There_is_independence_from_coding---\n                Both_application_for_user_and_SW_for_programming_expertise_are_employed---\n                Testers_use_different_tools_from_coders---\n                Examining_only_the_usual_case_is_insufficient---\n                Test_documentation_permits_its_reuse---\n                Test_documentation_permits_independent_confirmation_---\n                of_pass/fail_test_outcome_during_subsequent_review\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n\nend\n\n\n\n\n\n\n\n\n\n\nCode Based Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n            subgraph Code_Based_Testing\n                direction LR\n                subgraph white_box_testing\n                    direction TB\n                    identify_dead_code_never_executed---\n                    conduct_unit_test---\n                    conduct_other_level_tests\n                end\n                subgraph Evaluation_of_level_of_white_box_testing\n                    direction TB\n                    use_coverage_metrics---\n                    metrics_of_completeness_of_test_selection_criteria---\n                    coverage_should_be_commensurate_with_level_of_SW_risk---\n                    coverage_means_100_prcnt_coverage\n                end\n                subgraph Coverage_Metrics_of_White_Box_Testing\n                    direction TB\n                    Statement_Coverage---\n                    Decision_or_Branch_Coverage---\n                    Condition_Coverage---\n                    Multi_Condition_Coverage\n                    Loop_Coverage---\n                    Path_Coverage---\n                    Data_Flow_Coverage\n                end\n            white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n            end\nend\n\n\n\n\n\n\n\n\n\n\nSolution to White Box Testing\n\n\n\n\n\n \nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Alternatives_to_White_Box_Testing\n            direction LR\n            subgraph Types_of_Testing_Increasing_Cost\n                direction TB\n                    Normal_Case---\n                    Output_Forcing---\n                    Robustness---\n                    Combinations_of_Inputs\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n                difficulty_in_linking_---\n                tests_completion_criteria_to_SW_reliability\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n                statistical_testing---\n                provide_further_assurance_of_reliability---\n                generate_randomly_test_data_from_defined_distributions---\n                distribution_defined_by_expected_use---\n                distribution_defined_by_hazardous_use---\n                distribution_defined_by_malicious_use---\n                large_test_data_cover_particular_areas_or_concerns---\n                statistical_testing_provides_high_structural_coverage---\n                statistical_testing_requires_stable_system---\n                structural_and_functional_testing_are_prerequisites_for_statistical_testing\n            end\n            subgraph Change_in_SW\n                direction TB\n                conduct_regression_analysis_and_testing---\n                should_demonstrate_correct_implementation---\n                should_demonstrate_no_adverse_impact_on_other_modules   \n            end\n            subgraph Testing_Tasks\n                direction TB\n                Test_Planning---\n                Structural_Test_Case_Identification---\n                Functional_Test_Case_Identification---\n                Traceability_Analysis_Testing---\n                Unit_Tests_to_Detailed_Design---\n                Integration_Tests_to_High_Level_Design---\n                System_Tests_to_Software_Requirements---\n                Unit_Test_Execution---\n                Integration_Test_Execution---\n                Functional_Test_Execution---\n                System_Test_Execution---\n                Acceptance_Test_Execution---\n                Test_Results_Evaluation---\n                Error_Evaluation_Resolution---\n                Final_Test_Report\n            end\n        Types_of_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW---Testing_Tasks\n        end\nend\n\n\n\n\n\n\n\n\n\n\nDevelopment Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Development_Testing\n            direction LR\n            subgraph unit_level_testing\n                direction TB    \n                focus_on_early_examination_of_sub_program_functionality---\n                ensure_functionality_invisible_at_system_level_examined---\n                ensure_quality_software_units_furnished_for_integration\n            end\n            subgraph integration_level_testing\n                direction TB\n                focuses_on_transfer_of_data---\n                focuses_on_control_across_program's_internal_and_external_interfaces\n            end\n            subgraph system_level_testing\n                direction TB\n                demonstrate_all_specified_functionality_exists---\n                demonstrate_SW_is_trustworthy---\n                verifies_as_built_program's_functionality_and_performance_on_requirements---\n                addresses_functional_concerns_and_intended_uses---\n                like_Performance_issues---\n                like_Responses_to_stress_conditions---\n                like_Operation_of_internal_and_external_security_features---\n                like_Effectiveness_of_recovery_procedures---\n                like_disaster_recovery---\n                like_Usability---\n                like_Compatibility_with_other_SW---\n                like_Behavior_in_each_of_the_defined_hardware_configurations---\n                like_Accuracy_of_documentation\n            end\n            subgraph Error_Detected\n                direction TB        \n                should_be_logged---\n                should_be_classified---\n                should_be_reviewed---\n                should_be_resolved_before_SW_release\n            end\n        unit_level_testing-->integration_level_testing-->system_level_testing-->Error_Detected\n        end\nend\n\n\n\n\n\n\n\n\n\n\nUser Site Testing\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph User_Site_Testing\n            direction LR\n            subgraph Quality_System_Rregulation\n                direction TB\n                installation---\n                inspection_procedures---\n                testing_appropriateness---\n                documentation_of_inspection---\n                testing_to_demonstrate_proper_installation\n            end\n            subgraph Understand_Terminology\n                direction TB\n                beta_test---\n                site_validation---\n                user_acceptance_test---\n                installation_verification---\n                installation_testing\n            end\n            subgraph Testing\n                direction TB\n                subgraph Requirements\n                    direction TB\n                    either_actual_or_simulated_use---\n                    verification_of_intended_functionality---\n                    constant_contact_FDA_center\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n    \n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_System_Ability\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_User_Ability\n                        direction TB\n        \n                    end \n                    subgraph Evaluation_of_Operator_Ability\n                        direction TB\n        \n                    end\n                constant_contact_FDA_center-->Follow_Predefiened_Plan-->Documented_Evidence-->\n            Evaluation_of_System_Ability-->Evaluation_of_User_Ability-->Evaluation_of_Operator_Ability    \n                end \n                        \n            \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements-->User_Site_Testing_Task\n        end\n        Quality_System_Rregulation-->    Understand_Terminology-->Testing-->User_Site_Testing_Task\n        end\nend\n\n\n\n\n\n\n\n\n\n\nTesting\n\n\n\n\nflowchart TB\n            subgraph Testing\n                direction LR\n                subgraph Requirements\n                    direction LR\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n                        either_actual_or_simulated_use---\n                        verification_of_intended_functionality---\n                        constant_contact_FDA_center---\n                        formal_summary_of_testing---\n                        record_of_formal_acceptance\n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n                        testing_plan_of_full_range_of_operating_conditions---\n                        testing_plan_to_detect_any_latent_faults---\n                        all_testing_procedures---\n                        test_input_data---\n                        test_results---\n                        hardware_installation_and_configuration---\n                        software_installation_and_configuration---\n                        exercising_measure_of_all_system_components---\n                        versions_of_all_system_components           \n                    end\n                    subgraph Evaluation\n                        direction TB\n                      subgraph Evaluation_of_System_Ability\n                            direction TB\n                            high_volume_of_data---\n                            heavy_loads_or_stresses---\n                            security\n                            subgraph fault_testing\n                                direction TB\n                                avoidance---\n                                detection---\n                                tolerance---\n                                recovery\n                            end\n                        security---fault_testing---\n                        error_message---\n                        implementation_of_safety_requirements\n                        end\n                      subgraph Evaluation_of_User_Ability\n                            direction TB\n                            ability_to_understand_system---\n                            ability_to_interface_with_system\n                        end \n                        subgraph Evaluation_of_Operator_Ability\n                            direction TB\n                            ability_to_perform_intended_functions---\n                            ability_to_respond_in_alarms---\n                            ability_to_respond_in_warnings---\n                            ability_to_respond_in_error_messages\n                        end\n\n                    end\n            Follow_Predefiened_Plan-->Documented_Evidence-->\n            Evaluation_of_System_Ability-->Evaluation_of_User_Ability-->Evaluation_of_Operator_Ability    \n            end     \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements-->User_Site_Testing_Task\n        end"
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#maintenance-and-software-changes",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#maintenance-and-software-changes",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Maintenance and Software Changes",
    "text": "Maintenance and Software Changes\n\n\n\n\nflowchart LR\n    subgraph Hardware_VS_Software\n        direction LR\n        subgraph HW_maintenance_Inclusion\n            direction TB\n            preventive_hardware_maintenance_actions--- \n            component_replacement---\n            corrective_changes\n        end\n        subgraph SW_maintenance_Inclusion\n            direction TB\n            corrective---\n            perfective---\n            adaptive_maintenance---\n            not_include_preventive_maintenance_actions---\n            not_include_software_component_replacement\n        end\n    end\n    subgraph Maintenance_Type\n        direction TB\n        Corrective_maintenance---\n        Perfective_maintenance---\n        Adaptive_maintenance---\n        Sufficient_regression_analysis---\n        Sufficient_regression_testing\n    end\n    subgraph Factors_of_Validation_for_SW_change\n        direction TB\n        type_of_change---\n        development_products_affected---\n        impact_of_those_products_on_operation\n    end\n    subgraph Factors_of_Limitting_Validation_Effort\n        direction TB\n        documentation_of_design_structure---\n        documentation_of_interrelationships_of_modules---\n        documentation_of_interrelationships_of_interfaces---\n        test_documentation---\n    test_cases---\n        results_of_previous_verification_and_validation_testing\n    end\n    subgraph Maintenance_tasks\n        direction TB\n        Software_Validation_Plan_Revision---\n        Anomaly_Evaluation---\n        Problem_Identification_and_Resolution_Tracking---\n        Proposed_Change_Assessment---\n        Task_Iteration---\n        Documentation_Updating\n    end\nHardware_VS_Software-->Maintenance_Type-->Factors_of_Validation_for_SW_change-->\nFactors_of_Limitting_Validation_Effort-->Maintenance_tasks"
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation-of-quality-system-software",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/FDA_SW_validation_diagram.html#validation-of-quality-system-software",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation of Quality System Software",
    "text": "Validation of Quality System Software\n\nOverview\n\n\n\n\nflowchart LR\n    subgraph Use_of_Computers_and_automated_equipment\n        direction TB\n        medical_device_design---\n        laboratory_testing_and_analysis---\n        product_inspection_and_acceptance---\n        production_and_process_control---\n        environmental_controls---\n        packaging---\n        labeling---\n        traceability---\n        document_control---\n        complaint_management---\n        programmable_logic_controllers---\n        digital_function_controllers---\n        statistical_process_control---\n        supervisory_control_and_data_acquisition---\n        robotics---\n        human_machine_interfaces---\n        input_output_devices---\n        computer_operating_systems\n    end\n    subgraph Factors_in_Validation\n        direction TB\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System-->Validation_Supporting_Factors-->Factors_of_Validation_Evidence_Level-->\nFactors_of_Easing_Validation_Effort\n    end\n    subgraph Documented_User_Requirements\n        direction TB\n        intended_use_of_software_or_automated_equipment---\n      level_of_dependency_on_software_or_equipment\n    end\n    subgraph List_That_Must_Be_Defined_by_User\n        direction TB\n        \n    end\n    subgraph Documentation_List\n        direction TB\n        documented_protocol---\n        documented_validation_results\n        subgraph Documented_Test_Cases\n            direction TB\n        \n        end\n        documented_validation_results---Documented_Test_Cases\n    end\n\n    subgraph Manufaturer's_Responsbility\n        direction TB\n        \n    end\nUse_of_Computers_and_automated_equipment---Factors_in_Validation---Documented_User_Requirements---\nList_That_Must_Be_Defined_by_User---Documentation_List---Manufaturer's_Responsbility\n\n\n\n\n\n\n\n\n\n\nFactors in Validation\n\n\n\n\nflowchart LR\n    subgraph Factors_in_Validation\n        direction LR\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n                electronic_records_regulation---\n                electronic_signatures_regulation---\n                regulations_establishment---\n                security---\n                data_integrity---\n                validation_requirements \n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n            verifications_of_outputs_from_each_stage--- \n            verifications_of_outputs_throught_SW_life_cycle---\n            checking_for_proper_operation_in_intended_use_environment\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n            risk_posed_by_automated_operation---\n            complexity_of_process_software---\n            degree_of_dependence_on_automated_process\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System-->Validation_Supporting_Factors-->Factors_of_Validation_Evidence_Level-->\nFactors_of_Easing_Validation_Effort\n    end"
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/index.html",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/index.html",
    "title": "FDA Software Validation Guidance Summary",
    "section": "",
    "text": "I am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto.\nThe FDA validation guidance document is a bit difficult to understand because its explanations provides abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\n\n\n2022-12-28, download this article as PDF\n2022-12-28, summary with diagrams\n\n\n\n\nFDA: General Principles of Software Validation\n\n\n\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\n\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/index.html#purpose",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/index.html#purpose",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.1 Purpose",
    "text": "2.1 Purpose\nThe purpose is to make a sketch of general validation principle of the validation of medical device software or software used to design or develop."
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/index.html#scope",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/index.html#scope",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.2 Scope",
    "text": "2.2 Scope\nThe scope of this guidance is broad. The important activities for the software validation include at least:\n\nplanning,\nverfication,\ntesting,\ntraceability, and\nconfiguration management.\n\nAll of the activities above should be\n\nintegrated\nbe able to describe software life cycle management and\nbe able to describe software risk management.\n\nThe software validation and verification activities should be focused into the entire software life cycle. (It does not necessarily mean that the activies must follow any technical models.)\nThe guidance is applicable to any software related to a regulated medical device and anyone who is employed in a bio or medical industry.\n\n2.2.1 The Least Burdensome Approach\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\n2.2.2 Regulatory Requirements for Software Validation\n\nSoftware validation: a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997. (See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\nSpecific requirements for validation of device software are found in 21 CFR §820.30(g). Other design controls, such as planning, input, verification, and reviews, are required for medical device software. (See 21 CFR §820.30.)\ncomputer systems used to create, modify, and maintain electronic records and to manage electronic signatures are also subject to the validation requirements. (See 21 CFR §11.10(a).)\n\n\n2.2.2.1 Objective\nThe objective of software validation is to ensure:\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\n2.2.2.2 What to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\n\n2.2.3 Quality System Regulation vs Pre-market Submissions\nThis document does not address any specific requirements but general ones. Specific issues should be addressed to\n\nthe Office of Device Evaluation (ODE),\nCenter for Devices and Radiological Health (CDRH)\nthe Office of Blood Research and Review,\nCenter for Biologics Evaluation and Research (CBER). See the references in Appendix A for applicable FDA guidance documents for pre-market submissions."
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/index.html#context-for-software-validation",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/index.html#context-for-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.3 Context for Software Validation",
    "text": "2.3 Context for Software Validation\n\nValidation elements that FDA expects to do for the Quality System regulation, using the principles and tasks are listed in Sections 4 and 5.\nAdditional specific information is available from many of the references listed in Appendix A\n\n\n2.3.1 Definition and Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology between the medical device Quality System regulation and the software industry:\n\nrequirements,\nspecification,\nverification, and\nvalidation.\n\n\n\n2.3.1.1 Requirements and Specifications\nThe Quality System regulation states\n\nthat design input requirements must be documented and\nthat specified requirements must be verified\n\nBut, the regulation does not further clarify the distinction between the terms “requirement” and “specification.”\n\nRequirement\n\ncan be any need or expectation for a system or for its software.\nreflects the stated or implied needs of the customer: requirements may be\n\nmarket-based,\ncontractual,\nstatutory, or\nan organization’s internal requirements.\n\nvarious examples of requirements\n\ndesign, functional, implementation, interface, performance, or physical requirements\n\nSoftware requirements derived from the system requirements for those aspects of system functionality\nSoftware requirements are typically stated in functional terms and are defined, refined, and updated as a development project progresses.\nSuccess in accurately and completely documenting software requirements is a crucial factor in successful validation of the resulting software.\n\nSpecification\n\ndefined as “a document that states requirements.” (See 21 CFR §820.3(y).)\nIt may refer to or include drawings, patterns, or other relevant documents\nIt usually indicates the means and the criteria whereby conformity with the requirement can be checked.\nVarious examples of written specifications\n\nsystem requirements specification,\nsoftware requirements specification,\nsoftware design specification,\nsoftware test specification,\nsoftware integration specification, etc.\n\nAll of these documents are design outputs for which various forms of verification are necessary.\n\n\n\n\n2.3.1.2 Verifiaction and Validation\nThe Quality System regulation is harmonized with ISO 8402:1994, which treats “verification” and “validation” as separate and distinct terms.\n\nSoftware verification\n\nIt provides objective evidence that the design outputs of a particular phase of the software development life cycle meet all of the specified requirements for that phase.\nIt looks for\n\nconsistency,\ncompleteness, and\ncorrectness of the software and its supporting documentation\n\nSoftware testing\n\nverification activities intended to confirm that software development output meets its input requirements.\n\nTypes of verification activities include\n\nvarious static and dynamic analyses,\ncode and document inspections,\nwalkthroughs, and other techniques.\n\n\nSoftware Validation\n\nConfirmation by examination and provision of the following objective evidence:\nEvidence 1: software specifications conform to user needs and intended uses, and\nEvidnece 2: the particular requirements implemented through software can be consistently fulfilled.\nEvidnece 3: all software requirements have been implemented correctly and completely and are traceable to system requirements.\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device.\n\nDifficulty in Software verification and validation\n\na developer cannot test forever, and\nit is difficult to know how much evidence is enough.\nIn large measure, software validation is a matter of developing a “level of confidence” that the device meets all requirements and user expectations for the software automated functions and features of the device.\nConsiderations for an acceptable level of confidence\n\nmeasures such as defects found in specifications documents,\nestimates of defects remaining,\ntesting coverage, and other techniques are all used to develop before shipping the product.\nHowever, a level of confidence varies depending upon the safety risk (hazard) posed by the automated functions of the device. (Info on safety risk is found in Section 4 and in the international standards ISO/IEC 14971-1 and IEC 60601-1-4 referenced in Appendix A).\n\n\n\n\n\n2.3.1.3 IQ/OQ/PQ\nIQ/OQ/PQ are the terminology related to user site software validation\n\nInstallation qualification (IQ)\nOperational qualification (OQ)\nPerformance qualification (PQ).\n\nDefinitions of these terms may be found in FDA’s Guideline on General Principles of Process Validation, dated May 11, 1987, and in FDA’s Glossary of Computerized System and Software Development Terminology, dated August 1995. Both FDA personnel and device manufacturers need to be aware of these differences in terminology as they ask for and provide information regarding software validation.\n\n\n\n2.3.2 Software Development as Part of System Design\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nthe user’s needs\nintended uses from which the product is developed.\n\nA primary goal of software validation is to then demonstrate that all completed software products comply with all documented software and system requirements.\n\n\n2.3.3 Software Is Different from Hardware\nSoftware engineering needs an even greater level of managerial scrutiny and control than does hardware engineering.\n\n\n2.3.4 Benefits of Software Validation\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nSoftware validation can also reduce long term costs by making it easier and less costly to reliably modify software and revalidate software changes.\n\n\n\n2.3.5 Design Review\nDesign reviews are documented, comprehensive, and systematic examinations of a design to evaluate\n\nthe adequacy of the design requirements,\nthe capability of the design to meet these requirements, and\nto identify problems.\n\nDesign review is a primary tool for managing and evaluating development projects.\n\nIt is strongly recommended that it should be formal design because it is more structured than the informal one.\nIt includes participation from others outside the development team.\nIt may review reference or include results from other formal and informal reviews.\nDesign reviews should include\n\nexamination of development plans,\nrequirements specifications,\ndesign specifications,\ntesting plans and procedures,\nall other documents and activities associated with the project,\nverification results from each stage of the defined life cycle, and\nvalidation results for the overall device.\n\nThe Quality System regulation requires that at least one formal design review be conducted during the device design process. However, it is recommended that multiple design reviews be conducted\n\n(e.g., at the end of each software life cycle activity, in preparation for proceeding to the next activity).\n\nFormal design reviews documented should include:\n\nthe appropriate tasks and expected results, outputs, or products been established for each software life cycle activity\ncorrectness, completeness, consistency, and accuracy\nsatisfaction for the standards, practices, and conventions of that activity\nestablishment of a proper basis for initiating tasks for the next software life cycle activity"
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/index.html#principles-of-software-validation",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/index.html#principles-of-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.4 Principles of Software Validation",
    "text": "2.4 Principles of Software Validation\n\n2.4.1 Requirements\nA documented software requirements specification provides a baseline for both validation and verification. The software validation process must include an established software requirements specification (Ref: 21 CFR 820.3(z) and (aa) and 820.30(f) and (g)).\n\n\n2.4.2 Defect Prevention\nIn order to establish that confidence, software developers should use a mixture of methods and techniques to prevent software errors and to detect software errors that do occur.\n\n\n2.4.3 Time and Effort\nPreparation for software validation should begin early, i.e., during design and development planning and design input. The final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\n2.4.4 Software Life Cycle\n\nSoftware validation takes place within the environment of an established software life cycle.\nThe software life cycle contains software engineering tasks and documentation necessary to support the software validation effort.\nspecific verification and validation tasks need to be appropriate for the intended use of the software\n\n\n\n2.4.5 Plans\n\nThe software validation process is defined and controlled through the use of a plan.\nThe software validation plan defines “what” is to be accomplished through the software validation effort.\nSoftware validation plans specify areas such as\n\nscope,\napproach,\nresources,\nschedules and the types and extent of activities,\ntasks, and\nwork items.\n\n\n\n\n2.4.6 Procedures\nThe software validation process is executed through the use of procedures. These procedures establish “how” to conduct the software validation effort. The procedures should identify the specific actions or sequence of actions that must be taken to complete individual validation activities, tasks, and work items.\n\n\n2.4.7 Software Validation After a Change\n\nDue to the complexity of software, a small local change may have a significant global system impact.\nIf a change exists in the software, the whole validation status of the software needs to be re-established.\nneed to determine the extent and impact of that change on the entire software system.\nthe software developer should then conduct an appropriate level of software regression testing to show that unchanged but vulnerable portions of the system have not been adversely affected.\n\n\n\n2.4.8 Validation Coverage\n\nValidation coverage should be based on the software’s complexity and safety risk.\nThe selection of validation activities, tasks, and work items should be commensurate with the complexity of the software design and the risk associated with the use of the software for the specified intended use.\n\n\n\n2.4.9 Independence of Review\n\nValidation activities should be based on the basic quality assurance precept of “independence of review.”\nSelf-validation is extremely difficult.\nWhen possible, an independent evaluation is always better (like a contracted third-party independent verification and validation)\nAnother approach is to assign internal staff members that are not involved in a particular design or its implementation, but who have sufficient knowledge to evaluate the project and conduct the verification and validation activities.\n\n\n\n2.4.10 Flexibility and Responsibility\nThe device manufacturer has flexibility in choosing how to apply these validation principles, but retains ultimate responsibility for demonstrating that the software has been validated. FDA regulated medical device applications include software that:\n\nIs a component, part, or accessory of a medical device;\n\ncomponents: e.g., application software, operating systems, compilers, debuggers, configuration management tools, and many more\n\nIs itself a medical device; or\nIs used in manufacturing, design and development, or other parts of the quality system.\nNo matter how complex and disperse the software is, the manufacturer is in charge of responsibility for software validation."
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/index.html#activities-and-tasks",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/index.html#activities-and-tasks",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.5 Activities and Tasks",
    "text": "2.5 Activities and Tasks\nSoftware validation is accomplished through a series of activities and tasks that are planned and executed at various stages of the software development life cycle. These tasks may be\n\none time occurrences\niterated many times\n\n\n2.5.1 Software Life Cycle Activities\n\nSoftware developers should establish a software life cycle model that is appropriate for their product and organization.\nThe selected software life cycle model should cover the software from its birth to its retirement.\nActivities in a typical software life cycle model:\n\nQuality Planning\nSystem Requirements Definition\nDetailed Software Requirements Specification\nSoftware Design Specification\nConstruction or Coding\nTesting\nInstallation\nOperation and Support\nMaintenance\nRetirement\n\nVerification, testing, and other tasks that support software validation occur during each of these activities.\nSeveral software life cycle models defined in FDA’s Glossary of Computerized System and Software Development\n\nTerminology dated August 1995:\n\nwaterfall\nspiral\nrapid prototyping\nincremental development, etc.\n\n\n\n2.5.2 Typical Tasks Supporting Validation\nthe software developer should at least consider each of the risk-related tasks and should define and document which tasks are or are not appropriate for their specific application.\n\n2.5.2.1 Quality Planning\nDesign and development planning should culminate in a plan that identifies\n\nnecessary tasks,\nprocedures for anomaly reporting and resolution,\nnecessary resources, and\nmanagement review requirements including formal design reviews.\n\nThe plan should include:\n\nThe specific tasks for each life cycle activity;\nEnumeration of important quality factors (e.g., reliability, maintainability, and usability);\nMethods and procedures for each task;\nTask acceptance criteria;\nCriteria for defining and documenting outputs in terms that will allow evaluation of their conformance to input requirements;\nInputs for each task;\nOutputs from each task;\nRoles, resources, and responsibilities for each task;\nRisks and assumptions; and\nDocumentation of user needs.\n\nThe plan should identify\n\nthe personnel,\nthe facility and equipment resources for each task, and\nthe role that risk (hazard) management will play.\n\nA configuration management plan should be developed that will guide and control multiple parallel development activities and ensure proper communications and documentation.\nControls are necessary to ensure positive and correct correspondence among all approved versions of the specifications documents, source code, object code, and test suites that comprise a software system. The controls also should ensure accurate identification of, and access to, the currently approved versions.\nProcedures should be created for reporting and resolving software anomalies found through validation or other activities.\nManagement should identify the reports and specify the contents, format, and responsible organizational elements for each report. Procedures also are necessary for the review and approval of software development results, including the responsible organizational elements for such reviews and approvals.\nTypical Tasks – Quality Planning\n\nRisk (Hazard) Management Plan\nConfiguration Management Plan\nSoftware Quality Assurance Plan\n\nSoftware Verification and Validation Plan\n\nVerification and Validation Tasks, and Acceptance Criteria\nSchedule and Resource Allocation (for software verification and validation activities)\nReporting Requirements\n\nFormal Design Review Requirements\nOther Technical Review Requirements\n\nProblem Reporting and Resolution Procedures\nOther Support Activities\n\n\n\n2.5.2.2 Requirements\nRequirements development includes the\n\nidentification,\nanalysis, and\ndocumentation of information about the device and its intended use.\n\nAreas of special importance include allocation of system functions to\n\nhardware/software,\noperating conditions,\nuser characteristics,\npotential hazards, and\nanticipated tasks.\n\nIn addition, the requirements should state clearly the intended use of the software. It is not possible to validate software without predetermined and documented software requirements. Typical software requirements specify the following:\n\nAll software system inputs;\nAll software system outputs;\nAll functions that the software system will perform;\nAll performance requirements that the software will meet, (e.g., data throughput, reliability, and timing);\nThe definition of all external and user interfaces, as well as any internal software-to-system interfaces;\nHow users will interact with the system;\nWhat constitutes an error and how errors should be handled;\nRequired response times;\nThe intended operating environment for the software, if this is a design constraint (e.g., hardware platform, operating system);\nAll ranges, limits, defaults, and specific values that the software will accept; and\nAll safety related requirements, specifications, features, or functions that will be implemented in software.\n\nSoftware requirement specifications should identify clearly the potential hazards that can result from a software failure in the system as well as any safety requirements to be implemented in software.\nThe consequences of software failure should be evaluated, along with means of mitigating such failures (e.g., hardware mitigation, defensive programming, etc.).\nThe Quality System regulation requires a mechanism for addressing incomplete, ambiguous, or conflicting requirements. (See 21 CFR 820.30(c).) Each requirement (e.g., hardware, software, user, operator interface, and safety) identified in the software requirements specification should be evaluated for accuracy, completeness, consistency, testability, correctness, and clarity.\nFor example, software requirements should be evaluated to verify that:\n\nThere are no internal inconsistencies among requirements;\nAll of the performance requirements for the system have been spelled out;\nFault tolerance, safety, and security requirements are complete and correct;\nAllocation of software functions is accurate and complete;\nSoftware requirements are appropriate for the system hazards; and\nAll requirements are expressed in terms that are measurable or objectively verifiable.\n\nA software requirements traceability analysis should be conducted to trace software requirements to (and from) system requirements and to risk analysis results. In addition to any other analyses and documentation used to verify software requirements, a formal design review is recommended to confirm that requirements are fully specified and appropriate before extensive software design efforts begin. Requirements can be approved and released incrementally, but care should be taken that interactions and interfaces among software (and hardware) requirements are properly reviewed, analyzed, and controlled.\nTypical Tasks – Requirements\n\nPreliminary Risk Analysis\nTraceability Analysis\n\nSoftware Requirements to System Requirements (and vice versa)\nSoftware Requirements to Risk Analysis\n\nDescription of User Characteristics\nListing of Characteristics and Limitations of Primary and Secondary Memory\nSoftware Requirements Evaluation\nSoftware User Interface Requirements Analysis\nSystem Test Plan Generation\nAcceptance Test Plan Generation\nAmbiguity Review or Analysis\n\n\n\n2.5.2.3 Design\nIn the design process, the software requirements specification is translated into a logical and physical representation of the software to be implemented. The software design specification is a description of what the software should do and how it should do it. The design specification may contain both a high level summary of the design and detailed design information. Human factors engineering should be woven into\n\nthe entire design and development process,\nthe device design requirements,\nanalyses, and\ntests.\n\nDevice safety and usability issues should be considered when developing\n\nflowcharts,\nstate diagrams,\nprototyping tools, and\ntest plans.\n\nAlso, task and function analyses, risk analyses, prototype tests and reviews, and full usability tests should be performed. Participants from the user population should be included when applying these methodologies.\nThe software design specification should include:\n\nSoftware requirements specification, including predetermined criteria for acceptance of the software;\nSoftware risk analysis;\nDevelopment procedures and coding guidelines (or other programming procedures);\nSystems documentation (e.g., a narrative or a context diagram) that describes the systems context in which the program is intended to function, including the relationship of hardware, software, and the physical environment;\nHardware to be used;\nParameters to be measured or recorded;\nLogical structure (including control logic) and logical processing steps (e.g., algorithms);\nData structures and data flow diagrams;\nDefinitions of variables (control and data) and description of where they are used;\nError, alarm, and warning messages;\nSupporting software (e.g., operating systems, drivers, other application software);\nCommunication links (links among internal modules of the software, links with the supporting software, links with the hardware, and links with the user);\nSecurity measures (both physical and logical security); and\nAny additional constraints not identified in the above elements.\n\nThe first four of the elements noted above usually are separate pre-existing documents that are included by reference in the software design specification. Software requirements specification was discussed in the preceding section, as was software risk analysis.\nSoftware design evaluations criteria:\n\ncomplete,\ncorrect,\nconsistent,\nunambiguous,\nfeasible,\nmaintainable,\nanalyses of control flow,\ndata flow,\ncomplexity,\ntiming,\nsizing,\nmemory allocation,\ncriticality analysis, and many other aspects of the design\n\nAppropriate consideration of software architecture (e.g., modular structure) during design can reduce the magnitude of future validation efforts when software changes are needed.\nA traceability analysis should be conducted to verify that the software design implements all of the software requirements. As a technique for identifying where requirements are not sufficient, the traceability analysis should also verify that all aspects of the design are traceable to software requirements.\nAn analysis of communication links should be conducted to evaluate the proposed design with respect to hardware, user, and related software requirements. At the end of the software design activity, a Formal Design Review should be conducted to verify that the design is correct, consistent, complete, accurate, and testable, before moving to implement the design.\nSeveral versions of both the software requirement specification and the software design specification should be maintained. All approved versions should be archived and controlled in accordance with established configuration management procedures.\nTypical Tasks – Design\n\nUpdated Software Risk Analysis\nTraceability Analysis - Design Specification to Software Requirements (and vice versa)\nSoftware Design Evaluation\nDesign Communication Link Analysis\nModule Test Plan Generation\nIntegration Test Plan Generation\nTest Design Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.4 Construction or Coding\nSoftware may be constructed either by coding. Coding is the software activity where the detailed design specification is implemented as source code. It is the last stage in decomposition of the software requirements where module specifications are translated into a programming language.\nCoding usually involves the use of a high-level programming language, but may also entail the use of assembly language (or microcode) for time-critical operations.\nA source code traceability analysis is an important tool to verify that all code is linked to established specifications and established test procedures. A source code traceability analysis should be conducted and documented to verify that:\n\nEach element of the software design specification has been implemented in code;\nModules and functions implemented in code can be traced back to an element in the software design specification and to the risk analysis;\nTests for modules and functions can be traced back to an element in the software design specification and to the risk analysis; and\nTests for modules and functions can be traced to source code for the same modules and functions.\n\nTypical Tasks – Construction or Coding\n\nTraceability Analyses\n\nSource Code to Design Specification (and vice versa)\nTest Cases to Source Code and to Design Specification\n\nSource Code and Source Code Documentation Evaluation\nSource Code Interface Analysis\nTest Procedure and Test Case Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.5 Testing by the Software Developer\nSoftware testing entails running software products under known conditions with defined inputs and documented outcomes that can be compared to their predefined expectations. It is a time consuming, difficult, and imperfect activity.\nAs such, it requires early planning in order to be effective and efficient. Test plans and test cases should be created as early in the software development process as feasible.\nThey should identify\n\nthe schedules,\nenvironments,\nresources (personnel, tools, etc.),\nmethodologies,\ncases (inputs, procedures, outputs, expected results),\ndocumentation, and\nreporting criteria.\n\nDescriptions of categories of software and software testing effort appear in the literature\n\nNIST Special Publication 500-235, Structured Testing: A Testing Methodology Using the Cyclomatic Complexity Metric;\nNUREG/CR-6293, Verification and Validation Guidelines for High Integrity Systems; and\nIEEE Computer Society Press, Handbook of Software Reliability Engineering.\n\nTesting of all program functionality does not mean all of the program has been tested. Testing of all of a program’s code does not mean all necessary functionality is present in the program. Testing of all program functionality and all program code does not mean the program is 100% correct! Software testing that finds no errors should not be interpreted to mean that errors do not exist in the software product; it may mean the testing was superficial.\nAn essential element of a software test case is the expected result. It is the key detail that permits objective evaluation of the actual test result. This necessary testing information is obtained from the corresponding, predefined definition or specification.\nA software testing process should be based on principles that foster effective examinations of a software product. Applicable software testing tenets include:\n\nThe expected test outcome is predefined;\nA good test case has a high probability of exposing an error;\nA successful test is one that finds an error;\nThere is independence from coding;\nBoth application (user) and software (programming) expertise are employed;\nTesters use different tools from coders;\nExamining only the usual case is insufficient;\nTest documentation permits its reuse and an independent confirmation of the pass/fail status of a test outcome during subsequent review.\n\nCode-based testing is also known as structural testing or “white-box” testing. It identifies test cases based on knowledge obtained from the source code, detailed design specification, and other development documents. Structural testing can identify “dead” code that is never executed when the program is run. Structural testing is accomplished primarily with unit (module) level testing, but can be extended to other levels of software testing.\nThe level of structural testing can be evaluated using metrics that are designed to show what percentage of the software structure has been evaluated during structural testing. These metrics are typically referred to as “coverage” and are a measure of completeness with respect to test selection criteria. The amount of structural coverage should be commensurate with the level of risk posed by the software. Use of the term “coverage” usually means 100% coverage. Common structural coverage metrics include:\n\nStatement Coverage – This criteria requires sufficient test cases for each program statement to be executed at least once; however, its achievement is insufficient to provide confidence in a software product’s behavior.\nDecision (Branch) Coverage – This criteria requires sufficient test cases for each program decision or branch to be executed so that each possible outcome occurs at least once. It is considered to be a minimum level of coverage for most software products, but decision coverage alone is insufficient for high-integrity applications.\nCondition Coverage – This criteria requires sufficient test cases for each condition in a program decision to take on all possible outcomes at least once. It differs from branch coverage only when multiple conditions must be evaluated to reach a decision.\nMulti-Condition Coverage – This criteria requires sufficient test cases to exercise all possible combinations of conditions in a program decision.\nLoop Coverage – This criteria requires sufficient test cases for all program loops to be executed for zero, one, two, and many iterations covering initialization, typical running and termination (boundary) conditions.\nPath Coverage – This criteria requires sufficient test cases for each feasible path, basis path, etc., from start to exit of a defined program segment, to be executed at least once. Because of the very large number of possible paths through a software program, path coverage is generally not achievable. The amount of path coverage is normally established based on the risk or criticality of the software under test.\nData Flow Coverage – This criteria requires sufficient test cases for each feasible data flow to be executed at least once. A number of data flow testing strategies are available.\n\nThe following types of functional software testing involve generally increasing levels of effort:\n\nNormal Case – Testing with usual inputs is necessary. However, testing a software product only with expected, valid inputs does not thoroughly test that software product. By itself, normal case testing cannot provide sufficient confidence in the dependability of the software product.\nOutput Forcing – Choosing test inputs to ensure that selected (or all) software outputs are generated by testing.\nRobustness – Software testing should demonstrate that a software product behaves correctly when given unexpected, invalid inputs. Methods for identifying a sufficient set of such test cases include Equivalence Class Partitioning, Boundary Value Analysis, and Special Case Identification (Error Guessing). While important and necessary, these techniques do not ensure that all of the most appropriate challenges to a software product have been identified for testing.\nCombinations of Inputs – The functional testing methods identified above all emphasize individual or single test inputs. Most software products operate with multiple inputs under their conditions of use. Thorough software product testing should consider the combinations of inputs a software unit or system may encounter during operation. Error guessing can be extended to identify combinations of inputs, but it is an ad hoc technique. Cause-effect graphing is one functional software testing technique that systematically identifies combinations of inputs to a software product for inclusion in test cases.\n\nFunctional and structural software test case identification techniques provide specific inputs for testing, rather than random test inputs. One weakness of these techniques is the difficulty in linking structural and functional test completion criteria to a software product’s reliability.\nAdvanced software testing methods, such as statistical testing, can be employed to provide further assurance that a software product is dependable. Statistical testing uses randomly generated test data from defined distributions based on an operational profile (e.g., expected use, hazardous use, or malicious use of the software product). Large amounts of test data are generated and can be targeted to cover particular areas or concerns, providing an increased possibility of identifying individual and multiple rare operating conditions that were not anticipated by either the software product’s designers or its testers. Statistical testing also provides high structural coverage. It does require a stable software product. Thus, structural and functional testing are prerequisites for statistical testing of a software product.\nAnother aspect of software testing is the testing of software changes. Changes occur frequently during software development. These changes are the result of\n\ndebugging that finds an error and it is corrected,\nnew or changed requirements (“requirements creep”), and\nmodified designs as more effective or efficient implementations are found.\n\nOnce a software product has been baselined (approved), any change to that product should have its own “mini life cycle,” including testing. Testing of a changed software product requires additional effort. It should demonstrate\n\nthat the change was implemented correctly, and\nthat the change did not adversely impact other parts of the software product.\n\nRegression analysis is the determination of the impact of a change based on review of the relevant documentation in order to identify the necessary regression tests to be run. Regression testing is the rerunning of test cases that a program has previously executed correctly and comparing the current result to the previous result in order to detect unintended effects of a software change. Regression analysis and regression testing should also be employed when using integration methods to build a software product to ensure that newly integrated modules do not adversely impact the operation of previously integrated modules.\nIn order to provide a thorough and rigorous examination of a software product, development testing is typically organized into levels: unit, integration, and system levels of testing.\n\nUnit (module or component) level testing focuses on the early examination of sub-program functionality and ensures that functionality not visible at the system level is examined by testing. Unit testing ensures that quality software units are furnished for integration into the finished software product.\nIntegration level testing focuses on the transfer of data and control across a program’s internal and external interfaces. External interfaces are those with\n\nother software (including operating system software),\nsystem hardware, and\nthe users and can be described as communications links.\n\nSystem level testing demonstrates that all specified functionality exists and that the software product is trustworthy. This testing verifies the as-built program’s functionality and performance with respect to the requirements for the software product as exhibited on the specified operating platform(s). System level software testing addresses functional concerns and the following elements of a device’s software that are related to the intended use(s):\n\nPerformance issues (e.g., response times, reliability measurements);\nResponses to stress conditions, e.g., behavior under maximum load, continuous use;\nOperation of internal and external security features;\nEffectiveness of recovery procedures, including disaster recovery;\nUsability; (Usability vs Utility??)\nCompatibility with other software products;\nBehavior in each of the defined hardware configurations; and\nAccuracy of documentation.\n\n\nControl measures (e.g., a traceability analysis) should be used to ensure that the intended coverage is achieved.\nSystem level testing also exhibits the software product’s behavior in the intended operating environment. The location of such testing is dependent upon the software developer’s ability to produce the target operating environment(s). Depending upon the circumstances, simulation and/or testing at (potential) customer locations may be utilized.\nTest plans should identify the controls needed to ensure\n\nthat the intended coverage is achieved and\nthat proper documentation is prepared when planned system level testing is conducted at sites not directly controlled by the software developer.\n\nTest procedures, test data, and test results\n\nshould be documented in a manner permitting objective pass/fail decisions to be reached.\nshould also be suitable for review and objective decision making subsequent to running the test,\nshould be suitable for use in any subsequent regression testing.\n\nErrors detected during testing should be\n\nlogged,\nclassified,\nreviewed, and\nresolved prior to release of the software.\n\nSoftware error data that is collected and analyzed during a development life cycle may be used to determine the suitability of the software product for release for commercial distribution. Test reports should comply with the requirements of the corresponding test plans.\nSoftware testing tools are frequently used to ensure consistency, thoroughness, and efficiency in the testing of such software products and to fulfill the requirements of the planned testing activities.\nAppropriate documentation providing evidence of the validation of these software tools for their intended use should be maintained (see section 6 of this guidance).\nTypical Tasks – Testing by the Software Developer\n\nTest Planning\nStructural Test Case Identification\nFunctional Test Case Identification\nTraceability Analysis - Testing\nUnit (Module) Tests to Detailed Design\nIntegration Tests to High Level Design\nSystem Tests to Software Requirements\nUnit (Module) Test Execution\nIntegration Test Execution\nFunctional Test Execution\nSystem Test Execution\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.6 User Site Testing\nTesting at the user site is an essential part of software validation. The Quality System regulation requires\n\ninstallation and\ninspection procedures (including testing where appropriate) as well as\ndocumentation of inspection and\ntesting to demonstrate proper installation. (See 21 CFR §820.170.)\n\nLikewise, manufacturing equipment must meet specified requirements, and automated systems must be validated for their intended use. (See 21 CFR §820.70(g) and 21 CFR §820.70(i) respectively.)\nTerminology regarding user site testing can be confusing. Terms such as\n\nbeta test,\nsite validation,\nuser acceptance test,\ninstallation verification, and\ninstallation testing have all been used to describe user site testing.\n\nFor the purposes of this guidance, the term “user site testing” encompasses all of these and any other testing that takes place outside of the developer’s controlled environment.\nThis testing should take place at a user’s site with the actual hardware and software that will be part of the installed system configuration. The testing is accomplished through either actual or simulated use of the software being tested within the context in which it is intended to function.\nTest planners should check with the FDA Center(s) with the corresponding product jurisdiction to determine whether there are any additional regulatory requirements for user site testing.\nUser site testing should follow a pre-defined written plan with\n\na formal summary of testing and\na record of formal acceptance.\n\nThe following documented evidence should be retained:\n\nall testing procedures,\ntest input data, and\ntest results\n\nThere should be evidence that hardware and software are installed and configured as specified. Measures should ensure that all system components are exercised during the testing and that the versions of these components are those specified. The testing plan should specify testing throughout the full range of operating conditions and should specify continuation for a sufficient time to allow the system to encounter a wide spectrum of conditions and events in an effort to detect any latent faults that are not apparent during more normal activities.\nSome of the evaluations of the system’s ability that have been performed earlier by the software developer at the developer’s site should be repeated at the site of actual use. These may include tests for:\n\na high volume of data,\nheavy loads or stresses,\nsecurity,\nfault testing (avoidance, detection, tolerance, and recovery),\nerror messages, and\nimplementation of safety requirements.\n\nThere should be an evaluation of the ability of the users of the system to understand and correctly interface with it.\nOperators should be able to perform the intended functions and respond in an appropriate and timely manner to all alarms, warnings, and error messages.\nRecords should be maintained of both proper system performance and any system failures that are encountered.\nThe revision of the system to compensate for faults detected during this user site testing should follow the same procedures and controls as for any other software change.\nThe developers of the software may or may not be involved in the user site testing.\n\nIf the developers are involved, they may seamlessly carry over to the user’s site the last portions of design-level systems testing.\nIf the developers are not involved, it is all the more important that the user have persons who understand the importance of careful test planning, the definition of expected test results, and the recording of all test outputs.\n\nTypical Tasks – User Site Testing\n\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.7 Maintenance and Software Changes\n\n2.5.2.7.1 Hardware vs Software\nHardware maintenance typically includes\n\npreventive hardware maintenance actions,\ncomponent replacement, and\ncorrective changes.\n\nSoftware maintenance includes\n\ncorrective,\nperfective, and\nadaptive maintenance\nbut does not include preventive maintenance actions or software component replacement.\n\n\n\n2.5.2.7.2 Maintenance Types\n\nCorrective maintenance: Changes made to correct errors and faults in the software.\nPerfective maintenance: Changes made to the software to improve the performance, maintainability, or other attributes of the software system .\nAdaptive maintenance: Changes to make the software system usable in a changed environment.\n\nSufficient regression analysis and testing should be conducted to demonstrate that portions of the software not involved in the change were not adversely impacted. When changes are made to a software system,\n\neither during initial development or\nduring post release maintenance,\n\nThis is in addition to testing that evaluates the correctness of the implemented change(s). The specific validation effort necessary for each software change is determined by\n\nthe type of change,\nthe development products affected, and the\nimpact of those products on the operation of the software.\n\n\n\n2.5.2.7.3 Factors of Limitting Validation Effort Needed When a Change Is Made\n\ncareful and complete documentation of the design structure and\ncareful and complete documentation of interrelationships of various modules,\ninterfaces, etc.\nFor example,\n\ntest documentation,\ntest cases, and\nresults of previous verification and validation testing All of them need to be archived if they are to be available for performing subsequent regression testing.\n\n\nThe following additional maintenance tasks should be addressed:\n\nSoftware Validation Plan Revision - For software that was previously validated, the existing software validation plan should be revised to support the validation of the revised software. If no previous software validation plan exists, such a plan should be established to support the validation of the revised software.\nAnomaly Evaluation – Software organizations frequently maintain documentation, such as software problem reports that describe software anomalies discovered and the specific corrective action taken to fix each anomaly.\n\nToo often, however, mistakes are repeated because software developers do not take the next step to determine the root causes of problems and make the process and procedural changes needed to avoid recurrence of the problem.\nSoftware anomalies should be evaluated in terms of their severity and their effects on system operation and safety,\nbut they should also be treated as symptoms of process deficiencies in the quality system.\nA root cause analysis of anomalies can identify specific quality system deficiencies.\nWhere trends are identified (e.g., recurrence of similar software anomalies), appropriate corrective and preventive actions must be implemented and documented to avoid further recurrence of similar quality problems. (See 21 CFR 820.100.)\n\nProblem Identification and Resolution Tracking - All problems discovered during maintenance of the software should be documented. The resolution of each problem should be tracked to ensure it is fixed, for historical reference, and for trending.\nProposed Change Assessment - All proposed modifications, enhancements, or additions should be assessed to determine the effect each change would have on the system. This information should determine the extent to which verification and/or validation tasks need to be iterated.\nTask Iteration - For approved software changes, all necessary verification and validation tasks should be performed to ensure that planned changes are implemented correctly, all documentation is complete and up to date, and no unacceptable changes have occurred in software performance.\nDocumentation Updating – Documentation should be carefully reviewed to determine which documents have been impacted by a change. All approved documents (e.g., specifications, test procedures, user manuals, etc.) that have been affected should be updated in accordance with configuration management procedures. Specifications should be updated before any maintenance and software changes are made."
  },
  {
    "objectID": "docs/blog/posts/PublicHealth/2022-12-10-FDA/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "href": "docs/blog/posts/PublicHealth/2022-12-10-FDA/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.6 Validation of Automated Process Equipment and Quality System Software",
    "text": "2.6 Validation of Automated Process Equipment and Quality System Software\nThe Quality System regulation requires that “when computers or automated data processing systems are used as part of production or the quality system, the [device] manufacturer shall validate computer software for its intended use according to an established protocol.” (See 21 CFR §820.70(i)). This has been a regulatory requirement of FDA’s medical device Good Manufacturing Practice (GMP) regulations since 1978.\nComputer systems that implement part of a device manufacturer’s production processes or quality system (or that are used to create and maintain records required by any other FDA regulation) are subject to the Electronic Records; Electronic Signatures regulation. (See 21 CFR Part 11.) This regulation establishes additional security, data integrity, and validation requirements when records are created or maintained electronically. These additional Part 11 requirements should be carefully considered and included in system requirements and software requirements for any automated record keeping systems. System validation and software validation should demonstrate that all Part 11 requirements have been met.\nComputers and automated equipment are used extensively throughout all aspects of\n\nmedical device design,\nlaboratory testing and analysis,\nproduct inspection and acceptance,\nproduction and process control,\nenvironmental controls,\npackaging,\nlabeling,\ntraceability,\ndocument control,\ncomplaint management, and many other aspects of the quality system.\n\nIncreasingly, automated plant floor operations can involve extensive use of embedded systems in:\n\nprogrammable logic controllers;\ndigital function controllers;\nstatistical process control;\nsupervisory control and data acquisition;\nrobotics;\nhuman-machine interfaces;\ninput/output devices; and\ncomputer operating systems.\n\nAll software tools used for software design are subject to the requirement for software validation, but the validation approach used for each application can vary widely.\nValidation is typically supported by:\n\nverifications of the outputs from each stage of that software development life cycle; and\nchecking for proper operation of the finished software in the device manufacturer’s intended use environment.\n\n\n2.6.1 How Much Validation Evidence Is Needed?\nThe level of validation effort should be commensurate with\n\nthe risk posed by the automated operation,\nthe complexity of the process software,\nthe degree to which the device manufacturer is dependent upon that automated process to produce a safe and effective device\n\nDocumented requirements and risk analysis of the automated process help to define the scope of the evidence needed to show that the software is validated for its intended use. Without a plan, extensive testing may be needed for:\n\na plant-wide electronic record and electronic signature system;\nan automated controller for a sterilization cycle; or\nautomated test equipment used for inspection and acceptance of finished circuit boards in a lifesustaining / life-supporting device.\n\nHigh risk applications should not be running in the same operating environment with non-validated software functions, even if those software functions are not used. Risk mitigation techniques such as memory partitioning or other approaches to resource protection may need to be considered when high risk applications and lower risk applications are to be used in the same operating environment.\nWhen software is upgraded or any changes are made to the software, the device manufacturer should consider how those changes may impact the “used portions” of the software and must reconfirm the validation of those portions of the software that are used. (See 21 CFR §820.70(i).)\n\n\n2.6.2 Defined User Equipment\nA very important key to software validation is a documented user requirements specification that defines:\n\nthe “intended use” of the software or automated equipment; and\nthe extent to which the device manufacturer is dependent upon that software or equipment for production of a quality medical device.\n\nThe device manufacturer (user) needs to define the expected operating environment including any required hardware and software configurations, software versions, utilities, etc. The user also needs to:\n\ndocument requirements for system performance, quality, error handling, startup, shutdown, security, etc.;\nidentify any safety related functions or features, such as sensors, alarms, interlocks, logical processing steps, or command sequences; and\ndefine objective criteria for determining acceptable performance.\n\nThe validation must be conducted in accordance with a documented protocol, and the validation results must also be documented. (See 21 CFR §820.70(i).) Test cases should be documented that will exercise the system to challenge its performance against the pre-determined criteria, especially for its most critical parameters.\nTest cases should address\n\nerror and alarm conditions,\nstartup, shutdown,\nall applicable user functions and operator controls,\npotential operator errors,\nmaximum and minimum ranges of allowed values, and\nstress conditions applicable to the intended use of the equipment.\n\nThe test cases should be executed and the results should be recorded and evaluated to determine whether the results support a conclusion that the software is validated for its intended use.\nA device manufacturer may conduct a validation using their own personnel or may depend on a third party such as the equipment/software vendor or a consultant. In any case, the device manufacturer retains the ultimate responsibility for ensuring that the production and quality system software:\n\nis validated according to a written procedure for the particular intended use; and\nwill perform as intended in the chosen application.\n\nThe device manufacturer should have documentation including:\n\ndefined user requirements;\nvalidation protocol used;\nacceptance criteria;\ntest cases and results; and\na validation summary that objectively confirms that the software is validated for its intended use.\n\n\n\n2.6.3 Validation of Off-The-Shelf Software and Automated Equipment\nMost of the automated equipment and systems used by device manufacturers are supplied by thirdparty vendors and are purchased off-the-shelf (OTS). The device manufacturer is responsible for ensuring that the product development methodologies used by the OTS software developer are appropriate and sufficient for the device manufacturer’s intended use of that OTS software.\nWhere possible and depending upon the device risk involved, the device manufacturer should consider auditing the vendor’s design and development methodologies used in the construction of the OTS software and should assess the development and validation documentation generated for the OTS software. Such audits can be conducted by the device manufacturer or by a qualified third party.\nThe audit should demonstrate that the vendor’s procedures for and results of the verification and validation activities performed the OTS software are appropriate and sufficient for the safety and effectiveness requirements of the medical device to be produced using that software."
  },
  {
    "objectID": "docs/blog/posts/statistics/2022-12-08-P-value/index.html",
    "href": "docs/blog/posts/statistics/2022-12-08-P-value/index.html",
    "title": "p-values",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n실험 결과가 우연히 생성된 것보다 더 극단적인 경우를 통계적으로 유의하다고 한다. (결과가 귀무 가설 하의 분포와 다른 분포에서 나올 수 있다고 생각해 보십시오.)\n\n\n\n’p-값’의 p는 ’확률’을 나타냅니다. p-값은 실험에서 관찰된 결과가 귀무 가설 하에서 발생할 수 있는 극단적인 결과를 얻을 확률의 합계입니다. 즉, p-값은 실험 결과가 우연히 얻어질 확률입니다.\n\n\n\n우연한 결과가 통계적으로 유의미하다고 하기 위해 실험의 실제 결과를 넘어서야 하는 극단적이거나 드문 결과의 확률 임계값입니다.\n\n\n\n귀무가설이 참인데 실수로 귀무가설을 기각하는 오류\n\n\n\n대립가설이 참인데 실수로 귀무가설을 기각하지 못하는 오류\n\n\n\n\n\np-값은 테스트 결과의 유의성을 측정할 때 효율적이고 효과적인 통계 지표입니다. 회귀 분석을 수행했다고 가정해 봅시다. 그런 다음 회귀 모델의 결과로 베타 계수와 표준 오차를 얻을 수 있습니다.\n\nNumber of Cases of How You Interpret Regresssion Result\n\n\n\nhigh Standard Error\nlow Standard Error\n\n\n\n\nhigh \\(\\beta\\)\nUnclear Interpretation\nOK\n\n\nlow \\(\\beta\\)\nOK\nUnclear Interpretation\n\n\n\n위의 표는 회귀 모델의 결과를 해석할 수 있는 경우의 수를 보여줍니다. 각 계수 \\(\\beta\\) 에 대해 4개의 경우가 있습니다.\n\nhigh \\(\\beta\\) and high Standard Error mean that 해당 변수가 강한 영향을 미치나 그 영향이 변동될 수 있음을 의미하므로 회귀 모델에서 도출된 \\(\\beta\\) 계수는 유의하지 않을 가능성이 높습니다. 그 효과가 통계적으로 유의미한지 확신할 수 없습니다.\nhigh \\(\\beta\\) and low Standard Error mean that the corresponding variable has a strong effect, and its variation is small, so the \\(\\beta\\) coefficient resulted from your regression model is likely to be significant.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, its effect has a high variation. So, we can clearly interprete the variable with the \\(\\beta\\) as a variable that is not significantly associated with your response variable.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, but its effect has a low variation. So, it is difficult to conclude that the variable is significant.\n\nThe p-value could be used to provide a clearer interpretation of the unclear situation (i.e. (high \\(\\beta\\), high Standard Error), (low \\(\\beta\\), high Standard Error) ) by looking at the ratio of the estimated value of a parameter(= \\(\\beta\\)) to its standard error on the distribution under the null hypothesis. By general convention, the cut-off of p-value indicating statistical signficance is 0.05.\n\n\n\nDespite the goodness of p-value, it is controversial to make a decision based solely on the p-value. As mentioned above, p-value is the probability that the result of your experiment is due to chance. In addition, looking into \\(\\frac{\\beta}{\\frac{s.e}{\\sqrt{n}}}\\), the p-value gets smaller as the sample size becomes larger and larger. It should be avoided that something is proved just because a low p-value is calucated.\nEven if a result is statistically significant, that does not necessarily mean it has real significance. A small difference that has no practical meaning can be statistically significant if the sample size is large enough. It is because large samples ensure that meaningless effects can become big enough to possibly exclude chance due to simple math.\nThe American Statistical Association (ASA) has released a statement of six principles for researchers and journal editors on p-values:\nSource: ASA Statement on Statistical Significance and p-values\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nProper inference requires full reporting and transparency.\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n\n\n\n\n\nPersonally, I make use of p-values as a tool in data science to just check whether a model result or a set of variables that appears interesting and useful is in the range of normal variability by chance in the exploratory data analysis(EDA) or data mining step.\nIf you want to get a statistical significance level through p-values, other methodologies could help increase the accuracy of real significance such as permuted p-values, q-values, and penalization on multiple comparison tests\n\n\n\n\n\nIt is said to be statistically significant if a result of your experiment is more extreme than one that is produced by chance. (Try thinking that your result could have come from a different distribution from the one under the null hypothesis.)\n\n\n\np of ‘p-value’ stands for ‘probability’. The p-value is the summation of the probabilities of obtaining results as extreme as the observed results from your experiments could occur under the null hypothesis. In other words, p-value is the probability that the result of your experiment is obtained by chance.\n\n\n\nThe probability threshold of the extreme or rarer results that chance results must be beyond actual results of your experiments in order to be said to be statistically significant.\n\n\n\nconcluding \\(H_o\\) or the null hypothesis is true by mistake.\n\n\n\nconcluding \\(H_a\\) or the alternative hypothesis is true by mistake.\n\n\n\n\n\np-value is an efficient and effective statistical index when to measure the significance of your test result. Let’s make an assumption that you have conducted a regression analysis. Then, you can get beta coefficients and their standard errors as results of your regression model.\n\nNumber of Cases of How You Interpret Regresssion Result\n\n\n\nhigh Standard Error\nlow Standard Error\n\n\n\n\nhigh \\(\\beta\\)\nUnclear Interpretation\nOK\n\n\nlow \\(\\beta\\)\nOK\nUnclear Interpretation\n\n\n\nThe above table shows the number of cases you can interprete the results of your regression model. There are 4 cases for each coefficient \\(\\beta\\).\n\nhigh \\(\\beta\\) and high Standard Error mean that the corresponding variable has a strong effect but its effect may be fluctuated, so the \\(\\beta\\) coefficient resulted from your regression model is likely to be not significant. We are not sure that its effect is statistically significant.\nhigh \\(\\beta\\) and low Standard Error mean that the corresponding variable has a strong effect, and its variation is small, so the \\(\\beta\\) coefficient resulted from your regression model is likely to be significant.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, its effect has a high variation. So, we can clearly interprete the variable with the \\(\\beta\\) as a variable that is not significantly associated with your response variable.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, but its effect has a low variation. So, it is difficult to conclude that the variable is significant.\n\nThe p-value could be used to provide a clearer interpretation of the unclear situation (i.e. (high \\(\\beta\\), high Standard Error), (low \\(\\beta\\), high Standard Error) ) by looking at the ratio of the estimated value of a parameter(= \\(\\beta\\)) to its standard error on the distribution under the null hypothesis. By general convention, the cut-off of p-value indicating statistical signficance is 0.05.\n\n\n\nDespite the goodness of p-value, it is controversial to make a decision based solely on the p-value. As mentioned above, p-value is the probability that the result of your experiment is due to chance. In addition, looking into \\(\\frac{\\beta}{\\frac{s.e}{\\sqrt{n}}}\\), the p-value gets smaller as the sample size becomes larger and larger. It should be avoided that something is proved just because a low p-value is calucated.\nEven if a result is statistically significant, that does not necessarily mean it has real significance. A small difference that has no practical meaning can be statistically significant if the sample size is large enough. It is because large samples ensure that meaningless effects can become big enough to possibly exclude chance due to simple math.\nThe American Statistical Association (ASA) has released a statement of six principles for researchers and journal editors on p-values:\nSource: ASA Statement on Statistical Significance and p-values\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nProper inference requires full reporting and transparency.\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n\n\n\n\n\nPersonally, I make use of p-values as a tool in data science to just check whether a model result or a set of variables that appears interesting and useful is in the range of normal variability by chance in the exploratory data analysis(EDA) or data mining step.\nIf you want to get a statistical significance level through p-values, other methodologies could help increase the accuracy of real significance such as permuted p-values, q-values, and penalization on multiple comparison tests\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-07-anova/index.html",
    "href": "docs/blog/posts/statistics/2023-01-07-anova/index.html",
    "title": "ANOVA",
    "section": "",
    "text": "(draft)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-07-anova/index.html#terms",
    "href": "docs/blog/posts/statistics/2023-01-07-anova/index.html#terms",
    "title": "ANOVA",
    "section": "3 Terms",
    "text": "3 Terms\n\npairwise comparison: A hypothesis test (e.g., of means) between two groups among multiple groups.\nobmnibus set: A single hypothesis test of the overall variance among multiple group means.\ndecomposition of variance : Separation of components contributing to an individual value (e.g., from the overall average, from a treatment mean, and from a residual error).\nF-test\nF statistic: A standardized statistic that measures the extent to which differences among group means exceed what might be expected in a chance model.\nsum of squares: deviations from some average value"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-07-anova/index.html#application-to-example",
    "href": "docs/blog/posts/statistics/2023-01-07-anova/index.html#application-to-example",
    "title": "ANOVA",
    "section": "2 Application to Example",
    "text": "2 Application to Example\n\n2.1 Data Description\n\n2.1.1 Raw Data\n(…민감 정보 제거 및 데이터 변환 후 컨설팅 내용 일부 발췌…)\n\n\n\nexample data는 Day, Run, response의 변수들을 포함하고 있습니다. 공유해주신 정보에 따르면 아마도 Run은 오전과 오후를 나누는 변수인 것으로 생각 됩니다. 이 data만 보면 아마도 같은 샘플에 대해서 시약 제품이 시간에 따라 얼마나 안정적인 performance를 보여주는지 검사하는 실험으로 추측됩니다. 좀 더 분석하기 용이한 형태로 data structure를 바꾸겠습니다.\n\n\n2.1.2 Processed Data\n\n\n\n\n  \n\n\n\n재가공된 data는 120개의 샘플과 5개의 변수를 갖고있습니다. 변수 목록은 다음과 같습니다.\n\nid: 열번호, 총 20일간 하루 2회 구동(AM, PM) 구동, 오전 오후 각 각 3번씩 구동 총 120 \\((=20 \\times 3 \\times 2)\\) 샘플\n\nDay: Day1~20\n\nnoon: AM= before noon, PM= after noon\nRun: 1회 구동당 3번 반복씩1, 2, 3\n\nresponse: response variable, 낮을 수록 좋음\n\nANOVA의 Assumption\n\nresponse variable should follow normal distribution.\n\nhomoscedasticity, equality of variance: 각 집단의 분포는 모두 동일한 분산을 가짐\nANOVA의 가정들을 반드시 충족하지 않아도 되지만 충족하면 Power 가 올라감\n\n\n\n\n2.2 EDA (Explorator Data Analysis)\n이 data는 아래 처럼 1의 결측치를 갖고 있습니다.\n\n\n\n\n\nid\nDay\nnoon\nRun\nresponse\n\n\n\n\n117\n20\nAM\n3\nNA\n\n\n\n\n\nCt에 대한 Global Statistics는 다음과 같습니다.\n\n\n\n\n\ncount\nglobal_response_mean\nglobal_response_sd\nglobal_response_CV\n\n\n\n\n119\n38.727\n18.47\n47.694 %\n\n\n\n\n\nDay groups의 Statistics은 다음과 같습니다.\n\n\n\n\n  \n\n\n\nAM/PM groups의 Statistics은 다음과 같습니다.\n\n\n\n\n  \n\n\n\nDays와 AM/PM 조합 groups의 Statistics은 다음과 같습니다.\n\n\n\n\n  \n\n\n\n이제 ANOVA를 수행하기 위한 basic statistics는 모두 구했습니다. ANOVA를 수행하기 위해 집단 간 분산과 집단 내 분산을 계산하도록 하겠습니다.\n\n\n2.3 집단 간 분산\n앞에서 설명 드린바로 유추해보면 예시 data의 집단 간 분산의 범주형 변수는 Day로 설정하는 것이 합리적인 것으로 보입니다.\n\n\\(g=g\\) Day의 sample size = 20, 자유도 = 20-1 = 19 입니다.\n\\(n_g=g\\) group의 sample size, \\(\\overline{X}_g=g\\) 의 sample mean은 다음과 같습니다.\n\\(\\overline{X}\\) = global sample mean = 38.72681\n집단 간 분산: \\(\\frac{집단 간 제곱합}{자유도}=\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n2.3.1 SS_Day (집단간 분산 Day)\nDay sq = \\([(\\overline{X}_g-\\overline{X})^2n_g]\\) 집단(Day)간 분산 계산, 집단(Day)간 분산 = \\(\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n\n\n  \n\n\n\nAnalysis-In program의 ANOVA결과값과 일치하는 것을 볼 수 있습니다. $SS_{day} $= 7025.97838 with \\(df=19\\).\n\n\n2.3.2 SS_noon (집단간 분산 noon)\nnoon sq = \\([(\\overline{X}_g-\\overline{X})^2n_g]\\) 집단(noon)간 분산 계산, 집단(noon)간 분산 = \\(\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n\n\n  \n\n\n\nAnalysis-In program의 결과에서 찾아 볼 수 없죠? 이 결과는 숨어 있습니다. 상호 작용에 대한 분산값을 구하고 나면 정체를 알 수 있습니다.\n\\(SS_{noon}\\) = 319.76458 with \\(df=1\\).\n\n\n2.3.3 SS_error (집단내 분산)\n\n집단 내 분산 (within-groups variability)\n\n\n\n\n\n  \n\n\n\n\\(SS_{error}\\) = 2.47041^{4}\nAnalysis-In program의 결과와 일치하는 것을 확인할 수 있습니다.\n\n\n2.3.4 SS_total\n\n\n\n\n  \n\n\n\n\\(SS_{total}\\) = 4.02557^{4}\nAnalysis-In program의 ANOVA 결과 table에 있는 SS들의 합과 일치하는 것을 확인할 수 있습니다.\n\n\n2.3.5 상호 작용 분산\n\n\n\n\\(SS_{interaction}=SS_{DayNoon}= SS_{total}-SS_{Day}-SS_{noon}-SS_{error}\\)\n= 4.02557{4}-2.47041{4}-319.76458-7025.97838 = 8205.93974\nAnalysis-In program의 ANOVA 결과 table과 일치하는 것을 확인할 수 있습니다.\n위의 결과들을 종합하면 아래와 같이 요약됩니다.\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   7026     370    1.18   0.29\nnoon         1    339     339    1.08   0.30\nDay:noon    19   8187     431    1.38   0.16\nResiduals   79  24704     313               \n1 observation deleted due to missingness\n\n\n\nRepeatability SD = \\(\\sqrt{V_{error}}=\\sqrt{MS_{error}}\\) = 17.6836\nRepeatability CV = \\(\\frac{repeatability \\space SD}{global \\space mean \\space response}\\) = 0.45662\n\n위의 결과를 간단히 해석해 보면\n\n집단간 범주 변수인 Day는 p-value =0.29>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, 일별로 평균 response값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, 일별로 평균 response값이 다르지 않습니다.\n\n집단간 범주 변수인 noon은 p-value =0.30>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, 오전/오후별 평균 response값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, 오전/오후별 평균 response값이 다르지 않습니다.\n\nDay와 noon두 변수의 상호작용 변수는 p-value =0.16>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, Day별 Noon(오전/오후)별 평균 response값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, Day별 Noon(오전/오후)별 평균 response값이 다르지 않습니다.\n\n\n최종 결론, 제품의 response값이 Day별 오전/오후별 안정적인 performance를 보인다고 조심스럽게 결론을 내릴 수 있습니다.\n이제 까지는 질문에 대한 답이 되는 ANOVA의 원리 및 통계량의 재현 및 해석법에 대하여 알아봤습니다. 하지만 직관적으로 어떤 의미가 있을 까요? 원래는 시각화를 통해 데이터의 패턴을 짐작하고 통계 검정 결과를 예상하는데 우리는 반대로 가고 있네요 ㅎㅎ 시각화를 통해 ANOVA 결과가 얼마나 직관적인지 알아보겠습니다.\n\n\n\n2.4 Visualization\n\n2.4.1 One-way: Day\n\n\n\n\n\n\n\n\n자세히 보면 일별로 시간의 경과에 따라 전체적으로 평균 response값이 약간 하향하는 것으로 보입니다. 하지만 좀 더 세부적으로 관찰하면 1일~8일 평균 response의 경향이 constant한 패턴을 보입니다. 9일~13일 평균 response가 진동 하향하는 패턴을 보입니다. 14일~20일 평균 response가 상향하는 패턴을 보입니다.\n\n\nTables of means\nGrand mean\n       \n38.727 \n\n Day \n        1    2     3     4     5     6     7     8     9    10    11    12\n    47.34 41.2 43.26 33.04 44.01 40.53 40.92 42.98 55.89 34.64 49.19 26.94\nrep  6.00  6.0  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00\n       13    14    15    16    17    18    19    20\n    43.86 28.82 28.68 31.52 30.06 35.28 32.97 44.35\nrep  6.00  6.00  6.00  6.00  6.00  6.00  6.00  5.00\n\n\nTables of effects\n\n Day \n        1     2     3      4    5     6     7     8     9     10    11     12\n    8.615 2.476 4.534 -5.685 5.28 1.802 2.193 4.251 17.17 -4.088 10.46 -11.79\nrep 6.000 6.000 6.000  6.000 6.00 6.000 6.000 6.000  6.00  6.000  6.00   6.00\n       13     14     15     16     17     18     19   20\n    5.138 -9.907 -10.05 -7.211 -8.666 -3.449 -5.756 5.62\nrep 6.000  6.000   6.00  6.000  6.000  6.000  6.000 5.00\n\n\n위에 첫 번째표에서 Global Sample response Mean = 38.727 과 각 집단의 평균 response를 확인할 수 있습니다. 위에 두 번째표에서 Global Sample response Mean = 37.322 과 각 집단의 평균 response의 차이를 확인할 수 있습니다.\n\nDay 9에서 차이가 가장 큰 것으로 보아 9일째 실험에서 performance가 가장 낮은 것이 관측됐습니다.\n반대로, 12일에 performance 가장 좋은 것으로 관측됐습니다.\n\n9일과 12일에 response값에 영향을 미쳤던 요인이 있었는지 복기 하는것도 도움이 되겠군요.\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   7026     370     1.1   0.36\nResiduals   99  33230     336               \n1 observation deleted due to missingness\n\n\nOne-way ANOVA의 결과값입니다. Day별 평균 response의 차이는 거의 없는 것으로 보입니다. 따라서 Day 별 평균 response의 경향이 일관되지 않고 One-way ANOVA에서 역시 통계적으로 유의하지 않아 Day 변수는 평균 response에 영향을 미치지 않는 것 같습니다.\n\n\n2.4.2 One-way: AM/PM\n\n\n\n\n\n\n\n\n오후에 시간의 경과에 따라 전체적으로 평균 response값이 약간 하향하는 것으로 보입니다.\n\n\nTables of means\nGrand mean\n       \n38.727 \n\n noon \n       AM   PM\n    40.38 37.1\nrep 59.00 60.0\n\n\nTables of effects\n\n noon \n        AM     PM\n     1.653 -1.626\nrep 59.000 60.000\n\n\n위 첫 번째 표에서 AM/PM 간의 평균 response차이는 0.15 (농도가 약 0.5배) 차이가 나는 것을 확인할 수 있습니다. 생물학적으로 의미가 있는 수치일까요? 위 두 번째 표에서 Global Sample Mean 37.322와 오전/오후 별 약 0.07씩(농도가 약 0.25배) 차이가 납니다.\n\n\n             Df Sum Sq Mean Sq F value Pr(>F)\nnoon          1    320     320    0.94   0.34\nResiduals   117  39936     341               \n1 observation deleted due to missingness\n\n\n오전 오후별 One way ANOVA를 실행한 결과가 오전/오부 평균 response값의 차이가 다르지 않다는 것을 시사하고 있습니다. 아무래도 위의 차이는 우연에 의해 발생한 현상인 것 같습니다.\n\n\n\n\n\n일별로 평균 response값이 차이를 모든 경우의 수에서 차이가 나는지 확인한 결과 신뢰구간 모두가 0을 포함하고 있으므로 Day 변수는 유의하다고 볼 수 없습니다.\n\n\n\n\n\n오전/오후별로 평균 response값이 차이를 모든 경우의 수에서 차이가 나는지 확인한 결과 신뢰구간 모두가 0을 포함하고 있으므로 오전/오후 변수는 유의하다고 볼 수 없습니다.\n여기 까지 각 변수별 평균 response로의 영향도를 통계적으로 시각적으로 관찰했습니다. 하지만 Day별 오전/오후별 영향도가 있는지 확인하겠습니다. (이미 위에서 통계적으로 없다고 검정됐습니다.)\n\n\n\n2.5 Two way Anova\n\n\n       AM     PM\n1  35.987 58.697\n2  40.032 42.374\n3  33.148 53.374\n4  36.697 29.387\n5  44.432 43.581\n6  45.781 35.277\n7  40.458 41.381\n8  51.103 34.852\n9  69.981 41.806\n10 26.619 42.658\n11 67.923 30.452\n12 25.981 27.897\n13 40.600 47.129\n14 25.839 31.800\n15 18.458 38.897\n16 36.910 26.123\n17 41.594 18.529\n18 38.471 32.084\n19 45.923 20.019\n20 42.303 45.710\n\n\n        AM      PM\n1  20.9797  7.4516\n2  18.6291 12.0856\n3  22.1450 34.0565\n4  16.3483  7.5583\n5   4.6107 11.0032\n6  23.6862 11.7618\n7  12.3380 30.0128\n8  33.4416 12.0135\n9   7.7381 30.4715\n10  3.4483  7.8927\n11 28.8752  9.1557\n12  5.9053 12.9054\n13 15.2088  7.3824\n14  8.4556 27.2253\n15  4.5063 35.2180\n16 18.3756  1.6899\n17 13.3474  8.0068\n18  6.0067 21.7672\n19 19.7931  1.4176\n20  3.9142 14.0145\n\n\n\n\nTables of means\nGrand mean\n       \n38.727 \n\n Day \n        1    2     3     4     5     6     7     8     9    10    11    12\n    47.34 41.2 43.26 33.04 44.01 40.53 40.92 42.98 55.89 34.64 49.19 26.94\nrep  6.00  6.0  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00\n       13    14    15    16    17    18    19    20\n    43.86 28.82 28.68 31.52 30.06 35.28 32.97 44.35\nrep  6.00  6.00  6.00  6.00  6.00  6.00  6.00  5.00\n\n noon \n       AM    PM\n    40.43 37.05\nrep 59.00 60.00\n\n Day:noon \n     noon\nDay   AM    PM   \n  1   35.99 58.70\n  rep  3.00  3.00\n  2   40.03 42.37\n  rep  3.00  3.00\n  3   33.15 53.37\n  rep  3.00  3.00\n  4   36.70 29.39\n  rep  3.00  3.00\n  5   44.43 43.58\n  rep  3.00  3.00\n  6   45.78 35.28\n  rep  3.00  3.00\n  7   40.46 41.38\n  rep  3.00  3.00\n  8   51.10 34.85\n  rep  3.00  3.00\n  9   69.98 41.81\n  rep  3.00  3.00\n  10  26.62 42.66\n  rep  3.00  3.00\n  11  67.92 30.45\n  rep  3.00  3.00\n  12  25.98 27.90\n  rep  3.00  3.00\n  13  40.60 47.13\n  rep  3.00  3.00\n  14  25.84 31.80\n  rep  3.00  3.00\n  15  18.46 38.90\n  rep  3.00  3.00\n  16  36.91 26.12\n  rep  3.00  3.00\n  17  41.59 18.53\n  rep  3.00  3.00\n  18  38.47 32.08\n  rep  3.00  3.00\n  19  45.92 20.02\n  rep  3.00  3.00\n  20  42.30 45.71\n  rep  2.00  3.00\n\n\nTables of effects\n\n Day \n        1     2     3      4    5     6     7     8     9     10    11     12\n    8.615 2.476 4.534 -5.685 5.28 1.802 2.193 4.251 17.17 -4.088 10.46 -11.79\nrep 6.000 6.000 6.000  6.000 6.00 6.000 6.000 6.000  6.00  6.000  6.00   6.00\n       13     14     15     16     17     18     19   20\n    5.138 -9.907 -10.05 -7.211 -8.666 -3.449 -5.756 5.62\nrep 6.000  6.000   6.00  6.000  6.000  6.000  6.000 5.00\n\n noon \n        AM     PM\n     1.701 -1.672\nrep 59.000 60.000\n\n Day:noon \n     noon\nDay   AM      PM     \n  1   -13.044  13.044\n  rep   3.000   3.000\n  2    -2.860   2.860\n  rep   3.000   3.000\n  3   -11.802  11.802\n  rep   3.000   3.000\n  4     1.966  -1.966\n  rep   3.000   3.000\n  5    -1.263   1.263\n  rep   3.000   3.000\n  6     3.562  -3.562\n  rep   3.000   3.000\n  7    -2.151   2.151\n  rep   3.000   3.000\n  8     6.437  -6.437\n  rep   3.000   3.000\n  9    12.398 -12.398\n  rep   3.000   3.000\n  10   -9.709   9.709\n  rep   3.000   3.000\n  11   17.046 -17.046\n  rep   3.000   3.000\n  12   -2.647   2.647\n  rep   3.000   3.000\n  13   -4.954   4.954\n  rep   3.000   3.000\n  14   -4.670   4.670\n  rep   3.000   3.000\n  15  -11.909  11.909\n  rep   3.000   3.000\n  16    3.704  -3.704\n  rep   3.000   3.000\n  17    9.843  -9.843\n  rep   3.000   3.000\n  18    1.504  -1.504\n  rep   3.000   3.000\n  19   11.262 -11.262\n  rep   3.000   3.000\n  20   -4.071   2.714\n  rep   2.000   3.000\n\n\none way ANOVA와 같이 해석\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   7026     370    1.18   0.29\nnoon         1    339     339    1.08   0.30\nDay:noon    19   8187     431    1.38   0.16\nResiduals   79  24704     313               \n1 observation deleted due to missingness\n\n\n위 그림을 보듯이 두 변수의 영향도가 없음, ANOVA 역시 유의하지 않음\n\n\n\n\n\n\n Missing rows: 117 \n\n\n\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = response ~ Day + noon + Day:noon, data = data)\n\n$Day\n            diff     lwr     upr   p adj\n2-1    -6.138710 -43.575 31.2979 1.00000\n3-1    -4.080645 -41.517 33.3560 1.00000\n4-1   -14.300000 -51.737 23.1366 0.99700\n5-1    -3.335484 -40.772 34.1011 1.00000\n6-1    -6.812903 -44.250 30.6237 1.00000\n7-1    -6.422581 -43.859 31.0140 1.00000\n8-1    -4.364516 -41.801 33.0721 1.00000\n9-1     8.551613 -28.885 45.9882 1.00000\n10-1  -12.703226 -50.140 24.7334 0.99934\n11-1    1.845161 -35.591 39.2818 1.00000\n12-1  -20.403226 -57.840 17.0334 0.89330\n13-1   -3.477419 -40.914 33.9592 1.00000\n14-1  -18.522581 -55.959 18.9140 0.95243\n15-1  -18.664516 -56.101 18.7721 0.94905\n16-1  -15.825806 -53.262 21.6108 0.99024\n17-1  -17.280645 -54.717 20.1560 0.97539\n18-1  -12.064516 -49.501 25.3721 0.99968\n19-1  -14.370968 -51.808 23.0657 0.99682\n20-1   -2.994839 -42.259 36.2690 1.00000\n3-2     2.058065 -35.379 39.4947 1.00000\n4-2    -8.161290 -45.598 29.2753 1.00000\n5-2     2.803226 -34.633 40.2399 1.00000\n6-2    -0.674194 -38.111 36.7624 1.00000\n7-2    -0.283871 -37.721 37.1528 1.00000\n8-2     1.774194 -35.662 39.2108 1.00000\n9-2    14.690323 -22.746 52.1270 0.99585\n10-2   -6.564516 -44.001 30.8721 1.00000\n11-2    7.983871 -29.453 45.4205 1.00000\n12-2  -14.264516 -51.701 23.1721 0.99709\n13-2    2.661290 -34.775 40.0979 1.00000\n14-2  -12.383871 -49.821 25.0528 0.99953\n15-2  -12.525806 -49.962 24.9108 0.99946\n16-2   -9.687097 -47.124 27.7495 0.99999\n17-2  -11.141935 -48.579 26.2947 0.99990\n18-2   -5.925806 -43.362 31.5108 1.00000\n19-2   -8.232258 -45.669 29.2044 1.00000\n20-2    3.143871 -36.120 42.4077 1.00000\n4-3   -10.219355 -47.656 27.2173 0.99997\n5-3     0.745161 -36.691 38.1818 1.00000\n6-3    -2.732258 -40.169 34.7044 1.00000\n7-3    -2.341935 -39.779 35.0947 1.00000\n8-3    -0.283871 -37.721 37.1528 1.00000\n9-3    12.632258 -24.804 50.0689 0.99939\n10-3   -8.622581 -46.059 28.8140 1.00000\n11-3    5.925806 -31.511 43.3624 1.00000\n12-3  -16.322581 -53.759 21.1140 0.98634\n13-3    0.603226 -36.833 38.0399 1.00000\n14-3  -14.441935 -51.879 22.9947 0.99662\n15-3  -14.583871 -52.021 22.8528 0.99620\n16-3  -11.745161 -49.182 25.6915 0.99978\n17-3  -13.200000 -50.637 24.2366 0.99891\n18-3   -7.983871 -45.421 29.4528 1.00000\n19-3  -10.290323 -47.727 27.1463 0.99997\n20-3    1.085806 -38.178 40.3497 1.00000\n5-4    10.964516 -26.472 48.4011 0.99992\n6-4     7.487097 -29.950 44.9237 1.00000\n7-4     7.877419 -29.559 45.3140 1.00000\n8-4     9.935484 -27.501 47.3721 0.99998\n9-4    22.851613 -14.585 60.2882 0.76824\n10-4    1.596774 -35.840 39.0334 1.00000\n11-4   16.145161 -21.291 53.5818 0.98785\n12-4   -6.103226 -43.540 31.3334 1.00000\n13-4   10.822581 -26.614 48.2592 0.99993\n14-4   -4.222581 -41.659 33.2140 1.00000\n15-4   -4.364516 -41.801 33.0721 1.00000\n16-4   -1.525806 -38.962 35.9108 1.00000\n17-4   -2.980645 -40.417 34.4560 1.00000\n18-4    2.235484 -35.201 39.6721 1.00000\n19-4   -0.070968 -37.508 37.3657 1.00000\n20-4   11.305161 -27.959 50.5690 0.99994\n6-5    -3.477419 -40.914 33.9592 1.00000\n7-5    -3.087097 -40.524 34.3495 1.00000\n8-5    -1.029032 -38.466 36.4076 1.00000\n9-5    11.887097 -25.550 49.3237 0.99974\n10-5   -9.367742 -46.804 28.0689 0.99999\n11-5    5.180645 -32.256 42.6173 1.00000\n12-5  -17.067742 -54.504 20.3689 0.97827\n13-5   -0.141935 -37.579 37.2947 1.00000\n14-5  -15.187097 -52.624 22.2495 0.99387\n15-5  -15.329032 -52.766 22.1076 0.99318\n16-5  -12.490323 -49.927 24.9463 0.99948\n17-5  -13.945161 -51.382 23.4915 0.99780\n18-5   -8.729032 -46.166 28.7076 1.00000\n19-5  -11.035484 -48.472 26.4011 0.99991\n20-5    0.340645 -38.923 39.6045 1.00000\n7-6     0.390323 -37.046 37.8270 1.00000\n8-6     2.448387 -34.988 39.8850 1.00000\n9-6    15.364516 -22.072 52.8011 0.99300\n10-6   -5.890323 -43.327 31.5463 1.00000\n11-6    8.658065 -28.779 46.0947 1.00000\n12-6  -13.590323 -51.027 23.8463 0.99841\n13-6    3.335484 -34.101 40.7721 1.00000\n14-6  -11.709677 -49.146 25.7270 0.99979\n15-6  -11.851613 -49.288 25.5850 0.99975\n16-6   -9.012903 -46.450 28.4237 1.00000\n17-6  -10.467742 -47.904 26.9689 0.99996\n18-6   -5.251613 -42.688 32.1850 1.00000\n19-6   -7.558065 -44.995 29.8786 1.00000\n20-6    3.818065 -35.446 43.0819 1.00000\n8-7     2.058065 -35.379 39.4947 1.00000\n9-7    14.974194 -22.462 52.4108 0.99480\n10-7   -6.280645 -43.717 31.1560 1.00000\n11-7    8.267742 -29.169 45.7044 1.00000\n12-7  -13.980645 -51.417 23.4560 0.99773\n13-7    2.945161 -34.491 40.3818 1.00000\n14-7  -12.100000 -49.537 25.3366 0.99966\n15-7  -12.241935 -49.679 25.1947 0.99960\n16-7   -9.403226 -46.840 28.0334 0.99999\n17-7  -10.858065 -48.295 26.5786 0.99993\n18-7   -5.641935 -43.079 31.7947 1.00000\n19-7   -7.948387 -45.385 29.4882 1.00000\n20-7    3.427742 -35.836 42.6916 1.00000\n9-8    12.916129 -24.521 50.3528 0.99918\n10-8   -8.338710 -45.775 29.0979 1.00000\n11-8    6.209677 -31.227 43.6463 1.00000\n12-8  -16.038710 -53.475 21.3979 0.98869\n13-8    0.887097 -36.550 38.3237 1.00000\n14-8  -14.158065 -51.595 23.2786 0.99735\n15-8  -14.300000 -51.737 23.1366 0.99700\n16-8  -11.461290 -48.898 25.9753 0.99984\n17-8  -12.916129 -50.353 24.5205 0.99918\n18-8   -7.700000 -45.137 29.7366 1.00000\n19-8  -10.006452 -47.443 27.4302 0.99998\n20-8    1.369677 -37.894 40.6335 1.00000\n10-9  -21.254839 -58.691 16.1818 0.85576\n11-9   -6.706452 -44.143 30.7302 1.00000\n12-9  -28.954839 -66.391  8.4818 0.35269\n13-9  -12.029032 -49.466 25.4076 0.99969\n14-9  -27.074194 -64.511 10.3624 0.47719\n15-9  -27.216129 -64.653 10.2205 0.46731\n16-9  -24.377419 -61.814 13.0592 0.66832\n17-9  -25.832258 -63.269 11.6044 0.56535\n18-9  -20.616129 -58.053 16.8205 0.88455\n19-9  -22.922581 -60.359 14.5140 0.76389\n20-9  -11.546452 -50.810 27.7174 0.99991\n11-10  14.548387 -22.888 51.9850 0.99631\n12-10  -7.700000 -45.137 29.7366 1.00000\n13-10   9.225806 -28.211 46.6624 0.99999\n14-10  -5.819355 -43.256 31.6173 1.00000\n15-10  -5.961290 -43.398 31.4753 1.00000\n16-10  -3.122581 -40.559 34.3140 1.00000\n17-10  -4.577419 -42.014 32.8592 1.00000\n18-10   0.638710 -36.798 38.0753 1.00000\n19-10  -1.667742 -39.104 35.7689 1.00000\n20-10   9.708387 -29.555 48.9723 0.99999\n12-11 -22.248387 -59.685 15.1882 0.80373\n13-11  -5.322581 -42.759 32.1140 1.00000\n14-11 -20.367742 -57.804 17.0689 0.89472\n15-11 -20.509677 -57.946 16.9270 0.88898\n16-11 -17.670968 -55.108 19.7657 0.96936\n17-11 -19.125806 -56.562 18.3108 0.93692\n18-11 -13.909677 -51.346 23.5270 0.99787\n19-11 -16.216129 -53.653 21.2205 0.98726\n20-11  -4.840000 -44.104 34.4239 1.00000\n13-12  16.925806 -20.511 54.3624 0.98004\n14-12   1.880645 -35.556 39.3173 1.00000\n15-12   1.738710 -35.698 39.1753 1.00000\n16-12   4.577419 -32.859 42.0140 1.00000\n17-12   3.122581 -34.314 40.5592 1.00000\n18-12   8.338710 -29.098 45.7753 1.00000\n19-12   6.032258 -31.404 43.4689 1.00000\n20-12  17.408387 -21.855 56.6723 0.98369\n14-13 -15.045161 -52.482 22.3915 0.99450\n15-13 -15.187097 -52.624 22.2495 0.99387\n16-13 -12.348387 -49.785 25.0882 0.99955\n17-13 -13.803226 -51.240 23.6334 0.99807\n18-13  -8.587097 -46.024 28.8495 1.00000\n19-13 -10.893548 -48.330 26.5431 0.99992\n20-13   0.482581 -38.781 39.7464 1.00000\n15-14  -0.141935 -37.579 37.2947 1.00000\n16-14   2.696774 -34.740 40.1334 1.00000\n17-14   1.241935 -36.195 38.6786 1.00000\n18-14   6.458065 -30.979 43.8947 1.00000\n19-14   4.151613 -33.285 41.5882 1.00000\n20-14  15.527742 -23.736 54.7916 0.99545\n16-15   2.838710 -34.598 40.2753 1.00000\n17-15   1.383871 -36.053 38.8205 1.00000\n18-15   6.600000 -30.837 44.0366 1.00000\n19-15   4.293548 -33.143 41.7302 1.00000\n20-15  15.669677 -23.594 54.9335 0.99493\n17-16  -1.454839 -38.891 35.9818 1.00000\n18-16   3.761290 -33.675 41.1979 1.00000\n19-16   1.454839 -35.982 38.8915 1.00000\n20-16  12.830968 -26.433 52.0948 0.99961\n18-17   5.216129 -32.221 42.6528 1.00000\n19-17   2.909677 -34.527 40.3463 1.00000\n20-17  14.285806 -24.978 53.5497 0.99837\n19-18  -2.306452 -39.743 35.1302 1.00000\n20-18   9.069677 -30.194 48.3335 1.00000\n20-19  11.376129 -27.888 50.6400 0.99993\n\n$noon\n         diff     lwr    upr   p adj\nPM-AM -3.3731 -9.8265 3.0804 0.30135\n\n$`Day:noon`\n                  diff      lwr     upr   p adj\n2:AM-1:AM     4.045161  -54.344 62.4344 1.00000\n3:AM-1:AM    -2.838710  -61.228 55.5505 1.00000\n4:AM-1:AM     0.709677  -57.680 59.0989 1.00000\n5:AM-1:AM     8.445161  -49.944 66.8344 1.00000\n6:AM-1:AM     9.793548  -48.596 68.1827 1.00000\n7:AM-1:AM     4.470968  -53.918 62.8602 1.00000\n8:AM-1:AM    15.116129  -43.273 73.5053 1.00000\n9:AM-1:AM    33.993548  -24.396 92.3827 0.92887\n10:AM-1:AM   -9.367742  -67.757 49.0215 1.00000\n11:AM-1:AM   31.935484  -26.454 90.3247 0.96592\n12:AM-1:AM  -10.006452  -68.396 48.3827 1.00000\n13:AM-1:AM    4.612903  -53.776 63.0021 1.00000\n14:AM-1:AM  -10.148387  -68.538 48.2408 1.00000\n15:AM-1:AM  -17.529032  -75.918 40.8602 1.00000\n16:AM-1:AM    0.922581  -57.467 59.3118 1.00000\n17:AM-1:AM    5.606452  -52.783 63.9956 1.00000\n18:AM-1:AM    2.483871  -55.905 60.8731 1.00000\n19:AM-1:AM    9.935484  -48.454 68.3247 1.00000\n20:AM-1:AM    6.316129  -58.965 71.5972 1.00000\n1:PM-1:AM    22.709677  -35.680 81.0989 0.99990\n2:PM-1:AM     6.387097  -52.002 64.7763 1.00000\n3:PM-1:AM    17.387097  -41.002 75.7763 1.00000\n4:PM-1:AM    -6.600000  -64.989 51.7892 1.00000\n5:PM-1:AM     7.593548  -50.796 65.9827 1.00000\n6:PM-1:AM    -0.709677  -59.099 57.6795 1.00000\n7:PM-1:AM     5.393548  -52.996 63.7827 1.00000\n8:PM-1:AM    -1.135484  -59.525 57.2537 1.00000\n9:PM-1:AM     5.819355  -52.570 64.2086 1.00000\n10:PM-1:AM    6.670968  -51.718 65.0602 1.00000\n11:PM-1:AM   -5.535484  -63.925 52.8537 1.00000\n12:PM-1:AM   -8.090323  -66.480 50.2989 1.00000\n13:PM-1:AM   11.141935  -47.247 69.5311 1.00000\n14:PM-1:AM   -4.187097  -62.576 54.2021 1.00000\n15:PM-1:AM    2.909677  -55.480 61.2989 1.00000\n16:PM-1:AM   -9.864516  -68.254 48.5247 1.00000\n17:PM-1:AM  -17.458065  -75.847 40.9311 1.00000\n18:PM-1:AM   -3.903226  -62.292 54.4860 1.00000\n19:PM-1:AM  -15.967742  -74.357 42.4215 1.00000\n20:PM-1:AM    9.722581  -48.667 68.1118 1.00000\n3:AM-2:AM    -6.883871  -65.273 51.5053 1.00000\n4:AM-2:AM    -3.335484  -61.725 55.0537 1.00000\n5:AM-2:AM     4.400000  -53.989 62.7892 1.00000\n6:AM-2:AM     5.748387  -52.641 64.1376 1.00000\n7:AM-2:AM     0.425806  -57.963 58.8150 1.00000\n8:AM-2:AM    11.070968  -47.318 69.4602 1.00000\n9:AM-2:AM    29.948387  -28.441 88.3376 0.98570\n10:AM-2:AM  -13.412903  -71.802 44.9763 1.00000\n11:AM-2:AM   27.890323  -30.499 86.2795 0.99521\n12:AM-2:AM  -14.051613  -72.441 44.3376 1.00000\n13:AM-2:AM    0.567742  -57.821 58.9569 1.00000\n14:AM-2:AM  -14.193548  -72.583 44.1956 1.00000\n15:AM-2:AM  -21.574194  -79.963 36.8150 0.99997\n16:AM-2:AM   -3.122581  -61.512 55.2666 1.00000\n17:AM-2:AM    1.561290  -56.828 59.9505 1.00000\n18:AM-2:AM   -1.561290  -59.950 56.8279 1.00000\n19:AM-2:AM    5.890323  -52.499 64.2795 1.00000\n20:AM-2:AM    2.270968  -63.010 67.5521 1.00000\n1:PM-2:AM    18.664516  -39.725 77.0537 1.00000\n2:PM-2:AM     2.341935  -56.047 60.7311 1.00000\n3:PM-2:AM    13.341935  -45.047 71.7311 1.00000\n4:PM-2:AM   -10.645161  -69.034 47.7440 1.00000\n5:PM-2:AM     3.548387  -54.841 61.9376 1.00000\n6:PM-2:AM    -4.754839  -63.144 53.6344 1.00000\n7:PM-2:AM     1.348387  -57.041 59.7376 1.00000\n8:PM-2:AM    -5.180645  -63.570 53.2086 1.00000\n9:PM-2:AM     1.774194  -56.615 60.1634 1.00000\n10:PM-2:AM    2.625806  -55.763 61.0150 1.00000\n11:PM-2:AM   -9.580645  -67.970 48.8086 1.00000\n12:PM-2:AM  -12.135484  -70.525 46.2537 1.00000\n13:PM-2:AM    7.096774  -51.292 65.4860 1.00000\n14:PM-2:AM   -8.232258  -66.621 50.1569 1.00000\n15:PM-2:AM   -1.135484  -59.525 57.2537 1.00000\n16:PM-2:AM  -13.909677  -72.299 44.4795 1.00000\n17:PM-2:AM  -21.503226  -79.892 36.8860 0.99997\n18:PM-2:AM   -7.948387  -66.338 50.4408 1.00000\n19:PM-2:AM  -20.012903  -78.402 38.3763 0.99999\n20:PM-2:AM    5.677419  -52.712 64.0666 1.00000\n4:AM-3:AM     3.548387  -54.841 61.9376 1.00000\n5:AM-3:AM    11.283871  -47.105 69.6731 1.00000\n6:AM-3:AM    12.632258  -45.757 71.0215 1.00000\n7:AM-3:AM     7.309677  -51.080 65.6989 1.00000\n8:AM-3:AM    17.954839  -40.434 76.3440 1.00000\n9:AM-3:AM    36.832258  -21.557 95.2215 0.84321\n10:AM-3:AM   -6.529032  -64.918 51.8602 1.00000\n11:AM-3:AM   34.774194  -23.615 93.1634 0.90946\n12:AM-3:AM   -7.167742  -65.557 51.2215 1.00000\n13:AM-3:AM    7.451613  -50.938 65.8408 1.00000\n14:AM-3:AM   -7.309677  -65.699 51.0795 1.00000\n15:AM-3:AM  -14.690323  -73.080 43.6989 1.00000\n16:AM-3:AM    3.761290  -54.628 62.1505 1.00000\n17:AM-3:AM    8.445161  -49.944 66.8344 1.00000\n18:AM-3:AM    5.322581  -53.067 63.7118 1.00000\n19:AM-3:AM   12.774194  -45.615 71.1634 1.00000\n20:AM-3:AM    9.154839  -56.126 74.4359 1.00000\n1:PM-3:AM    25.548387  -32.841 83.9376 0.99897\n2:PM-3:AM     9.225806  -49.163 67.6150 1.00000\n3:PM-3:AM    20.225806  -38.163 78.6150 0.99999\n4:PM-3:AM    -3.761290  -62.150 54.6279 1.00000\n5:PM-3:AM    10.432258  -47.957 68.8215 1.00000\n6:PM-3:AM     2.129032  -56.260 60.5182 1.00000\n7:PM-3:AM     8.232258  -50.157 66.6215 1.00000\n8:PM-3:AM     1.703226  -56.686 60.0924 1.00000\n9:PM-3:AM     8.658065  -49.731 67.0473 1.00000\n10:PM-3:AM    9.509677  -48.880 67.8989 1.00000\n11:PM-3:AM   -2.696774  -61.086 55.6924 1.00000\n12:PM-3:AM   -5.251613  -63.641 53.1376 1.00000\n13:PM-3:AM   13.980645  -44.409 72.3698 1.00000\n14:PM-3:AM   -1.348387  -59.738 57.0408 1.00000\n15:PM-3:AM    5.748387  -52.641 64.1376 1.00000\n16:PM-3:AM   -7.025806  -65.415 51.3634 1.00000\n17:PM-3:AM  -14.619355  -73.009 43.7698 1.00000\n18:PM-3:AM   -1.064516  -59.454 57.3247 1.00000\n19:PM-3:AM  -13.129032  -71.518 45.2602 1.00000\n20:PM-3:AM   12.561290  -45.828 70.9505 1.00000\n5:AM-4:AM     7.735484  -50.654 66.1247 1.00000\n6:AM-4:AM     9.083871  -49.305 67.4731 1.00000\n7:AM-4:AM     3.761290  -54.628 62.1505 1.00000\n8:AM-4:AM    14.406452  -43.983 72.7956 1.00000\n9:AM-4:AM    33.283871  -25.105 91.6731 0.94386\n10:AM-4:AM  -10.077419  -68.467 48.3118 1.00000\n11:AM-4:AM   31.225806  -27.163 89.6150 0.97452\n12:AM-4:AM  -10.716129  -69.105 47.6731 1.00000\n13:AM-4:AM    3.903226  -54.486 62.2924 1.00000\n14:AM-4:AM  -10.858065  -69.247 47.5311 1.00000\n15:AM-4:AM  -18.238710  -76.628 40.1505 1.00000\n16:AM-4:AM    0.212903  -58.176 58.6021 1.00000\n17:AM-4:AM    4.896774  -53.492 63.2860 1.00000\n18:AM-4:AM    1.774194  -56.615 60.1634 1.00000\n19:AM-4:AM    9.225806  -49.163 67.6150 1.00000\n20:AM-4:AM    5.606452  -59.675 70.8876 1.00000\n1:PM-4:AM    22.000000  -36.389 80.3892 0.99995\n2:PM-4:AM     5.677419  -52.712 64.0666 1.00000\n3:PM-4:AM    16.677419  -41.712 75.0666 1.00000\n4:PM-4:AM    -7.309677  -65.699 51.0795 1.00000\n5:PM-4:AM     6.883871  -51.505 65.2731 1.00000\n6:PM-4:AM    -1.419355  -59.809 56.9698 1.00000\n7:PM-4:AM     4.683871  -53.705 63.0731 1.00000\n8:PM-4:AM    -1.845161  -60.234 56.5440 1.00000\n9:PM-4:AM     5.109677  -53.280 63.4989 1.00000\n10:PM-4:AM    5.961290  -52.428 64.3505 1.00000\n11:PM-4:AM   -6.245161  -64.634 52.1440 1.00000\n12:PM-4:AM   -8.800000  -67.189 49.5892 1.00000\n13:PM-4:AM   10.432258  -47.957 68.8215 1.00000\n14:PM-4:AM   -4.896774  -63.286 53.4924 1.00000\n15:PM-4:AM    2.200000  -56.189 60.5892 1.00000\n16:PM-4:AM  -10.574194  -68.963 47.8150 1.00000\n17:PM-4:AM  -18.167742  -76.557 40.2215 1.00000\n18:PM-4:AM   -4.612903  -63.002 53.7763 1.00000\n19:PM-4:AM  -16.677419  -75.067 41.7118 1.00000\n20:PM-4:AM    9.012903  -49.376 67.4021 1.00000\n6:AM-5:AM     1.348387  -57.041 59.7376 1.00000\n7:AM-5:AM    -3.974194  -62.363 54.4150 1.00000\n8:AM-5:AM     6.670968  -51.718 65.0602 1.00000\n9:AM-5:AM    25.548387  -32.841 83.9376 0.99897\n10:AM-5:AM  -17.812903  -76.202 40.5763 1.00000\n11:AM-5:AM   23.490323  -34.899 81.8795 0.99980\n12:AM-5:AM  -18.451613  -76.841 39.9376 1.00000\n13:AM-5:AM   -3.832258  -62.221 54.5569 1.00000\n14:AM-5:AM  -18.593548  -76.983 39.7956 1.00000\n15:AM-5:AM  -25.974194  -84.363 32.4150 0.99860\n16:AM-5:AM   -7.522581  -65.912 50.8666 1.00000\n17:AM-5:AM   -2.838710  -61.228 55.5505 1.00000\n18:AM-5:AM   -5.961290  -64.350 52.4279 1.00000\n19:AM-5:AM    1.490323  -56.899 59.8795 1.00000\n20:AM-5:AM   -2.129032  -67.410 63.1521 1.00000\n1:PM-5:AM    14.264516  -44.125 72.6537 1.00000\n2:PM-5:AM    -2.058065  -60.447 56.3311 1.00000\n3:PM-5:AM     8.941935  -49.447 67.3311 1.00000\n4:PM-5:AM   -15.045161  -73.434 43.3440 1.00000\n5:PM-5:AM    -0.851613  -59.241 57.5376 1.00000\n6:PM-5:AM    -9.154839  -67.544 49.2344 1.00000\n7:PM-5:AM    -3.051613  -61.441 55.3376 1.00000\n8:PM-5:AM    -9.580645  -67.970 48.8086 1.00000\n9:PM-5:AM    -2.625806  -61.015 55.7634 1.00000\n10:PM-5:AM   -1.774194  -60.163 56.6150 1.00000\n11:PM-5:AM  -13.980645  -72.370 44.4086 1.00000\n12:PM-5:AM  -16.535484  -74.925 41.8537 1.00000\n13:PM-5:AM    2.696774  -55.692 61.0860 1.00000\n14:PM-5:AM  -12.632258  -71.021 45.7569 1.00000\n15:PM-5:AM   -5.535484  -63.925 52.8537 1.00000\n16:PM-5:AM  -18.309677  -76.699 40.0795 1.00000\n17:PM-5:AM  -25.903226  -84.292 32.4860 0.99867\n18:PM-5:AM  -12.348387  -70.738 46.0408 1.00000\n19:PM-5:AM  -24.412903  -82.802 33.9763 0.99957\n20:PM-5:AM    1.277419  -57.112 59.6666 1.00000\n7:AM-6:AM    -5.322581  -63.712 53.0666 1.00000\n8:AM-6:AM     5.322581  -53.067 63.7118 1.00000\n9:AM-6:AM    24.200000  -34.189 82.5892 0.99964\n10:AM-6:AM  -19.161290  -77.550 39.2279 1.00000\n11:AM-6:AM   22.141935  -36.247 80.5311 0.99994\n12:AM-6:AM  -19.800000  -78.189 38.5892 1.00000\n13:AM-6:AM   -5.180645  -63.570 53.2086 1.00000\n14:AM-6:AM  -19.941935  -78.331 38.4473 1.00000\n15:AM-6:AM  -27.322581  -85.712 31.0666 0.99660\n16:AM-6:AM   -8.870968  -67.260 49.5182 1.00000\n17:AM-6:AM   -4.187097  -62.576 54.2021 1.00000\n18:AM-6:AM   -7.309677  -65.699 51.0795 1.00000\n19:AM-6:AM    0.141935  -58.247 58.5311 1.00000\n20:AM-6:AM   -3.477419  -68.759 61.8037 1.00000\n1:PM-6:AM    12.916129  -45.473 71.3053 1.00000\n2:PM-6:AM    -3.406452  -61.796 54.9827 1.00000\n3:PM-6:AM     7.593548  -50.796 65.9827 1.00000\n4:PM-6:AM   -16.393548  -74.783 41.9956 1.00000\n5:PM-6:AM    -2.200000  -60.589 56.1892 1.00000\n6:PM-6:AM   -10.503226  -68.892 47.8860 1.00000\n7:PM-6:AM    -4.400000  -62.789 53.9892 1.00000\n8:PM-6:AM   -10.929032  -69.318 47.4602 1.00000\n9:PM-6:AM    -3.974194  -62.363 54.4150 1.00000\n10:PM-6:AM   -3.122581  -61.512 55.2666 1.00000\n11:PM-6:AM  -15.329032  -73.718 43.0602 1.00000\n12:PM-6:AM  -17.883871  -76.273 40.5053 1.00000\n13:PM-6:AM    1.348387  -57.041 59.7376 1.00000\n14:PM-6:AM  -13.980645  -72.370 44.4086 1.00000\n15:PM-6:AM   -6.883871  -65.273 51.5053 1.00000\n16:PM-6:AM  -19.658065  -78.047 38.7311 1.00000\n17:PM-6:AM  -27.251613  -85.641 31.1376 0.99674\n18:PM-6:AM  -13.696774  -72.086 44.6924 1.00000\n19:PM-6:AM  -25.761290  -84.150 32.6279 0.99880\n20:PM-6:AM   -0.070968  -58.460 58.3182 1.00000\n8:AM-7:AM    10.645161  -47.744 69.0344 1.00000\n9:AM-7:AM    29.522581  -28.867 87.9118 0.98840\n10:AM-7:AM  -13.838710  -72.228 44.5505 1.00000\n11:AM-7:AM   27.464516  -30.925 85.8537 0.99629\n12:AM-7:AM  -14.477419  -72.867 43.9118 1.00000\n13:AM-7:AM    0.141935  -58.247 58.5311 1.00000\n14:AM-7:AM  -14.619355  -73.009 43.7698 1.00000\n15:AM-7:AM  -22.000000  -80.389 36.3892 0.99995\n16:AM-7:AM   -3.548387  -61.938 54.8408 1.00000\n17:AM-7:AM    1.135484  -57.254 59.5247 1.00000\n18:AM-7:AM   -1.987097  -60.376 56.4021 1.00000\n19:AM-7:AM    5.464516  -52.925 63.8537 1.00000\n20:AM-7:AM    1.845161  -63.436 67.1263 1.00000\n1:PM-7:AM    18.238710  -40.150 76.6279 1.00000\n2:PM-7:AM     1.916129  -56.473 60.3053 1.00000\n3:PM-7:AM    12.916129  -45.473 71.3053 1.00000\n4:PM-7:AM   -11.070968  -69.460 47.3182 1.00000\n5:PM-7:AM     3.122581  -55.267 61.5118 1.00000\n6:PM-7:AM    -5.180645  -63.570 53.2086 1.00000\n7:PM-7:AM     0.922581  -57.467 59.3118 1.00000\n8:PM-7:AM    -5.606452  -63.996 52.7827 1.00000\n9:PM-7:AM     1.348387  -57.041 59.7376 1.00000\n10:PM-7:AM    2.200000  -56.189 60.5892 1.00000\n11:PM-7:AM  -10.006452  -68.396 48.3827 1.00000\n12:PM-7:AM  -12.561290  -70.950 45.8279 1.00000\n13:PM-7:AM    6.670968  -51.718 65.0602 1.00000\n14:PM-7:AM   -8.658065  -67.047 49.7311 1.00000\n15:PM-7:AM   -1.561290  -59.950 56.8279 1.00000\n16:PM-7:AM  -14.335484  -72.725 44.0537 1.00000\n17:PM-7:AM  -21.929032  -80.318 36.4602 0.99996\n18:PM-7:AM   -8.374194  -66.763 50.0150 1.00000\n19:PM-7:AM  -20.438710  -78.828 37.9505 0.99999\n20:PM-7:AM    5.251613  -53.138 63.6408 1.00000\n9:AM-8:AM    18.877419  -39.512 77.2666 1.00000\n10:AM-8:AM  -24.483871  -82.873 33.9053 0.99954\n11:AM-8:AM   16.819355  -41.570 75.2086 1.00000\n12:AM-8:AM  -25.122581  -83.512 33.2666 0.99925\n13:AM-8:AM  -10.503226  -68.892 47.8860 1.00000\n14:AM-8:AM  -25.264516  -83.654 33.1247 0.99916\n15:AM-8:AM  -32.645161  -91.034 25.7440 0.95531\n16:AM-8:AM  -14.193548  -72.583 44.1956 1.00000\n17:AM-8:AM   -9.509677  -67.899 48.8795 1.00000\n18:AM-8:AM  -12.632258  -71.021 45.7569 1.00000\n19:AM-8:AM   -5.180645  -63.570 53.2086 1.00000\n20:AM-8:AM   -8.800000  -74.081 56.4811 1.00000\n1:PM-8:AM     7.593548  -50.796 65.9827 1.00000\n2:PM-8:AM    -8.729032  -67.118 49.6602 1.00000\n3:PM-8:AM     2.270968  -56.118 60.6602 1.00000\n4:PM-8:AM   -21.716129  -80.105 36.6731 0.99996\n5:PM-8:AM    -7.522581  -65.912 50.8666 1.00000\n6:PM-8:AM   -15.825806  -74.215 42.5634 1.00000\n7:PM-8:AM    -9.722581  -68.112 48.6666 1.00000\n8:PM-8:AM   -16.251613  -74.641 42.1376 1.00000\n9:PM-8:AM    -9.296774  -67.686 49.0924 1.00000\n10:PM-8:AM   -8.445161  -66.834 49.9440 1.00000\n11:PM-8:AM  -20.651613  -79.041 37.7376 0.99999\n12:PM-8:AM  -23.206452  -81.596 35.1827 0.99985\n13:PM-8:AM   -3.974194  -62.363 54.4150 1.00000\n14:PM-8:AM  -19.303226  -77.692 39.0860 1.00000\n15:PM-8:AM  -12.206452  -70.596 46.1827 1.00000\n16:PM-8:AM  -24.980645  -83.370 33.4086 0.99933\n17:PM-8:AM  -32.574194  -90.963 25.8150 0.95647\n18:PM-8:AM  -19.019355  -77.409 39.3698 1.00000\n19:PM-8:AM  -31.083871  -89.473 27.3053 0.97602\n20:PM-8:AM   -5.393548  -63.783 52.9956 1.00000\n10:AM-9:AM  -43.361290 -101.750 15.0279 0.52759\n11:AM-9:AM   -2.058065  -60.447 56.3311 1.00000\n12:AM-9:AM  -44.000000 -102.389 14.3892 0.49403\n13:AM-9:AM  -29.380645  -87.770 29.0086 0.98920\n14:AM-9:AM  -44.141935 -102.531 14.2473 0.48663\n15:AM-9:AM  -51.522581 -109.912  6.8666 0.17781\n16:AM-9:AM  -33.070968  -91.460 25.3182 0.94789\n17:AM-9:AM  -28.387097  -86.776 30.0021 0.99364\n18:AM-9:AM  -31.509677  -89.899 26.8795 0.97131\n19:AM-9:AM  -24.058065  -82.447 34.3311 0.99968\n20:AM-9:AM  -27.677419  -92.959 37.6037 0.99943\n1:PM-9:AM   -11.283871  -69.673 47.1053 1.00000\n2:PM-9:AM   -27.606452  -85.996 30.7827 0.99595\n3:PM-9:AM   -16.606452  -74.996 41.7827 1.00000\n4:PM-9:AM   -40.593548  -98.983 17.7956 0.67322\n5:PM-9:AM   -26.400000  -84.789 31.9892 0.99812\n6:PM-9:AM   -34.703226  -93.092 23.6860 0.91136\n7:PM-9:AM   -28.600000  -86.989 29.7892 0.99284\n8:PM-9:AM   -35.129032  -93.518 23.2602 0.89961\n9:PM-9:AM   -28.174194  -86.563 30.2150 0.99436\n10:PM-9:AM  -27.322581  -85.712 31.0666 0.99660\n11:PM-9:AM  -39.529032  -97.918 18.8602 0.72631\n12:PM-9:AM  -42.083871 -100.473 16.3053 0.59536\n13:PM-9:AM  -22.851613  -81.241 35.5376 0.99989\n14:PM-9:AM  -38.180645  -96.570 20.2086 0.78850\n15:PM-9:AM  -31.083871  -89.473 27.3053 0.97602\n16:PM-9:AM  -43.858065 -102.247 14.5311 0.50145\n17:PM-9:AM  -51.451613 -109.841  6.9376 0.17988\n18:PM-9:AM  -37.896774  -96.286 20.4924 0.80070\n19:PM-9:AM  -49.961290 -108.350  8.4279 0.22767\n20:PM-9:AM  -24.270968  -82.660 34.1182 0.99962\n11:AM-10:AM  41.303226  -17.086 99.6924 0.63649\n12:AM-10:AM  -0.638710  -59.028 57.7505 1.00000\n13:AM-10:AM  13.980645  -44.409 72.3698 1.00000\n14:AM-10:AM  -0.780645  -59.170 57.6086 1.00000\n15:AM-10:AM  -8.161290  -66.550 50.2279 1.00000\n16:AM-10:AM  10.290323  -48.099 68.6795 1.00000\n17:AM-10:AM  14.974194  -43.415 73.3634 1.00000\n18:AM-10:AM  11.851613  -46.538 70.2408 1.00000\n19:AM-10:AM  19.303226  -39.086 77.6924 1.00000\n20:AM-10:AM  15.683871  -49.597 80.9650 1.00000\n1:PM-10:AM   32.077419  -26.312 90.4666 0.96397\n2:PM-10:AM   15.754839  -42.634 74.1440 1.00000\n3:PM-10:AM   26.754839  -31.634 85.1440 0.99763\n4:PM-10:AM    2.767742  -55.621 61.1569 1.00000\n5:PM-10:AM   16.961290  -41.428 75.3505 1.00000\n6:PM-10:AM    8.658065  -49.731 67.0473 1.00000\n7:PM-10:AM   14.761290  -43.628 73.1505 1.00000\n8:PM-10:AM    8.232258  -50.157 66.6215 1.00000\n9:PM-10:AM   15.187097  -43.202 73.5763 1.00000\n10:PM-10:AM  16.038710  -42.350 74.4279 1.00000\n11:PM-10:AM   3.832258  -54.557 62.2215 1.00000\n12:PM-10:AM   1.277419  -57.112 59.6666 1.00000\n13:PM-10:AM  20.509677  -37.880 78.8989 0.99999\n14:PM-10:AM   5.180645  -53.209 63.5698 1.00000\n15:PM-10:AM  12.277419  -46.112 70.6666 1.00000\n16:PM-10:AM  -0.496774  -58.886 57.8924 1.00000\n17:PM-10:AM  -8.090323  -66.480 50.2989 1.00000\n18:PM-10:AM   5.464516  -52.925 63.8537 1.00000\n19:PM-10:AM  -6.600000  -64.989 51.7892 1.00000\n20:PM-10:AM  19.090323  -39.299 77.4795 1.00000\n12:AM-11:AM -41.941935 -100.331 16.4473 0.60287\n13:AM-11:AM -27.322581  -85.712 31.0666 0.99660\n14:AM-11:AM -42.083871 -100.473 16.3053 0.59536\n15:AM-11:AM -49.464516 -107.854  8.9247 0.24541\n16:AM-11:AM -31.012903  -89.402 27.3763 0.97675\n17:AM-11:AM -26.329032  -84.718 32.0602 0.99821\n18:AM-11:AM -29.451613  -87.841 28.9376 0.98880\n19:AM-11:AM -22.000000  -80.389 36.3892 0.99995\n20:AM-11:AM -25.619355  -90.900 39.6618 0.99988\n1:PM-11:AM   -9.225806  -67.615 49.1634 1.00000\n2:PM-11:AM  -25.548387  -83.938 32.8408 0.99897\n3:PM-11:AM  -14.548387  -72.938 43.8408 1.00000\n4:PM-11:AM  -38.535484  -96.925 19.8537 0.77279\n5:PM-11:AM  -24.341935  -82.731 34.0473 0.99959\n6:PM-11:AM  -32.645161  -91.034 25.7440 0.95531\n7:PM-11:AM  -26.541935  -84.931 31.8473 0.99794\n8:PM-11:AM  -33.070968  -91.460 25.3182 0.94789\n9:PM-11:AM  -26.116129  -84.505 32.2731 0.99846\n10:PM-11:AM -25.264516  -83.654 33.1247 0.99916\n11:PM-11:AM -37.470968  -95.860 20.9182 0.81833\n12:PM-11:AM -40.025806  -98.415 18.3634 0.70190\n13:PM-11:AM -20.793548  -79.183 37.5956 0.99999\n14:PM-11:AM -36.122581  -94.512 22.2666 0.86852\n15:PM-11:AM -29.025806  -87.415 29.3634 0.99101\n16:PM-11:AM -41.800000 -100.189 16.5892 0.61038\n17:PM-11:AM -49.393548 -107.783  8.9956 0.24802\n18:PM-11:AM -35.838710  -94.228 22.5505 0.87792\n19:PM-11:AM -47.903226 -106.292 10.4860 0.30702\n20:PM-11:AM -22.212903  -80.602 36.1763 0.99994\n13:AM-12:AM  14.619355  -43.770 73.0086 1.00000\n14:AM-12:AM  -0.141935  -58.531 58.2473 1.00000\n15:AM-12:AM  -7.522581  -65.912 50.8666 1.00000\n16:AM-12:AM  10.929032  -47.460 69.3182 1.00000\n17:AM-12:AM  15.612903  -42.776 74.0021 1.00000\n18:AM-12:AM  12.490323  -45.899 70.8795 1.00000\n19:AM-12:AM  19.941935  -38.447 78.3311 1.00000\n20:AM-12:AM  16.322581  -48.959 81.6037 1.00000\n1:PM-12:AM   32.716129  -25.673 91.1053 0.95413\n2:PM-12:AM   16.393548  -41.996 74.7827 1.00000\n3:PM-12:AM   27.393548  -30.996 85.7827 0.99645\n4:PM-12:AM    3.406452  -54.983 61.7956 1.00000\n5:PM-12:AM   17.600000  -40.789 75.9892 1.00000\n6:PM-12:AM    9.296774  -49.092 67.6860 1.00000\n7:PM-12:AM   15.400000  -42.989 73.7892 1.00000\n8:PM-12:AM    8.870968  -49.518 67.2602 1.00000\n9:PM-12:AM   15.825806  -42.563 74.2150 1.00000\n10:PM-12:AM  16.677419  -41.712 75.0666 1.00000\n11:PM-12:AM   4.470968  -53.918 62.8602 1.00000\n12:PM-12:AM   1.916129  -56.473 60.3053 1.00000\n13:PM-12:AM  21.148387  -37.241 79.5376 0.99998\n14:PM-12:AM   5.819355  -52.570 64.2086 1.00000\n15:PM-12:AM  12.916129  -45.473 71.3053 1.00000\n16:PM-12:AM   0.141935  -58.247 58.5311 1.00000\n17:PM-12:AM  -7.451613  -65.841 50.9376 1.00000\n18:PM-12:AM   6.103226  -52.286 64.4924 1.00000\n19:PM-12:AM  -5.961290  -64.350 52.4279 1.00000\n20:PM-12:AM  19.729032  -38.660 78.1182 1.00000\n14:AM-13:AM -14.761290  -73.150 43.6279 1.00000\n15:AM-13:AM -22.141935  -80.531 36.2473 0.99994\n16:AM-13:AM  -3.690323  -62.080 54.6989 1.00000\n17:AM-13:AM   0.993548  -57.396 59.3827 1.00000\n18:AM-13:AM  -2.129032  -60.518 56.2602 1.00000\n19:AM-13:AM   5.322581  -53.067 63.7118 1.00000\n20:AM-13:AM   1.703226  -63.578 66.9843 1.00000\n1:PM-13:AM   18.096774  -40.292 76.4860 1.00000\n2:PM-13:AM    1.774194  -56.615 60.1634 1.00000\n3:PM-13:AM   12.774194  -45.615 71.1634 1.00000\n4:PM-13:AM  -11.212903  -69.602 47.1763 1.00000\n5:PM-13:AM    2.980645  -55.409 61.3698 1.00000\n6:PM-13:AM   -5.322581  -63.712 53.0666 1.00000\n7:PM-13:AM    0.780645  -57.609 59.1698 1.00000\n8:PM-13:AM   -5.748387  -64.138 52.6408 1.00000\n9:PM-13:AM    1.206452  -57.183 59.5956 1.00000\n10:PM-13:AM   2.058065  -56.331 60.4473 1.00000\n11:PM-13:AM -10.148387  -68.538 48.2408 1.00000\n12:PM-13:AM -12.703226  -71.092 45.6860 1.00000\n13:PM-13:AM   6.529032  -51.860 64.9182 1.00000\n14:PM-13:AM  -8.800000  -67.189 49.5892 1.00000\n15:PM-13:AM  -1.703226  -60.092 56.6860 1.00000\n16:PM-13:AM -14.477419  -72.867 43.9118 1.00000\n17:PM-13:AM -22.070968  -80.460 36.3182 0.99995\n18:PM-13:AM  -8.516129  -66.905 49.8731 1.00000\n19:PM-13:AM -20.580645  -78.970 37.8086 0.99999\n20:PM-13:AM   5.109677  -53.280 63.4989 1.00000\n15:AM-14:AM  -7.380645  -65.770 51.0086 1.00000\n16:AM-14:AM  11.070968  -47.318 69.4602 1.00000\n17:AM-14:AM  15.754839  -42.634 74.1440 1.00000\n18:AM-14:AM  12.632258  -45.757 71.0215 1.00000\n19:AM-14:AM  20.083871  -38.305 78.4731 0.99999\n20:AM-14:AM  16.464516  -48.817 81.7456 1.00000\n1:PM-14:AM   32.858065  -25.531 91.2473 0.95170\n2:PM-14:AM   16.535484  -41.854 74.9247 1.00000\n3:PM-14:AM   27.535484  -30.854 85.9247 0.99612\n4:PM-14:AM    3.548387  -54.841 61.9376 1.00000\n5:PM-14:AM   17.741935  -40.647 76.1311 1.00000\n6:PM-14:AM    9.438710  -48.950 67.8279 1.00000\n7:PM-14:AM   15.541935  -42.847 73.9311 1.00000\n8:PM-14:AM    9.012903  -49.376 67.4021 1.00000\n9:PM-14:AM   15.967742  -42.421 74.3569 1.00000\n10:PM-14:AM  16.819355  -41.570 75.2086 1.00000\n11:PM-14:AM   4.612903  -53.776 63.0021 1.00000\n12:PM-14:AM   2.058065  -56.331 60.4473 1.00000\n13:PM-14:AM  21.290323  -37.099 79.6795 0.99998\n14:PM-14:AM   5.961290  -52.428 64.3505 1.00000\n15:PM-14:AM  13.058065  -45.331 71.4473 1.00000\n16:PM-14:AM   0.283871  -58.105 58.6731 1.00000\n17:PM-14:AM  -7.309677  -65.699 51.0795 1.00000\n18:PM-14:AM   6.245161  -52.144 64.6344 1.00000\n19:PM-14:AM  -5.819355  -64.209 52.5698 1.00000\n20:PM-14:AM  19.870968  -38.518 78.2602 1.00000\n16:AM-15:AM  18.451613  -39.938 76.8408 1.00000\n17:AM-15:AM  23.135484  -35.254 81.5247 0.99986\n18:AM-15:AM  20.012903  -38.376 78.4021 0.99999\n19:AM-15:AM  27.464516  -30.925 85.8537 0.99629\n20:AM-15:AM  23.845161  -41.436 89.1263 0.99998\n1:PM-15:AM   40.238710  -18.150 98.6279 0.69123\n2:PM-15:AM   23.916129  -34.473 82.3053 0.99972\n3:PM-15:AM   34.916129  -23.473 93.3053 0.90560\n4:PM-15:AM   10.929032  -47.460 69.3182 1.00000\n5:PM-15:AM   25.122581  -33.267 83.5118 0.99925\n6:PM-15:AM   16.819355  -41.570 75.2086 1.00000\n7:PM-15:AM   22.922581  -35.467 81.3118 0.99988\n8:PM-15:AM   16.393548  -41.996 74.7827 1.00000\n9:PM-15:AM   23.348387  -35.041 81.7376 0.99983\n10:PM-15:AM  24.200000  -34.189 82.5892 0.99964\n11:PM-15:AM  11.993548  -46.396 70.3827 1.00000\n12:PM-15:AM   9.438710  -48.950 67.8279 1.00000\n13:PM-15:AM  28.670968  -29.718 87.0602 0.99256\n14:PM-15:AM  13.341935  -45.047 71.7311 1.00000\n15:PM-15:AM  20.438710  -37.950 78.8279 0.99999\n16:PM-15:AM   7.664516  -50.725 66.0537 1.00000\n17:PM-15:AM   0.070968  -58.318 58.4602 1.00000\n18:PM-15:AM  13.625806  -44.763 72.0150 1.00000\n19:PM-15:AM   1.561290  -56.828 59.9505 1.00000\n20:PM-15:AM  27.251613  -31.138 85.6408 0.99674\n17:AM-16:AM   4.683871  -53.705 63.0731 1.00000\n18:AM-16:AM   1.561290  -56.828 59.9505 1.00000\n19:AM-16:AM   9.012903  -49.376 67.4021 1.00000\n20:AM-16:AM   5.393548  -59.888 70.6747 1.00000\n1:PM-16:AM   21.787097  -36.602 80.1763 0.99996\n2:PM-16:AM    5.464516  -52.925 63.8537 1.00000\n3:PM-16:AM   16.464516  -41.925 74.8537 1.00000\n4:PM-16:AM   -7.522581  -65.912 50.8666 1.00000\n5:PM-16:AM    6.670968  -51.718 65.0602 1.00000\n6:PM-16:AM   -1.632258  -60.021 56.7569 1.00000\n7:PM-16:AM    4.470968  -53.918 62.8602 1.00000\n8:PM-16:AM   -2.058065  -60.447 56.3311 1.00000\n9:PM-16:AM    4.896774  -53.492 63.2860 1.00000\n10:PM-16:AM   5.748387  -52.641 64.1376 1.00000\n11:PM-16:AM  -6.458065  -64.847 51.9311 1.00000\n12:PM-16:AM  -9.012903  -67.402 49.3763 1.00000\n13:PM-16:AM  10.219355  -48.170 68.6086 1.00000\n14:PM-16:AM  -5.109677  -63.499 53.2795 1.00000\n15:PM-16:AM   1.987097  -56.402 60.3763 1.00000\n16:PM-16:AM -10.787097  -69.176 47.6021 1.00000\n17:PM-16:AM -18.380645  -76.770 40.0086 1.00000\n18:PM-16:AM  -4.825806  -63.215 53.5634 1.00000\n19:PM-16:AM -16.890323  -75.280 41.4989 1.00000\n20:PM-16:AM   8.800000  -49.589 67.1892 1.00000\n18:AM-17:AM  -3.122581  -61.512 55.2666 1.00000\n19:AM-17:AM   4.329032  -54.060 62.7182 1.00000\n20:AM-17:AM   0.709677  -64.571 65.9908 1.00000\n1:PM-17:AM   17.103226  -41.286 75.4924 1.00000\n2:PM-17:AM    0.780645  -57.609 59.1698 1.00000\n3:PM-17:AM   11.780645  -46.609 70.1698 1.00000\n4:PM-17:AM  -12.206452  -70.596 46.1827 1.00000\n5:PM-17:AM    1.987097  -56.402 60.3763 1.00000\n6:PM-17:AM   -6.316129  -64.705 52.0731 1.00000\n7:PM-17:AM   -0.212903  -58.602 58.1763 1.00000\n8:PM-17:AM   -6.741935  -65.131 51.6473 1.00000\n9:PM-17:AM    0.212903  -58.176 58.6021 1.00000\n10:PM-17:AM   1.064516  -57.325 59.4537 1.00000\n11:PM-17:AM -11.141935  -69.531 47.2473 1.00000\n12:PM-17:AM -13.696774  -72.086 44.6924 1.00000\n13:PM-17:AM   5.535484  -52.854 63.9247 1.00000\n14:PM-17:AM  -9.793548  -68.183 48.5956 1.00000\n15:PM-17:AM  -2.696774  -61.086 55.6924 1.00000\n16:PM-17:AM -15.470968  -73.860 42.9182 1.00000\n17:PM-17:AM -23.064516  -81.454 35.3247 0.99987\n18:PM-17:AM  -9.509677  -67.899 48.8795 1.00000\n19:PM-17:AM -21.574194  -79.963 36.8150 0.99997\n20:PM-17:AM   4.116129  -54.273 62.5053 1.00000\n19:AM-18:AM   7.451613  -50.938 65.8408 1.00000\n20:AM-18:AM   3.832258  -61.449 69.1134 1.00000\n1:PM-18:AM   20.225806  -38.163 78.6150 0.99999\n2:PM-18:AM    3.903226  -54.486 62.2924 1.00000\n3:PM-18:AM   14.903226  -43.486 73.2924 1.00000\n4:PM-18:AM   -9.083871  -67.473 49.3053 1.00000\n5:PM-18:AM    5.109677  -53.280 63.4989 1.00000\n6:PM-18:AM   -3.193548  -61.583 55.1956 1.00000\n7:PM-18:AM    2.909677  -55.480 61.2989 1.00000\n8:PM-18:AM   -3.619355  -62.009 54.7698 1.00000\n9:PM-18:AM    3.335484  -55.054 61.7247 1.00000\n10:PM-18:AM   4.187097  -54.202 62.5763 1.00000\n11:PM-18:AM  -8.019355  -66.409 50.3698 1.00000\n12:PM-18:AM -10.574194  -68.963 47.8150 1.00000\n13:PM-18:AM   8.658065  -49.731 67.0473 1.00000\n14:PM-18:AM  -6.670968  -65.060 51.7182 1.00000\n15:PM-18:AM   0.425806  -57.963 58.8150 1.00000\n16:PM-18:AM -12.348387  -70.738 46.0408 1.00000\n17:PM-18:AM -19.941935  -78.331 38.4473 1.00000\n18:PM-18:AM  -6.387097  -64.776 52.0021 1.00000\n19:PM-18:AM -18.451613  -76.841 39.9376 1.00000\n20:PM-18:AM   7.238710  -51.150 65.6279 1.00000\n20:AM-19:AM  -3.619355  -68.900 61.6618 1.00000\n1:PM-19:AM   12.774194  -45.615 71.1634 1.00000\n2:PM-19:AM   -3.548387  -61.938 54.8408 1.00000\n3:PM-19:AM    7.451613  -50.938 65.8408 1.00000\n4:PM-19:AM  -16.535484  -74.925 41.8537 1.00000\n5:PM-19:AM   -2.341935  -60.731 56.0473 1.00000\n6:PM-19:AM  -10.645161  -69.034 47.7440 1.00000\n7:PM-19:AM   -4.541935  -62.931 53.8473 1.00000\n8:PM-19:AM  -11.070968  -69.460 47.3182 1.00000\n9:PM-19:AM   -4.116129  -62.505 54.2731 1.00000\n10:PM-19:AM  -3.264516  -61.654 55.1247 1.00000\n11:PM-19:AM -15.470968  -73.860 42.9182 1.00000\n12:PM-19:AM -18.025806  -76.415 40.3634 1.00000\n13:PM-19:AM   1.206452  -57.183 59.5956 1.00000\n14:PM-19:AM -14.122581  -72.512 44.2666 1.00000\n15:PM-19:AM  -7.025806  -65.415 51.3634 1.00000\n16:PM-19:AM -19.800000  -78.189 38.5892 1.00000\n17:PM-19:AM -27.393548  -85.783 30.9956 0.99645\n18:PM-19:AM -13.838710  -72.228 44.5505 1.00000\n19:PM-19:AM -25.903226  -84.292 32.4860 0.99867\n20:PM-19:AM  -0.212903  -58.602 58.1763 1.00000\n1:PM-20:AM   16.393548  -48.888 81.6747 1.00000\n2:PM-20:AM    0.070968  -65.210 65.3521 1.00000\n3:PM-20:AM   11.070968  -54.210 76.3521 1.00000\n4:PM-20:AM  -12.916129  -78.197 52.3650 1.00000\n5:PM-20:AM    1.277419  -64.004 66.5585 1.00000\n6:PM-20:AM   -7.025806  -72.307 58.2553 1.00000\n7:PM-20:AM   -0.922581  -66.204 64.3585 1.00000\n8:PM-20:AM   -7.451613  -72.733 57.8295 1.00000\n9:PM-20:AM   -0.496774  -65.778 64.7843 1.00000\n10:PM-20:AM   0.354839  -64.926 65.6359 1.00000\n11:PM-20:AM -11.851613  -77.133 53.4295 1.00000\n12:PM-20:AM -14.406452  -79.688 50.8747 1.00000\n13:PM-20:AM   4.825806  -60.455 70.1069 1.00000\n14:PM-20:AM -10.503226  -75.784 54.7779 1.00000\n15:PM-20:AM  -3.406452  -68.688 61.8747 1.00000\n16:PM-20:AM -16.180645  -81.462 49.1005 1.00000\n17:PM-20:AM -23.774194  -89.055 41.5069 0.99998\n18:PM-20:AM -10.219355  -75.500 55.0618 1.00000\n19:PM-20:AM -22.283871  -87.565 42.9972 1.00000\n20:PM-20:AM   3.406452  -61.875 68.6876 1.00000\n2:PM-1:PM   -16.322581  -74.712 42.0666 1.00000\n3:PM-1:PM    -5.322581  -63.712 53.0666 1.00000\n4:PM-1:PM   -29.309677  -87.699 29.0795 0.98958\n5:PM-1:PM   -15.116129  -73.505 43.2731 1.00000\n6:PM-1:PM   -23.419355  -81.809 34.9698 0.99982\n7:PM-1:PM   -17.316129  -75.705 41.0731 1.00000\n8:PM-1:PM   -23.845161  -82.234 34.5440 0.99973\n9:PM-1:PM   -16.890323  -75.280 41.4989 1.00000\n10:PM-1:PM  -16.038710  -74.428 42.3505 1.00000\n11:PM-1:PM  -28.245161  -86.634 30.1440 0.99413\n12:PM-1:PM  -30.800000  -89.189 27.5892 0.97882\n13:PM-1:PM  -11.567742  -69.957 46.8215 1.00000\n14:PM-1:PM  -26.896774  -85.286 31.4924 0.99740\n15:PM-1:PM  -19.800000  -78.189 38.5892 1.00000\n16:PM-1:PM  -32.574194  -90.963 25.8150 0.95647\n17:PM-1:PM  -40.167742  -98.557 18.2215 0.69480\n18:PM-1:PM  -26.612903  -85.002 31.7763 0.99784\n19:PM-1:PM  -38.677419  -97.067 19.7118 0.76637\n20:PM-1:PM  -12.987097  -71.376 45.4021 1.00000\n3:PM-2:PM    11.000000  -47.389 69.3892 1.00000\n4:PM-2:PM   -12.987097  -71.376 45.4021 1.00000\n5:PM-2:PM     1.206452  -57.183 59.5956 1.00000\n6:PM-2:PM    -7.096774  -65.486 51.2924 1.00000\n7:PM-2:PM    -0.993548  -59.383 57.3956 1.00000\n8:PM-2:PM    -7.522581  -65.912 50.8666 1.00000\n9:PM-2:PM    -0.567742  -58.957 57.8215 1.00000\n10:PM-2:PM    0.283871  -58.105 58.6731 1.00000\n11:PM-2:PM  -11.922581  -70.312 46.4666 1.00000\n12:PM-2:PM  -14.477419  -72.867 43.9118 1.00000\n13:PM-2:PM    4.754839  -53.634 63.1440 1.00000\n14:PM-2:PM  -10.574194  -68.963 47.8150 1.00000\n15:PM-2:PM   -3.477419  -61.867 54.9118 1.00000\n16:PM-2:PM  -16.251613  -74.641 42.1376 1.00000\n17:PM-2:PM  -23.845161  -82.234 34.5440 0.99973\n18:PM-2:PM  -10.290323  -68.680 48.0989 1.00000\n19:PM-2:PM  -22.354839  -80.744 36.0344 0.99993\n20:PM-2:PM    3.335484  -55.054 61.7247 1.00000\n4:PM-3:PM   -23.987097  -82.376 34.4021 0.99970\n5:PM-3:PM    -9.793548  -68.183 48.5956 1.00000\n6:PM-3:PM   -18.096774  -76.486 40.2924 1.00000\n7:PM-3:PM   -11.993548  -70.383 46.3956 1.00000\n8:PM-3:PM   -18.522581  -76.912 39.8666 1.00000\n9:PM-3:PM   -11.567742  -69.957 46.8215 1.00000\n10:PM-3:PM  -10.716129  -69.105 47.6731 1.00000\n11:PM-3:PM  -22.922581  -81.312 35.4666 0.99988\n12:PM-3:PM  -25.477419  -83.867 32.9118 0.99902\n13:PM-3:PM   -6.245161  -64.634 52.1440 1.00000\n14:PM-3:PM  -21.574194  -79.963 36.8150 0.99997\n15:PM-3:PM  -14.477419  -72.867 43.9118 1.00000\n16:PM-3:PM  -27.251613  -85.641 31.1376 0.99674\n17:PM-3:PM  -34.845161  -93.234 23.5440 0.90755\n18:PM-3:PM  -21.290323  -79.680 37.0989 0.99998\n19:PM-3:PM  -33.354839  -91.744 25.0344 0.94247\n20:PM-3:PM   -7.664516  -66.054 50.7247 1.00000\n5:PM-4:PM    14.193548  -44.196 72.5827 1.00000\n6:PM-4:PM     5.890323  -52.499 64.2795 1.00000\n7:PM-4:PM    11.993548  -46.396 70.3827 1.00000\n8:PM-4:PM     5.464516  -52.925 63.8537 1.00000\n9:PM-4:PM    12.419355  -45.970 70.8086 1.00000\n10:PM-4:PM   13.270968  -45.118 71.6602 1.00000\n11:PM-4:PM    1.064516  -57.325 59.4537 1.00000\n12:PM-4:PM   -1.490323  -59.880 56.8989 1.00000\n13:PM-4:PM   17.741935  -40.647 76.1311 1.00000\n14:PM-4:PM    2.412903  -55.976 60.8021 1.00000\n15:PM-4:PM    9.509677  -48.880 67.8989 1.00000\n16:PM-4:PM   -3.264516  -61.654 55.1247 1.00000\n17:PM-4:PM  -10.858065  -69.247 47.5311 1.00000\n18:PM-4:PM    2.696774  -55.692 61.0860 1.00000\n19:PM-4:PM   -9.367742  -67.757 49.0215 1.00000\n20:PM-4:PM   16.322581  -42.067 74.7118 1.00000\n6:PM-5:PM    -8.303226  -66.692 50.0860 1.00000\n7:PM-5:PM    -2.200000  -60.589 56.1892 1.00000\n8:PM-5:PM    -8.729032  -67.118 49.6602 1.00000\n9:PM-5:PM    -1.774194  -60.163 56.6150 1.00000\n10:PM-5:PM   -0.922581  -59.312 57.4666 1.00000\n11:PM-5:PM  -13.129032  -71.518 45.2602 1.00000\n12:PM-5:PM  -15.683871  -74.073 42.7053 1.00000\n13:PM-5:PM    3.548387  -54.841 61.9376 1.00000\n14:PM-5:PM  -11.780645  -70.170 46.6086 1.00000\n15:PM-5:PM   -4.683871  -63.073 53.7053 1.00000\n16:PM-5:PM  -17.458065  -75.847 40.9311 1.00000\n17:PM-5:PM  -25.051613  -83.441 33.3376 0.99929\n18:PM-5:PM  -11.496774  -69.886 46.8924 1.00000\n19:PM-5:PM  -23.561290  -81.950 34.8279 0.99979\n20:PM-5:PM    2.129032  -56.260 60.5182 1.00000\n7:PM-6:PM     6.103226  -52.286 64.4924 1.00000\n8:PM-6:PM    -0.425806  -58.815 57.9634 1.00000\n9:PM-6:PM     6.529032  -51.860 64.9182 1.00000\n10:PM-6:PM    7.380645  -51.009 65.7698 1.00000\n11:PM-6:PM   -4.825806  -63.215 53.5634 1.00000\n12:PM-6:PM   -7.380645  -65.770 51.0086 1.00000\n13:PM-6:PM   11.851613  -46.538 70.2408 1.00000\n14:PM-6:PM   -3.477419  -61.867 54.9118 1.00000\n15:PM-6:PM    3.619355  -54.770 62.0086 1.00000\n16:PM-6:PM   -9.154839  -67.544 49.2344 1.00000\n17:PM-6:PM  -16.748387  -75.138 41.6408 1.00000\n18:PM-6:PM   -3.193548  -61.583 55.1956 1.00000\n19:PM-6:PM  -15.258065  -73.647 43.1311 1.00000\n20:PM-6:PM   10.432258  -47.957 68.8215 1.00000\n8:PM-7:PM    -6.529032  -64.918 51.8602 1.00000\n9:PM-7:PM     0.425806  -57.963 58.8150 1.00000\n10:PM-7:PM    1.277419  -57.112 59.6666 1.00000\n11:PM-7:PM  -10.929032  -69.318 47.4602 1.00000\n12:PM-7:PM  -13.483871  -71.873 44.9053 1.00000\n13:PM-7:PM    5.748387  -52.641 64.1376 1.00000\n14:PM-7:PM   -9.580645  -67.970 48.8086 1.00000\n15:PM-7:PM   -2.483871  -60.873 55.9053 1.00000\n16:PM-7:PM  -15.258065  -73.647 43.1311 1.00000\n17:PM-7:PM  -22.851613  -81.241 35.5376 0.99989\n18:PM-7:PM   -9.296774  -67.686 49.0924 1.00000\n19:PM-7:PM  -21.361290  -79.750 37.0279 0.99998\n20:PM-7:PM    4.329032  -54.060 62.7182 1.00000\n9:PM-8:PM     6.954839  -51.434 65.3440 1.00000\n10:PM-8:PM    7.806452  -50.583 66.1956 1.00000\n11:PM-8:PM   -4.400000  -62.789 53.9892 1.00000\n12:PM-8:PM   -6.954839  -65.344 51.4344 1.00000\n13:PM-8:PM   12.277419  -46.112 70.6666 1.00000\n14:PM-8:PM   -3.051613  -61.441 55.3376 1.00000\n15:PM-8:PM    4.045161  -54.344 62.4344 1.00000\n16:PM-8:PM   -8.729032  -67.118 49.6602 1.00000\n17:PM-8:PM  -16.322581  -74.712 42.0666 1.00000\n18:PM-8:PM   -2.767742  -61.157 55.6215 1.00000\n19:PM-8:PM  -14.832258  -73.221 43.5569 1.00000\n20:PM-8:PM   10.858065  -47.531 69.2473 1.00000\n10:PM-9:PM    0.851613  -57.538 59.2408 1.00000\n11:PM-9:PM  -11.354839  -69.744 47.0344 1.00000\n12:PM-9:PM  -13.909677  -72.299 44.4795 1.00000\n13:PM-9:PM    5.322581  -53.067 63.7118 1.00000\n14:PM-9:PM  -10.006452  -68.396 48.3827 1.00000\n15:PM-9:PM   -2.909677  -61.299 55.4795 1.00000\n16:PM-9:PM  -15.683871  -74.073 42.7053 1.00000\n17:PM-9:PM  -23.277419  -81.667 35.1118 0.99984\n18:PM-9:PM   -9.722581  -68.112 48.6666 1.00000\n19:PM-9:PM  -21.787097  -80.176 36.6021 0.99996\n20:PM-9:PM    3.903226  -54.486 62.2924 1.00000\n11:PM-10:PM -12.206452  -70.596 46.1827 1.00000\n12:PM-10:PM -14.761290  -73.150 43.6279 1.00000\n13:PM-10:PM   4.470968  -53.918 62.8602 1.00000\n14:PM-10:PM -10.858065  -69.247 47.5311 1.00000\n15:PM-10:PM  -3.761290  -62.150 54.6279 1.00000\n16:PM-10:PM -16.535484  -74.925 41.8537 1.00000\n17:PM-10:PM -24.129032  -82.518 34.2602 0.99966\n18:PM-10:PM -10.574194  -68.963 47.8150 1.00000\n19:PM-10:PM -22.638710  -81.028 35.7505 0.99991\n20:PM-10:PM   3.051613  -55.338 61.4408 1.00000\n12:PM-11:PM  -2.554839  -60.944 55.8344 1.00000\n13:PM-11:PM  16.677419  -41.712 75.0666 1.00000\n14:PM-11:PM   1.348387  -57.041 59.7376 1.00000\n15:PM-11:PM   8.445161  -49.944 66.8344 1.00000\n16:PM-11:PM  -4.329032  -62.718 54.0602 1.00000\n17:PM-11:PM -11.922581  -70.312 46.4666 1.00000\n18:PM-11:PM   1.632258  -56.757 60.0215 1.00000\n19:PM-11:PM -10.432258  -68.821 47.9569 1.00000\n20:PM-11:PM  15.258065  -43.131 73.6473 1.00000\n13:PM-12:PM  19.232258  -39.157 77.6215 1.00000\n14:PM-12:PM   3.903226  -54.486 62.2924 1.00000\n15:PM-12:PM  11.000000  -47.389 69.3892 1.00000\n16:PM-12:PM  -1.774194  -60.163 56.6150 1.00000\n17:PM-12:PM  -9.367742  -67.757 49.0215 1.00000\n18:PM-12:PM   4.187097  -54.202 62.5763 1.00000\n19:PM-12:PM  -7.877419  -66.267 50.5118 1.00000\n20:PM-12:PM  17.812903  -40.576 76.2021 1.00000\n14:PM-13:PM -15.329032  -73.718 43.0602 1.00000\n15:PM-13:PM  -8.232258  -66.621 50.1569 1.00000\n16:PM-13:PM -21.006452  -79.396 37.3827 0.99998\n17:PM-13:PM -28.600000  -86.989 29.7892 0.99284\n18:PM-13:PM -15.045161  -73.434 43.3440 1.00000\n19:PM-13:PM -27.109677  -85.499 31.2795 0.99702\n20:PM-13:PM  -1.419355  -59.809 56.9698 1.00000\n15:PM-14:PM   7.096774  -51.292 65.4860 1.00000\n16:PM-14:PM  -5.677419  -64.067 52.7118 1.00000\n17:PM-14:PM -13.270968  -71.660 45.1182 1.00000\n18:PM-14:PM   0.283871  -58.105 58.6731 1.00000\n19:PM-14:PM -11.780645  -70.170 46.6086 1.00000\n20:PM-14:PM  13.909677  -44.480 72.2989 1.00000\n16:PM-15:PM -12.774194  -71.163 45.6150 1.00000\n17:PM-15:PM -20.367742  -78.757 38.0215 0.99999\n18:PM-15:PM  -6.812903  -65.202 51.5763 1.00000\n19:PM-15:PM -18.877419  -77.267 39.5118 1.00000\n20:PM-15:PM   6.812903  -51.576 65.2021 1.00000\n17:PM-16:PM  -7.593548  -65.983 50.7956 1.00000\n18:PM-16:PM   5.961290  -52.428 64.3505 1.00000\n19:PM-16:PM  -6.103226  -64.492 52.2860 1.00000\n20:PM-16:PM  19.587097  -38.802 77.9763 1.00000\n18:PM-17:PM  13.554839  -44.834 71.9440 1.00000\n19:PM-17:PM   1.490323  -56.899 59.8795 1.00000\n20:PM-17:PM  27.180645  -31.209 85.5698 0.99689\n19:PM-18:PM -12.064516  -70.454 46.3247 1.00000\n20:PM-18:PM  13.625806  -44.763 72.0150 1.00000\n20:PM-19:PM  25.690323  -32.699 84.0795 0.99886\n\n\n  Tukey multiple comparisons of means\n    99% family-wise confidence level\n\nFit: aov(formula = response ~ Day + noon + Day:noon, data = data)\n\n$Day\n            diff     lwr    upr   p adj\n2-1    -6.138710 -48.905 36.628 1.00000\n3-1    -4.080645 -46.847 38.686 1.00000\n4-1   -14.300000 -57.066 28.466 0.99700\n5-1    -3.335484 -46.102 39.431 1.00000\n6-1    -6.812903 -49.579 35.953 1.00000\n7-1    -6.422581 -49.189 36.344 1.00000\n8-1    -4.364516 -47.131 38.402 1.00000\n9-1     8.551613 -34.215 51.318 1.00000\n10-1  -12.703226 -55.470 30.063 0.99934\n11-1    1.845161 -40.921 44.611 1.00000\n12-1  -20.403226 -63.170 22.363 0.89330\n13-1   -3.477419 -46.244 39.289 1.00000\n14-1  -18.522581 -61.289 24.244 0.95243\n15-1  -18.664516 -61.431 24.102 0.94905\n16-1  -15.825806 -58.592 26.941 0.99024\n17-1  -17.280645 -60.047 25.486 0.97539\n18-1  -12.064516 -54.831 30.702 0.99968\n19-1  -14.370968 -57.137 28.395 0.99682\n20-1   -2.994839 -47.849 41.859 1.00000\n3-2     2.058065 -40.708 44.824 1.00000\n4-2    -8.161290 -50.928 34.605 1.00000\n5-2     2.803226 -39.963 45.570 1.00000\n6-2    -0.674194 -43.441 42.092 1.00000\n7-2    -0.283871 -43.050 42.482 1.00000\n8-2     1.774194 -40.992 44.541 1.00000\n9-2    14.690323 -28.076 57.457 0.99585\n10-2   -6.564516 -49.331 36.202 1.00000\n11-2    7.983871 -34.782 50.750 1.00000\n12-2  -14.264516 -57.031 28.502 0.99709\n13-2    2.661290 -40.105 45.428 1.00000\n14-2  -12.383871 -55.150 30.382 0.99953\n15-2  -12.525806 -55.292 30.241 0.99946\n16-2   -9.687097 -52.453 33.079 0.99999\n17-2  -11.141935 -53.908 31.624 0.99990\n18-2   -5.925806 -48.692 36.841 1.00000\n19-2   -8.232258 -50.999 34.534 1.00000\n20-2    3.143871 -41.710 47.998 1.00000\n4-3   -10.219355 -52.986 32.547 0.99997\n5-3     0.745161 -42.021 43.511 1.00000\n6-3    -2.732258 -45.499 40.034 1.00000\n7-3    -2.341935 -45.108 40.424 1.00000\n8-3    -0.283871 -43.050 42.482 1.00000\n9-3    12.632258 -30.134 55.399 0.99939\n10-3   -8.622581 -51.389 34.144 1.00000\n11-3    5.925806 -36.841 48.692 1.00000\n12-3  -16.322581 -59.089 26.444 0.98634\n13-3    0.603226 -42.163 43.370 1.00000\n14-3  -14.441935 -57.208 28.324 0.99662\n15-3  -14.583871 -57.350 28.182 0.99620\n16-3  -11.745161 -54.511 31.021 0.99978\n17-3  -13.200000 -55.966 29.566 0.99891\n18-3   -7.983871 -50.750 34.782 1.00000\n19-3  -10.290323 -53.057 32.476 0.99997\n20-3    1.085806 -43.768 45.939 1.00000\n5-4    10.964516 -31.802 53.731 0.99992\n6-4     7.487097 -35.279 50.253 1.00000\n7-4     7.877419 -34.889 50.644 1.00000\n8-4     9.935484 -32.831 52.702 0.99998\n9-4    22.851613 -19.915 65.618 0.76824\n10-4    1.596774 -41.170 44.363 1.00000\n11-4   16.145161 -26.621 58.911 0.98785\n12-4   -6.103226 -48.870 36.663 1.00000\n13-4   10.822581 -31.944 53.589 0.99993\n14-4   -4.222581 -46.989 38.544 1.00000\n15-4   -4.364516 -47.131 38.402 1.00000\n16-4   -1.525806 -44.292 41.241 1.00000\n17-4   -2.980645 -45.747 39.786 1.00000\n18-4    2.235484 -40.531 45.002 1.00000\n19-4   -0.070968 -42.837 42.695 1.00000\n20-4   11.305161 -33.549 56.159 0.99994\n6-5    -3.477419 -46.244 39.289 1.00000\n7-5    -3.087097 -45.853 39.679 1.00000\n8-5    -1.029032 -43.795 41.737 1.00000\n9-5    11.887097 -30.879 54.653 0.99974\n10-5   -9.367742 -52.134 33.399 0.99999\n11-5    5.180645 -37.586 47.947 1.00000\n12-5  -17.067742 -59.834 25.699 0.97827\n13-5   -0.141935 -42.908 42.624 1.00000\n14-5  -15.187097 -57.953 27.579 0.99387\n15-5  -15.329032 -58.095 27.437 0.99318\n16-5  -12.490323 -55.257 30.276 0.99948\n17-5  -13.945161 -56.711 28.821 0.99780\n18-5   -8.729032 -51.495 34.037 1.00000\n19-5  -11.035484 -53.802 31.731 0.99991\n20-5    0.340645 -44.513 45.194 1.00000\n7-6     0.390323 -42.376 43.157 1.00000\n8-6     2.448387 -40.318 45.215 1.00000\n9-6    15.364516 -27.402 58.131 0.99300\n10-6   -5.890323 -48.657 36.876 1.00000\n11-6    8.658065 -34.108 51.424 1.00000\n12-6  -13.590323 -56.357 29.176 0.99841\n13-6    3.335484 -39.431 46.102 1.00000\n14-6  -11.709677 -54.476 31.057 0.99979\n15-6  -11.851613 -54.618 30.915 0.99975\n16-6   -9.012903 -51.779 33.753 1.00000\n17-6  -10.467742 -53.234 32.299 0.99996\n18-6   -5.251613 -48.018 37.515 1.00000\n19-6   -7.558065 -50.324 35.208 1.00000\n20-6    3.818065 -41.036 48.672 1.00000\n8-7     2.058065 -40.708 44.824 1.00000\n9-7    14.974194 -27.792 57.741 0.99480\n10-7   -6.280645 -49.047 36.486 1.00000\n11-7    8.267742 -34.499 51.034 1.00000\n12-7  -13.980645 -56.747 28.786 0.99773\n13-7    2.945161 -39.821 45.711 1.00000\n14-7  -12.100000 -54.866 30.666 0.99966\n15-7  -12.241935 -55.008 30.524 0.99960\n16-7   -9.403226 -52.170 33.363 0.99999\n17-7  -10.858065 -53.624 31.908 0.99993\n18-7   -5.641935 -48.408 37.124 1.00000\n19-7   -7.948387 -50.715 34.818 1.00000\n20-7    3.427742 -41.426 48.281 1.00000\n9-8    12.916129 -29.850 55.682 0.99918\n10-8   -8.338710 -51.105 34.428 1.00000\n11-8    6.209677 -36.557 48.976 1.00000\n12-8  -16.038710 -58.805 26.728 0.98869\n13-8    0.887097 -41.879 43.653 1.00000\n14-8  -14.158065 -56.924 28.608 0.99735\n15-8  -14.300000 -57.066 28.466 0.99700\n16-8  -11.461290 -54.228 31.305 0.99984\n17-8  -12.916129 -55.682 29.850 0.99918\n18-8   -7.700000 -50.466 35.066 1.00000\n19-8  -10.006452 -52.773 32.760 0.99998\n20-8    1.369677 -43.484 46.223 1.00000\n10-9  -21.254839 -64.021 21.511 0.85576\n11-9   -6.706452 -49.473 36.060 1.00000\n12-9  -28.954839 -71.721 13.811 0.35269\n13-9  -12.029032 -54.795 30.737 0.99969\n14-9  -27.074194 -69.841 15.692 0.47719\n15-9  -27.216129 -69.982 15.550 0.46731\n16-9  -24.377419 -67.144 18.389 0.66832\n17-9  -25.832258 -68.599 16.934 0.56535\n18-9  -20.616129 -63.382 22.150 0.88455\n19-9  -22.922581 -65.689 19.844 0.76389\n20-9  -11.546452 -56.400 33.307 0.99991\n11-10  14.548387 -28.218 57.315 0.99631\n12-10  -7.700000 -50.466 35.066 1.00000\n13-10   9.225806 -33.541 51.992 0.99999\n14-10  -5.819355 -48.586 36.947 1.00000\n15-10  -5.961290 -48.728 36.805 1.00000\n16-10  -3.122581 -45.889 39.644 1.00000\n17-10  -4.577419 -47.344 38.189 1.00000\n18-10   0.638710 -42.128 43.405 1.00000\n19-10  -1.667742 -44.434 41.099 1.00000\n20-10   9.708387 -35.145 54.562 0.99999\n12-11 -22.248387 -65.015 20.518 0.80373\n13-11  -5.322581 -48.089 37.444 1.00000\n14-11 -20.367742 -63.134 22.399 0.89472\n15-11 -20.509677 -63.276 22.257 0.88898\n16-11 -17.670968 -60.437 25.095 0.96936\n17-11 -19.125806 -61.892 23.641 0.93692\n18-11 -13.909677 -56.676 28.857 0.99787\n19-11 -16.216129 -58.982 26.550 0.98726\n20-11  -4.840000 -49.694 40.014 1.00000\n13-12  16.925806 -25.841 59.692 0.98004\n14-12   1.880645 -40.886 44.647 1.00000\n15-12   1.738710 -41.028 44.505 1.00000\n16-12   4.577419 -38.189 47.344 1.00000\n17-12   3.122581 -39.644 45.889 1.00000\n18-12   8.338710 -34.428 51.105 1.00000\n19-12   6.032258 -36.734 48.799 1.00000\n20-12  17.408387 -27.445 62.262 0.98369\n14-13 -15.045161 -57.811 27.721 0.99450\n15-13 -15.187097 -57.953 27.579 0.99387\n16-13 -12.348387 -55.115 30.418 0.99955\n17-13 -13.803226 -56.570 28.963 0.99807\n18-13  -8.587097 -51.353 34.179 1.00000\n19-13 -10.893548 -53.660 31.873 0.99992\n20-13   0.482581 -44.371 45.336 1.00000\n15-14  -0.141935 -42.908 42.624 1.00000\n16-14   2.696774 -40.070 45.463 1.00000\n17-14   1.241935 -41.524 44.008 1.00000\n18-14   6.458065 -36.308 49.224 1.00000\n19-14   4.151613 -38.615 46.918 1.00000\n20-14  15.527742 -29.326 60.381 0.99545\n16-15   2.838710 -39.928 45.605 1.00000\n17-15   1.383871 -41.382 44.150 1.00000\n18-15   6.600000 -36.166 49.366 1.00000\n19-15   4.293548 -38.473 47.060 1.00000\n20-15  15.669677 -29.184 60.523 0.99493\n17-16  -1.454839 -44.221 41.311 1.00000\n18-16   3.761290 -39.005 46.528 1.00000\n19-16   1.454839 -41.311 44.221 1.00000\n20-16  12.830968 -32.023 57.685 0.99961\n18-17   5.216129 -37.550 47.982 1.00000\n19-17   2.909677 -39.857 45.676 1.00000\n20-17  14.285806 -30.568 59.139 0.99837\n19-18  -2.306452 -45.073 40.460 1.00000\n20-18   9.069677 -35.784 53.923 1.00000\n20-19  11.376129 -33.478 56.230 0.99993\n\n\n유의한 패턴 없음 Tukey 검정 결과 (보통 유의할때 함) 테이블 도 유의한 수치가 없음"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-16_normality/index.html",
    "href": "docs/blog/posts/statistics/2023-01-16_normality/index.html",
    "title": "Normality Check",
    "section": "",
    "text": "Wiki\n논문\n원래는 표본의 크기가 50이하인 작은 데이터 셋의 정규성 검정을 위해 고안됨. R 에서는 3~5천개 사이의 표본까지 다룰 수 있도록 조정됨\n\n정규 분포 전용 검정: 모든 검정 대비 최고의 검정력을 보임 (Power), 이상치가 있으면 p value가 너무 작아짐\n\\(H_0\\): 데이터가 정규분포를 따른다\n\\(H_a\\): 데이터가 정규분포를 따르지 않는다.\n검정 통계량 \\[\n   \\mathbf W=\\frac{(\\sum_{i=1}^{n}a_ix_{(i)})^2}{\\sum_{i=1}^{n}(x_i-\\overline{x})^2}\n   \\]\n\n\\(a_i\\) : 미리 정해진 숫자들, \\(x\\)의 개수에 의해 정해짐\n\\(x_{(i)}\\) 들은 순위 표본, 즉 i 번째로 큰 표본\n분자는 순서 통계량으로 계산한 정규분포의 분산, 분모는 데이터의 표본 분산 (표본 Sum of Squares)\n이미 이론적으로 세팅된 값과 표본 분산의 비율을 보는 것\n귀무 가설이 참이면 이론적으로 1 이 나와야 함\n\\(\\mathbf W \\in (0,1)\\), 상관계수의 제곱을 측정한 계량 값이라고 생각해도 된다.\n\n\\(\\mathbf W\\) 값이 1에서 멀어질 수록 정규분포와는 다르게 분포되어 있음을 의미\n단점: 너무 민감함, 조그만 달라도 p value가 너무 작게 나와 귀무가설이 기각됨\n해결책 : 시각화 기법과 같이 사용해서 보여준다\n\nqqplot과 density 같이 사용"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html",
    "href": "docs/blog/posts/statistics/guide_map/index.html",
    "title": "Content List, Statistics",
    "section": "",
    "text": "(Draft)"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#basic-statistics",
    "href": "docs/blog/posts/statistics/guide_map/index.html#basic-statistics",
    "title": "Blog Guide Map, Statistics",
    "section": "Basic Statistics",
    "text": "Basic Statistics\n\n1111-11-11, Hypothesis Testing\n2022-12-28, p-values\n1111-11-11, Permutation Test\n1111-11-11, Power\n1111-11-11, Multiple Testing\n1111-11-11, Sample Size Calculation\n1111-11-11, A/B Testing\n2023-01-07, ANOVA\n\n1111-11-11, ANCOVA\n1111-11-11, repeated measures ANOVA\n1111-11-11, MANOVA\n1111-11-11, MANCOVA"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_python_list/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-17_python_list/index.html",
    "title": "Data Structure Python List",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\npython_list"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_array/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-17_array/index.html",
    "title": "Data Structure, Array",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n가장 기본적인 자료구조다.\n여러 개의 변수를 담는 공간으로 이해할 수 있다.\n배열은 인덱스(index)가 존재하며, 인덱스는 0부터 시작한다.\n특정한 인덱스에 직접적으로 접근 가능 → 수행 시간: \\(O (1)\\)\n\n\n\n\n\n컴퓨터의 메인 메모리에서 배열의 공간은 연속적으로 할당된다.\n장점: 캐시(cache) 히트 가능성이 높으며, 조회가 빠르다.\n단점: 배열의 크기를 미리 지정해야 하는 것이 일반적이므로, 데이터의 추가 및 삭제에 한계가 있다.\n\n\n\n\n\n컴퓨터의 메인 메모리상에서 주소가 연속적이지 않다.\n배열과 다르게 크기가 정해져 있지 않고, 리스트의 크기는 동적으로 변경 가능하다.\n\n장점: 포인터(pointer)를 통해 다음 데이터의 위치를 가리킨다는 점에서 삽입과 삭제가 간편하다.\n단점: 원소를 검색할 때는 앞에서부터 원소를 찾아야 하므로, 데이터 검색 속도가 느리다.\n\n\n\n파이썬의 리스트(List) 자료형\n\n파이썬에서는 리스트 자료형을 제공한다.\n\n[알아 둘 점]\n\n컴퓨터 공학에서의 연결 리스트와 다른 의미를 가진다.\n일반적인 프로그래밍 언어에서의 배열로 이해할 수 있다.\n파이썬의 리스트는 배열처럼 임의의 인덱스를 이용해 직접적인 접근이 가능하다.\n파이썬의 리스트 자료형은 동적 배열이다.\n배열의 용량이 가득 차면, 자동으로 크기를 증가시킨다.\n내부적으로 포인터(pointer)를 사용하여, 연결 리스트의 장점도 가지고 있다.\n배열(array) 혹은 스택(stack)의 기능이 필요할 때 리스트 자료형을 그대로 사용할 수 있다.\n큐(queue)의 기능을 제공하지 못한다. (비효율적)\n\n\n\n\n\n파이썬에서는 임의의 크기를 가지는 배열을 만들 수 있다.\n일반적으로 리스트 컴프리헨션(list comprehension)을 사용한다.\n크기가 N인 1차원 배열을 만드는 방법은 다음과 같다.\n\n\n# [0, 0, 0, 0, 0]\nn = 5\narr = [0] * n\nprint(arr)\n\n# [0, 1, 2, 3, 4]\nn = 5\narr = [i for i in range(n)]\nprint(arr)\n\n[0, 0, 0, 0, 0]\n[0, 1, 2, 3, 4]\n\n\n\n크기가 N X M인 2차원 리스트(배열) 만들기 1\n\n2차원 배열이 필요할 때는 다음과 같이 초기화한다.\n\n\n\nn = 3\nm = 5\narr = [[0] * m for i in range(n)]\nprint(arr)\n\n[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\n\n\n\n크기가 N X M인 2차원 리스트(배열) 만들기 2\n\n\nn = 3\nm = 5\narr = [[i * m + j for j in range(m)] for i in range(n)]\nprint(arr)\n\n[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]\n\n\n\n\n\n\n리스트는 기본적으로 메모리 주소를 반환한다.\n따라서 단순히 0 ∗ m ∗ n 형태로 배열을 초기화하면 안 된다.\n\n→ 이렇게 초기화를 하게 되면, n개의 0 ∗ m 리스트는 모두 같은 객체로 인식된다.\n→ 다시 말해, 같은 메모리를(동일한 리스트를) 가리키는 n개의 원소를 담는 리스트가 된다.\n\n2차원 배열을 초기화할 때는 리스트 컴프리헨션을 이용하는 것이 일반적이다.\n\n\nn = 3\nm = 5\narr1 = [[0] * m] * n\narr2 = [[0] * m for i in range(n)]\n\narr1[1][3] = 7\narr2[1][3] = 7\n\nprint(arr1)\nprint(arr2)\n\n[[0, 0, 0, 7, 0], [0, 0, 0, 7, 0], [0, 0, 0, 7, 0]]\n[[0, 0, 0, 0, 0], [0, 0, 0, 7, 0], [0, 0, 0, 0, 0]]\n\n\n\n\n\n\n자신이 원하는 임의의 값을 넣어 곧바로 사용할 수 있다.\n\n\narr = [0, 1, 2, 3, 4, 5, 6, 7, 8]\nprint(arr)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_linked_list/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-17_linked_list/index.html",
    "title": "Data Structure Linked List",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n연결 리스트는 각 노드가 한 줄로 연결되어 있는 자료 구조다.\n각 노드는 (데이터, 포인터) 형태를 가진다.\n포인터: 다음 노드의 메모리 주소를 가리키는 목적으로 사용된다.\n연결 리스트는 각 노드가 한 줄로 연결되어 있는 자료 구조다.\n각 노드는 (데이터, 포인터) 형태를 가진다.\n연결성: 각 노드의 포인터는 다음 혹은 이전 노드를 가리킨다.\n\n연결 리스트를 이용하면 다양한 자료구조를 구현할 수 있다.\n예시) 스택, 큐 등\nPython은 연결 리스트를 활용하는 자료구조를 제공한다.\n그래서 연결 리스트를 실제 구현해야 하는 경우는 적지만, 그 원리에 대해서 이해해 보자.\n\n\n\n\n\n연결 리스트와 배열(array)을 비교하여 장단점을 이해할 필요가 있다.\n특정 위치의 데이터를 삭제할 때, 일반적인 배열에서는 \\(O(N)\\) 만큼의 시간이 소요된다.\n하지만, 연결 리스트를 이용하면 단순히 연결만 끊어주면 된다.\n따라서 삭제할 위치를 정확히 알고 있는 경우 \\(O(1)\\) 의 시간이 소요된다.\n\n\n\n\n\n배열에 새로운 원소를 삽입할 때, 최악의 경우 시간 복잡도를 계산하여라.\n예시) 아래 배열에서 인덱스 3에 원소 “59”를 삽입할 경우\n배열에 새로운 원소를 삽입할 때, 최악의 경우 시간 복잡도를 계산하여라.\n예시) 아래 배열에서 인덱스 3에 원소 “59”를 삽입할 경우\n배열에 새로운 원소를 삽입할 때, 최악의 경우 시간 복잡도를 계산하여라.\n예시) 아래 배열에서 인덱스 3에 원소 “59”를 삽입할 경우\n\n\n\n\n\n배열에 존재하는 원소를 삭제할 때, 최악의 경우 시간 복잡도를 계산하여라.\n예시) 아래 배열에서 인덱스 3에 해당하는 원소를 삭제하는 경우\n배열에 존재하는 원소를 삭제할 때, 최악의 경우 시간 복잡도를 계산하여라.\n예시) 아래 배열에서 인덱스 3에 해당하는 원소를 삭제하는 경우\n배열에 존재하는 원소를 삭제할 때, 최악의 경우 시간 복잡도를 계산하여라.\n예시) 아래 배열에서 인덱스 3에 해당하는 원소를 삭제하는 경우\n배열에 존재하는 원소를 삭제할 때, 최악의 경우 시간 복잡도를 계산하여라.\n예시) 아래 배열에서 인덱스 3에 해당하는 원소를 삭제하는 경우\n따라서, 최악의 경우 시간 복잡도는 \\(O(N)\\) 이다.\n\n\n\n\n\n삽입할 위치를 알고 있다면, 물리적인 위치를 한 칸씩 옮기지 않아도 삽입할 수 있다.\n인덱스 2의 위치에 원소를 삽입\n\n\n\n\n\n뒤에 붙일 때는 마지막 노드의 다음 위치에 원소를 넣으면 된다.\n\n마지막 위치에 새로운 원소를 추가"
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html",
    "href": "docs/projects/LLFS/project_description.html",
    "title": "Description",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n알츠하이머병(Alzheimer’s Disease, AD)은 수백만 명의 미국인에게 영향을 미치는 가장 흔한 형태의 치매이다. 알츠하이머병은 기억력, 사고력 및 행동에 영향을 주지만 증상이 나타나기까지 거의 20년에 걸쳐 진행이 된다. 따라서 전 임상 단계에서 생리학을 이해하는 것이 필수적이다. 유전적 요인이 AD에 거의 50% 기여하는 것으로 추정된다. 유전자가 세포 환경을 변경하여 알츠하이머병 위험에 어떻게 기여하는지 더 잘 이해하기 위해 AD와 연관이 있는 유전자인 APOE를 보유한 사람들의 대사체(Metabolome)를 조사했다. 대사체는 유전체(Genome)과 단백질체(Proteome)에서 생성된 산물을 의미한다. 이러한 생화학 부산물은 유전적 요인과 환경적 요인 모두의 영향을 받는다. 모집단은 장수마을에 사는 Caucasian (백인) 참여자들이다.\n\n\n\nLLFS(Long Life Family Study) 프로젝트의 목적은 유전체, 전사체, 단백질체 및 대사체 단계를 통해 유전체에서 대사체 단계에 이르는 여러 단계에서 통계 및 기계 학습을 사용하여 분석 파이프라인을 구축하고 알츠하이머병에 대한 중요한 바이오마커를 식별하는 것이다.\n\n\n\n읽기의 편의성을 위해 LLFS project에 대한 설명을 project와 self project 와 같이 2개의 section으로 나누었다. project는 내가 실제로 프로젝트를 수행했던 과정을 기술했고 self project는 그 방법론을 대략적으로 간소화된 형태로 기술했다.\n\nProject Description (Current)\nSelf Project\n\nSelf Project Description\nData Preparation\nEDA (Exploratory Data Analysis)\nMethod1: statistical approach\nMethod2: ML Approach\nConclusion\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    subgraph Data_Collection\n        direction TB\n        Multi_Centerd_Blood_Sampling---\n        Mass_Spectrometry---\n        Data_Transfer\n    end\n    subgraph Quality_Control\n        direction TB\n        Identify_Anomaly_Data---\n        Identify_Missing_Values\n    end\n    subgraph Analytics\n        direction TB \n        EDA---\n        Data_Mining---\n        Statistical_Analysis---\n        Machine_Learning\n    end\n    subgraph Reporting_and_Conclusion\n        direction TB \n        Share_with_Faculty\n    end\n\nData_Collection-->Quality_Control-->Analytics-->Reporting_and_Conclusion\n\n\n\n\n\n\n\n\nData는 장수 마을에 거주하는 백인을 대상으로 New York, Bonston, Pittsburgh 및 Denmark에 있는 여러 medical centers에서 sampled blood를 MS Spectromtetry로 Digitalization을 했다. 여러 과정을 통해 data를 csv형태로 받아 data의 QC(Quality Control)를 진행한뒤 Data 분석 업무를 수행했다. EDA (Exploratory Data Analysis) 와 Data Mining을 통해 data에 대한 이해도를 높였고 이를 토대로 통계 분석과 machine learning을 이용하여 이 data에 적합한 모형을 찾았다. 모든 결과물은 The Taub Institute for Research on Alzheimer’s Disease and the Aging Brain 의 biostaticians, medical doctors, biologists, neurologists, bioinformaticians 및 epidemiologists와 공유를 했다.\n\n\n\n\n\n\n\nflowchart LR\n    subgraph Data_Quality_Control\n        direction TB\n        identify_anomaly_data---\n        identify_missing_values\n        subgraph Missing_Value_Analysis\n            direction LR\n            MCAR---\n            MAR---\n            MNAR\n        end\n        either_imputation_or_omission---\n        communication_with_labs---\n        set_data_inclusion/exclusion_criteria\n    end\n    identify_missing_values---Missing_Value_Analysis---either_imputation_or_omission\n    subgraph Data_Preprocessing\n        direction TB\n        data_transformation---\n        log_transformation---\n        standardization\n    end\n\nData_Quality_Control-->Data_Preprocessing\n\n\n\n\n\n\n\n\n\nData의 품질 관리를 위해 data를 생성한 biochemists와 소통하여 실험실 기준에 따라 결측치와 이상치를 구분하여 labeling을 수행했고 missing value analysis를 통해 결과에 따라 medical doctors를 포함한 다른 faculty members와 상의하여 결측치 처리를 했다. data QC criteria는 rowwise 와 columwise sum의 합이 sample size에 대하여 missing values의 비율이 5%가 넘는 환자와 변수는 분석 대상에서 제외 됐다. 모든 metabolites data는 log transoformation 과 standardization을 통해 data의 단위를 표준화 했다.\n\n\n\n\n\n\n\nflowchart TB\n    subgraph Data_Analytics\n        direction TB\n        Exploratory_Data_Analysis---\n        Data_Minig---\n        Statistical_Analysis---\n        Machine_Learning\n    end\n\n\n\n\n\n\n\n\nData 분석은 크게 EDA (Exploratory Data Analysis), Statistical Analysis 및 Machine Learning과 같이 3 단계로 수행했다. 각 단계에서 나온 결과가 각 각의 단계에서 일관되게 나오는 metabolites를 선별했다.\n\n\nstudent t tests, Wilcoxon Man Whiteney tests, \\(\\chi^2\\) tests, Fisher Exact Tests, ANOVAs, Kruskal Wallis Tests 및 regression analysis이 수행됐고 visualization을 통해 검정 결과를 재확인하는 작업을 수행했다. 고차원 데이터를 시각화하여 data의 pattern을 관찰하기 위해 KNN, PCA, K means clustering 및 DB Scan을 이용했다.\n\n\n\nmultivariable linear regression, logistic regression 및 Cox PH(Proportional Hazards) regression anayses 가 수행됐고 질병과 유의한 metabolites를 선별했다. multiple testing으로 인한 1 종 오류를 범하는 것을 줄이기 위해 permuted p-values를 계산하여 유의성을 한번 더 확인했다.\n\n\n\nLasso, ridge regression, elastic net, decision tree, random rorests, ada boosting, gradient descent boosting, SVM (support vector machine), partial least square 및 sparse partial least square가 사용됐다. 질병을 가장 잘 예측하는 classifier를 평가하여 최적의 classifier를 선택했다.\n\n\n\n\n146개의 관측치와 약 3,000여개의 변수로 구성된 data에서 약 60개 내외의 대사물질이 질병과 5% 유의수준으로 유의한 관계가 있는 것으로 관찰됐고 partial least suare 가 가장 성능이 좋은 것으로 관찰됐다.\n\n\n\n\n\n\nAlzheimer Disease (AD) is the most common form of dementia that affects millions of Americans. AD affects memory, thinking and behavior, but its progression is slow, spanning nearly two decades before the symptoms appear. Thus, it is imperative to understand the physiology at the pre-clinical stage. It is estimated that genetic factors contribute nearly 50% to AD. To better understand how genes contribute to the risk of AD by altering cellular milieu, I have examined the metabolome of individuals with the AD-related genotype, APOE. The metabolome represents the products that were generated from the genome and proteome. These biochemical products represent influences of both genetic and environmental factors. The population is Caucasian participants living in longevity village.\n\n\n\nThe objective of the Long Life Family Study (LLFS) project was to build an analysis pipeline of identifying significant biomarkers for AD using statistics and machine learning at the multi-stages from the genomic to the metabolomic stage through the transcriptomic and proteomic stage.\n\n\n\nFor the convenience of reading, the LLFS project is divided into the two sections: project and self-project. The project section roughly described the process of the project I actually carried out, and the self project one described the methodology in a roughly simplified form.\n\nProject Description (Current)\nSelf Project\n\nSelf Project Description\nData Preparation\nEDA (Exploratory Data Analysis)\nMethod1: statistical approach\nMethod2: ML Approach\nConclusion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster0\n\n Data Collection  \n\ncluster1\n\n Quality Control  \n\ncluster2\n\n Analytics  \n\ncluster3\n\n Reporting and Conclusion   \n\nMulti_Centered_Blood_Sampling\n\n Multi_Centered_Blood_Sampling   \n\nMass_Spectrometry\n\n Mass_Spectrometry   \n\nData_Transfer\n\n Data_Transfer   \n\nIdentify_Anomaly_Data\n\n Identify_Anomaly_Data   \n\nData_Transfer->Identify_Anomaly_Data\n\n    \n\nIdentify_Missing_Values\n\n Identify_Missing_Values   \n\nEDA\n\n EDA   \n\nIdentify_Missing_Values->EDA\n\n    \n\nData_Mining\n\n Data_Mining   \n\nStatistical_Analysis\n\n Statistical_Analysis   \n\nMachine_Learning\n\n Machine_Learning   \n\nShare_with_Faculty\n\n Share_with_Faculty   \n\nMachine_Learning->Share_with_Faculty\n\n   \n\n\n\n\n\nData were obtained by digitization through MS Spectromtetry of blood samples from multiple medical centers in New York, Bonston, Pittsburgh, and Denmark for Caucasians residing in longevity villages. After receiving the data in a csv format through various processes, QC (Quality Control) of the data and data analysis were performed. To better understand data, exploratory data analysis (EDA) and data mining were conducted. Based on the analysis findings on data, the machine learning model to explain the data most was selcted. All findings were shared with biostatisticians, medical doctors, biologists, neurologists and epidemiologists at the neurology department and the Taub Institute for Research on Alzheimer’s Disease and the Aging Brain in the Columbia University Irving Medical Center.\n\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster2\n\n Data Preprocessing  \n\ncluster0\n\n Data Quality Control  \n\ncluster1\n\n Missing Value Analysis   \n\nidentify_anomaly_data\n\n identify_anomaly_data   \n\nidentify_missing_values\n\n identify_missing_values   \n\nidentify_anomaly_data->identify_missing_values\n\n    \n\nMissing_Completely_At_Random\n\n Missing_Completely_At_Random   \n\nidentify_missing_values->Missing_Completely_At_Random\n\n    \n\nMissing_At_Random\n\n Missing_At_Random   \n\nMissing_Completely_At_Random->Missing_At_Random\n\n    \n\nMissing_Not_at_Random\n\n Missing_Not_at_Random   \n\nMissing_At_Random->Missing_Not_at_Random\n\n    \n\neither_imputation_or_omission\n\n either_imputation_or_omission   \n\nMissing_Not_at_Random->either_imputation_or_omission\n\n    \n\ncommunication_with_labs\n\n communication_with_labs   \n\neither_imputation_or_omission->communication_with_labs\n\n    \n\nset_data_inclusion_exclusion_criteria\n\n set_data_inclusion_exclusion_criteria   \n\ncommunication_with_labs->set_data_inclusion_exclusion_criteria\n\n    \n\nData_Transformation\n\n Data_Transformation   \n\nset_data_inclusion_exclusion_criteria->Data_Transformation\n\n    \n\nLog_Transformation\n\n Log_Transformation   \n\nData_Transformation->Log_Transformation\n\n    \n\nStandardization\n\n Standardization   \n\nLog_Transformation->Standardization\n\n   \n\n\n\n\n\nFor data quality control, I communicated with whom generated the data, classified missing values ​​and outliers according to laboratory standards, and labeled them. Based on the results through missing value analysis, I processed the missing values through consultation with the faculty members several times. For the data QC criteria, patients and variables whose ratio of missing values ​​for the sum of the rowwise and columnwise sums exceeded 5% for the sample size were excluded from the analysis. All metabolites data were standardized through log transformation and standardization.\n\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster2\n\n Data Analytics   \n\nExploratory_Data_Analysis\n\n Exploratory_Data_Analysis   \n\nData_Minig\n\n Data_Minig   \n\nExploratory_Data_Analysis->Data_Minig\n\n    \n\nStatistical_Analysis\n\n Statistical_Analysis   \n\nData_Minig->Statistical_Analysis\n\n    \n\nMachine_Learning\n\n Machine_Learning   \n\nStatistical_Analysis->Machine_Learning\n\n   \n\n\n\n\n\nData analysis was performed in three stages: Exploratory Data Analysis (EDA), Statistical Analysis, and Machine Learning. In each stage, metabolites commonly associated with diseases were selected.\n\n\nStudent t tests, Wilcoxon Man Whiteney tests, \\(\\chi^2\\) tests, Fisher Exact Tests, ANOVAs, Kruskal Wallis Tests, and regression testing were performed, and I visualizaed data to reconfirm the test results. To visualize high-dimensional data and observe data patterns, KNN, PCA, K means, Clustering, and DB Scan were used.\n\n\n\nMultivariable linear regression, logistic regression, and Cox PH (Proportional Hazards) regression analyses were conducted and the metabolites that are signficantly associated with the disease status were selected. In order to reduce the possibility of making a type 1 error due to multiple testing, the significance was checked once more by calculating permuted p-values.\n\n\n\nLasso, ridge regression, elastic net, decision tree, random rorests, ada boosting, gradient descent boosting, support vector machine (SVM), partial least square, and sparse partial least square were used. The optimal classifier was selected by evaluating the classifier that best predicted the disease status.\n\n\n\n\nIn the data consisting of 146 observations and about 3,000 variables, about 60 metabolites were observed to have a significant relationship with the disease at the 5% significance level, and partial least suare was observed to perform the best."
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#data-collection",
    "href": "docs/projects/LLFS/project_description.html#data-collection",
    "title": "Description",
    "section": "Data Collection",
    "text": "Data Collection\n\n\n\n\nflowchart TB\n    subgraph Data_Collection\n        direction TB\n        multi-centered_medical_centers---\n        set_inclusion\\exclusion_criteria---\n        patient_visit---\n        blood_sampling---\n        store_samples_in_labs\n    end\n    subgraph Mass_Spectrometry\n        direction LR\n        conduct_mass_spectrometry---\n        digitalization---\n        store_in_database_system    \n    end\n    subgraph Data_Transfer\n        direction TB\n        subgraph Data\n            direction LR\n            metabolome_data---\n            clinical_data---\n            demographic_data---\n            dietary_habit_data\n        end\n        Data---csv_or_excel_format\n    end\nData_Collection-->Mass_Spectrometry-->Data_Transfer"
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#data-qc-quality-control",
    "href": "docs/projects/LLFS/project_description.html#data-qc-quality-control",
    "title": "Description",
    "section": "Data QC (Quality Control)",
    "text": "Data QC (Quality Control)\n\n\n\n\nflowchart LR\n    subgraph Data_Quality_Control\n        direction TB\n        identify_anomaly_data---\n        identify_missing_values\n        subgraph Missing_Value_Analysis\n            direction LR\n            MCAR---\n            MAR---\n            MNAR\n        end\n        either_imputation_or_omission---\n        communication_with_labs---\n        set_data_inclusion/exclusion_criteria\n    end\n    identify_missing_values---Missing_Value_Analysis---either_imputation_or_omission\n    subgraph Data_Preprocessing\n        direction TB\n        data_transformation---\n        log_transformation---\n        standardization\n    end\n\nData_Quality_Control-->Data_Preprocessing\n\n\n\n\n\n\n\n\n\nData의 품질 관리를 위해 data를 생성한 biochemists와 소통하여 실험실 기준에 따라 결측치와 이상치를 구분하여 labeling을 수행했고 missing value analysis를 통해 결과에 따라 medical doctors를 포함한 다른 faculty members와 상의하여 결측치 처리를 했다. data QC criteria는 rowwise 와 columwise sum의 합이 sample size에 대해 missing values의 비율이 5%가 넘는 환자와 변수는 분석 대상에서 제외 됐다. 모든 metabolites data는 log transoformation 와 standardization을 통해 data의 단위를 표준화 했다.\nFor data quality control, I communicated with the biochemists who generated the data, classified missing values ​​and outliers according to laboratory standards, and labeled them. Based on the results through missing value analysis, I processed the missing values through consultation with other faculty members several times, including medical doctors. For the data QC criteria, patients and variables whose ratio of missing values ​​for the sum of the rowwise and columnwise sums exceeded 5% for the sample size were excluded from the analysis. All metabolites data were standardized through log transformation and standardization."
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#data-analytics",
    "href": "docs/projects/LLFS/project_description.html#data-analytics",
    "title": "Description",
    "section": "Data Analytics",
    "text": "Data Analytics\n\n\n\n\nflowchart TB\n    subgraph data_analytics\n        direction LR\n        exploratory_data_analysis---\n        statistical_analysis---\n        machine_learning\n    end"
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#eda-exploratory-data-analysis",
    "href": "docs/projects/LLFS/project_description.html#eda-exploratory-data-analysis",
    "title": "Description",
    "section": "EDA (Exploratory Data Analysis)",
    "text": "EDA (Exploratory Data Analysis)"
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#data-mining",
    "href": "docs/projects/LLFS/project_description.html#data-mining",
    "title": "Description",
    "section": "Data Mining",
    "text": "Data Mining\n\nStatistical Analysis\n\n\nMachine Learning"
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#reporting",
    "href": "docs/projects/LLFS/project_description.html#reporting",
    "title": "Description",
    "section": "Reporting",
    "text": "Reporting\n\n\n\n\nflowchart TB\n    subgraph report_to_faculty_members\n        direction TB\n        share_with_medical_doctors---\n        share_with_epidemiologists---\n        share_with_neurologists---\n        share_with_biologists---\n        share_with_biostatisticians---\n        share_with_bioinformatician\n    end"
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#conclusion",
    "href": "docs/projects/LLFS/project_description.html#conclusion",
    "title": "Description",
    "section": "Conclusion",
    "text": "Conclusion\n146개의 관측치와 약 3,000여개의 변수로 구성된 data에서 약 50개의 대사물질이 질병과 1% 유의수준으로 유의한 관계가 있는 것으로 관찰됐고 partial least suare 가 가장 성능이 좋은 것으로 관찰됐다.\nIn the data consisting of 146 observations and about 3,000 variables, about 50 metabolites were observed to have a significant relationship with the disease at the 1% significance level, and partial least suare was observed to perform the best."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_array/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-18_array/index.html",
    "title": "Data Structure (2) Array",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n가장 기본적인 자료구조다.\n여러 개의 변수를 담는 공간으로 이해할 수 있다.\ndata가 연속적으로 들어가는 형태여서 배열은 인덱스(index)가 존재하며, 인덱스는 0부터 시작한다.\n특정한 인덱스에 직접적으로 접근 가능하여 수행 시간은 빠른 속도인 \\(O(1)\\) 이다.\n\n\n\n\n\n컴퓨터의 메인 메모리에서 배열의 공간은 연속적으로 할당된다.\n장점: Cache memory(속도면에서, \\(RAM<Cache<CPU\\), 공간면에서, \\(CPU<RAM<Cache\\), CPU옆에 위치) 히트(RAM에 있는 data를 Cache에 일부 옮기는 현상) 가능성이 높으며, 조회가 빠르다. 배열 같은 경우는 공간적으로 또는 연속적으로 붙어 있기때문에 cache memory 묶어서 옮길 수 있다.\n\nCache Hit: 원하는 data가 Cache Memory존재하는 것을 의미.\n특정 index에 접근하는 속도가 매우 빠르다, \\(O(1)\\).\n\n단점: 배열의 크기를 미리 지정해야 하는 것이 일반적이므로, 데이터의 추가 및 삭제에 한계가 있다.\n\n\n\n\n\n컴퓨터의 메인 메모리(RAM)상에서 주소가 연속적이지 않다.\n배열과 다르게 크기가 정해져 있지 않고, 리스트의 크기는 동적으로 변경 가능하다.\n장점: 포인터(pointer)를 통해 다음 데이터의 위치를 가리킨다는 점에서 삽입과 삭제가 간편하다.\n단점: 원소를 검색할 때는 포인터가 앞에서부터 원소를 찾아야 하므로, 데이터 검색 속도가 느리다.\n\n\n\n\n파이썬의 리스트(List) 자료형\n\n파이썬에서는 리스트 자료형을 제공한다. (컴퓨터 공학에서의 연결 리스트와는 다른 의미)\n일반적인 프로그래밍 언어에서의 배열로 이해할 수 있다. 그러므로, 파이썬의 리스트는 배열이라고 생각해야한다.\n\n파이썬의 리스트는 배열처럼 임의의 인덱스를 이용해 직접적인 접근이 가능하다.\n\n파이썬의 리스트 자료형은 동적 배열이다.\n\nappend를 이용해 데이터를 삽입할 때 배열의 용량이 가득 차면, 자동으로 크기를 증가시킨다.\n\n내부적으로 포인터(pointer)를 사용하여, 연결 리스트의 장점도 가지고 있다.\n배열(array) 혹은 스택(stack)의 기능이 필요할 때 리스트 자료형을 그대로 사용할 수 있다.\n큐(queue)의 기능을 제공하지 못한다. (비효율적)\n\n\n\n\n\n파이썬에서는 임의의 크기를 가지는 배열을 만들 수 있다.\n일반적으로 리스트를 초기화할 때 리스트 컴프리헨션(list comprehension)이 자주 사용된다. (매우 편리)\n크기가 N인 1차원 배열을 만드는 방법은 다음과 같다.\n\n\n\nCode\n# [0, 0, 0, 0, 0]\nn = 5\narr = [0] * n\nprint(arr)\n\n# [0, 1, 2, 3, 4]\nn = 5\narr = [i for i in range(n)]\nprint(arr)\n\n\n[0, 0, 0, 0, 0]\n[0, 1, 2, 3, 4]\n\n\n\n크기가 \\(N \\times M\\) 인 2차원 리스트(배열) 만들기 1\n\n2차원 배열이 필요할 때는 다음과 같이 초기화한다.\n\n\n\n\nCode\nn = 3\nm = 5\narr = [[0] * m for i in range(n)]\nprint(arr)\n\n\n[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\n\n\n\n크기가 \\(N \\times M\\) 인 2차원 리스트(배열) 만들기 2\n\n\n\nCode\nn = 3\nm = 5\narr = [[i * m + j for j in range(m)] for i in range(n)]\nprint(arr)\n\n\n[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]\n\n\n\n\n\n\n리스트는 기본적으로 메모리 주소를 반환한다.\n따라서 단순히 [[0]∗m]∗n 형태로 배열을 초기화하면 안 된다.\n이렇게 초기화를 하게 되면, n개의 [0]∗m 리스트는 모두 같은 객체로 인식된다.\n다시 말해, 같은 메모리를(동일한 리스트를) 가리키는 n개의 원소를 담는 리스트가 된다.\n2차원 배열을 초기화할 때는 리스트 컴프리헨션을 이용하는 것이 일반적이다.\n\n\n\nCode\nn = 3\nm = 5\narr1 = [[0] * m] * n # 잘못된 방식\narr2 = [[0] * m for i in range(n)] # 옳은 방식\n\narr1[1][3] = 7\narr2[1][3] = 7\n\nprint(arr1)\nprint(arr2)\n\n\n[[0, 0, 0, 7, 0], [0, 0, 0, 7, 0], [0, 0, 0, 7, 0]]\n[[0, 0, 0, 0, 0], [0, 0, 0, 7, 0], [0, 0, 0, 0, 0]]\n\n\n\n\n위의 결과를 보면, 잘못된 방식으로 초기화된 배열 arr1은 [[0, 0, 0, 7, 0], [0, 0, 0, 7, 0], [0, 0, 0, 7, 0]]와 같이 7의 삽입이 모든 행에 걸쳐서 적용됐다. 반면에, 올바른 방식으로 초기화된 arr2는 [[0, 0, 0, 0, 0], [0, 0, 0, 7, 0], [0, 0, 0, 0, 0]]는 의도된 대로 하나의 element가 [1][3] index에 삽입이 된 것을 볼 수 있다.\n\n\n\n\n\n\n자신이 원하는 임의의 값을 넣어 곧바로 사용할 수 있다.\n\n\n\nCode\narr = [0, 1, 2, 3, 4, 5, 6, 7, 8]\nprint(arr)\n\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_linked_list/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-18_linked_list/index.html",
    "title": "Data Structure (3) Linked List",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n연결 리스트는 각 노드가 한 줄로 연결되어 있는 자료 구조다.\n각 노드는 (데이터, 포인터) 형태를 가진다.\n포인터: 다음 노드의 메모리 주소를 가리키는 목적으로 사용된다.\n연결성: 각 노드의 포인터는 다음 혹은 이전 노드를 가리킨다.\n\n연결 리스트를 이용하면 다양한 자료구조를 구현할 수 있다.\n\n예시) 스택, 큐 등을 구현 가능\n\nPython은 연결 리스트를 활용하는 자료구조를 제공한다.\n연결 리스트를 실제 구현해야 하는 경우는 적지만, 그 원리 이해는 자료 구조와 클래스를 작성하는데 도움이 된다.\n\n\n\n\n\n연결 리스트와 배열(array)을 비교하여 장단점을 이해할 필요가 있다.\n특정 위치의 데이터를 삭제할 때, 일반적인 배열에서는 \\(O(N)\\) 만큼의 시간이 소요된다.\n하지만, 연결 리스트를 이용하면 단순히 연결만 끊어주면 된다.\n따라서 삭제할 위치를 정확히 알고 있는 경우 \\(O(1)\\) 의 시간이 소요된다.\n하지만 삭제할 위치를 정확히 알아내기 위해 앞의 코드를 자세히 보게 되는 소요 시간이 증가할 수 있다.\n\n\n\n\n배열에 새로운 원소를 삽입할 때, 최악의 경우 시간 복잡도를 계산하여라.\n예시) 배열에서 인덱스 3에 원소 “59”를 삽입할 경우, 인덱스 4 이후의 공간에 있는 데이터를 한칸씩 밀어내는 \\(O(n)\\) 만큼 소요\n\n\n\n\n\n배열에 존재하는 원소를 삭제할 때, 최악의 경우 시간 복잡도를 계산하여라.\n예시) 배열에서 인덱스 3에 해당하는 원소를 삭제한 후 데이터를 한칸 씩 당겨 이동 시키는 \\(O(n)\\) 만큼 소요\n따라서, 최악의 경우 시간 복잡도는 \\(O(N)\\) 이다.\n\n\n\n\n\n삽입할 위치를 알고 있다면, 물리적인 위치를 한 칸씩 옮기지 않아도 삽입할 수 있다.\n인덱스 2의 위치에 원소를 삽입할 경우 인덱스 1의 Node에서 인덱스 2에 위치할 데이터를 가리키고 인덱스 2의 node가 인덱스 3의 node를 가리키도록 만들면 된다.\n\n\n\n\n\n삭제할 위치를 알고 을 경우 연결 리스트 사용\n인덱스 2의 위치에 원소를 삭제할 경우 인덱스 1의 Node가 인덱스 3의 node를 가리키게 만들면 됨\n\n\n\n\n\n뒤에 붙일 때는 남는 공간에 마지막 노드의 다음 위치에 원소를 할당 시키면 된다.\n마지막 위치에 새로운 원소를 추가\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data # 데이터 할당\n        self.next = None # 다음 노드\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head = None # 첫 번째  node\n\n    # 가장 뒤에 노드 삽입\n    def append(self, data):\n        \n        if self.head == None: # 헤드(head)가 비어있는 경우\n            self.head = Node(data)\n            return\n        \n        currrent = self.head # 그렇지 않다면 마지막 노드에 새로운 노드 추가\n\n        while currrent.next is not None: # 다음 노드가 없을 때까지  \n            currrent = currrent.next # 다음 원소로 넘어감\n        currrent.next = Node(data) # 다음 노드가 없으면 새로운 데이터를 추가 \n\n    # 모든 노드를 하나씩 출력\n    def show(self):\n        currrent = self.head\n        while currrent is not None:\n            print(currrent.data, end=\" \")\n            currrent = currrent.next\n\n    # 특정 인덱스(index)의 노드 찾기\n    def search(self, index):\n        node = self.head\n        for _ in range(index):\n            node = node.next\n        return node\n\n    # 특정 인덱스(index)에 노드 삽입\n    def insert(self, index, data):\n        new = Node(data)\n        # 첫 위치에 추가하는 경우\n        if index == 0:\n            new.next = self.head\n            self.head = new\n            return\n        # 삽입할 위치의 앞 노드\n        node = self.search(index - 1)\n        next = node.next\n        node.next = new\n        new.next = next\n\n    # 특정 인덱스(index)의 노드 삭제\n    def remove(self, index):\n        # 첫 위치를 삭제하는 경우\n        if index == 0:\n            self.head = self.head.next\n            return\n        # 삭제할 위치의 앞 노드\n        front = self.search(index - 1)\n        front.next = front.next.next\n\n\nlinked_list = LinkedList()\ndata_list = [3, 5, 9, 8, 5, 6, 1, 7]\n\nfor data in data_list:\n    linked_list.append(data)\n\nprint(\"전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\nlinked_list.insert(4, 4)\nprint(\"\\n전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\nlinked_list.remove(7)\nprint(\"\\n전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\nlinked_list.insert(7, 2)\nprint(\"\\n전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\n\n전체 노드 출력: 3 5 9 8 5 6 1 7 \n전체 노드 출력: 3 5 9 8 4 5 6 1 7 \n전체 노드 출력: 3 5 9 8 4 5 6 7 \n전체 노드 출력: 3 5 9 8 4 5 6 2 7"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_python_list/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-18_python_list/index.html",
    "title": "Data Structure (4) Python List",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\nTable 1: a list of the list functions in Python\n\n\n\n\n\n\n\n\n\nNumber\nMethods\nTime Complexity\nExamples\nDescription\n\n\n\n\n1\nIndexing\n\\(O(1)\\)\narr[i]\n특정 i th 인덱스의 값 반환\n\n\n2\nStoring\n\\(O(1)\\)\narr[i] = 1\n특정 i th 인덱스에 값 (=1) 할당\n\n\n3\nAppend\n\\(O(1)\\)\narr.append(5)\n리스트의 가장 뒤에 데이터 추가\n\n\n4\nPop\n\\(O(1)\\)\narr.pop()\n리스트의 가장 뒤에서 원소 꺼내기\n\n\n5\nLength\n\\(O(1)\\)\nlen(arr)\n리스트의 길이 얻기\n\n\n6\nClear\n\\(O(1)\\)\narr.clear()\n리스트 내 모든 원소 제거하기\n\n\n7\nSlicing\n\\(O(b-a)\\)\narr[a:b]\n리스트에서 인덱스 a부터 b-1까지의 원소만 꺼내 새 리스트 만들기\n\n\n8\nExtend\n\\(O(len(other))\\)\narr.extend(list2)\n기존 리스트, list1에 다른 리스트, list2를 이어 붙이기\n\n\n9\nInsertion\n\\(O(N)\\)\narr.insert(index, x)\n특정 인덱스에 데이터 x를 삽입하기, 즉 i th index를 뒤로 밀고 추가\n\n\n10\nDelete\n\\(O(N)\\)\ndel arr[index]\n특정 인덱스의 데이터 삭제하기\n\n\n11\nConstruction\n\\(O(len(other))\\)\narr = list(other)\n다른 자료구조의 원소들을 이용해 리스트로 만들기\n\n\n12\nIn\n\\(O(N)\\)\nx in arr\n데이터 x가 리스트에 존재하는지 확인\n\n\n13\nNot in\n\\(O(N)\\)\nx not in arr\n데이터 x가 리스트에 존재하지 않는지 확인\n\n\n14\nPop\n\\(O(N)\\)\narr.pop(index)\n특정 인덱스의 데이터를 꺼내기 / 단, 가장 뒤 원소를 꺼내는 경우 O(1)\n\n\n15\nRemove\n\\(O(N)\\)\narr.remove(x)\n리스트 내에 존재하는 데이터 x를 삭제\n\n\n16\nCopy\n\\(O(N)\\)\narr.copy()\n리스트를 복제\n\n\n17\nMin\n\\(O(N)\\)\nmin(arr)\n리스트 내에 존재하는 가장 작은 원소\n\n\n18\nMax\n\\(O(N)\\)\nmax(arr)\n리스트 내에 존재하는 가장 큰 원소\n\n\n19\nIteration\n\\(O(N)\\)\nfor x in arr\n리스트 내에 존재하는 모든 원소 순회\n\n\n20\nMultiply\n\\(O(k*N)\\)\narr * k\n리스트를 k번 반복하여 길게 만들기\n\n\n21\nSort\n\\(O(NlogN)\\)\narr.sort()\n리스트 내 존재하는 원소를 정렬\n\n\n\n\nSee Table 1.\n\n1~6: 파이썬의 list는 동적 배열의 특징이 있다. 시간 복잡도는 모두 \\(O(1)\\) 이다.\n\n3~4: 사실상 stack의 기능과 동일\n\n\n\n\nCode\narr = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(arr[4]) # 인덱싱(indexing)\n\n# 저장(storing)\narr[7] = 10\n\n# 뒤에 붙이기(append)\narr.append(10)\nprint(arr)\n\n# 뒤에서 꺼내기(pop)\narr.pop()\nprint(arr)\n\n# 길이(length)\nprint(len(arr))\n\n# 배열 비우기(clear)\narr.clear()\nprint(arr)\n\n\n4\n[0, 1, 2, 3, 4, 5, 6, 10, 8, 9, 10]\n[0, 1, 2, 3, 4, 5, 6, 10, 8, 9]\n10\n[]\n\n\n\n7~11\n\n\n\nCode\narr = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nnew_arr = arr[2:7] # 슬라이싱(slicing)\nprint(new_arr)\n\narr1 = [0, 1, 2, 3, 4]\narr2 = [5, 6, 7, 8, 9]\narr1.extend(arr2) # 확장(extend)\nprint(arr1)\n\narr = [0, 1, 2, 3, 4]\narr.insert(3, 7) # 삽입(insertion)\nprint(arr)\n\ndel arr[3] # 삭제(delete)\nprint(arr)\n\ndata = {7, 8, 9}\narr = list(data) # 다른 자료구조로 리스트 만들기\nprint(arr)\n\n\n[2, 3, 4, 5, 6]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[0, 1, 2, 7, 3, 4]\n[0, 1, 2, 3, 4]\n[8, 9, 7]\n\n\n\n12~16\n\n\n\nCode\narr = [0, 1, 2, 3, 4]\n\nprint(3 in arr) # 존재 여부(in)\nprint(7 not in arr) # 비존재 여부(not in)\n\narr.pop(1) # 인덱스 1에 해당하는 원소 꺼내기(pop)\nprint(arr)\n\narr.remove(3) # 리스트의 특정 원소 삭제(remove)\nprint(arr)\n\nnew_arr = arr.copy() # 복제(copy)\nprint(new_arr)\n\n\nTrue\nTrue\n[0, 2, 3, 4]\n[0, 2, 4]\n[0, 2, 4]\n\n\n\n17~21\n\n\n\nCode\narr = [3, 5, 4, 1, 2]\n\nprint(min(arr)) # 최소(min)\nprint(max(arr)) # 최대(max)\n\nfor x in arr: # 원소 순회(iteration)\n    print(x, end=\" \")\nprint()\n\nprint(arr * 2) # 리스트 반복하여 곱하기(multiply)\n\narr.sort() # 정렬(sorting)\nprint(arr)\n\n\n1\n5\n3 5 4 1 2 \n[3, 5, 4, 1, 2, 3, 5, 4, 1, 2]\n[1, 2, 3, 4, 5]"
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html",
    "href": "docs/projects/LLFS/data_preparation.html",
    "title": "Data Preparation",
    "section": "",
    "text": "flowchart TB\n    subgraph Simulation\n        direction TB\n        subgraph Assign_Setting_Values\n            direction LR\n            Assign_Sample_Size---\n            Assign_Dimension_Size---\n            Assign_Covariance_Correlation---\n            Assign_Several_Proportions---\n            Assign_Noise_Intensity\n        end\n        subgraph Generate_Metabolite_Variables\n            direction LR\n            Generate_Covariance_Matrix---\n            Apply_Noise_to_Covariance---\n            Generate_Weights_Matrix---\n            Use_MVN_Distribution---\n            Generate_Metabolite_Data\n        end\n        subgraph Generate_Outcome_Variable\n            direction LR\n            Calculate_Score_Matrix---\n            Use_Logit_Link---\n            Calculate_Outcome_Probabilities---\n            Use_Binomial_Distribution1---\n            Generate_Binary_Outcome_Data\n        end\n        subgraph Generate_Sex_Variable\n            direction LR\n            Use_Binomial_Distribution2---\n            Generate_Sex_Data\n        end\n        subgraph Generate_Age_Variable\n            direction LR\n            Search_the_Strongest_Metabolite---\n            Rescale_It_to_Age---\n            Generate_Age_Data\n        end\n        subgraph Generate_Genotype_Variable\n            direction LR       \n            Calculate_Marginal_Proportions---\n            Calcualte_Joint_Proportions---\n            Generate_Genotype_data\n        end\n        subgraph Merge_All_Data\n            direction LR\n            Outcome_Variable---\n            Sex_Variable---\n            Age_Variable---\n            Genotype_Variable---\n            Metabolite_Data\n        end\n        Assign_Setting_Values-->Generate_Metabolite_Variables-->Generate_Outcome_Variable-->\n        Generate_Sex_Variable-->Generate_Age_Variable-->Generate_Genotype_Variable-->\n        Merge_All_Data\n    end\n    subgraph Data_Analytics\n        direction LR\n        exploratory_data_analysis---\n        statistical_analysis---\n        machine_learning\n    end\n    subgraph Conclusion\n        direction LR\n    end\n    Simulation-->Data_Analytics-->Conclusion\n\n\n\n\n\n\n\n\n\nMVN: Multivariate Normal Distirubtion\n\n\n\n\nKorean\n\n\n\n\nEnglish\n\n\n\n\n\n대략적인 분석 방법론을 간단히 보여주기 위해 Simulation을 수행했다. 이해를 돕기위해 Simulation 순서도를 간략히 설명하자면 크게 7 단계로 Simulation을 수행했다.\n\n\nData Set Size Setting\nSimulation에 필요한 몇 가지 설정값들을 Global Variables로 설정하여 후차적으로 작성된 스크립트에서 호출이 자유롭도록 작성했다. 변수들은 아래의 Simulation section에 있는 Global Variables (see Section 2.2) 에서 확인 가능하다.\nCategorial Data Setting\n먼저, 고차원 데이터의 차원을 설정하기 위해 Sample Size와 변수의 수를 설정한 후 Categorical predictors를 만들기 위해 잘 알려진 분포, 내가 정한 분포, 혹은 임의로 발생하게 만든 분포를 설정하였다. (see Section 2.2, Section 2.3, and Section 2.4)\nSex Variable Setting\nSex 변수는 \\(X \\sim \\text{Bernoulli}(0.5)\\) 을 통해 data를 생성했다. (see Section 2.4)\nGenotype Variable Setting\nGenotype 변수의 data는 아직도 어떻게 통계적으로 생성해야하는지 감을 못잡은 상태이기 때문에 더 연구가 필요하다. 하지만, 질병에 대한 유전적 영향도는 반영해야하기 때문에 outcome variable과 이미 잘 알려진 genotype의 분포를 반영하려고 노력했다. 두 변수에 연관성을 갖게하기 위해 각 변수의 proportion을 marginal distirubtion으로 설정하여 두 변수의 joint proportion을 계산하여 Genotype data를 생성했다. (see Section 2.3 and Section 2.4)\nMetabolite Data Setting\n고차원의 metabolite data를 만드는 설정으로, 고차원이면서 그룹내 서로 상관 관계가 있는 변수들을 생성하기 위해 난수에 의해 발생되는 임의의 Covariance를 생성하여 MVN (Multivariate Normal Distribution)에 반영되게 했고 각 그룹의 반응 변수로의 영향(또는 가중치)도 또한 난수로 임의적으로 발생되게 설정했다. 이때, 난수에 의해 임의적으로 발생하는 수치는 내가 임의적으로 범위를 한정했다. 재현성을 위해 seed number를 고정했다. (see Section 2.4)\nOutcome Variable Setting\nMVN에 의해 만들어진 Data와 미리 만들어 놓은 가중치 Matrix의 곱을 통해 Score Matrix를 만들고 Logit Link를 이용하여 각 Sample의 확률값을 만들었다. 각 Sample의 확률값을 기반으로 \\(X \\sim \\text{Bernoulli}(p)\\), (여기서 \\(p\\)는 각 sample이 갖는 확률값을 뜻한다), 을 통해 disease status의 정보를 담은 binary outcome variable를 만들었다. (see Section 2.4)\nAge Variable Setting\nAge 변수는 생물학적, 의학적으로 치매와 연관성이 높은 요인으로 Outcome 변수로 가장 설명이 잘되는 metabolite를 탐색해 선별하여 Age 형태로 변환을 했다. 제일 어린 사람을 65세 그리고 제일 연장자를 105세로 설정하여 min max normalization을 적용했다. (see (see Section 2.3 and Section 2.4)\nMerge All Data\nSimulation을 통해 만들어진 각 변수들을 index를 만들어 병합시켜 data frame의 형태로 만들었다. (see Section 2.4)\nAnalytics & Conclusion\n분석 부분은 이 data preparation section에서는 자세히 기술하지 않고 EDA, Statistical Approach 및 ML Approach Section에서 자세히 다룰예정이다. 간략히 말하면, outcome 변수와 통계적으로 유의한 관계를 갖는 metabolite를 선별하고 그 결과가 machine learning을 이용하여 얻은 결과와 얼마나 같은지 비교 분석을 하여 Outcome variable에 가장 연관성이 있는 변수들을 규명하는 방법을 기술할 예정이다.\n\n\n\n\n\n\n\n\n\n\nShow the code\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nShow the code\n# the number of samples\nsample_size <- 500 #1000\n# the number of predictors\npredictor_size <- 1000 #5000\n# the number of groups\ngroup_size <- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors <- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors<-floor(significant_predictors*0.4) \nnegatively_associated_predictors<-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list<-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%>%round(3) \nnames(group_proportion_list)<-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix <- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data<-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%>%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%>%round(0),\n            # the 1st index of predictors in each group\n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE),\n            # effect of each group on an outcome variable \n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]<-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]<-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata<-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix<-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients <- rnorm(predictor_size,0,0.05)\n\nanswer_list<-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nShow the code\n## Function List\n\nscale_function=function(vector=x,min=NULL,max=NULL,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data \n    # that are statistically associated with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)<-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite<-\n        temp_df%>%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%>%\n        filter(abs_mean_diff==max(abs_mean_diff))%>%\n        dplyr::select(metabolite)%>%pull\n    \n    ## generate age data with min max normalization\n    age_data<-\n        data%>%\n        dplyr::select(strong_metabolite)%>%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%>%\n        rename(age=1)%>%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated with \n    # some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n    binary_proportion<-as.numeric(table(in_response)/sample_size) #the simulated proportion for the disease vs non-disease cases\n    genotype_proportion<-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion<-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nShow the code\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range <- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] <- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) <- 1\n    data[, group_range] <- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] <-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names<-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] <- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]<-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities <- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%>%\n    ifelse(.>1,1,.)%>%\n    ifelse(.<0,0,.)\n\nresponse <-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%>%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data <- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data <- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data <- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data<-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%>%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nShow the code\n# load simulation data\nsimulated_data<-read_rds(datapath)\n\n# simple data pre-processing\nall_data<-\n    simulated_data%>%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]<-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\nThis data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers.\n\n\n\n\nSimulations were performed to outline the approximate analysis methodology. For your better understanding, I will briefly describe the simulation flow diagram. The simulation was conducted in 9 steps.\n\n\nData Set Size Setting\nSome of the setting values ​​required for simulation are set as global variables so that they can be freely called in the later scripts. (see Section 2.2)\nCategorial Data Setting\nI first set the dimensions of my high-dimensional data by setting the sample size and number of variables, then I created categorical data by choosing a well-known distribution, a distribution I determined, or a distribution that occurred randomly set. (see Section 2.2, Section 2.3, and Section 2.4)\nSex Variable Setting\nThe data of the sex variable were generated through \\(X \\sim \\text{Bernoulli}(0.5)\\). (Section 2.4)\nGenotype Variable Setting\nPersonally, I have not yet figured out how to generate data for genotype (categorical) variables statistically, so further research is needed. However, since the genetic influence on the disease should be reflected, I tried to reflect the distribution of outcome variables and well-known distribution of the genotypes, APOE (from Wiki). To make an association between the two variables, the proportions of each variable were set as marginal distirubtion and the joint distribution of the two variables was calculated to generate genotype data. (Section 2.3 and Section 2.4)\nMetabolite Data Setting\nAs a setting for generating high-dimensional metabolomic data, a covariance matrix generated by random numbers is generated to create high-dimensional and mutually correlated metabolites within a group, which is used as input in the MVN (Multivariate Normal Distribution) function, and for each group, the metabolites’ effect (or weight) toward the outcome variable is also set to be randomly generated with a random number. At this time, the range of numbers randomly generated by random numbers was arbitrarily limited by myself. A seed number was fixed for reproducibility. (Section 2.4)\nOutcome Variable Setting\nA score matrix was created through the matrix multiplication of the data created by MVN and a pre-made weight matrix with the probability values of samples that were created using the Logit Link. Based on the probability value of each sample, a binary outcome variable representing disease status information was created through \\(X \\sim \\text{Bernoulli}(p)\\), (where \\(p\\) means the probability value of each sample). (Section 2.4)\nAge Variable Setting\nSince the Age variable is a important factor related to dementia biologically and medically, the metabolite best explained as an Outcome variable was selected and converted into an age scale using min-max normalization by setting the youngest to 65 and the oldest to 105. (Section 2.3 and Section 2.4)\nMerge All Data\nEach variable created through simulation was merged into a data frame. (Section 2.4)\nAnalytics & Conclusion\nThe analysis part will not be discussed in detail in this data preparation section, but will be covered in detail in the EDA, Statistical Approaches and ML Approaches section. Briefly, I describe a method to identify the variables most associated with the outcome variable by selecting metabolites that have a statistically significant relationship with the outcome variable and comparing how similar the results are to those obtained through machine learning.\n\n\n\n\n\n\n\n\n\n\nShow the code\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nShow the code\n# the number of samples\nsample_size <- 500 #1000\n# the number of predictors\npredictor_size <- 1000 #5000\n# the number of groups\ngroup_size <- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors <- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors<-floor(significant_predictors*0.4) \nnegatively_associated_predictors<-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list<-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%>%round(3) \nnames(group_proportion_list)<-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix <- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data<-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%>%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%>%round(0),\n            # the 1st index of predictors in each group \n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE), \n            # effect of each group on an outcome variable\n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]<-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]<-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata<-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix<-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients <- rnorm(predictor_size,0,0.05)\n\nanswer_list<-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nShow the code\n## Function List\n\nscale_function=function(vector=x,min=a,max=b,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))/(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data that are statistically associated \n    # with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)<-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite<-\n        temp_df%>%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%>%\n        filter(abs_mean_diff==max(abs_mean_diff))%>%\n        dplyr::select(metabolite)%>%pull\n    \n    ## generate age data with min max normalization\n    age_data<-\n        data%>%\n        dplyr::select(strong_metabolite)%>%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%>%\n        rename(age=1)%>%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated \n    # with some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n\n    ##the simulated proportion for the disease vs non-disease cases\n    binary_proportion<-as.numeric(table(in_response)/sample_size) \n    genotype_proportion<-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion<-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nShow the code\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range <- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] <- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) <- 1\n    data[, group_range] <- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] <-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names<-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] <- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]<-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities <- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%>%\n    ifelse(.>1,1,.)%>%\n    ifelse(.<0,0,.)\n\nresponse <-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%>%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data <- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data <- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data <- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data<-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%>%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nShow the code\n# load simulation data\nsimulated_data<-read_rds(datapath)\n\n# simple data pre-processing\nall_data<-\n    simulated_data%>%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]<-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\n\n\n\nThis data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers."
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#준비중",
    "href": "docs/projects/LLFS/data_preparation.html#준비중",
    "title": "Data Preparation",
    "section": "2 준비중",
    "text": "2 준비중\nplease, read the English section first."
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#data-preparation",
    "href": "docs/projects/LLFS/data_preparation.html#data-preparation",
    "title": "Data Preparation",
    "section": "2 Data Preparation",
    "text": "2 Data Preparation\n\n2.1 Package Loading and Option Settings\n\n\nShow the code\nrm(list=ls())\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(mixOmics)\nset.seed(20230121) # the date writing this\nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n2.2 Global Functions & Variables\n\n\nShow the code\n## Function List\n\nscale_function=function(vector=x,min=a,max=b,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\ngenotype_data_generator=function(temp_data,fun=scale_function,start_quantile,end_quantile){\n    temp=temp_data%>%\n    pull%>%\n    fun(vector=.,min=0,max=1,method=\"customized\")\n    return(ifelse(temp>temp%>%quantile(.,probs=start_quantile)&temp<temp%>%quantile(.,probs=end_quantile),1,0))\n}\n\n# the number of samples\nsample_size <- 1000\n# the number of predictors\npredictor_size <- 5000\n# the number of groups\ngroup_size <- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors <- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors<-floor(significant_predictors*0.4) \nnegatively_associated_predictors<-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list<-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%>%round(3) \nnames(group_proportion_list)<-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix <- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data<-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%>%\n        mutate(\n            group_n=(predictor_size*group_proportion_list)%>%round(0), # the number of predictors within each group \n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), # the 1st index of predictors in each group\n            last_index=cumsum(group_n), # the last index of predictors in each group\n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE), # within-group correlations among the within-group predictors\n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) # effect of each group on an outcome variable\n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]<-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]<-(-0.5)\n\n\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata<-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.01), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix<-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients <- rnorm(predictor_size,0,0.05)\n\n\n\n\n2.3 Data Simulation\n\n\nShow the code\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(meta_data)) {\n    \n    group_range <- meta_data[i, \"first_index\"]:meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] <- meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) <- 1\n    data[, group_range] <- \n        mvrnorm(n = sample_size, \n                mu = rep(0,meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[meta_data[i, \"first_index\"]:meta_data[i, \"last_index\"]] <-\n        beta_coefficients[meta_data[i, \"first_index\"]:meta_data[i, \"last_index\"]]+\n        meta_data[i,\"group_effect\"]\n    predictor_names<-paste0(meta_data[i,\"group_name\"],\"_\",1:meta_data[i,\"group_n\"])\n    names(beta_coefficients)[meta_data[i, \"first_index\"]:meta_data[i, \"last_index\"]] <- predictor_names\n    names(data)[meta_data[i, \"first_index\"]:meta_data[i, \"last_index\"]]<-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities <- ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%>%\n    ifelse(.>1,1,\n    ifelse(.<0,0,.))\nresponse <- rbinom(sample_size, 1, probabilities) \n\n\nanswer_list<-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.01),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function' = 'logistic function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0.2,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=list(\n        'e2/e2'='8%, the bottom 8 % of the percentiles of the outcome probabilities with noises',\n        'e2/e3'='15%, the bottom 23 % of the percentiles of the outcome probabilities with noises',\n        'e2/e4'='8.6%, the bottom 31.6 % of the percentiles of the outcome probabilities with noises',\n        'e3/e3'='30%, the bottom 61.6 % of the percentiles of the outcome probabilities with noises',\n        'e3/e4'='25%, the bottom 86.6 % of the percentiles of the outcome probabilities with noises',\n        'e4/e4'='13.4%, above the bottom 86.6 % of the percentiles of the outcome probabilities with noises'),\n    'treatment_distirbution'=list(\n        'control' = '70%, the bottom 70 % of the percentiles of the outcome probabilities with noises',\n        'treatment1'='15%, the bottom 85 % of the percentiles of the outcome probabilities with noises',\n        'treatment2'='10%, bottom 95 % of the percentiles of the outcome probabilities with noises',\n        'treatment3'='5%, above the bottom 8 % of the percentiles of the outcome probabilities with noises'\n    ));answer_list\n\n# used data of a variable with the highest effect on outcome\nage_distribution=data%>%\ndplyr::select(grep(group_meta_data[,\"group_name\"][group_meta_data[,\"group_effect\"]==0.7],names(data))[1])%>%\npull()%>%\nscale_function(vector=.,min=65,max=105,method=\"customized\")%>%round(0)\n\n#rbinom(n=sample size,p=0.5)\nsex_distribution=rbinom(sample_size,1,0.5)\n\ngenotype_quantile_data<-quantile((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05),\nprobs=cumsum(c(0.08,0.15,0.086,0.30,0.25)))\ntreatment_quantile_data<-quantile((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05),\nprobs=cumsum(c(0.7,0.15,0.1)))\n\ngenotype_distribution<-data.frame(\n    probability=((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05)))%>%\n    mutate(\n        genotype=ifelse(probability<genotype_quantile_data[1],'e2e2',\n        ifelse(probability<genotype_quantile_data[2],'e2e3',\n        ifelse(probability<genotype_quantile_data[3],'e2e4',\n        ifelse(probability<genotype_quantile_data[4],'e3e3',\n        ifelse(probability<genotype_quantile_data[5],'e3e4','e4e4'))))))\ntreatment_distribution<-data.frame(\n    probability=((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05)))%>%\n    mutate(\n        treatment=ifelse(probability<treatment_quantile_data[1],'control',\n        ifelse(probability<treatment_quantile_data[2],'trtmnt1',\n        ifelse(probability<treatment_quantile_data[3],'trtmnt2','trtmnt3'))))\n\nphenotype_data<-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        probabilities=probabilities,\n        age=age_distribution,\n        sex=sex_distribution,\n        genotype=genotype_distribution[,'genotype'],\n        treatment=treatment_distribution[,'treatment'])\n\n\nall_data=inner_join(phenotype_data,data%>%mutate(id=1:n()),by=\"id\")\n\n#write_rds(all_data,\"./docs/projects/data/llfs_simulated_data.rds\")\n\n\n\n\n2.4 Load Data\n\n\nShow the code\ndatapath<-\"C:/Users/kmkim/Desktop/my_project/website/docs/projects/data/llfs_simulated_data.rds\"\n#datapath<-\"C:/Users/kkm/Desktop/projects/website/docs/projects/data/llfs_simulated_data.rds\"\n simulated_data=read_rds(datapath)%>%\n     dplyr::select(-1,-probabilities)\n all_data=simulated_data%>%\n mutate(\n      outcome=ifelse(outcome==0,\"negative\",\"positive\"),\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      treatment=ifelse(treatment==0,\"trmnt1\",ifelse(treatment==1,\"trmnt2\",\"trmnt3\")),\n      treatment=factor(treatment,levels=c(\"trmnt1\",\"trmnt2\",\"trmnt3\")),\n      genotype=ifelse(genotype==0,\"e2/e2\",\n      ifelse(genotype==1,\"e2/e3\",\n      ifelse(genotype==2,\"e2/e4\",\n      ifelse(genotype==3,\"e3/e3\",\n      ifelse(genotype==4,\"e3/e4\",\"e4/e4\"))))),\n      genotype=factor(genotype,levels=c(\"e2/e2\",\"e2/e3\",\"e2/e4\",\"e3/e3\",\"e3/e4\",\"e4/e4\"))\n      )\n names(all_data)[6:ncol(all_data)]<-paste0(\"meta\",1:predictor_size)"
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#data-description",
    "href": "docs/projects/LLFS/data_preparation.html#data-description",
    "title": "Data Preparation",
    "section": "3 Data Description",
    "text": "3 Data Description\nThis data include 1000 samples and 5005 variables:\n\nid: sample ID.\noutcome: a disease status (negative, positive), negative is an affected status, positive is an unaffected status, and the reference group is negative.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e2, e4\n\n\nmeta1 ~ meta5000: a list of metabolites that were blood-sampled from the APOE carriers.\n\n:::\n\n\n\n\n\n4 Simulation\n\n4.1 Package Loading and Option Settings\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(mixOmics)\nset.seed(20230121) # the date writing this\nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n4.2 Global Variables\n\n\nShow the code\n# the number of samples\nsample_size <- 1000\n# the number of predictors\npredictor_size <- 5000\n# the number of groups\ngroup_size <- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors <- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors<-floor(significant_predictors*0.4) \nnegatively_associated_predictors<-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list<-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%>%round(3) \nnames(group_proportion_list)<-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix <- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data<-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%>%\n        mutate(\n            group_n=(predictor_size*group_proportion_list)%>%round(0), # the number of predictors within each group \n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), # the 1st index of predictors in each group\n            last_index=cumsum(group_n), # the last index of predictors in each group\n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE), # within-group correlations among the within-group predictors\n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) # effect of each group on an outcome variable\n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]<-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]<-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata<-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix<-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients <- rnorm(predictor_size,0,0.05)\n\nanswer_list<-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n4.3 Functions\n\n\nShow the code\n## Function List\n\nscale_function=function(vector=x,min=a,max=b,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data that are statistically associated with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)<-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite<-\n        temp_df%>%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%>%\n        filter(abs_mean_diff==max(abs_mean_diff))%>%\n        dplyr::select(metabolite)%>%pull\n    \n    ## generate age data with min max normalization\n    age_data<-\n        data%>%\n        dplyr::select(strong_metabolite)%>%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%>%\n        rename(age=1)%>%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data that are jointly and statistically associated with a continuous data and a binary data (I am not so sure if I can generate data that are statistically associated with some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n    binary_proportion<-as.numeric(table(in_response)/sample_size) #the simulated proportion for the disease vs non-disease cases\n    genotype_proportion<-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion<-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n4.4 Simulation Algorithm\n\n\nShow the code\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range <- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] <- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) <- 1\n    data[, group_range] <- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] <-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names<-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] <- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]<-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities <- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%>%\n    ifelse(.>1,1,.)%>%\n    ifelse(.<0,0,.)\n\nresponse <-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%>%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data <- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data <- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data <- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data<-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%>%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n4.5 Load Data\n\n\nShow the code\n# load simulation data\nsimulated_data<-read_rds(datapath)\n\n# simple data pre-processing\nall_data<-\n    simulated_data%>%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]<-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\n\n\n5 Data Description\nThis data include 1000 samples and 5005 variables:\n\nid: sample ID.\noutcome: a disease status (negative, positive), negative is an affected status, positive is an unaffected status, and the reference group is negative.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e2, e4\n\n\nmeta1 ~ meta5000: a list of metabolites that were blood-sampled from the APOE carriers."
  },
  {
    "objectID": "docs/projects/LLFS/eda.html",
    "href": "docs/projects/LLFS/eda.html",
    "title": "EDA",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\nplease, read the English section first.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsimulated_data=read_rds(datapath)%>%dplyr::select(-1,-probabilities)\nall_data=simulated_data\n\nall_data=all_data%>%\nmutate(\n      outcome=ifelse(outcome==0,\"negative\",\"positive\"),\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      treatment=factor(treatment,levels=c(\"control\",\"trtmnt1\",\"trtmnt2\",\"trtmnt3\")),\n      genotype=factor(genotype,levels=c(\"e3e3\",\"e2e2\",\"e2e3\",\"e2e4\",\"e3e4\",\"e4e4\"))\n      )\n names(all_data)[6:ncol(all_data)]<-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\nmean, sd, normality check with p value\n\n\n\n\n\n\n\n\n\n\nCode\ncolor_function<-function(summary_data){\nreturn(\n    if(nrow(summary_data)==2){\n        c(\"darkblue\",\"darkred\")\n    }else if(nrow(summary_data)==3){\n        c(\"darkblue\",\"darkred\",\"yellow4\")\n    }else if(nrow(summary_data)==4){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\")\n    }else if(nrow(summary_data)==5){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\")\n    }else{\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\",\"darkgreen\")\n    }\n    )\n}\n\ngroup_variable=\"outcome\"\ngroup_variable2=\"sex\"\nsummary_variable=\"age\"\nstr(group_variable)\n\n\n chr \"outcome\"\n\n\nCode\ngetNumericSummary=function(data=all_data,group_variable,summary_variable,set_color=color_function,...){\n    # table\n    temp<-data %>% \n    #group_by_at(vars(...)) %>% \n    group_by_at(vars(group_variable)) %>% \n    mutate(count=n())%>%\n    summarise_at(vars(summary_variable,count),\n                 list(mean=mean,\n                 sd=sd,\n                 min=min,\n                 Q1=~quantile(., probs = 0.25),\n                 median=median, \n                 Q3=~quantile(., probs = 0.75),\n                 max=max))%>%\n                 as.data.frame()%>%\n                 rename(\n                 n=count_mean)%>%\n                 dplyr::select(-contains('count'))%>%\n                 as.data.frame()\n    names(temp)<-c(\"group\",\n    sapply(names(temp)[-1],function(x)str_replace(x,paste0(summary_variable,\"_\"),\"\")))\n    temp<-temp%>%\n    mutate(\n        variable=group_variable,\n        summary=summary_variable,\n        mean=mean%>%round(2),\n        sd=sd%>%round(2),\n        min=min%>%round(2),\n        Q1=Q1%>%round(2),\n        Q4=Q3%>%round(2),\n        max=max%>%round(2),\n        IQR_min=Q1-(Q3-Q1)*1.5%>%round(2),\n    IQR_max=Q3+(Q3-Q1)*1.5%>%round(2),\n    proportion=paste0(round(n/nrow(all_data)*100,2),\"%\"))%>%\n    dplyr::select(variable,group,summary,n,proportion,mean,sd,min,IQR_min,Q1,median,Q3,IQR_max,max)\n\n    # plot\n    temp2=temp\n    names(temp2)[2]=group_variable\n    plot<-\n    data%>%\n    dplyr::select(group_variable,summary_variable)%>%\n    inner_join(.,temp2,by=group_variable)%>%\n    ggplot(aes(x=age,fill=get(group_variable),color=get(group_variable)))+\n    geom_histogram(aes(y=..density..),binwidth=1,alpha=0.5, position=\"identity\")+\n    geom_vline(aes(xintercept=mean,color=get(group_variable)), linetype=\"dashed\", size=1.5) + \n    geom_density(aes(y=..density..),alpha=0.3) +\n    scale_color_manual(values=set_color(temp2))+\n    scale_fill_manual(values=set_color(temp2))+\n    theme_bw()+\n    theme(legend.position = c(.95, .95),\n    legend.justification = c(\"right\", \"top\"),\n    legend.margin = margin(6, 6, 6, 6),\n    legend.text = element_text(size = 10))+\n    guides(fill=guide_legend(title=group_variable),\n    color=FALSE)+\n    geom_text(aes(label=round(mean,1),y=0,x=mean),\n                vjust=-1,col='yellow',size=5)+\n    ggtitle(paste0(\"Histogram & Density, \", summary_variable, \" Grouped by \", group_variable))+\n        labs(x=summary_variable, y = \"Density\")\n\n    result<-list(temp,plot)\n    return(result)\n}\n\nad_age_summary=getNumericSummary(data=all_data,group_variable=\"outcome\",summary_variable=\"age\")[[1]]\nsex_age_summary=getNumericSummary(data=all_data,group_variable=\"sex\",summary_variable=\"age\")[[1]]\ntreatment_age_summary=getNumericSummary(data=all_data,group_variable=\"treatment\",summary_variable=\"age\")[[1]]\ngenotype_age_summary=getNumericSummary(data=all_data,group_variable=\"genotype\",summary_variable=\"age\")[[1]]\nage_summary=rbind(\n    ad_age_summary,\n    sex_age_summary,\n    treatment_age_summary,\n    genotype_age_summary)\n\n\nAge is known as a strong risk factor of AD or dementia. Human nerve system gets damaged as people are aged and the nerve fibrosis symptoms progress gradually (some reference, to be added after figuring out how to add bibliography in quarto). For this reason, we need to look into how the sample data are distributed in terms of age.\n\n\nCode\nage_summary%>%knitr::kable()\n\n\nThe table above shows the summary statistics of age grouped by the affected status, AD and non AD. The difference of age between the two groups are about 0.09, but their standard deviations are 6.54 and 6.27. Thus, it is hard to say their age in average differ in the affected status because the age variations of the two groups are overlapped. This research has two conflicting characteristics at the population level. First, the participants are the elderly whose age in average is 84.66, which indicates they are likely to develop dementia, an aging disease. Second, the data were collected from the longevity village where people live long and healthy lives and it is expected that where will be some protective factors against dementia. These two conflicting traits may have contributed to this unclear difference.\nThe glaring difference of age is also shown below. As you can see, the people with a negative status a bit younger than those with a positive one.\n\n\nCode\nplot<- ggarrange(\n    getNumericSummary(data=all_data,group_variable=\"outcome\",summary_variable=\"age\")[[2]],\n    getNumericSummary(data=all_data,group_variable=\"sex\",summary_variable=\"age\")[[2]],\n    getNumericSummary(data=all_data,group_variable=\"treatment\",summary_variable=\"age\")[[2]],\n    getNumericSummary(data=all_data,group_variable=\"genotype\",summary_variable=\"age\")[[2]],\n    ncol=2, nrow=2,legend=\"bottom\")\n\nd=lapply(all_data[,sapply(all_data,function(x)is.numeric(x))],function(x)t.test(x~outcome,data=all_data))\n\nstr(d[[1]])\nlapply(d,function(x)\nc('variable'=names(d),\nx$estimate[1],\nx$estimate[2],\n'p'=x$p.value,\n'lower'=x$conf.int[1],\n'upper'=x$conf.int[2]))\nsd=lapply(d,function(x)c('p'=x$p.value))%>%do.call(\"rbind\",.)%>%as.data.frame%>%mutate(variable=rownames(.))\nnames(sd)[1]='p'\n\nsdf=sd%>%mutate(test=ifelse(p<0.05/5000,1,0))\ntable(sdf$test)\n\n\n\n\n\n\n\nt-test\n\n\nCode\n#F-test to test for homogeneity in variances\n#| echo: false\n#| eval: false\nvar.test\n\n\nfunction (x, ...) \nUseMethod(\"var.test\")\n<bytecode: 0x00000288f8ea1010>\n<environment: namespace:stats>\n\n\n\n\n\nOne way Anova\n\n\n\nOne way Anova\n\n\n\n\n\n\n\n\n\n\n\n\n\nChisquare test\n\n\n\nchisquare test"
  },
  {
    "objectID": "docs/projects/LLFS/ml_approach.html",
    "href": "docs/projects/LLFS/ml_approach.html",
    "title": "ML Approach",
    "section": "",
    "text": "alpha      mse fit.name\n1    0.0 2217.331   alpha0\n2    0.1 2217.331 alpha0.1\n3    0.2 2217.331 alpha0.2\n4    0.3 2217.331 alpha0.3\n5    0.4 2217.331 alpha0.4\n6    0.5 2217.331 alpha0.5\n7    0.6 2217.331 alpha0.6\n8    0.7 2217.331 alpha0.7\n9    0.8 2217.331 alpha0.8\n10   0.9 2217.331 alpha0.9\n11   1.0 2217.331   alpha1"
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#overview",
    "href": "docs/projects/LLFS/project_description.html#overview",
    "title": "Description",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nflowchart LR\n    subgraph Data_Collection\n        direction TB\n        Multi_Centerd_Blood_Sampling---\n        Mass_Spectrometry---\n        Data_Transfer\n    end\n    subgraph Quality_Control\n        direction TB\n        Identify_Anomaly_Data---\n        Identify_Missing_Values\n    end\n    subgraph Analytics\n        direction TB \n        EDA---\n        Data_Mining---\n        Statistical_Analysis---\n        Machine_Learning\n    end\n    subgraph Reporting_and_Conclusion\n        direction TB \n        Share_with_Faculty\n    end\n\nData_Collection-->Quality_Control-->Analytics-->Reporting_and_Conclusion\n\n\n\n\n\n\n\n\n\n\n\nKorean\n\n\n\n\nEnglish\n\n\n\n\n\nData는 장수 마을에 거주하는 백인 남성을 대상으로 New York, Bonston, Pittsburgh 및 Denmark에 있는 여러 medical centers에서 sampled blood를 MS Spectromtetry로 Digitalization을 했다. 여러 과정을 통해 data를 csv형태로 받아 data의 QC(Quality Control)를 진행한뒤 Data 분석 업무를 수행했다. EDA (Exploratory Data Analysis) 와 Data Mining을 통해 data에 대한 이해도를 높였고 이를 토대로 통계 분석과 machine learning을 이용하여 이 data에 적합한 모형을 찾았다. 모든 결과물은 The Taub Institute for Research on Alzheimer’s Disease and the Aging Brain 의 biostaticians, medical doctors, biologists, neurologists, bioinformaticians 및 epidemiologists와 공유를 했다.\n\n\nData were obtained by MS Spectromtetry digitization of blood samples from multiple medical centers in New York, Bonston, Pittsburgh, and Denmark for Caucasians residing in longevity villages. After receiving the data in csv format through various processes, QC (Quality Control) of the data and data analysis were performed. To better understand data, exploratory data analyses (EDA) and data mining were conducted. Based on the information on data, the machine learning model to explain the data moset was selcted. All findings were shared with biostatisticians, medical doctors, biologists, neurologists and epidemiologists at the neurology department and the Taub Institute for Research on Alzheimer’s Disease and the Aging Brain in the Columbia University Irving Medical Center."
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html#analytics",
    "href": "docs/projects/LLFS/project_description.html#analytics",
    "title": "Description",
    "section": "Analytics",
    "text": "Analytics\n\n\n\n\nflowchart TB\n    subgraph Data_Analytics\n        direction TB\n        Exploratory_Data_Analysis---\n        Data_Minig---\n        Statistical_Analysis---\n        Machine_Learning\n    end\n\n\n\n\n\n\n\n\nData 분석은 크게 EDA (Exploratory Data Analysis), Statistical Analysis 및 Machine Learning, 이렇게 3 단계로 수행했다. 각 단계에서 나온 결과가 각 각의 단계에서 일관되게 나오는지 확인하는 분석을 수행했다.\nData analysis was performed in three stages: Exploratory Data Analysis (EDA), Statistical Analysis, and Machine Learning. An analysis was performed to ensure that the results from each stage were consistent at each stage.\n\nEDA and Data Mining\nstudent t tests, Wilcoxon Man Whiteney tests, \\(\\chi^2\\) tests, Fisher Exact Tests, ANOVAs, Kruskal Wallis Tests 및 regression testing이 수행됐고 visualization을 통해 검정 결과를 재확인하는 작업을 수행했다. 고차원 데이터를 시각화하여 data의 pattern 관찰하기 위해 KNN, PCA, K means, Clustering 및 DB Scan을 이용했다.\nStudent t tests, Wilcoxon Man Whiteney tests, \\(\\chi^2\\) tests, Fisher Exact Tests, ANOVAs, Kruskal Wallis Tests, and regression testing were performed, and I visualizaed data to reconfirm the test results. To visualize high-dimensional data and observe data patterns, KNN, PCA, K means, Clustering, and DB Scan were used.\n\n\nStatistical Analysis\nmultivariable linear regression, logistic regression 및 Cox PH(Proportional Hazards) regression anayses 가 수행됐고 질병과 유의한 metabolites를 선별했다. multiple testing으로 인해 1 종 오류를 범하는 것을 줄이기 위해 permuted p-values를 계산하여 유의성을 한번 더 확인했다.\nMultivariable linear regression, logistic regression, and Cox PH (Proportional Hazards) regression analyses were conducted and the metabolites that are signficantly associated with the disease status were selected. In order to reduce the possibility of making a type 1 error due to multiple testing, the significance was confirmed once more by calculating permuted p-values.\n\n\nMachine Learning\nLasso, ridge regression, elastic net, decision tree, random rorests, ada boosting, gradient descent boosting, SVM (support vector machine), partial least square 및 sparse partial least square 사용됐다. 질병을 가장 잘 예측하는 classifier를 평가하여 최적의 classifier를 선택했다.\nLasso, ridge regression, elastic net, decision tree, random rorests, ada boosting, gradient descent boosting, support vector machine (SVM), partial least square, and sparse partial least square were used. The optimal classifier was selected by evaluating the classifier that best predicted the disease status."
  },
  {
    "objectID": "docs/projects/index.html",
    "href": "docs/projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\n\nPLAN, MATH, SP, STAT, ML, and DL stands for 'Intellectual Property Planning', 'Mathematics', 'Signal Processing', 'Statistics', 'Machine Learning', and \"Deep Learning\", respectively.\n\n\n\n[STAT][ML] LLFS (Long Life Family Study) (working on)\n[PLAN] Platform IP Planning (To be written)\n[ML] Data-Driven Diagnostic Algorithm (To be written)\n[STAT][SP] Clinical Data Analysis for QC (To be written)\n[STAT][ML] Diagnostic Device QC Platform (To be written)\n[STAT][ML] Noise Test Result Prediction (To be written)\n[ML][MATH][Biology] Heavy Metal Removal Algorihtm using Tea Leaves (To be written)\n[ML] Diffusion Model of Social Networks using Genetic Algorithm (To be written)\n[Biochemistry] Effects of Phellinus Linteus toward Formation of Lymphatic Vessel (To be written)\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-07-anova/index.html#anova",
    "href": "docs/blog/posts/statistics/2023-01-07-anova/index.html#anova",
    "title": "ANOVA",
    "section": "4 ANOVA",
    "text": "4 ANOVA\nA basic idea of the ANOVA is to partition variation. It is not concerend with analyzing variances but with analyzing variation in means. To be specific, it is a method of estimating the means of several populations that are often assumed to be normally distributed.\n\n4.1 One Way ANOVA\n\nwe\nAssumptions\n\n\\[ Y_{ij}=\\mu_i + \\epsilon_{ij}, \\space \\space i=1, ..., k, \\space j=1, ...,n_i \\] \\[ EY_{ij}=\\mu_i, \\space \\space , i=1, ..., k, \\space j=1, ...,n_i \\]\nwhere the \\(\\mu_i\\) are unkown parameters and the \\(\\epsilon_{ij}\\) are error random variables.\n\n\\(\\text{E}\\epsilon_{ij}=0\\), \\(\\text{Var}\\epsilon_{ij}=0<\\infty\\), for all \\(i, j\\)\n\\(\\text{Cov}(\\epsilon_{ij},\\epsilon_{i'j'})=0\\), for all \\(i, i', j\\), and \\(j'\\) unless \\(i=i'\\) and \\(j=j'\\).\nThe \\(\\epsilon_{ij}\\) are independent and normally distributed (normal errors).\n\\(\\sigma^2_{i}=\\sigma^2\\) for all \\(i\\) (homoscedasticity)"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_stack/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-19_stack/index.html",
    "title": "Data Structure (5) Stack",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n다양한 알고리즘과 프로그램에서 사용됨\n스택: 먼저 들어온 데이터가 나중에 나가는 자료구조\n흔히 박스가 쌓인 형태를 스택(stack)이라고 한다. 예) ‘Deep Learning 알고리즘의 구조가 stacked 되어 있는 구조다’ 라고 표현\n\n우리가 박스를 쌓은 뒤에 꺼낼 때는, 가장 마지막에 올렸던 박스부터 꺼내야 한다.\n\n새로운 원소를 삽입할 때는 마지막 위치에 삽입한다. (가장 최근에 삽입된 원소가 가장 끝에 위치)\n새로운 원소를 삭제할 때는 마지막 원소가 삭제된다. (가장 최근에 삽입된 원소가 제거됨)\nhead = 최상위 원소 = 가장 최근에 삽입이된 원소\n\n\n\n\n\n스택은 굉장히 기본적인 자료구조이다.\n기계 학습 분야뿐 아니라 다양한 프로그램을 개발할 때 빠지지 않고 사용된다.\n\n\n\n\n\n스택은 여러 가지 연산을 제공한다.\n\n\n\nTable 1: a list of the stack functions in Python\n\n\n\n\n\n\n\n\nNumber\nMethods\nTime Complexity\nDescription\n\n\n\n\n1\n삽입(Push)\n\\(O(1)\\)\n스택에 원소를 삽입하는 연산\n\n\n2\n추출(Pop)\n\\(O(1)\\)\n스택에서 원소를 추출하는 연산\n\n\n3\n최상위 원소 (Top)\n\\(O(1)\\)\n스택의 최상위 원소(마지막에 들어온 원소) 를 확인(조회)하는 연산\n\n\n4\nEmpty\n\\(O(1)\\)\n스택이 비어 있는지 확인하는 연산\n\n\n\n\nSee Table 1.\n\n\n\n\n파이썬의 기본적인 리스트 자료형은 다음의 두 가지 메서드를 제공한다.\nappend() 메서드: 마지막 위치에 원소를 삽입하며, 시간 복잡도는 \\(O(1)\\) 이다.\npop() 메서드: 마지막 위치에서 원소를 추출하며, 시간 복잡도는 \\(O(1)\\) 이다.\n따라서 일반적으로 스택을 구현할 때, 파이썬의 리스트(list) 자료형을 사용한다.\n\n\n\nCode\nclass Stack:\n    def __init__(self):\n        self.stack = []\n\n    def push(self, data):\n        # 마지막 위치에 원소 삽입\n        self.stack.append(data)\n\n    def pop(self):\n        if self.is_empty():\n            return None\n        # 마지막 원소 추출\n        return self.stack.pop()\n\n    def top(self):\n        if self.is_empty():\n            return None\n        # 마지막 원소 반환\n        return self.stack[-1]\n\n    def is_empty(self):\n        return len(self.stack) == 0\n\n\nstack = Stack()\narr = [9, 7, 2, 5, 6, 4, 2]\nfor x in arr:\n    stack.push(x)\n\nwhile not stack.is_empty():\n    print(stack.pop())\n\n\n2\n4\n6\n5\n2\n7\n9\n\n\n\n\n\n\n스택을 연결 리스트로 구현하면, 삽입과 삭제에 있어서 \\(O(1)\\) 을 보장한다.\n연결 리스트로 구현할 때는 머리(head)를 가리키는 하나의 포인터만 가진다.\n머리(head): 남아있는 원소 중 가장 마지막에 들어 온 데이터를 가리키는 포인터\n\n\n\n\n삽입할 때는 기존의 머리 뒤에 데이터가 들어가고 포인터가 가장 최근에 삽입된 데이터를 가리키도록 머리(head) 위치를 바꿔준다.\n삭제할 때는 머리(head) 위치에서 데이터를 꺼낸다.\n\n즉, 포인터를 삭제할 데이터에 앞에 있는 데이터로 머리 위치를 바꾸는 것만으로 삭제는 이루어진다.\n\n\n\n\n\n\n삭제할 때는 머리(head) 위치에서 데이터를 꺼낸다.\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\n\nclass Stack:\n    def __init__(self):\n        self.head = None\n\n    # 원소 삽입\n    def push(self, data):\n        node = Node(data)\n        node.next = self.head\n        self.head = node\n\n    # 원소 추출하기\n    def pop(self):\n        if self.is_empty():\n            return None\n\n        # 머리(head) 위치에서 노드 꺼내기\n        data = self.head.data\n        self.head = self.head.next\n\n        return data\n\n    # 최상위 원소(top)\n    def top(self):\n        if self.is_empty():\n            return None\n        return self.head.data\n\n    # 먼저 추출할 원소부터 출력\n    def show(self):\n        cur = self.head\n        while cur:\n            print(cur.data, end=\" \")\n            cur = cur.next\n\n    # 스택이 비어있는지 확인\n    def is_empty(self):\n        return self.head is None\n\n\nstack = Stack()\narr = [9, 7, 2, 5, 6, 4, 2]\nfor x in arr:\n    stack.push(x)\nstack.show()\nprint()\n\nwhile not stack.is_empty():\n    print(stack.pop())\n\n\n2 4 6 5 2 7 9 \n2\n4\n6\n5\n2\n7\n9"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_graph/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-20_graph/index.html",
    "title": "Data Structure (10) Graph",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n• 그래프(graph)란 사물을 정점(vertex)과 간선(edge)으로 나타내기 위한 도구다.\n• 그래프는 두 가지 방식으로 구현할 수 있다.\n\n인접 행렬(adjacency matrix): 2차원 배열을 사용하는 방식\n인접 리스트(adjacency list): 연결 리스트를 이용하는 방식\n\n\n\n\n• 인접 행렬(adjacency matrix)에서는 그래프를 2차원 배열로 표현한다.\n\n\n• 모든 간선이 방향성을 가지지 않는 그래프를 무방향 그래프라고 한다.\n• 모든 간선에 가중치가 없는 그래프를 무가중치 그래프라고 한다.\n• 무방향 무가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 행렬로 출력할 수 있다.\n\n\n\n• 모든 간선이 방향을 가지는 그래프를 방향 그래프라고 한다.\n• 모든 간선에 가중치가 있는 그래프를 가중치 그래프라고 한다.\n• 방향 가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 행렬로 출력할 수 있다.\n\n\n\n\n• 인접 리스트(adjacency list)에서는 그래프를 리스트로 표현한다.\n\n\n• 모든 간선이 방향성을 가지지 않는 그래프를 무방향 그래프라고 한다.\n• 모든 간선에 가중치가 없는 그래프를 무가중치 그래프라고 한다.\n• 무방향 무가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 리스트로 출력할 수 있다.\n\n\n\n• 모든 간선이 방향을 가지는 그래프를 방향 그래프라고 한다.\n• 모든 간선에 가중치가 있는 그래프를 가중치 그래프라고 한다.\n• 방향 가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 리스트로 출력할 수 있다.\n\n\n\n\n\n인접 행렬: 모든 정점들의 연결 여부를 저장해 O V\n\n2 의 공간을 요구한다.\n• 공간 효율성이 떨어지지만, 두 노드의 연결 여부를 O 1 에 확인할 수 있다.\n\n인접 리스트: 연결된 간선의 정보만을 저장하여 O V + E 의 공간을 요구한다.\n\n• 공간 효율성이 우수하지만, 두 노드의 연결 여부를 확인하기 위해 O V 의 시간이 필요하다.\n\n\nTable 1: a list of the stack functions in Python\n\n\n\n\n\n\n\n\nNumber\nCategory\n필요한 메모리\n연결 여부 확인\n\n\n\n\n1\n인접 행렬\n\\(O(V^2)\\)\n\\(O(1)\\)\n\n\n2\n인접 리스트\n\\(O(V+E)\\)\n\\(O(V)\\)\n\n\n\n\nSee Table 1.\n\n\n\n• 최단 경로 알고리즘을 구현할 때, 어떤 자료구조가 유용할까?\n• 각각 근처의 노드와 연결되어 있는 경우가 많으므로, 간선 개수가 적어 인접 리스트가 유리하다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html",
    "title": "Data Structure (8) Binary Search Tree",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n트리는 가계도와 같이 계층적인 구조를 표현할 때 사용할 수 있는 자료구조다.\n나무(tree)의 형태를 뒤집은 것과 같이 생겼다.\n다수의 데이터를 관리하기에 적합한 트리 자료 구조의 가장 기본적인 형태\n\n\n\n\n\n루트 노드(root node): 부모가 없는 최상위 노드\n단말 노드(leaf node): 자식이 없는 노드\n트리(tree)에서는 부모와 자식 관계가 성립한다 (직계).\n형제 관계 (sibling, 방계): 부모 node로 부터 왼쪽 자식과 오른쪽 자식과의 관계\n깊이(depth): 루트 노드에서의 길이(length), 루트 노드로부터 손자까지의 depth=2\n\n이때, 길이란 출발 노드에서 목적지 노드까지 거쳐야 하는 간선의 수를 의미한다.\n\n트리의 높이(height)은 루트 노드에서 가장 깊은 노드까지의 길이를 의미한다.\n\n\n\n\n\n이진 트리는 최대 2개의 자식을 가질 수 있는 트리를 말한다.\n\n\n\n\n\n다수의 데이터를 관리(조회, 저장, 삭제)하기 위한 가장 기본적인 자료구조 중 하나다.\n이진 탐색 트리의 성질: 순서가 있음\n\n왼쪽 자식 노드 < 부모 노드 < 오른쪽 자식 노드\n루트 노드 기준 모든 왼쪽 노드들은 루트 노드보다 작음\n루트 노드 기준 모든 오른쪽 노드들은 루트 노드보다 큼\n2진 탐색을 가능하게 하는 구조\n\n\n\n\n\n특정한 노드의 키(key) 값보다 그 왼쪽 자식 노드의 키(key) 값이 더 작다.\n특정한 노드의 키(key) 값보다 그 오른쪽 자식 노드의 키(key) 값이 더 크다.\n특정한 노드의 왼쪽 서브 트리, 오른쪽 서브 트리 모두 이진 탐색 트리다.\nworst case: 찾는게 없을 때 혹은 찾고자 하는 데이터가 가장 마지막에 있을 때\n\n탐색시 재귀적으로 중앙값을 기준으로 오른쪽만 찾음\n매 실행마다 데이터의 개수가 절반씩 줄어듬\n그러면, 몇 번만에 사이즈가 1이 되는가?\n수식 유도, input size를 N이라고 가정했을때\n\\(N \\times {(\\frac{1}{2})}^{k}=1 \\rightarrow N=2^k \\rightarrow k = log_2N\\)\n위의 수식을 점근적 표기법으로 표현하면 \\(\\Theta(logN)\\)\n\nbest case: 한번에 찾았을 때\n\n\\(\\Theta(1)\\)\n\n그러므로, lower bound = \\(\\Theta(1)\\), upper bound = \\(O(logN)\\)\n\n\n\n\n\n루트 노드에서 출발하여 아래쪽으로 내려오면서, 삽입할 위치를 찾는다.\n\n삽입할 노드의 키(key)가 작으면 왼쪽으로,\n삽입할 노드의 키(key)가 크면 오른쪽으로 삽입\n\n삽입할 노드 목록 예시: [7,4,5,9,6,2,3,2,8]으로 트리 생성해보기\n\n\n\n\nBinary Tree\n\n\nSorcue: 코딩 테스트를 위한 트리(Tree) 자료구조 10분 핵심 요약 By 동빈나\n\n\n\n\n루트 노드에서 출발하여 아래쪽으로 내려오면서, 찾고자 하는 원소를 조회한다. 삽입 연산과 같은 로직을 따름\n1 삽입할 노드의 키(key)가 작으면 왼쪽으로, 2 삽입할 노드의 키(key)가 크면 오른쪽으로 조회\n조회할 노드 목록 예시: 5번 노드\n\n\n\n\n\n루트 노드에서 출발하여 아래쪽으로 내려오면서, 삭제할 원소에 접근한다.\n삭제할 노드 목록 예시: 7번 노드\n\nCase #1 왼쪽 자식이 없는 경우 → 오른쪽 자식으로 대체\nCase #2 오른쪽 자식이 없는 경우 → 왼쪽 자식으로 대체\nCase #3 왼쪽, 오른쪽이 모두 있는 경우 → 오른쪽 서브\n\n트리에서 가장 작은 노드로 대체\n삭제할 노드 목록 예시: 4번 노드\n\n\n\n\nBinary Tree Deletion\n\n\nSorcue: 코딩 테스트를 위한 트리(Tree) 자료구조 10분 핵심 요약 By 동빈나\n\n\n\n\n\n트리에 포함되어 있는 정보를 모두 출력하고자 할 때, 어떤 방식을 사용할 수 있을까?\n바로 순회(traversal)를 사용할 수 있다.\n트리의 모든 노드를 특정한 순서(조건)에 따라서 방문하는 방법을 순회(traversal)라고 한다.\n\n\n전위 순회(pre-order traverse): 루트 방문 → 왼쪽 자식 방문 → 오른쪽 자식 방문\n중위 순회(in-order traverse): 왼쪽 자식 방문 → 루트 방문 → 오른쪽 자식 방문\n후위 순회(post-order traverse): 왼쪽 자식 방문 → 오른쪽 자식 방문 → 루트 방문\n\n\n\n\n전위 순회(pre-order traverse): A → B → D → E → C → F → G\n중위 순회(in-order traverse): D → B → E → A → F → C → G\n후위 순회(post-order traverse): D → E → B → F → G → C → A\n\n\n\n\nBinary Tree Traverse\n\n\nSorcue: 코딩 테스트를 위한 트리(Tree) 자료구조 10분 핵심 요약 By 동빈나\n\n\n\n• 방문 방법: 현재 노드 → 왼쪽 자식 노드 → 오른쪽 자식 노드\n\n\nCode\ndef _preorder(self, node):\n  if node:\n    print(node.key, end=' ')\n    self._preorder(node.left)\n    self._preorder(node.right)\n\n\n\n\n\n\n방문 방법: 왼쪽 자식 노드 → 현재 노드 → 오른쪽 자식 노드\n\n\n\nCode\ndef _inorder(self, node):\n  if node:\n    self._inorder(node.left)\n    print(node.key, end=' ')\n    self._inorder(node.right)\n\n\n\n\n\n\n방문 방법: 왼쪽 자식 노드 → 오른쪽 자식 노드 → 현재 노드\n\n\n\nCode\ndef _postorder(self, node):\n  if node:\n    self._postorder(node.left)\n    self._postorder(node.right)\n    print(node.key, end=' ')\n\n\n\n\n\n\n낮은 레벨(루트)부터 높은 레벨까지 순차적으로 방문한다.\n단순히 루트 노드에서부터 너비 우선 탐색(BST)를 진행하면 된다.\n레벨 순회 순회(level-order traverse): A → B → C → D → E → F → G\n\n\n\n\n\n\n다른 메서드 안에서 사용되는 메서드는 이름 앞에 언더바(_) 기호를 붙인다.\n\n\n\nCode\ndef search(self, node, key):\n  return self._search(self.root, key) # search: recursively 조회\n\ndef _search(self, node, key):\n  if node is None or node.key == key:\n    return node\n\n  # 현재 노드의 key보다 작은 경우\n  if node.key > key:\n    return self._search(node.left, key)\n\n  # 현재 노드의 key보다 큰 경우\n  elif node.key < key:\n    return self._search(node.right, key)\n\n\n\n\n\n편향 이진 트리는 다음의 두 가지 속성을 가진다.\n\n\n같은 높이의 이진 트리 중 최소 개수의 노드 개수를 가진다.\n왼쪽 혹은 오른쪽으로 한 방향에 대한 서브 트리를 가진다.\n\n\n\n\n\n노드의 개수가 N개일 때, 시간 복잡도는 다음과 같다.\n트리의 높이(height)을 H라고 할 때, 엄밀한 시간 복잡도는 \\(O(H)\\) 다.\n이상적인 경우 H = log2 N로 볼 수 있다.\n하지만 최악의 경우(편향된 경우) H = N로 볼 수 있다.\n\n\n\nTable 1: a list of the time complexity of the binary search trees in Python\n\n\n\n\n\n\n\n\n\n\nNumber\nMethods\n조회\n삽입\n삭제\n수정\n\n\n\n\n1\n균형 잡힌 이진 탐색 트리\n\\(O(logN)\\)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\n\n2\n편향 이진 탐색 트리\n\\(O(N)\\)\n\\(O(N)\\)\n\\(O(N)\\)\n\\(O(N)\\)\n\n\n\n\nSee Table 1.\n\n\n\n\nAVL stands for Adelson-Velsky and Landis\n이진 탐색 트리는 편향 트리가 될 수 있으므로, 최악의 경우 \\(O(N)\\) 을 요구한다.\n반면에 AVL 트리는 균형이 갖춰진 이진 트리다.\n간단한 구현 과정으로 완전 이진 트리에 가까운 형태를 유지하도록 한다.\n\n\n\nCode\nfrom collections import deque\n\n\nclass Node:\n    def __init__(self, key):\n        self.key = key\n        self.left = None\n        self.right = None\n\n\nclass BinarySearchTree:\n    def __init__(self):\n        self.root = None\n\n    def search(self, node, key):\n        return self._search(self.root, key)\n\n    def _search(self, node, key):\n        if node is None or node.key == key:\n            return node\n\n        # 현재 노드의 key보다 작은 경우\n        if node.key > key:\n            return self._search(node.left, key)\n        # 현재 노드의 key보다 큰 경우\n        elif node.key < key:\n            return self._search(node.right, key)\n\n    def insert(self, key):\n        self.root = self._insert(self.root, key)\n\n    def _insert(self, node, key):\n        if node is None:\n            return Node(key)\n\n        # 현재 노드의 key보다 작은 경우\n        if node.key > key:\n            node.left = self._insert(node.left, key)\n        # 현재 노드의 key보다 큰 경우\n        elif node.key < key:\n            node.right = self._insert(node.right, key)\n\n        return node\n\n    def delete(self, key):\n        self.root = self._delete(self.root, key)\n\n    def _delete(self, node, key):\n        if node is None:\n            return None\n\n        # 현재 노드의 key보다 작은 경우\n        if node.key > key:\n            node.left = self._delete(node.left, key)\n        # 현재 노드의 key보다 큰 경우\n        elif node.key < key:\n            node.right = self._delete(node.right, key)\n        # 삭제할 노드를 찾은 경우\n        else:\n            # 왼쪽 자식이 없는 경우\n            if node.left is None:\n                return node.right\n            # 오른쪽 자식이 없는 경우\n            elif node.right is None:\n                return node.left\n            # 왼쪽과 오른쪽 자식 모두 있는 경우\n            node.key = self._get_min(node.right)\n            node.right = self._delete(node.right, node.key)\n\n        return node\n\n    def _get_min(self, node):\n        key = node.key\n        while node.left:\n            key = node.left.key\n            node = node.left\n        return key\n\n    def preorder(self):\n        self._preorder(self.root)\n\n    def _preorder(self, node):\n        if node:\n            print(node.key, end=' ')\n            self._preorder(node.left)\n            self._preorder(node.right)\n\n    def inorder(self):\n        self._inorder(self.root)\n\n    def _inorder(self, node):\n        if node:\n            self._inorder(node.left)\n            print(node.key, end=' ')\n            self._inorder(node.right)\n\n    def postorder(self):\n        self._postorder(self.root)\n\n    def _postorder(self, node):\n        if node:\n            self._postorder(node.left)\n            self._postorder(node.right)\n            print(node.key, end=' ')\n\n    def levelorder(self):\n        return self._levelorder(self.root)\n\n    def _levelorder(self, node):\n        if node is None:\n            return\n\n        result = []\n\n        queue = deque()\n        queue.append((0, node))  # (level, node)\n\n        while queue:\n            level, node = queue.popleft()\n            if node:\n                result.append((level, node.key))\n                queue.append((level + 1, node.left))\n                queue.append((level + 1, node.right))\n\n        for level, key in result:\n            print(f\"level: {level}, key: {key}\")\n\n    def to_list(self):\n        return self._to_list(self.root)\n\n    def _to_list(self, node):\n        if node is None:\n            return []\n        return self._to_list(node.left) + [node.key] + self._to_list(\n            node.right)\n\n\narr = [7, 4, 5, 9, 6, 3, 2, 8]\nbst = BinarySearchTree()\nfor x in arr:\n    bst.insert(x)\nprint('전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nbst.delete(7)\nprint('\\n전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nbst.delete(4)\nprint('\\n전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nbst.delete(3)\nprint('\\n전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nprint(bst.to_list())\n\n\n전위 순회: 7 4 3 2 5 6 9 8 \n중위 순회: 2 3 4 5 6 7 8 9 \n후위 순회: 2 3 6 5 4 8 9 7 \n[레벨 순회]\nlevel: 0, key: 7\nlevel: 1, key: 4\nlevel: 1, key: 9\nlevel: 2, key: 3\nlevel: 2, key: 5\nlevel: 2, key: 8\nlevel: 3, key: 2\nlevel: 3, key: 6\n\n전위 순회: 8 4 3 2 5 6 9 \n중위 순회: 2 3 4 5 6 8 9 \n후위 순회: 2 3 6 5 4 9 8 \n[레벨 순회]\nlevel: 0, key: 8\nlevel: 1, key: 4\nlevel: 1, key: 9\nlevel: 2, key: 3\nlevel: 2, key: 5\nlevel: 3, key: 2\nlevel: 3, key: 6\n\n전위 순회: 8 5 3 2 6 9 \n중위 순회: 2 3 5 6 8 9 \n후위 순회: 2 3 6 5 9 8 \n[레벨 순회]\nlevel: 0, key: 8\nlevel: 1, key: 5\nlevel: 1, key: 9\nlevel: 2, key: 3\nlevel: 2, key: 6\nlevel: 3, key: 2\n\n전위 순회: 8 5 2 6 9 \n중위 순회: 2 5 6 8 9 \n후위 순회: 2 6 5 9 8 \n[레벨 순회]\nlevel: 0, key: 8\nlevel: 1, key: 5\nlevel: 1, key: 9\nlevel: 2, key: 2\nlevel: 2, key: 6\n[2, 5, 6, 8, 9]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_deque/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-19_deque/index.html",
    "title": "Data Structure (7) Deque",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n덱은 스택(stack)과 큐(queue)의 기능을 모두 가지고 있다.\n그래서, 스택과 큐대신 덱을 사용해도 괜찮음\n다만, 포인터 변수가 더 많이 필요하기 때문에, 메모리는 상대적으로 더 많이 필요하다.\nPython에서는 큐(queue)의 기능이 필요할 때 간단히 덱(deque)을 사용한다.\n데이터의 삭제와 삽입 모두에서 \\(O(1)\\) 의 시간 복잡도가 소요된다.\n덱에 여러 개의 데이터를 삽입하고 삭제하는 예시를 확인해 보자.\n\n[12개의 전체 연산]\n\n좌측으로부터 삽입 연산이 가능\n우측으로부터 삽입 연산이 가능\n삭제 연산시 우측/좌측 선택적 삭제가 가능\n\n\n\n\n• 데이터의 삭제와 삽입 모두에서 \\(O(1)\\) 의 시간 복잡도가 소요된다.\n\n\nTable 1: a list of the deque functions in Python\n\n\n\n\n\n\n\n\nNumber\nMethods\nTime Complexity\nDescription\n\n\n\n\n1\nappend left\n\\(O(1)\\)\n덱의 가장 왼쪽에 새 데이터를 삽입\n\n\n2\npop left\n\\(O(1)\\)\n덱의 가장 왼쪽에서 데이터를 추출\n\n\n3\nappend right\n\\(O(1)\\)\n덱의 가장 오른쪽에 새 데이터를 삽입\n\n\n4\npop right\n\\(O(1)\\)\n덱의 가장 오른쪽에서 데이터를 추출\n\n\n\n\nSee Table 1.\n\n\n\n\nPython에서는 덱(deque) 라이브러리를 사용할 수 있다.\n아래의 모든 메서드는 최악의 경우 시간 복잡도 O 1 을 보장한다.\n우측 삽입: append()\n좌측 삽입: appendleft()\n우측 추출: pop()\n좌측 추출: popleft()\n\n\n\nCode\nfrom collections import deque\n\n\nd = deque()\narr = [5, 6, 7, 8] \nfor x in arr:\n    d.append(x) # 오른쪽 삽입\narr = [4, 3, 2, 1]\nfor x in arr:\n    d.appendleft(x) # 좌측 삽입\nprint(d)\n\nwhile d:\n    print(d.popleft()) # 좌측 삭제\n\narr = [1, 2, 3, 4, 5, 6, 7, 8]\nfor x in arr:\n    d.appendleft(x)\nprint(d)\n\nwhile True:\n    print(d.pop())\n    if not d:\n        break\n    print(d.popleft())\n    if not d:\n        break\n\n\ndeque([1, 2, 3, 4, 5, 6, 7, 8])\n1\n2\n3\n4\n5\n6\n7\n8\ndeque([8, 7, 6, 5, 4, 3, 2, 1])\n1\n8\n2\n7\n3\n6\n4\n5\n\n\n\n\n\n기본적인 Python의 리스트 자료형은 큐(queue)의 기능을 제공하지 않는다.\n가능하다면 Python에서 제공하는 덱(deque) 라이브러리를 사용한다.\n큐(queue)의 기능이 필요할 때는 덱 라이브러리를 사용하는 것을 추천한다.\n삽입과 삭제에 대하여 모두 시간 복잡도 \\(O(1)\\) 이 요구된다.\n\n\n\n\n\n\n덱(deque)을 연결 리스트로 구현하면, 삽입과 삭제에 있어서 O 1 을 보장할 수 있다.\n연결 리스트로 구현할 때는 앞(front)과 뒤(rear) 두 개의 포인터를 가진다.\n앞(front): 가장 좌측에 있는 데이터를 가리키는 포인터\n뒤(rear): 가장 우측에 있는 데이터를 가리키는 포인터\n삽입과 삭제의 구현 방법은 스택 및 큐와 유사하다.\n앞(front)과 뒤(rear)에 대하여 대칭적으로 로직이 구현될 수 있다.\n\n\n\n\n좌측 삽입할 때는 앞(front) 위치에 데이터를 넣는다.\n새로운 데이터가 삽입되었을 때 front data와 연결이 먼저 된 후 front data의 이전 노드가 새로운 데이터가 되도록 설정\n\n\n\n\n\n삭제할 때는 앞(front) 위치에서 데이터를 꺼낸다. 즉, 그냥 front를 그 다음 데이터로 설정하면 됨\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.prev = None\n        self.next = None\n\n\nclass Deque:\n    def __init__(self):\n        self.front = None\n        self.rear = None\n        self.size = 0\n\n    def appendleft(self, data):\n        node = Node(data)\n        if self.front == None:\n            self.front = node\n            self.rear = node\n        else:\n            node.next = self.front\n            self.front.prev = node\n            self.front = node\n        self.size += 1\n\n    def append(self, data):\n        node = Node(data)\n        if self.rear == None:\n            self.front = node\n            self.rear = node\n        else:\n            node.prev = self.rear\n            self.rear.next = node\n            self.rear = node\n        self.size += 1\n\n    def popleft(self):\n        if self.size == 0:\n            return None\n        # 앞에서 노드 꺼내기\n        data = self.front.data\n        self.front = self.front.next\n        # 삭제로 인해 노드가 하나도 없는 경우\n        if self.front == None:\n            self.rear = None\n        else:\n            self.front.prev = None\n        self.size -= 1\n        return data\n\n    def pop(self):\n        if self.size == 0:\n            return None\n        # 뒤에서 노드 꺼내기\n        data = self.rear.data\n        self.rear = self.rear.prev\n        # 삭제로 인해 노드가 하나도 없는 경우\n        if self.rear == None:\n            self.front = None\n        else:\n            self.rear.next = None\n        self.size -= 1\n        return data\n\n    def front(self):\n        if self.size == 0:\n            return None\n        return self.front.data\n\n    def rear(self):\n        if self.size == 0:\n            return None\n        return self.rear.data\n\n    # 앞에서부터 원소 출력\n    def show(self):\n        cur = self.front\n        while cur:\n            print(cur.data, end=\" \")\n            cur = cur.next\n\n\nd = Deque()\narr = [5, 6, 7, 8]\nfor x in arr:\n    d.append(x)\narr = [4, 3, 2, 1]\nfor x in arr:\n    d.appendleft(x)\nd.show()\n\nprint()\nwhile d.size != 0:\n    print(d.popleft())\n\narr = [1, 2, 3, 4, 5, 6, 7, 8]\nfor x in arr:\n    d.appendleft(x)\nd.show()\n\nprint()\nwhile True:\n    print(d.pop())\n    if d.size == 0:\n        break\n    print(d.popleft())\n    if d.size == 0:\n        break\n\n\n1 2 3 4 5 6 7 8 \n1\n2\n3\n4\n5\n6\n7\n8\n8 7 6 5 4 3 2 1 \n1\n8\n2\n7\n3\n6\n4\n5"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_queue/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-19_queue/index.html",
    "title": "Data Structure (6) Queue",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n큐(queue)는 먼저 삽입된 데이터가 먼저 추출되는 자료구조(data structure)다. (First-In First-Out)\n딥러닝 모델에 들어가는 데이터 순서대로 들어가는데 먼저 들어간 데이터는 먼저 나오게 할때 사용되는 자료 구조이다.\n\n\n\n\n\n큐를 연결 리스트로 구현하면, 삽입과 삭제에 있어서 \\(O(1)\\) 을 보장할 수 있다.\n연결 리스트로 구현할 때는 머리(head)와 꼬리(tail) 두 개의 포인터를 가진다.\n머리(head): 남아있는 원소 중 가장 먼저 들어 온 데이터를 가리키는 포인터\n꼬리(tail): 남아있는 원소 중 가장 마지막에 들어 온 데이터를 가리키는 포인터\n\n\n\n\n삽입할 때는 꼬리(tail) 위치에 데이터를 넣는다.\n값으로 8을 갖는 새로운 데이터가 삽입되었을 때 예시)\n\n\n\n\n\n삭제할 때는 머리(head) 위치에서 데이터를 꺼낸다.\n하나의 데이터를 삭제할 때의 예시)\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\n\nclass Queue:\n    def __init__(self):\n        self.head = None\n        self.tail = None\n\n    def enqueue(self, data):\n        node = Node(data)\n        if self.head == None:\n            self.head = node\n            self.tail = node\n        # 꼬리(tail) 위치에 새로운 노드 삽입\n        else:\n            self.tail.next = node\n            self.tail = self.tail.next\n\n    def dequeue(self):\n        if self.head == None:\n            return None\n\n        # 머리(head) 위치에서 노드 꺼내기\n        data = self.head.data\n        self.head = self.head.next\n\n        return data\n\n    def show(self):\n        cur = self.head\n        while cur:\n            print(cur.data, end=\" \")\n            cur = cur.next\n\n\nqueue = Queue()\ndata_list = [3, 5, 9, 8, 5, 6, 1, 7]\n\nfor data in data_list:\n    queue.enqueue(data)\n\nprint(\"\\n전체 노드 출력:\", end=\" \")\nqueue.show()\n\nprint(\"\\n[원소 삭제]\")\nprint(queue.dequeue())\nprint(queue.dequeue())\nprint(queue.dequeue())\n\nprint(\"[원소 삽입]\")\nqueue.enqueue(2)\nqueue.enqueue(5)\nqueue.enqueue(3)\n\nprint(\"전체 노드 출력:\", end=\" \")\nqueue.show()\n\n\n\n전체 노드 출력: 3 5 9 8 5 6 1 7 \n[원소 삭제]\n3\n5\n9\n[원소 삽입]\n전체 노드 출력: 8 5 6 1 7 2 5 3 \n\n\n\n\n\n\n다수의 데이터를 삽입 및 삭제할 때에 대하여, 수행 시간을 측정할 수 있다.\n단순히 Python의 리스트 자료형을 이용할 때보다 수행 시간 관점에서 효율적이다.\n\n\n\nCode\nimport time\n\ndata_list = [i for i in range(100000)]\n\nstart_time = time.time()\n\nqueue = []\nfor data in data_list:\n    queue.append(data)\nwhile queue:\n    queue.pop(0)\n\nprint(f\"Elapsed time: {time.time() - start_time} seconds.\")\nprint(queue)\n\nstart_time = time.time()\n\nqueue = Queue()\nfor data in data_list:\n    queue.enqueue(data)\nwhile queue.head != None:\n    queue.dequeue()\n\nprint(f\"Elapsed time: {time.time() - start_time} seconds.\")\nqueue.show()\n\n\nElapsed time: 0.8915739059448242 seconds.\n[]\nElapsed time: 0.10299944877624512 seconds."
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#docker",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#docker",
    "title": "Content List, Engineering",
    "section": "4 Docker",
    "text": "4 Docker\n\n2023-01-30, Docker Install\n2023-01-31, Docker Compose\n2023-02-01, Docker Container"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#data-structure",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#data-structure",
    "title": "Content List, Engineering",
    "section": "2 Data Structure",
    "text": "2 Data Structure\n\n2023-01-17, Overview\n2023-01-18, Array\n2023-01-18, Linked List\n2023-01-18, Python List\n2023-01-19, Stack\n2023-01-19, Queue\n2023-01-26, Deque\n2023-01-26, Binary Search Tree\n2023-01-20, Priority Queue\n2023-01-20, Graph"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#conda",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#conda",
    "title": "Content List, Engineering",
    "section": "3 Conda",
    "text": "3 Conda"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#dynamic-documentation",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#dynamic-documentation",
    "title": "Content List, Engineering",
    "section": "5 Dynamic Documentation",
    "text": "5 Dynamic Documentation\n\n2023-01-19, Quarto\n2023-01-19, xaringan[R]\n2023-01-19, Bookdown[R]\n2023-01-19, DISTL\n2023-01-26, Sphinx[Python]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#aws-cloud",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#aws-cloud",
    "title": "Content List, Engineering",
    "section": "6 AWS Cloud",
    "text": "6 AWS Cloud\nCoursera Course: AWS Fundamentals\n\n2023-03-09, Computing and Networking\n2023-03-12, Storage and Database"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#azure-cloud",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#azure-cloud",
    "title": "Content List, Engineering",
    "section": "7 Azure Cloud",
    "text": "7 Azure Cloud"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#data-modeling",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#data-modeling",
    "title": "Content List, Engineering",
    "section": "8 Data Modeling",
    "text": "8 Data Modeling"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#apache-airflow",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#apache-airflow",
    "title": "Content List, Engineering",
    "section": "9 Apache Airflow",
    "text": "9 Apache Airflow"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#apache-spark",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#apache-spark",
    "title": "Content List, Engineering",
    "section": "10 Apache Spark",
    "text": "10 Apache Spark"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#front-end",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#front-end",
    "title": "Content List, Engineering",
    "section": "11 Front End",
    "text": "11 Front End"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#back-end",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#back-end",
    "title": "Content List, Engineering",
    "section": "12 Back End",
    "text": "12 Back End"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#basic",
    "href": "docs/blog/posts/statistics/guide_map/index.html#basic",
    "title": "Content List, Statistics",
    "section": "Basic",
    "text": "Basic\n\nProbability Theory\n\n2023-02-05, Set Theory\n2023-02-05, [Basics of Probability Theory - Axiomatic Foundations]\n2023-02-05, [Basics of Probability Theory - Calculus of Probabilities]\n2023-02-05, Basics of Probability Theory - Probability\n2023-02-05, Conditional Probability\n2023-02-05, [Independence]\n2023-02-05, Bayes’ Rule\n2023-02-05, Random Variable\n1111-11-11, Probability Distribution\n\n\n\nTransformations and Expectations\n\n2023-02-21, Transformation of Random Variables\n1111-11-11, Expected Value vs Realizaed Value\n1111-11-11, Variance\n1111-11-11, Covariance and Correlation\n2023-02-28, Moment Generating Function, MGF\n\n\n\nExponential Family Distributions\n\nDiscrete Random Variable\n\n2023-02-27,Bernoulli Distribution\n2023-02-28,Binomial Distribution\n2023-03-01,Poisson Distribution\n2023-03-01,Geometric Distribution\n1111-11-11, Hypergeometric Distribution\n\nContinuous Random Variable\n\n1111-11-11, Normal Distribution\n1111-11-11, Exponential Distribution\n1111-11-11, Beta Distribution\n1111-11-11, Chi-squared Distribution\n\n1111-11-11,\n\n\n\nMultiple Random Variables\n\n1111-11-11, Joint Distribution and Marginal Distribution\n\n\n\nPoint Estimation\n\n1111-11-11, Estimation Methods - Method of Moments\n1111-11-11, Estimation Methods - Maximum Likelihood Estimation (MLE)\n1111-11-11, Estimation Methods - Bayesian Estimation\n1111-11-11, Estimation Methods - The EM Algorithm\n1111-11-11, Evaluation Methods of Estimators - Mean Squared Error\n1111-11-11, Evaluation Methods of Estimators - Best Unbiased Estimators\n1111-11-11, Evaluation Methods of Estimators - Sufficiency and Unbiasedness\n1111-11-11, Evaluation Methods of Estimators - Loss Function Optimality"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#inferencce",
    "href": "docs/blog/posts/statistics/guide_map/index.html#inferencce",
    "title": "Blog Guide Map, Statistics",
    "section": "Inferencce",
    "text": "Inferencce\n\n1111-11-11, Hypothesis Testing\n2022-12-28, p-values\n1111-11-11, Permutation Test\n1111-11-11, Power\n1111-11-11, Multiple Testing\n1111-11-11, Sample Size Calculation\n1111-11-11, A/B Testing\n2023-01-07, ANOVA\n\n2023-01-27, ANCOVA\n2023-01-27, repeated measures ANOVA\n2023-01-28, MANOVA\n2023-01-28, MANCOVA"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#regression",
    "href": "docs/blog/posts/statistics/guide_map/index.html#regression",
    "title": "Content List, Statistics",
    "section": "Regression",
    "text": "Regression\n\n1111-11-11, Least Square and Simple Linear Regression\n1111-11-11, Multiple Linear Regression\n\n\nGeneralized Linear Models\n\n1111-11-11, Logistic Regression\n1111-11-11, Multinomial Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#generalized-additive-models",
    "href": "docs/blog/posts/statistics/guide_map/index.html#generalized-additive-models",
    "title": "Content List, Statistics",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#survival-analysis",
    "href": "docs/blog/posts/statistics/guide_map/index.html#survival-analysis",
    "title": "Content List, Statistics",
    "section": "Survival Analysis",
    "text": "Survival Analysis\n\n1111-11-11, Cox-Hazard Model"
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#simulation",
    "href": "docs/projects/LLFS/data_preparation.html#simulation",
    "title": "Data Preparation",
    "section": "2 Simulation",
    "text": "2 Simulation\n\n2.1 Package Loading and Option Settings\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(mixOmics)\nset.seed(20230121) # the date writing this\nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n2.2 Global Variables\n\n\nShow the code\n# the number of samples\nsample_size <- 1000\n# the number of predictors\npredictor_size <- 5000\n# the number of groups\ngroup_size <- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors <- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors<-floor(significant_predictors*0.4) \nnegatively_associated_predictors<-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list<-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%>%round(3) \nnames(group_proportion_list)<-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix <- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data<-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%>%\n        mutate(\n            group_n=(predictor_size*group_proportion_list)%>%round(0), # the number of predictors within each group \n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), # the 1st index of predictors in each group\n            last_index=cumsum(group_n), # the last index of predictors in each group\n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE), # within-group correlations among the within-group predictors\n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) # effect of each group on an outcome variable\n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]<-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]<-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata<-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix<-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients <- rnorm(predictor_size,0,0.05)\n\nanswer_list<-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n2.3 Functions\n\n\nShow the code\n## Function List\n\nscale_function=function(vector=x,min=a,max=b,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data that are statistically associated with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)<-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite<-\n        temp_df%>%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%>%\n        filter(abs_mean_diff==max(abs_mean_diff))%>%\n        dplyr::select(metabolite)%>%pull\n    \n    ## generate age data with min max normalization\n    age_data<-\n        data%>%\n        dplyr::select(strong_metabolite)%>%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%>%\n        rename(age=1)%>%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data that are jointly and statistically associated with a continuous data and a binary data (I am not so sure if I can generate data that are statistically associated with some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n    binary_proportion<-as.numeric(table(in_response)/sample_size) #the simulated proportion for the disease vs non-disease cases\n    genotype_proportion<-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion<-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n2.4 Simulation Algorithm\n\n\nShow the code\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range <- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] <- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) <- 1\n    data[, group_range] <- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] <-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names<-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] <- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]<-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities <- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%>%\n    ifelse(.>1,1,.)%>%\n    ifelse(.<0,0,.)\n\nresponse <-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%>%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data <- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data <- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data <- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data<-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%>%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n2.5 Load Data\n\n\nShow the code\n# load simulation data\nsimulated_data<-read_rds(datapath)\n\n# simple data pre-processing\nall_data<-\n    simulated_data%>%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]<-paste0(\"meta\",1:predictor_size)"
  },
  {
    "objectID": "docs/blog/posts/Validation/guide_map/index.html",
    "href": "docs/blog/posts/Validation/guide_map/index.html",
    "title": "Content List, Validation",
    "section": "",
    "text": "0000-00-00, EN62304"
  },
  {
    "objectID": "docs/blog/posts/Validation/guide_map/index.html#fda",
    "href": "docs/blog/posts/Validation/guide_map/index.html#fda",
    "title": "Content List, Validation",
    "section": "2 FDA",
    "text": "2 FDA\n\n2023-01-27, General Principles of SW Validation\n2023-01-27, General Principles of SW Validation - Diagram Summary\n1111-11-11, Guidance for the Content of Premarket Submissions for Software Contained in Medical Devices"
  },
  {
    "objectID": "docs/blog/posts/Validation/guide_map/index.html#dhf",
    "href": "docs/blog/posts/Validation/guide_map/index.html#dhf",
    "title": "Content List, Validation",
    "section": "3 DHF",
    "text": "3 DHF"
  },
  {
    "objectID": "docs/blog/posts/Validation/guide_map/index.html#public-health",
    "href": "docs/blog/posts/Validation/guide_map/index.html#public-health",
    "title": "Content List, Validation",
    "section": "4 Public Health",
    "text": "4 Public Health"
  },
  {
    "objectID": "docs/blog/posts/Validation/guide_map/index.html#wet-lab",
    "href": "docs/blog/posts/Validation/guide_map/index.html#wet-lab",
    "title": "Content List, Validation",
    "section": "5 Wet Lab",
    "text": "5 Wet Lab\n\n0000-00-00, PCR (Polymerase Chain Reaction) Experiment"
  },
  {
    "objectID": "docs/blog/posts/Validation/2022-12-10_FDA_sw_general_guidance/index.html",
    "href": "docs/blog/posts/Validation/2022-12-10_FDA_sw_general_guidance/index.html",
    "title": "FDA Software Validation Guidance Summary",
    "section": "",
    "text": "I am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto.\nThe FDA validation guidance document is a bit difficult to understand because its explanations provides abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\n\n\n2022-12-28, download this article as PDF\n2022-12-28, summary with diagrams\n\n\n\n\nFDA: General Principles of Software Validation\n\n\n\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\n\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/Validation/2022-12-10_FDA_sw_general_guidance/index.html#purpose",
    "href": "docs/blog/posts/Validation/2022-12-10_FDA_sw_general_guidance/index.html#purpose",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.1 Purpose",
    "text": "2.1 Purpose\nThe purpose is to make a sketch of general validation principle of the validation of medical device software or software used to design or develop."
  },
  {
    "objectID": "docs/blog/posts/Validation/2022-12-10_FDA_sw_general_guidance/index.html#scope",
    "href": "docs/blog/posts/Validation/2022-12-10_FDA_sw_general_guidance/index.html#scope",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.2 Scope",
    "text": "2.2 Scope\nThe scope of this guidance is broad. The important activities for the software validation include at least:\n\nplanning,\nverfication,\ntesting,\ntraceability, and\nconfiguration management.\n\nAll of the activities above should be\n\nintegrated\nbe able to describe software life cycle management and\nbe able to describe software risk management.\n\nThe software validation and verification activities should be focused into the entire software life cycle. (It does not necessarily mean that the activies must follow any technical models.)\nThe guidance is applicable to any software related to a regulated medical device and anyone who is employed in a bio or medical industry.\n\n2.2.1 The Least Burdensome Approach\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\n2.2.2 Regulatory Requirements for Software Validation\n\nSoftware validation: a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997. (See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\nSpecific requirements for validation of device software are found in 21 CFR §820.30(g). Other design controls, such as planning, input, verification, and reviews, are required for medical device software. (See 21 CFR §820.30.)\ncomputer systems used to create, modify, and maintain electronic records and to manage electronic signatures are also subject to the validation requirements. (See 21 CFR §11.10(a).)\n\n\n2.2.2.1 Objective\nThe objective of software validation is to ensure:\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\n2.2.2.2 What to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\n\n2.2.3 Quality System Regulation vs Pre-market Submissions\nThis document does not address any specific requirements but general ones. Specific issues should be addressed to\n\nthe Office of Device Evaluation (ODE),\nCenter for Devices and Radiological Health (CDRH)\nthe Office of Blood Research and Review,\nCenter for Biologics Evaluation and Research (CBER). See the references in Appendix A for applicable FDA guidance documents for pre-market submissions."
  },
  {
    "objectID": "docs/blog/posts/Validation/2022-12-10_FDA_sw_general_guidance/index.html#context-for-software-validation",
    "href": "docs/blog/posts/Validation/2022-12-10_FDA_sw_general_guidance/index.html#context-for-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.3 Context for Software Validation",
    "text": "2.3 Context for Software Validation\n\nValidation elements that FDA expects to do for the Quality System regulation, using the principles and tasks are listed in Sections 4 and 5.\nAdditional specific information is available from many of the references listed in Appendix A\n\n\n2.3.1 Definition and Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology between the medical device Quality System regulation and the software industry:\n\nrequirements,\nspecification,\nverification, and\nvalidation.\n\n\n\n2.3.1.1 Requirements and Specifications\nThe Quality System regulation states\n\nthat design input requirements must be documented and\nthat specified requirements must be verified\n\nBut, the regulation does not further clarify the distinction between the terms “requirement” and “specification.”\n\nRequirement\n\ncan be any need or expectation for a system or for its software.\nreflects the stated or implied needs of the customer: requirements may be\n\nmarket-based,\ncontractual,\nstatutory, or\nan organization’s internal requirements.\n\nvarious examples of requirements\n\ndesign, functional, implementation, interface, performance, or physical requirements\n\nSoftware requirements derived from the system requirements for those aspects of system functionality\nSoftware requirements are typically stated in functional terms and are defined, refined, and updated as a development project progresses.\nSuccess in accurately and completely documenting software requirements is a crucial factor in successful validation of the resulting software.\n\nSpecification\n\ndefined as “a document that states requirements.” (See 21 CFR §820.3(y).)\nIt may refer to or include drawings, patterns, or other relevant documents\nIt usually indicates the means and the criteria whereby conformity with the requirement can be checked.\nVarious examples of written specifications\n\nsystem requirements specification,\nsoftware requirements specification,\nsoftware design specification,\nsoftware test specification,\nsoftware integration specification, etc.\n\nAll of these documents are design outputs for which various forms of verification are necessary.\n\n\n\n\n2.3.1.2 Verifiaction and Validation\nThe Quality System regulation is harmonized with ISO 8402:1994, which treats “verification” and “validation” as separate and distinct terms.\n\nSoftware verification\n\nIt provides objective evidence that the design outputs of a particular phase of the software development life cycle meet all of the specified requirements for that phase.\nIt looks for\n\nconsistency,\ncompleteness, and\ncorrectness of the software and its supporting documentation\n\nSoftware testing\n\nverification activities intended to confirm that software development output meets its input requirements.\n\nTypes of verification activities include\n\nvarious static and dynamic analyses,\ncode and document inspections,\nwalkthroughs, and other techniques.\n\n\nSoftware Validation\n\nConfirmation by examination and provision of the following objective evidence:\nEvidence 1: software specifications conform to user needs and intended uses, and\nEvidnece 2: the particular requirements implemented through software can be consistently fulfilled.\nEvidnece 3: all software requirements have been implemented correctly and completely and are traceable to system requirements.\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device.\n\nDifficulty in Software verification and validation\n\na developer cannot test forever, and\nit is difficult to know how much evidence is enough.\nIn large measure, software validation is a matter of developing a “level of confidence” that the device meets all requirements and user expectations for the software automated functions and features of the device.\nConsiderations for an acceptable level of confidence\n\nmeasures such as defects found in specifications documents,\nestimates of defects remaining,\ntesting coverage, and other techniques are all used to develop before shipping the product.\nHowever, a level of confidence varies depending upon the safety risk (hazard) posed by the automated functions of the device. (Info on safety risk is found in Section 4 and in the international standards ISO/IEC 14971-1 and IEC 60601-1-4 referenced in Appendix A).\n\n\n\n\n\n2.3.1.3 IQ/OQ/PQ\nIQ/OQ/PQ are the terminology related to user site software validation\n\nInstallation qualification (IQ)\nOperational qualification (OQ)\nPerformance qualification (PQ).\n\nDefinitions of these terms may be found in FDA’s Guideline on General Principles of Process Validation, dated May 11, 1987, and in FDA’s Glossary of Computerized System and Software Development Terminology, dated August 1995. Both FDA personnel and device manufacturers need to be aware of these differences in terminology as they ask for and provide information regarding software validation.\n\n\n\n2.3.2 Software Development as Part of System Design\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nthe user’s needs\nintended uses from which the product is developed.\n\nA primary goal of software validation is to then demonstrate that all completed software products comply with all documented software and system requirements.\n\n\n2.3.3 Software Is Different from Hardware\nSoftware engineering needs an even greater level of managerial scrutiny and control than does hardware engineering.\n\n\n2.3.4 Benefits of Software Validation\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nSoftware validation can also reduce long term costs by making it easier and less costly to reliably modify software and revalidate software changes.\n\n\n\n2.3.5 Design Review\nDesign reviews are documented, comprehensive, and systematic examinations of a design to evaluate\n\nthe adequacy of the design requirements,\nthe capability of the design to meet these requirements, and\nto identify problems.\n\nDesign review is a primary tool for managing and evaluating development projects.\n\nIt is strongly recommended that it should be formal design because it is more structured than the informal one.\nIt includes participation from others outside the development team.\nIt may review reference or include results from other formal and informal reviews.\nDesign reviews should include\n\nexamination of development plans,\nrequirements specifications,\ndesign specifications,\ntesting plans and procedures,\nall other documents and activities associated with the project,\nverification results from each stage of the defined life cycle, and\nvalidation results for the overall device.\n\nThe Quality System regulation requires that at least one formal design review be conducted during the device design process. However, it is recommended that multiple design reviews be conducted\n\n(e.g., at the end of each software life cycle activity, in preparation for proceeding to the next activity).\n\nFormal design reviews documented should include:\n\nthe appropriate tasks and expected results, outputs, or products been established for each software life cycle activity\ncorrectness, completeness, consistency, and accuracy\nsatisfaction for the standards, practices, and conventions of that activity\nestablishment of a proper basis for initiating tasks for the next software life cycle activity"
  },
  {
    "objectID": "docs/blog/posts/Validation/2022-12-10_FDA_sw_general_guidance/index.html#principles-of-software-validation",
    "href": "docs/blog/posts/Validation/2022-12-10_FDA_sw_general_guidance/index.html#principles-of-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.4 Principles of Software Validation",
    "text": "2.4 Principles of Software Validation\n\n2.4.1 Requirements\nA documented software requirements specification provides a baseline for both validation and verification. The software validation process must include an established software requirements specification (Ref: 21 CFR 820.3(z) and (aa) and 820.30(f) and (g)).\n\n\n2.4.2 Defect Prevention\nIn order to establish that confidence, software developers should use a mixture of methods and techniques to prevent software errors and to detect software errors that do occur.\n\n\n2.4.3 Time and Effort\nPreparation for software validation should begin early, i.e., during design and development planning and design input. The final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\n2.4.4 Software Life Cycle\n\nSoftware validation takes place within the environment of an established software life cycle.\nThe software life cycle contains software engineering tasks and documentation necessary to support the software validation effort.\nspecific verification and validation tasks need to be appropriate for the intended use of the software\n\n\n\n2.4.5 Plans\n\nThe software validation process is defined and controlled through the use of a plan.\nThe software validation plan defines “what” is to be accomplished through the software validation effort.\nSoftware validation plans specify areas such as\n\nscope,\napproach,\nresources,\nschedules and the types and extent of activities,\ntasks, and\nwork items.\n\n\n\n\n2.4.6 Procedures\nThe software validation process is executed through the use of procedures. These procedures establish “how” to conduct the software validation effort. The procedures should identify the specific actions or sequence of actions that must be taken to complete individual validation activities, tasks, and work items.\n\n\n2.4.7 Software Validation After a Change\n\nDue to the complexity of software, a small local change may have a significant global system impact.\nIf a change exists in the software, the whole validation status of the software needs to be re-established.\nneed to determine the extent and impact of that change on the entire software system.\nthe software developer should then conduct an appropriate level of software regression testing to show that unchanged but vulnerable portions of the system have not been adversely affected.\n\n\n\n2.4.8 Validation Coverage\n\nValidation coverage should be based on the software’s complexity and safety risk.\nThe selection of validation activities, tasks, and work items should be commensurate with the complexity of the software design and the risk associated with the use of the software for the specified intended use.\n\n\n\n2.4.9 Independence of Review\n\nValidation activities should be based on the basic quality assurance precept of “independence of review.”\nSelf-validation is extremely difficult.\nWhen possible, an independent evaluation is always better (like a contracted third-party independent verification and validation)\nAnother approach is to assign internal staff members that are not involved in a particular design or its implementation, but who have sufficient knowledge to evaluate the project and conduct the verification and validation activities.\n\n\n\n2.4.10 Flexibility and Responsibility\nThe device manufacturer has flexibility in choosing how to apply these validation principles, but retains ultimate responsibility for demonstrating that the software has been validated. FDA regulated medical device applications include software that:\n\nIs a component, part, or accessory of a medical device;\n\ncomponents: e.g., application software, operating systems, compilers, debuggers, configuration management tools, and many more\n\nIs itself a medical device; or\nIs used in manufacturing, design and development, or other parts of the quality system.\nNo matter how complex and disperse the software is, the manufacturer is in charge of responsibility for software validation."
  },
  {
    "objectID": "docs/blog/posts/Validation/2022-12-10_FDA_sw_general_guidance/index.html#activities-and-tasks",
    "href": "docs/blog/posts/Validation/2022-12-10_FDA_sw_general_guidance/index.html#activities-and-tasks",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.5 Activities and Tasks",
    "text": "2.5 Activities and Tasks\nSoftware validation is accomplished through a series of activities and tasks that are planned and executed at various stages of the software development life cycle. These tasks may be\n\none time occurrences\niterated many times\n\n\n2.5.1 Software Life Cycle Activities\n\nSoftware developers should establish a software life cycle model that is appropriate for their product and organization.\nThe selected software life cycle model should cover the software from its birth to its retirement.\nActivities in a typical software life cycle model:\n\nQuality Planning\nSystem Requirements Definition\nDetailed Software Requirements Specification\nSoftware Design Specification\nConstruction or Coding\nTesting\nInstallation\nOperation and Support\nMaintenance\nRetirement\n\nVerification, testing, and other tasks that support software validation occur during each of these activities.\nSeveral software life cycle models defined in FDA’s Glossary of Computerized System and Software Development\n\nTerminology dated August 1995:\n\nwaterfall\nspiral\nrapid prototyping\nincremental development, etc.\n\n\n\n2.5.2 Typical Tasks Supporting Validation\nthe software developer should at least consider each of the risk-related tasks and should define and document which tasks are or are not appropriate for their specific application.\n\n2.5.2.1 Quality Planning\nDesign and development planning should culminate in a plan that identifies\n\nnecessary tasks,\nprocedures for anomaly reporting and resolution,\nnecessary resources, and\nmanagement review requirements including formal design reviews.\n\nThe plan should include:\n\nThe specific tasks for each life cycle activity;\nEnumeration of important quality factors (e.g., reliability, maintainability, and usability);\nMethods and procedures for each task;\nTask acceptance criteria;\nCriteria for defining and documenting outputs in terms that will allow evaluation of their conformance to input requirements;\nInputs for each task;\nOutputs from each task;\nRoles, resources, and responsibilities for each task;\nRisks and assumptions; and\nDocumentation of user needs.\n\nThe plan should identify\n\nthe personnel,\nthe facility and equipment resources for each task, and\nthe role that risk (hazard) management will play.\n\nA configuration management plan should be developed that will guide and control multiple parallel development activities and ensure proper communications and documentation.\nControls are necessary to ensure positive and correct correspondence among all approved versions of the specifications documents, source code, object code, and test suites that comprise a software system. The controls also should ensure accurate identification of, and access to, the currently approved versions.\nProcedures should be created for reporting and resolving software anomalies found through validation or other activities.\nManagement should identify the reports and specify the contents, format, and responsible organizational elements for each report. Procedures also are necessary for the review and approval of software development results, including the responsible organizational elements for such reviews and approvals.\nTypical Tasks – Quality Planning\n\nRisk (Hazard) Management Plan\nConfiguration Management Plan\nSoftware Quality Assurance Plan\n\nSoftware Verification and Validation Plan\n\nVerification and Validation Tasks, and Acceptance Criteria\nSchedule and Resource Allocation (for software verification and validation activities)\nReporting Requirements\n\nFormal Design Review Requirements\nOther Technical Review Requirements\n\nProblem Reporting and Resolution Procedures\nOther Support Activities\n\n\n\n2.5.2.2 Requirements\nRequirements development includes the\n\nidentification,\nanalysis, and\ndocumentation of information about the device and its intended use.\n\nAreas of special importance include allocation of system functions to\n\nhardware/software,\noperating conditions,\nuser characteristics,\npotential hazards, and\nanticipated tasks.\n\nIn addition, the requirements should state clearly the intended use of the software. It is not possible to validate software without predetermined and documented software requirements. Typical software requirements specify the following:\n\nAll software system inputs;\nAll software system outputs;\nAll functions that the software system will perform;\nAll performance requirements that the software will meet, (e.g., data throughput, reliability, and timing);\nThe definition of all external and user interfaces, as well as any internal software-to-system interfaces;\nHow users will interact with the system;\nWhat constitutes an error and how errors should be handled;\nRequired response times;\nThe intended operating environment for the software, if this is a design constraint (e.g., hardware platform, operating system);\nAll ranges, limits, defaults, and specific values that the software will accept; and\nAll safety related requirements, specifications, features, or functions that will be implemented in software.\n\nSoftware requirement specifications should identify clearly the potential hazards that can result from a software failure in the system as well as any safety requirements to be implemented in software.\nThe consequences of software failure should be evaluated, along with means of mitigating such failures (e.g., hardware mitigation, defensive programming, etc.).\nThe Quality System regulation requires a mechanism for addressing incomplete, ambiguous, or conflicting requirements. (See 21 CFR 820.30(c).) Each requirement (e.g., hardware, software, user, operator interface, and safety) identified in the software requirements specification should be evaluated for accuracy, completeness, consistency, testability, correctness, and clarity.\nFor example, software requirements should be evaluated to verify that:\n\nThere are no internal inconsistencies among requirements;\nAll of the performance requirements for the system have been spelled out;\nFault tolerance, safety, and security requirements are complete and correct;\nAllocation of software functions is accurate and complete;\nSoftware requirements are appropriate for the system hazards; and\nAll requirements are expressed in terms that are measurable or objectively verifiable.\n\nA software requirements traceability analysis should be conducted to trace software requirements to (and from) system requirements and to risk analysis results. In addition to any other analyses and documentation used to verify software requirements, a formal design review is recommended to confirm that requirements are fully specified and appropriate before extensive software design efforts begin. Requirements can be approved and released incrementally, but care should be taken that interactions and interfaces among software (and hardware) requirements are properly reviewed, analyzed, and controlled.\nTypical Tasks – Requirements\n\nPreliminary Risk Analysis\nTraceability Analysis\n\nSoftware Requirements to System Requirements (and vice versa)\nSoftware Requirements to Risk Analysis\n\nDescription of User Characteristics\nListing of Characteristics and Limitations of Primary and Secondary Memory\nSoftware Requirements Evaluation\nSoftware User Interface Requirements Analysis\nSystem Test Plan Generation\nAcceptance Test Plan Generation\nAmbiguity Review or Analysis\n\n\n\n2.5.2.3 Design\nIn the design process, the software requirements specification is translated into a logical and physical representation of the software to be implemented. The software design specification is a description of what the software should do and how it should do it. The design specification may contain both a high level summary of the design and detailed design information. Human factors engineering should be woven into\n\nthe entire design and development process,\nthe device design requirements,\nanalyses, and\ntests.\n\nDevice safety and usability issues should be considered when developing\n\nflowcharts,\nstate diagrams,\nprototyping tools, and\ntest plans.\n\nAlso, task and function analyses, risk analyses, prototype tests and reviews, and full usability tests should be performed. Participants from the user population should be included when applying these methodologies.\nThe software design specification should include:\n\nSoftware requirements specification, including predetermined criteria for acceptance of the software;\nSoftware risk analysis;\nDevelopment procedures and coding guidelines (or other programming procedures);\nSystems documentation (e.g., a narrative or a context diagram) that describes the systems context in which the program is intended to function, including the relationship of hardware, software, and the physical environment;\nHardware to be used;\nParameters to be measured or recorded;\nLogical structure (including control logic) and logical processing steps (e.g., algorithms);\nData structures and data flow diagrams;\nDefinitions of variables (control and data) and description of where they are used;\nError, alarm, and warning messages;\nSupporting software (e.g., operating systems, drivers, other application software);\nCommunication links (links among internal modules of the software, links with the supporting software, links with the hardware, and links with the user);\nSecurity measures (both physical and logical security); and\nAny additional constraints not identified in the above elements.\n\nThe first four of the elements noted above usually are separate pre-existing documents that are included by reference in the software design specification. Software requirements specification was discussed in the preceding section, as was software risk analysis.\nSoftware design evaluations criteria:\n\ncomplete,\ncorrect,\nconsistent,\nunambiguous,\nfeasible,\nmaintainable,\nanalyses of control flow,\ndata flow,\ncomplexity,\ntiming,\nsizing,\nmemory allocation,\ncriticality analysis, and many other aspects of the design\n\nAppropriate consideration of software architecture (e.g., modular structure) during design can reduce the magnitude of future validation efforts when software changes are needed.\nA traceability analysis should be conducted to verify that the software design implements all of the software requirements. As a technique for identifying where requirements are not sufficient, the traceability analysis should also verify that all aspects of the design are traceable to software requirements.\nAn analysis of communication links should be conducted to evaluate the proposed design with respect to hardware, user, and related software requirements. At the end of the software design activity, a Formal Design Review should be conducted to verify that the design is correct, consistent, complete, accurate, and testable, before moving to implement the design.\nSeveral versions of both the software requirement specification and the software design specification should be maintained. All approved versions should be archived and controlled in accordance with established configuration management procedures.\nTypical Tasks – Design\n\nUpdated Software Risk Analysis\nTraceability Analysis - Design Specification to Software Requirements (and vice versa)\nSoftware Design Evaluation\nDesign Communication Link Analysis\nModule Test Plan Generation\nIntegration Test Plan Generation\nTest Design Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.4 Construction or Coding\nSoftware may be constructed either by coding. Coding is the software activity where the detailed design specification is implemented as source code. It is the last stage in decomposition of the software requirements where module specifications are translated into a programming language.\nCoding usually involves the use of a high-level programming language, but may also entail the use of assembly language (or microcode) for time-critical operations.\nA source code traceability analysis is an important tool to verify that all code is linked to established specifications and established test procedures. A source code traceability analysis should be conducted and documented to verify that:\n\nEach element of the software design specification has been implemented in code;\nModules and functions implemented in code can be traced back to an element in the software design specification and to the risk analysis;\nTests for modules and functions can be traced back to an element in the software design specification and to the risk analysis; and\nTests for modules and functions can be traced to source code for the same modules and functions.\n\nTypical Tasks – Construction or Coding\n\nTraceability Analyses\n\nSource Code to Design Specification (and vice versa)\nTest Cases to Source Code and to Design Specification\n\nSource Code and Source Code Documentation Evaluation\nSource Code Interface Analysis\nTest Procedure and Test Case Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.5 Testing by the Software Developer\nSoftware testing entails running software products under known conditions with defined inputs and documented outcomes that can be compared to their predefined expectations. It is a time consuming, difficult, and imperfect activity.\nAs such, it requires early planning in order to be effective and efficient. Test plans and test cases should be created as early in the software development process as feasible.\nThey should identify\n\nthe schedules,\nenvironments,\nresources (personnel, tools, etc.),\nmethodologies,\ncases (inputs, procedures, outputs, expected results),\ndocumentation, and\nreporting criteria.\n\nDescriptions of categories of software and software testing effort appear in the literature\n\nNIST Special Publication 500-235, Structured Testing: A Testing Methodology Using the Cyclomatic Complexity Metric;\nNUREG/CR-6293, Verification and Validation Guidelines for High Integrity Systems; and\nIEEE Computer Society Press, Handbook of Software Reliability Engineering.\n\nTesting of all program functionality does not mean all of the program has been tested. Testing of all of a program’s code does not mean all necessary functionality is present in the program. Testing of all program functionality and all program code does not mean the program is 100% correct! Software testing that finds no errors should not be interpreted to mean that errors do not exist in the software product; it may mean the testing was superficial.\nAn essential element of a software test case is the expected result. It is the key detail that permits objective evaluation of the actual test result. This necessary testing information is obtained from the corresponding, predefined definition or specification.\nA software testing process should be based on principles that foster effective examinations of a software product. Applicable software testing tenets include:\n\nThe expected test outcome is predefined;\nA good test case has a high probability of exposing an error;\nA successful test is one that finds an error;\nThere is independence from coding;\nBoth application (user) and software (programming) expertise are employed;\nTesters use different tools from coders;\nExamining only the usual case is insufficient;\nTest documentation permits its reuse and an independent confirmation of the pass/fail status of a test outcome during subsequent review.\n\nCode-based testing is also known as structural testing or “white-box” testing. It identifies test cases based on knowledge obtained from the source code, detailed design specification, and other development documents. Structural testing can identify “dead” code that is never executed when the program is run. Structural testing is accomplished primarily with unit (module) level testing, but can be extended to other levels of software testing.\nThe level of structural testing can be evaluated using metrics that are designed to show what percentage of the software structure has been evaluated during structural testing. These metrics are typically referred to as “coverage” and are a measure of completeness with respect to test selection criteria. The amount of structural coverage should be commensurate with the level of risk posed by the software. Use of the term “coverage” usually means 100% coverage. Common structural coverage metrics include:\n\nStatement Coverage – This criteria requires sufficient test cases for each program statement to be executed at least once; however, its achievement is insufficient to provide confidence in a software product’s behavior.\nDecision (Branch) Coverage – This criteria requires sufficient test cases for each program decision or branch to be executed so that each possible outcome occurs at least once. It is considered to be a minimum level of coverage for most software products, but decision coverage alone is insufficient for high-integrity applications.\nCondition Coverage – This criteria requires sufficient test cases for each condition in a program decision to take on all possible outcomes at least once. It differs from branch coverage only when multiple conditions must be evaluated to reach a decision.\nMulti-Condition Coverage – This criteria requires sufficient test cases to exercise all possible combinations of conditions in a program decision.\nLoop Coverage – This criteria requires sufficient test cases for all program loops to be executed for zero, one, two, and many iterations covering initialization, typical running and termination (boundary) conditions.\nPath Coverage – This criteria requires sufficient test cases for each feasible path, basis path, etc., from start to exit of a defined program segment, to be executed at least once. Because of the very large number of possible paths through a software program, path coverage is generally not achievable. The amount of path coverage is normally established based on the risk or criticality of the software under test.\nData Flow Coverage – This criteria requires sufficient test cases for each feasible data flow to be executed at least once. A number of data flow testing strategies are available.\n\nThe following types of functional software testing involve generally increasing levels of effort:\n\nNormal Case – Testing with usual inputs is necessary. However, testing a software product only with expected, valid inputs does not thoroughly test that software product. By itself, normal case testing cannot provide sufficient confidence in the dependability of the software product.\nOutput Forcing – Choosing test inputs to ensure that selected (or all) software outputs are generated by testing.\nRobustness – Software testing should demonstrate that a software product behaves correctly when given unexpected, invalid inputs. Methods for identifying a sufficient set of such test cases include Equivalence Class Partitioning, Boundary Value Analysis, and Special Case Identification (Error Guessing). While important and necessary, these techniques do not ensure that all of the most appropriate challenges to a software product have been identified for testing.\nCombinations of Inputs – The functional testing methods identified above all emphasize individual or single test inputs. Most software products operate with multiple inputs under their conditions of use. Thorough software product testing should consider the combinations of inputs a software unit or system may encounter during operation. Error guessing can be extended to identify combinations of inputs, but it is an ad hoc technique. Cause-effect graphing is one functional software testing technique that systematically identifies combinations of inputs to a software product for inclusion in test cases.\n\nFunctional and structural software test case identification techniques provide specific inputs for testing, rather than random test inputs. One weakness of these techniques is the difficulty in linking structural and functional test completion criteria to a software product’s reliability.\nAdvanced software testing methods, such as statistical testing, can be employed to provide further assurance that a software product is dependable. Statistical testing uses randomly generated test data from defined distributions based on an operational profile (e.g., expected use, hazardous use, or malicious use of the software product). Large amounts of test data are generated and can be targeted to cover particular areas or concerns, providing an increased possibility of identifying individual and multiple rare operating conditions that were not anticipated by either the software product’s designers or its testers. Statistical testing also provides high structural coverage. It does require a stable software product. Thus, structural and functional testing are prerequisites for statistical testing of a software product.\nAnother aspect of software testing is the testing of software changes. Changes occur frequently during software development. These changes are the result of\n\ndebugging that finds an error and it is corrected,\nnew or changed requirements (“requirements creep”), and\nmodified designs as more effective or efficient implementations are found.\n\nOnce a software product has been baselined (approved), any change to that product should have its own “mini life cycle,” including testing. Testing of a changed software product requires additional effort. It should demonstrate\n\nthat the change was implemented correctly, and\nthat the change did not adversely impact other parts of the software product.\n\nRegression analysis is the determination of the impact of a change based on review of the relevant documentation in order to identify the necessary regression tests to be run. Regression testing is the rerunning of test cases that a program has previously executed correctly and comparing the current result to the previous result in order to detect unintended effects of a software change. Regression analysis and regression testing should also be employed when using integration methods to build a software product to ensure that newly integrated modules do not adversely impact the operation of previously integrated modules.\nIn order to provide a thorough and rigorous examination of a software product, development testing is typically organized into levels: unit, integration, and system levels of testing.\n\nUnit (module or component) level testing focuses on the early examination of sub-program functionality and ensures that functionality not visible at the system level is examined by testing. Unit testing ensures that quality software units are furnished for integration into the finished software product.\nIntegration level testing focuses on the transfer of data and control across a program’s internal and external interfaces. External interfaces are those with\n\nother software (including operating system software),\nsystem hardware, and\nthe users and can be described as communications links.\n\nSystem level testing demonstrates that all specified functionality exists and that the software product is trustworthy. This testing verifies the as-built program’s functionality and performance with respect to the requirements for the software product as exhibited on the specified operating platform(s). System level software testing addresses functional concerns and the following elements of a device’s software that are related to the intended use(s):\n\nPerformance issues (e.g., response times, reliability measurements);\nResponses to stress conditions, e.g., behavior under maximum load, continuous use;\nOperation of internal and external security features;\nEffectiveness of recovery procedures, including disaster recovery;\nUsability; (Usability vs Utility??)\nCompatibility with other software products;\nBehavior in each of the defined hardware configurations; and\nAccuracy of documentation.\n\n\nControl measures (e.g., a traceability analysis) should be used to ensure that the intended coverage is achieved.\nSystem level testing also exhibits the software product’s behavior in the intended operating environment. The location of such testing is dependent upon the software developer’s ability to produce the target operating environment(s). Depending upon the circumstances, simulation and/or testing at (potential) customer locations may be utilized.\nTest plans should identify the controls needed to ensure\n\nthat the intended coverage is achieved and\nthat proper documentation is prepared when planned system level testing is conducted at sites not directly controlled by the software developer.\n\nTest procedures, test data, and test results\n\nshould be documented in a manner permitting objective pass/fail decisions to be reached.\nshould also be suitable for review and objective decision making subsequent to running the test,\nshould be suitable for use in any subsequent regression testing.\n\nErrors detected during testing should be\n\nlogged,\nclassified,\nreviewed, and\nresolved prior to release of the software.\n\nSoftware error data that is collected and analyzed during a development life cycle may be used to determine the suitability of the software product for release for commercial distribution. Test reports should comply with the requirements of the corresponding test plans.\nSoftware testing tools are frequently used to ensure consistency, thoroughness, and efficiency in the testing of such software products and to fulfill the requirements of the planned testing activities.\nAppropriate documentation providing evidence of the validation of these software tools for their intended use should be maintained (see section 6 of this guidance).\nTypical Tasks – Testing by the Software Developer\n\nTest Planning\nStructural Test Case Identification\nFunctional Test Case Identification\nTraceability Analysis - Testing\nUnit (Module) Tests to Detailed Design\nIntegration Tests to High Level Design\nSystem Tests to Software Requirements\nUnit (Module) Test Execution\nIntegration Test Execution\nFunctional Test Execution\nSystem Test Execution\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.6 User Site Testing\nTesting at the user site is an essential part of software validation. The Quality System regulation requires\n\ninstallation and\ninspection procedures (including testing where appropriate) as well as\ndocumentation of inspection and\ntesting to demonstrate proper installation. (See 21 CFR §820.170.)\n\nLikewise, manufacturing equipment must meet specified requirements, and automated systems must be validated for their intended use. (See 21 CFR §820.70(g) and 21 CFR §820.70(i) respectively.)\nTerminology regarding user site testing can be confusing. Terms such as\n\nbeta test,\nsite validation,\nuser acceptance test,\ninstallation verification, and\ninstallation testing have all been used to describe user site testing.\n\nFor the purposes of this guidance, the term “user site testing” encompasses all of these and any other testing that takes place outside of the developer’s controlled environment.\nThis testing should take place at a user’s site with the actual hardware and software that will be part of the installed system configuration. The testing is accomplished through either actual or simulated use of the software being tested within the context in which it is intended to function.\nTest planners should check with the FDA Center(s) with the corresponding product jurisdiction to determine whether there are any additional regulatory requirements for user site testing.\nUser site testing should follow a pre-defined written plan with\n\na formal summary of testing and\na record of formal acceptance.\n\nThe following documented evidence should be retained:\n\nall testing procedures,\ntest input data, and\ntest results\n\nThere should be evidence that hardware and software are installed and configured as specified. Measures should ensure that all system components are exercised during the testing and that the versions of these components are those specified. The testing plan should specify testing throughout the full range of operating conditions and should specify continuation for a sufficient time to allow the system to encounter a wide spectrum of conditions and events in an effort to detect any latent faults that are not apparent during more normal activities.\nSome of the evaluations of the system’s ability that have been performed earlier by the software developer at the developer’s site should be repeated at the site of actual use. These may include tests for:\n\na high volume of data,\nheavy loads or stresses,\nsecurity,\nfault testing (avoidance, detection, tolerance, and recovery),\nerror messages, and\nimplementation of safety requirements.\n\nThere should be an evaluation of the ability of the users of the system to understand and correctly interface with it.\nOperators should be able to perform the intended functions and respond in an appropriate and timely manner to all alarms, warnings, and error messages.\nRecords should be maintained of both proper system performance and any system failures that are encountered.\nThe revision of the system to compensate for faults detected during this user site testing should follow the same procedures and controls as for any other software change.\nThe developers of the software may or may not be involved in the user site testing.\n\nIf the developers are involved, they may seamlessly carry over to the user’s site the last portions of design-level systems testing.\nIf the developers are not involved, it is all the more important that the user have persons who understand the importance of careful test planning, the definition of expected test results, and the recording of all test outputs.\n\nTypical Tasks – User Site Testing\n\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.7 Maintenance and Software Changes\n\n2.5.2.7.1 Hardware vs Software\nHardware maintenance typically includes\n\npreventive hardware maintenance actions,\ncomponent replacement, and\ncorrective changes.\n\nSoftware maintenance includes\n\ncorrective,\nperfective, and\nadaptive maintenance\nbut does not include preventive maintenance actions or software component replacement.\n\n\n\n2.5.2.7.2 Maintenance Types\n\nCorrective maintenance: Changes made to correct errors and faults in the software.\nPerfective maintenance: Changes made to the software to improve the performance, maintainability, or other attributes of the software system .\nAdaptive maintenance: Changes to make the software system usable in a changed environment.\n\nSufficient regression analysis and testing should be conducted to demonstrate that portions of the software not involved in the change were not adversely impacted. When changes are made to a software system,\n\neither during initial development or\nduring post release maintenance,\n\nThis is in addition to testing that evaluates the correctness of the implemented change(s). The specific validation effort necessary for each software change is determined by\n\nthe type of change,\nthe development products affected, and the\nimpact of those products on the operation of the software.\n\n\n\n2.5.2.7.3 Factors of Limitting Validation Effort Needed When a Change Is Made\n\ncareful and complete documentation of the design structure and\ncareful and complete documentation of interrelationships of various modules,\ninterfaces, etc.\nFor example,\n\ntest documentation,\ntest cases, and\nresults of previous verification and validation testing All of them need to be archived if they are to be available for performing subsequent regression testing.\n\n\nThe following additional maintenance tasks should be addressed:\n\nSoftware Validation Plan Revision - For software that was previously validated, the existing software validation plan should be revised to support the validation of the revised software. If no previous software validation plan exists, such a plan should be established to support the validation of the revised software.\nAnomaly Evaluation – Software organizations frequently maintain documentation, such as software problem reports that describe software anomalies discovered and the specific corrective action taken to fix each anomaly.\n\nToo often, however, mistakes are repeated because software developers do not take the next step to determine the root causes of problems and make the process and procedural changes needed to avoid recurrence of the problem.\nSoftware anomalies should be evaluated in terms of their severity and their effects on system operation and safety,\nbut they should also be treated as symptoms of process deficiencies in the quality system.\nA root cause analysis of anomalies can identify specific quality system deficiencies.\nWhere trends are identified (e.g., recurrence of similar software anomalies), appropriate corrective and preventive actions must be implemented and documented to avoid further recurrence of similar quality problems. (See 21 CFR 820.100.)\n\nProblem Identification and Resolution Tracking - All problems discovered during maintenance of the software should be documented. The resolution of each problem should be tracked to ensure it is fixed, for historical reference, and for trending.\nProposed Change Assessment - All proposed modifications, enhancements, or additions should be assessed to determine the effect each change would have on the system. This information should determine the extent to which verification and/or validation tasks need to be iterated.\nTask Iteration - For approved software changes, all necessary verification and validation tasks should be performed to ensure that planned changes are implemented correctly, all documentation is complete and up to date, and no unacceptable changes have occurred in software performance.\nDocumentation Updating – Documentation should be carefully reviewed to determine which documents have been impacted by a change. All approved documents (e.g., specifications, test procedures, user manuals, etc.) that have been affected should be updated in accordance with configuration management procedures. Specifications should be updated before any maintenance and software changes are made."
  },
  {
    "objectID": "docs/blog/posts/Validation/2022-12-10_FDA_sw_general_guidance/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "href": "docs/blog/posts/Validation/2022-12-10_FDA_sw_general_guidance/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.6 Validation of Automated Process Equipment and Quality System Software",
    "text": "2.6 Validation of Automated Process Equipment and Quality System Software\nThe Quality System regulation requires that “when computers or automated data processing systems are used as part of production or the quality system, the [device] manufacturer shall validate computer software for its intended use according to an established protocol.” (See 21 CFR §820.70(i)). This has been a regulatory requirement of FDA’s medical device Good Manufacturing Practice (GMP) regulations since 1978.\nComputer systems that implement part of a device manufacturer’s production processes or quality system (or that are used to create and maintain records required by any other FDA regulation) are subject to the Electronic Records; Electronic Signatures regulation. (See 21 CFR Part 11.) This regulation establishes additional security, data integrity, and validation requirements when records are created or maintained electronically. These additional Part 11 requirements should be carefully considered and included in system requirements and software requirements for any automated record keeping systems. System validation and software validation should demonstrate that all Part 11 requirements have been met.\nComputers and automated equipment are used extensively throughout all aspects of\n\nmedical device design,\nlaboratory testing and analysis,\nproduct inspection and acceptance,\nproduction and process control,\nenvironmental controls,\npackaging,\nlabeling,\ntraceability,\ndocument control,\ncomplaint management, and many other aspects of the quality system.\n\nIncreasingly, automated plant floor operations can involve extensive use of embedded systems in:\n\nprogrammable logic controllers;\ndigital function controllers;\nstatistical process control;\nsupervisory control and data acquisition;\nrobotics;\nhuman-machine interfaces;\ninput/output devices; and\ncomputer operating systems.\n\nAll software tools used for software design are subject to the requirement for software validation, but the validation approach used for each application can vary widely.\nValidation is typically supported by:\n\nverifications of the outputs from each stage of that software development life cycle; and\nchecking for proper operation of the finished software in the device manufacturer’s intended use environment.\n\n\n2.6.1 How Much Validation Evidence Is Needed?\nThe level of validation effort should be commensurate with\n\nthe risk posed by the automated operation,\nthe complexity of the process software,\nthe degree to which the device manufacturer is dependent upon that automated process to produce a safe and effective device\n\nDocumented requirements and risk analysis of the automated process help to define the scope of the evidence needed to show that the software is validated for its intended use. Without a plan, extensive testing may be needed for:\n\na plant-wide electronic record and electronic signature system;\nan automated controller for a sterilization cycle; or\nautomated test equipment used for inspection and acceptance of finished circuit boards in a lifesustaining / life-supporting device.\n\nHigh risk applications should not be running in the same operating environment with non-validated software functions, even if those software functions are not used. Risk mitigation techniques such as memory partitioning or other approaches to resource protection may need to be considered when high risk applications and lower risk applications are to be used in the same operating environment.\nWhen software is upgraded or any changes are made to the software, the device manufacturer should consider how those changes may impact the “used portions” of the software and must reconfirm the validation of those portions of the software that are used. (See 21 CFR §820.70(i).)\n\n\n2.6.2 Defined User Equipment\nA very important key to software validation is a documented user requirements specification that defines:\n\nthe “intended use” of the software or automated equipment; and\nthe extent to which the device manufacturer is dependent upon that software or equipment for production of a quality medical device.\n\nThe device manufacturer (user) needs to define the expected operating environment including any required hardware and software configurations, software versions, utilities, etc. The user also needs to:\n\ndocument requirements for system performance, quality, error handling, startup, shutdown, security, etc.;\nidentify any safety related functions or features, such as sensors, alarms, interlocks, logical processing steps, or command sequences; and\ndefine objective criteria for determining acceptable performance.\n\nThe validation must be conducted in accordance with a documented protocol, and the validation results must also be documented. (See 21 CFR §820.70(i).) Test cases should be documented that will exercise the system to challenge its performance against the pre-determined criteria, especially for its most critical parameters.\nTest cases should address\n\nerror and alarm conditions,\nstartup, shutdown,\nall applicable user functions and operator controls,\npotential operator errors,\nmaximum and minimum ranges of allowed values, and\nstress conditions applicable to the intended use of the equipment.\n\nThe test cases should be executed and the results should be recorded and evaluated to determine whether the results support a conclusion that the software is validated for its intended use.\nA device manufacturer may conduct a validation using their own personnel or may depend on a third party such as the equipment/software vendor or a consultant. In any case, the device manufacturer retains the ultimate responsibility for ensuring that the production and quality system software:\n\nis validated according to a written procedure for the particular intended use; and\nwill perform as intended in the chosen application.\n\nThe device manufacturer should have documentation including:\n\ndefined user requirements;\nvalidation protocol used;\nacceptance criteria;\ntest cases and results; and\na validation summary that objectively confirms that the software is validated for its intended use.\n\n\n\n2.6.3 Validation of Off-The-Shelf Software and Automated Equipment\nMost of the automated equipment and systems used by device manufacturers are supplied by thirdparty vendors and are purchased off-the-shelf (OTS). The device manufacturer is responsible for ensuring that the product development methodologies used by the OTS software developer are appropriate and sufficient for the device manufacturer’s intended use of that OTS software.\nWhere possible and depending upon the device risk involved, the device manufacturer should consider auditing the vendor’s design and development methodologies used in the construction of the OTS software and should assess the development and validation documentation generated for the OTS software. Such audits can be conducted by the device manufacturer or by a qualified third party.\nThe audit should demonstrate that the vendor’s procedures for and results of the verification and validation activities performed the OTS software are appropriate and sufficient for the safety and effectiveness requirements of the medical device to be produced using that software."
  },
  {
    "objectID": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html",
    "href": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "",
    "text": "Notice\nLast Update\nIntroduction\n\nDefinition of SW Validation\nSome Terminology\nRationale\nObjective of SW Validation\nWhat to validate\nMain Institutions\n\nQuality System Regulation\nVerification\nValidation\nBenefits and Difficulty in SW V&V\nSW Development as Part of System Design\n\nOverview\nDesign Reveiw\n\n\n\n\nValidation Pinciples\n\nOverview\nConditions\nPlanning\nAfter SW Change\nSW Lifecycle\n\nSW Lifecycle Tasks\n\nOverview\nQuality Planning\nConfiguration Management\nTask Requirements\nDesign Overview\n\nDesign Consideration\nDesign Specification\nDesign Activity and Task\n\n\n\n\n\nTesting Tasks\n\nOverview\nConsideration Before Testing Tasks\nCode Based Testing\nSolution to White Box Testing\nDevelopment Testing\nUser Site Testing\n\nOverview\nTesting\n\n\nMaintenance and SW Changes\nValidation of Quality System SW\n\nOverview\nFactors in Validation"
  },
  {
    "objectID": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#notice",
    "href": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#notice",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Notice",
    "text": "Notice\n\nI am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto. (it seems that Quarto system has some issues on mermaid diagrams.)\nThe FDA validation guidance document is a bit difficult to understand because its explanations provide abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\nLast Update\n\n2022-12-28, Summary of Document"
  },
  {
    "objectID": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#introduction",
    "href": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#introduction",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Introduction",
    "text": "Introduction\n\nDefinition of Software Validation\nSoftware Validation is a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997.\n(See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\n\n\nSome Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology: requirements, specification, verification, and validation.\n\n\n\nRationale\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\nObjective of SW validation is to ensure\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\nWhat to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\nMain Institutions\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#quality-system-regulation",
    "href": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#quality-system-regulation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Quality System Regulation",
    "text": "Quality System Regulation\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\n\n\nflowchart TB\n    subgraph Quality_System_Regulation\n        direction LR\n        subgraph Requirement\n            direction TB\n            user_requirements\n        end\n        subgraph Specification\n           direction TB\n           document_user_requirements \n        end \n        subgraph Verification\n           direction TB\n           verify_spacified_requirements\n        end\n        subgraph Validation\n           direction TB\n           Confirmation_by_Examinations\n           Provision_of_objective_3evidences\n        end\n        Requirement--> Specification --> Verification --> Validation                    \n    end\n    subgraph First_Detail\n        direction TB\n        subgraph User_Requirement\n            direction TB\n            any_need_for_customer---\n            any_need_for_system---\n            any_need_for_software\n        end\n            subgraph Document_User_Requirement\n            direction TB\n            define_means_for_requirements---\n          define_criteria_for_requirements\n        end         \n        subgraph Verify_Spacified_Requirement\n            direction TB\n            Objective_Evidence--->|needs|Software_Testing\n        end\n        subgraph SW_Validation\n            direction TB\n            subgraph Confirmation_by_Examination\n            direction TB\n                subgraph Examination_List_of_SW_LifeCycle\n                    direction TB\n                    comprehensiveness_of_software_testing---\n                    inspection_verification_test---\n                    analysis_verification_test---\n                    other_varification_tests    \n                end \n            end             \n            subgraph Provision_of_Objective_3evidences\n                direction TB\n                Software_specifications_conformity---\n                Consistent_SW_Implementation---\n                Correctness_Completeness_Traceability\n            end\n        end\n        Requirement---User_Requirement\n        Specification---Document_User_Requirement\n        Verification---Verify_Spacified_Requirement\n        Confirmation_by_Examinations---Confirmation_by_Examination\n        Provision_of_objective_3evidences---Provision_of_Objective_3evidences             \n    end"
  },
  {
    "objectID": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#verification",
    "href": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#verification",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Verification",
    "text": "Verification\n\n\n\n\nflowchart LR\n    subgraph Objective_Evidence\n        direction LR\n        subgraph Design_Outputs_of_SW_life_cycle_for_Specified_Requirements\n            direction TB\n            Consistency---\n            Completeness---\n            Correctness---\n            Documentation\n        end       \n        subgraph Software_Testing\n            direction LR\n            subgraph Testing_Environments\n                direction TB\n                satisfaction_for_input_requirements\n                satisfaction_for_input_requirements---Simulated_Use_Environment\n                subgraph User_Site_Testing\n                    direction TB                            \n                    Installation_Qualification---\n                    Operational_Qualification---\n                    Performance_Qualification\n                end\n            end\n            satisfaction_for_input_requirements---User_Site_Testing\n            subgraph Testing_Activities\n                direction TB\n                static_analyses---\n                dynamic_analyses---\n                code_and_document_inspections---\n                walkthroughs\n            end \n        Testing_Environments-->Testing_Activities\n        end\n    Design_Outputs_of_SW_life_cycle_for_Specified_Requirements-->Software_Testing-->Testing_Activities\nend    \n\n\n\n\n\n\n\n\n\n\nInstallation_Qualification (IQ): documentation of correct installations according to requirements, specifications, vendor’s recommendations, and the FDA’s guidance for all hardware, software, equipment and systems.\nOperational_Qualification (OQ): establishment of confidence that the software shows constant performances according to specified requirements.\nPerformance_Qualification (PQ): confirmation of the performance in the intended use according to the specified requirements for functionality and safety throughout the SW life cycle."
  },
  {
    "objectID": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation",
    "href": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation",
    "text": "Validation\n\n\n\n\nflowchart LR\n    subgraph Validation\n    direction LR\n        subgraph Confirmation_by_Examination\n            direction TB\n            subgraph Examination_List_at_each_stage_of_SW_Life_Cycle\n                direction TB\n                comprehensiveness_of_software_testing---\n                inspection_verification_test---\n                analysis_verification_test---\n                other_varification_tests    \n            end \n        end\n        subgraph Provision_of_objective_3evidences\n            direction TB\n            subgraph Software_specifications_conform_to\n                direction TB\n                user_needs \n                intended_uses\n            end\n            subgraph Consistent_SW_Implementation\n                direction TB\n                particular_requirements\n            end\n            subgraph Correctness_Completeness_Traceability\n                direction TB\n                correct_complete_implementation_by_all_SW_requirements---\n                traceable_to_system_requirements\n            end\n            Software_specifications_conform_to---\n            Consistent_SW_Implementation---\n            Correctness_Completeness_Traceability\n        end\n        Confirmation_by_Examination-->\n        Provision_of_objective_3evidences\n    end\n\n\n\n\n\n\n\n\n\n\n\n\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device."
  },
  {
    "objectID": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#benefits-and-difficulty-of-sw-vv",
    "href": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#benefits-and-difficulty-of-sw-vv",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Benefits and Difficulty of SW V&V",
    "text": "Benefits and Difficulty of SW V&V\n\nBenefits of SW V&V\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nReduce long term costs by making V&V easier and less costly to reliably modify software and revalidate software changes.\n\n\n\nDifficulty in SW V&V\n\na developer cannot test forever, and\n\nit is difficult to know how much evidence is enough.\na matter of developing a level of confidence that the device meets all requirements\n\nConsiderations for an acceptable level of confidence\nmeasures and estimates such as defects found in specifications documents\ntesting coverage, and other techniques are all used before shipping the product.\na level of confidence varies depending upon the safety risk (hazard) of a SW or device"
  },
  {
    "objectID": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-development-as-part-of-system-design",
    "href": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-development-as-part-of-system-design",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Development as Part of System Design",
    "text": "SW Development as Part of System Design\n\nOverview\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        purpose_design_review---\n        design_review_types---\n        design_review_requirements---\n        design_review_outputs\n    end\n\n\n\n\n\n\n\n\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nuser’s needs\nintended uses from which the product is developed.\n\nA primary goal of SW validation is to demonstrate that all completed SW products comply with all documented requirements.\n\n\nDesign Review\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        subgraph Purpose_Design_Review\n            direction TB\n            documented_structured_comprehensive_systematic_examinations---\n            adequacy_of_design_requirements---\n            capability_of_design_for_requirements---\n            identification_of_problem   \n        end\n        subgraph Design_Reivew_Types\n            direction TB\n            subgraph Formal_Design_Review\n                direction TB\n                3rd_parties_outside_development_team\n            end\n            subgraph Informal_Design_Review\n                direction TB\n                within_development_team\n            end\n        Formal_Design_Review---Informal_Design_Review    \n        end\n        subgraph Design_Review_Requirements\n            direction TB\n               necessary_at_least_one_formal_design_review---\n               optinal_informal_design_review---\n               recommended_multiple_design_reviews\n        end\n        subgraph Formal_Design_Review_Outputs\n            direction TB\n            more_than_10_outputs\n        end\n        Purpose_Design_Review--> Design_Reivew_Types--> Design_Review_Requirements\n        Design_Review_Requirements-->Formal_Design_Review_Outputs\n    end\n\n\n\n\n\n\n\n\n\n\nDesign review is a primary tool for managing and evaluating development projects.\nAt least one formal design review must be conducted during the device design process.\nIt is recommended that multiple design reviews be conducted.\nProblems found at this point can\n\nbe resolved more easily,\nsave time and money, and\nreduce the likelihood of missing a critical issue."
  },
  {
    "objectID": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-principles",
    "href": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-principles",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation Principles",
    "text": "Validation Principles\n\nOverview\n\n\n\n\nflowchart LR\n  subgraph Validation_Principles\n        direction LR\n        subgraph Validation_Starting_Point\n            direction TB\n            during_design_planning---\n            during_development_planning---\n            all_results_should_be_supported_by_evidence_collected_from_planning_SW_lifecylce\n        end\n        subgraph Validation_Conditions\n            direction TB\n            Requirements---Estabilishment_Confidence---SW_Lifecycle\n        end\n\n        subgraph Validation_Planning\n            direction TB\n            Specify_Areas\n            subgraph Validation_Coverage\n                direction TB\n            end\n            subgraph Validation_Process_Establishment\n                direction TB\n            end\n        Specify_Areas---Validation_Coverage---Validation_Process_Establishment\n        end\n\n        subgraph After_Self_Validation\n            direction TB\n            subgraph Validation_After_SW_Change\n        direction TB\n        end\n\n        subgraph Independence_of_Review\n        direction TB\n\n        end\n        Validation_After_SW_Change---Independence_of_Review\n        end\n            Validation_Starting_Point-->Validation_Conditions-->Validation_Planning-->\nAfter_Self_Validation\n    end\n\n\n\n\n\n\n\n\n\nPreparation for software validation should begin as early as possible because the final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\n\nConditions\n\n\n\n\nflowchart LR\n\nsubgraph Validation_Conditions\n    direction LR\n    subgraph SW_Requirments\n        direction TB\n        subgraph Documented_SW_Requirments_Specification\n            direction TB\n            Baseline_Provision_for_V&V---\n            establishment_of_software_requirements_specification\n        end\n    end\n    subgraph Estabilishment_Confidence\n        direction TB\n            mixture_of_methods_techinques---\n            preventing_SW_errors---\n            detecting_SW_errors                 \n    end\n    subgraph SW_Lifecycle\n        direction TB\n        validation_must_be_conducted_within_established_environment_across_lifecycle---\n        lifecycle_contains_SW_engineering_tasks_and_documentation---\n        V&V_tasks_must_reflect_intended_use\n    end\nend\nSW_Requirments---Estabilishment_Confidence---SW_Lifecycle\n\n\n\n\n\n\n\n\n\n\nPlanning\n\n\n\n\nflowchart LR\n    subgraph Validation_Planning\n        direction LR\n        define_what_to_accomplish\n        subgraph Specify_Areas\n            direction TB\n            scope---\n            approach---\n            resources---\n            schedules_activities---\n            types_activitieis---\n            extent_of_activities---\n            tasks---\n            work_items\n        end\n            define_what_to_accomplish-->Specify_Areas\n        subgraph Validation_Coverage\n               direction TB\n            depending_on_SW_complexity_of_SW_design---\n            depending_on_safety_risk_for_specified_intended_use---\n            select_activities_tasks_work_items_for_complexity_safety_risk\n        end\n        subgraph Validation_Process_Establishment\n            direction TB\n            establish_how_to_conduct-->\n            identify_sequence_of_specific_actions-->\n            identify_specific_activitieis-->\n            identify_specific_tasks-->\n            identify_specific_work_items\n        end\n    Specify_Areas-->Validation_Coverage-->Validation_Process_Establishment\n    end\n\n\n\n\n\n\n\n\n\n\nAfter SW Change\n\n\n\n\nflowchart LR\n\nsubgraph After_Self_Validation\n    direction LR\n    subgraph Validation_After_SW_Change\n        direction TB\n        determine_extent_of_change_on_entire_SW_system-->\n        determine_impact_of_change_on_entire_SW_system-->\n        conduct_SW_regression_testing_on_unchanged_but_vulnerable_modules\n    end\n    subgraph Independence_of_Review\n        direction TB\n        follow_basic_quality_assurance_precept_of_independence_of_review---\n        avoid_self_validation---\n        should_conduct_contracted_3rd_party_independent_V&V---\n        or_conduct_blind_test_with_internal_staff\n    end\n    Validation_After_SW_Change---Independence_of_Review\nend\n    \n\n\n\n\n\n\n\n\n\n\nSW Lifecycle\n\n\n\n\nflowchart LR\nsubgraph SW_Lifecycle\n    direction TB\n    validation_must_be_conducted_within_the_established_environment_across_lifecycle---\n    lifecycle_contains_SW_engineering_tasks_and_documentation---\n    V&V_tasks_must_reflect_intended_use\nend\n\nsubgraph SW_Lifecycle_Activities\n    direction TB\n    subgraph should_establish_lifecycle_model\n        direction TB\n        subgraph SW_Lifecycle_Model_List_Defined_in_FDA\n            direction TB\n            waterfall---\n            spiral---\n            rapid_prototyping---\n            incremental_development---\n            etc\n        end     \n    end\n    subgraph should_cover_SW_birth_to_retirement\n        direction TB\n        subgraph Lifecycle_Activities\n            direction TB\n            Quality_Plan-->\n            System_Requirements_Definition-->\n            Detailed_Software_Requirements_Specification-->\n            Software_Design_Specification-->\n            Construction_or_Coding-->\n            Testing-->\n            Installation-->\n            Operation_and_Support-->\n            Maintenance-->\n            Retirement\n        end\n    end\n    should_establish_lifecycle_model-->should_cover_SW_birth_to_retirement\n    should_cover_SW_birth_to_retirement-->Lifecycle_Activities\nend\nsubgraph SW_Lifecycle_Tasks\n    direction TB\n    should_define_and_document_risk_related_tasks---\n    should_define_and_document_which_tasks_are_appropriate_in_vice_versa---\n    Quality_Planning---\n    Quality_Planning_Tasks---\n    Inclusion_Task_List_for_Plan---\n    Identification_Task_List_for_Plan---\n    Configuration_Management---\n    Control---\n    Management---\n    Procedures---\n    ensure_proper_communications_and_documentation---\n    Task_Requirements\nend\nSW_Lifecycle-->SW_Lifecycle_Activities-->SW_Lifecycle_Tasks"
  },
  {
    "objectID": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-lifecycle-tasks",
    "href": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-lifecycle-tasks",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Lifecycle Tasks",
    "text": "SW Lifecycle Tasks\n\nOverview\n\n\n\n\n \nflowchart TB\n\nsubgraph SW_Lifecycle_Tasks\n    direction LR\n    subgraph Define_and_Document_List\n        direction TB\n        risk_related_tasks---\n        whether_or_not_tasks_are_appropriate\n    end\n    \n    subgraph Quality_Planning\n        direction TB\n        subgraph Quality_Planning_Tasks\n            direction TB\n        \n        end\n        subgraph Inclusion_List_for_Plan\n            direction TB\n            \n        end\n        subgraph Identification_List_for_Plan\n            direction TB\n            \n        end\n    Quality_Planning_Tasks-->Inclusion_List_for_Plan-->Identification_List_for_Plan\n    end\n    \n    subgraph Configuration_Management\n        direction TB\n        subgraph Control\n            direction TB\n            \n        end\n        subgraph Management\n            direction TB\n        end\n        subgraph Procedures\n            direction TB\n        end\n        ensure_proper_communications_and_documentation\n        Control-->Management-->Procedures-->ensure_proper_communications_and_documentation \n    end\n    subgraph Task_Requirements\n        direction TB\n        identification---\n        analysis---\n        predetermined_documentation_about_device_its_intended_use---\n        Requirements_Specification_List---\n        Verfification_List_by_Evaluation---\n        Requirements_Tasks    \n    end\nDefine_and_Document_List-->Quality_Planning-->Configuration_Management-->Task_Requirements\nend     \n\n\n\n\n\n\n\n\n\n\nQuality Planning\n\n\n\n\nflowchart TB\nsubgraph Quality_Planning\n    direction LR\n    subgraph Quality_Planning_Tasks\n        direction TB\n        Risk_Hazard_Management_Plan---\n        Configuration_Management_Plan---\n        Software_Quality_Assurance_Plan---\n        Software_Verification_and_Validation_Plan---\n        Verification_and_Validation_Tasks---\n        Acceptance_Criteria---\n        Schedule_and_Resource_Allocation_for_V&V_activities---\n        Reporting_Requirements---\n        Formal_Design_Review_Requirements---\n        Other_Technical_Review_Requirements---\n        Problem_Reporting_and_Resolution_Procedures---\n        Other_Support_Activities\n    end\n    subgraph Inclusion_List_for_Plan\n        direction TB\n        specific_tasks_for_each_life_cycle_activity---\n        Enumeration_of_important_quality_factors--- \n        like_reliability_maintainability_usability---\n        Methods_and_procedures_for_each_task---\n        Task_acceptance_criteria---\n        Criteria_for_defining_and_documenting_outputs_for_input_requirements---\n        Inputs_for_each_task---\n        Outputs_from_each_task---\n        Roles_resources_and_responsibilities_for_each_task---\n        Risks_and_assumptions---\n        Documentation_of_user_needs    \n    end\n    subgraph Identification_List_for_Plan\n        direction TB\n        personnel---\n        facility_and_equipment_resources_for_each_task---\n        role_that_risk_hazard_management        \n    end\nQuality_Planning_Tasks-->Inclusion_List_for_Plan-->Identification_List_for_Plan\nend\n\n\n\n\n\n\n\n\n\n\nConfiguration Management\n\n\n\n\nflowchart LR\nsubgraph Configuration_Management\n    direction LR\n    subgraph Control\n        direction TB\n        control_multiple_parallel_development_activities---\n        ensure_positive_and_correct_correspondence_of---\n        specifications_documents---\n        source_code---\n        object_code---\n        test_suites---\n        ensure_accurate_identification_of_approved_versions---\n        ensure_access_to_approved_versions---\n        create_procedures_for_reporting---\n        create_procedures_for_resolving_SW_anomalies                            \n    end\n    subgraph Management\n        direction TB\n        identify_reports---\n        specify_contents---\n        specify_format---\n        specify_responsible_organizational_elements_for_each_report\n    end\n    subgraph Procedures\n        direction TB\n        necessary_for_review_of_SW_development_results---\n        necessary_for_approval_of_SW_development_results\n    end\n    ensure_proper_communications_and_documentation\n    Control-->Management-->Procedures-->ensure_proper_communications_and_documentation \nend\n\n\n\n\n\n\n\n\n\n\nTask Requirements\n\n\n\n\n\nflowchart TB\n    subgraph Task_Requirements\n        direction LR\n        subgraph group\n            direction TB\n            identification---\n            analysis---\n            predetermined_documentation_about_device_its_intended_use\n        end\n        \n        subgraph Requirements_Specification_List\n            direction TB\n            All_software_system_inputs---\n            All_software_system_outputs---\n            All_functions_that_software_system_will_perform---\n            All_performance_requirements_that_software_will_meet---\n            requirement_example_data_throughput_reliability_timing---\n            definition_of_all_external_and_user_interfaces---\n            any_internal_software_to_system_interfaces---\n            How_users_will_interact_with_system---\n            What_constitutes_error---\n            how_errors_should_be_handled---\n            Required_response_times---\n            Intended_operating_environment_for_software---\n            All_acceptable_ranges_limits_defaults_specific_values---\n            All_safety_related_requirements_that_will_be_implemented_in_SW---\n            All_safety_related_specifications_that_will_be_implemented_in_SW---\n            All_safety_related_features_that_will_be_implemented_in_SW---\n            All_safety_related_functions_that_will_be_implemented_in_SW---\n            clearly_identify_potential_hazards---\n            risk_evaluation_for_accuracy---\n            risk_evaluation_for_completeness---\n            risk_evaluation_for_consistency---\n            risk_evaluation_for_testability---\n            risk_evaluation_for_correctness---\n            risk_evaluation_for_clarity\n        end\n        subgraph Verfification_List_by_Evaluation\n            direction TB\n            no_internal_inconsistencies_among_requirements---\n            All_of_performance_requirements_for_system---\n            Complete_correct_Fault_tolerance_safety_security_requirements---\n            Accurate_Complete_Allocation_of_software_functions---\n            Appropriate_Software_requirements_for_system_hazards---\n            mesurable_requirements---\n            objectively_verifiable_requirements---\n            traceable_requirements\n        end\n        subgraph Requirements_Tasks\n            direction TB\n            Preliminary_Risk_Analysis---\n            Traceability_Analysis---\n            ex_Software_Requirements_to_System_Requirements_vice_versa---\n            ex_Software_Requirements_to_Risk_Analysis---\n            Description_of_User_Characteristics---\n            Listing_of_Characteristics_and_Limitations_of_Memory---\n            Software_Requirements_Evaluation---\n            Software_User_Interface_Requirements_Analysis---\n            System_Test_Plan_Generation---\n            Acceptance_Test_Plan_Generation---\n            Ambiguity_Review_or_Analysis\n        end\n    group-->Requirements_Specification_List \n    Verfification_List_by_Evaluation-->Requirements_Tasks\n    end\n\n\n\n\n\n\n\n\n\n\nDesign Overview\n\n\n\n\nflowchart TB\n    subgraph Deign_Task\n        direction LR\n    subgraph Design_Consideration_List\n        direction TB\n        subgraph Description\n                    direction TB\n                end\n        subgraph Human_Factors_Engineering\n          direction TB\n    \n        end\n        subgraph Safety_Usability_Issues_Conisderation\n            direction TB\n\n            end\n        Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n    subgraph Design_Specificiation\n        direction TB\n        subgraph Performing_List\n            direction TB\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n        end\n    Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design \n    end\n    subgraph Design_Activity_and_Task_List\n        direction TB\n        subgraph Final_Design_activity\n            direction TB\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n            end\n            subgraph Coding_Tasks\n                direction TB\n            end\n        traceability_analysis-->Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end\n    Design_Consideration_List---Design_Specificiation---Design_Activity_and_Task_List\n\n    end\n\n\n\n\n\n\n\n\n\nDesign Consideration\n\n\n\n\nflowchart TB\nsubgraph Design_Consideration_List\n    direction LR\n        subgraph Requirement_Specification\n            direction TB\n            logical_representation---\n            physical_representation\n        end\n    subgraph Description\n            direction TB\n            what_to_do---\n            how_to_do                   \n        end\n    subgraph Human_Factors_Engineering\n      direction TB\n            entire_design_and_development_process---\n            device_design_requirements---\n            analyses---\n            tests\n    end\n    subgraph Safety_Usability_Issues_Conisderation\n        direction TB\n                flowcharts--- \n                state_diagrams--- \n                prototyping_tools---\n                test_plans\n        end\n        Requirement_Specification---Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n\n\n\n\n\n\n\n\n\n\nDesign Specification\n\n\n\n\nflowchart TB\nsubgraph Design_Specificiation\n        direction LR\n        subgraph Conceptual_Specification\n            direction TB\n            requirements_specification---\n            predetermined_criteria---\n            Software_risk_analysis---\n            Development_procedures---\n            coding_guidelines\n        end\n        subgraph Performing_List\n            direction TB\n            task---\n            function_analyses---\n            risk_analyses---\n            prototype_tests_and_reviews---\n            full_usability_tests\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n            SW_requirements_specification---\n            predetermined_criteria_for_SW_acceptance---\n            SW_risk_analysis---\n            Development_procedure_list---\n            coding_guidance---\n            Systems_documentation---\n            Hardware_to_be_used---\n            Parameters_to_be_measured---\n            Logical_structure---\n            Control_logic---\n            logical_processing_steps_aka_algorithms---\n            Data_structures_diagram---\n            data_flow_diagrams---\n            Definitions_of_variables---\n            description_of_where_they_are_used---\n            Error_alarm_and_warning_messages---\n            Supporting_software---\n            internal_modules_Communication_links---\n            supporting_sw_links---\n            link_with_hardware---\n            link_with_user---\n            physical_Security_measures---\n            logical_security_measures\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n            complete--- \n            correct---\n            consistent--- \n            unambiguous--- \n            feasible---\n            maintainable---\n            analyses_of_control_flow---\n            data_flow--- \n            complexity--- \n            timing--- \n            sizing--- \n            memory_allocation---\n            module_architecture---\n            traceability_analysis_of_modules--- \n            criticality_analysis\n        end\n    Conceptual_Specification---Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design  \n    end\n\n\n\n\n\n\n\n\n\n\nDesign Activity and Task\n\n\n\n\n\nflowchart TB\nsubgraph Design_Activity_and_Task_List\n        direction LR\n        subgraph Final_Design_activity\n            direction TB\n            Formal_Design_Review_Before_Design_Implementation---\n            correct_consistent_complete_accurate_testable\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n            Updated_Software_Risk_Analysis---\n            Traceability_Analysis---\n            Software_Design_Evaluation---\n            Design_Communication_Link_Analysis---\n            Module_Test_Plan_Generation---\n            Integration_Test_Plan_Generation---\n            module_Test_Design_Generation---\n            integration_Test_Design_Generation---\n            system_Test_Design_Generation---\n            acceptance_Test_Design_Generation   \n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n                each_element_implementation---\n                each_module_implementation_to_element_and_risk_analysis---\n                each_functions_implemented_to_element_and_risk_analysis---\n                Tests_for_modules_to_element_and_risk_analysis--- \n                Tests_for_functions_to_element_and_risk_analysis---\n                Tests_for_modules_to_source_code---\n                Tests_for_functions_to_source_code\n            end\n            subgraph Coding_Tasks\n                direction TB\n                Traceability_Analyses---\n                Source_Code_to_Design_Specification_and_vice_versa---\n                Test_Cases_to_Source_Code_and_to_Design_Specification---\n                Source_Code_and_Source_Code_Documentation_Evaluation---\n                Source_Code_Interface_Analysis---\n                Test_Procedure_and_Test_Case_Generation \n            end\n        traceability_analysis-->Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end"
  },
  {
    "objectID": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#testing-task",
    "href": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#testing-task",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Testing Task",
    "text": "Testing Task\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction TB\n            subgraph Test_Plans\n                direction TB\n            end\n            subgraph Conditions\n                direction TB\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n        subgraph Code_Based_Testing\n            direction TB\n            subgraph white_box_testing\n                direction TB\n            end\n            subgraph Evaluation_of_level_of_white_box_testing\n                direction TB\n            end\n            subgraph Coverage_Metrics_of_White_Box_Testing\n                direction TB\n            end\n        white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n        end\n        subgraph Alternatives_to_White_Box_Testing\n            direction TB\n            subgraph Types_of_Functional_Software_Testing_Increasing_Cost\n                direction TB\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n            end\n            subgraph Change_in_SW\n                direction TB    \n            end\n        Types_of_Functional_Software_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW\n        end\n        \n\n        subgraph Development_Testing\n            direction TB\n            subgraph unit_level_testing\n                direction TB    \n            end\n            subgraph integration_level_testing\n                direction TB\n            end\n            subgraph system_level_testing\n                direction TB\n            end\n            subgraph Error_Detected\n                direction TB        \n            end\n        unit_level_testing-->integration_level_testing-->system_level_testing-->Error_Detected\n        end\n\n        subgraph Testing_Tasks\n            direction TB\n        end\n        subgraph User_Site_Testing\n            direction TB\n            subgraph Quality_System_Rregulation\n                direction TB\n            end\n            subgraph Understand_Terminology\n                direction TB\n            end\n            subgraph Testing\n                direction TB\n            end\n            Quality_System_Rregulation---Understand_Terminology---Testing\n        end\nConsideration_Before_Testing_Tasks---Code_Based_Testing---Alternatives_to_White_Box_Testing\nDevelopment_Testing---Testing_Tasks---User_Site_Testing\n    end\n\n\n\n\n\n\n\n\n\n\nConsideration Before Testing Tasks\n\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction LR\n            subgraph Test_Plans\n                direction TB\n                should_identify_control_measures_like_traceability_analysis---\n                ensure_that_intended_coverage_is_achieved---\n                ensure_that_proper_documentation_is_prepared---\n                conduct_tests_not_by_SW_developers_but_in_other_sites\n            end\n            subgraph Conditions\n                direction TB\n                use_defined_inputs---\n                documented_outcomes---\n                gonnabe_time_consuming_activity---\n                gonnabe_difficult_activity---\n                gonnabe_imperfect_activity---\n                testing_all_program_functionality---\n                does_not_mean_100_prcnt_correction_perfection---\n                make_detailed_objective_evaluation---\n                requires_sophisticated_definition_specificiation---\n                all_test_procedures_data_results_are_documented---\n                all_test_procedures_data_results_are_suitable_for_review---\n                all_test_procedures_data_results_are_suitable_for_objective_decision_making---\n                all_test_procedures_data_results_are_suitable_for_subsequent_regression_testing\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n                make_test_plans---\n                make_test_cases---\n                plan_schedules---\n                plan_environments---\n                plan_resources_of_personnel_tools---\n                plan_methodologies---\n                plan_inputs_procedures_outputs_expected_results---\n                plan_documentation---\n                plan_reporting_criteria\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n                expected_test_outcome_is_predefined---\n                good_test_case_has_high_probability_of_exposing_errors---\n                successful_test_is_one_that_finds_errors---\n                There_is_independence_from_coding---\n                Both_application_for_user_and_SW_for_programming_expertise_are_employed---\n                Testers_use_different_tools_from_coders---\n                Examining_only_the_usual_case_is_insufficient---\n                Test_documentation_permits_its_reuse---\n                Test_documentation_permits_independent_confirmation_---\n                of_pass/fail_test_outcome_during_subsequent_review\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n\nend\n\n\n\n\n\n\n\n\n\n\nCode Based Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n            subgraph Code_Based_Testing\n                direction LR\n                subgraph white_box_testing\n                    direction TB\n                    identify_dead_code_never_executed---\n                    conduct_unit_test---\n                    conduct_other_level_tests\n                end\n                subgraph Evaluation_of_level_of_white_box_testing\n                    direction TB\n                    use_coverage_metrics---\n                    metrics_of_completeness_of_test_selection_criteria---\n                    coverage_should_be_commensurate_with_level_of_SW_risk---\n                    coverage_means_100_prcnt_coverage\n                end\n                subgraph Coverage_Metrics_of_White_Box_Testing\n                    direction TB\n                    Statement_Coverage---\n                    Decision_or_Branch_Coverage---\n                    Condition_Coverage---\n                    Multi_Condition_Coverage\n                    Loop_Coverage---\n                    Path_Coverage---\n                    Data_Flow_Coverage\n                end\n            white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n            end\nend\n\n\n\n\n\n\n\n\n\n\nSolution to White Box Testing\n\n\n\n\n\n \nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Alternatives_to_White_Box_Testing\n            direction LR\n            subgraph Types_of_Testing_Increasing_Cost\n                direction TB\n                    Normal_Case---\n                    Output_Forcing---\n                    Robustness---\n                    Combinations_of_Inputs\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n                difficulty_in_linking_---\n                tests_completion_criteria_to_SW_reliability\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n                statistical_testing---\n                provide_further_assurance_of_reliability---\n                generate_randomly_test_data_from_defined_distributions---\n                distribution_defined_by_expected_use---\n                distribution_defined_by_hazardous_use---\n                distribution_defined_by_malicious_use---\n                large_test_data_cover_particular_areas_or_concerns---\n                statistical_testing_provides_high_structural_coverage---\n                statistical_testing_requires_stable_system---\n                structural_and_functional_testing_are_prerequisites_for_statistical_testing\n            end\n            subgraph Change_in_SW\n                direction TB\n                conduct_regression_analysis_and_testing---\n                should_demonstrate_correct_implementation---\n                should_demonstrate_no_adverse_impact_on_other_modules   \n            end\n            subgraph Testing_Tasks\n                direction TB\n                Test_Planning---\n                Structural_Test_Case_Identification---\n                Functional_Test_Case_Identification---\n                Traceability_Analysis_Testing---\n                Unit_Tests_to_Detailed_Design---\n                Integration_Tests_to_High_Level_Design---\n                System_Tests_to_Software_Requirements---\n                Unit_Test_Execution---\n                Integration_Test_Execution---\n                Functional_Test_Execution---\n                System_Test_Execution---\n                Acceptance_Test_Execution---\n                Test_Results_Evaluation---\n                Error_Evaluation_Resolution---\n                Final_Test_Report\n            end\n        Types_of_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW---Testing_Tasks\n        end\nend\n\n\n\n\n\n\n\n\n\n\nDevelopment Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Development_Testing\n            direction LR\n            subgraph unit_level_testing\n                direction TB    \n                focus_on_early_examination_of_sub_program_functionality---\n                ensure_functionality_invisible_at_system_level_examined---\n                ensure_quality_software_units_furnished_for_integration\n            end\n            subgraph integration_level_testing\n                direction TB\n                focuses_on_transfer_of_data---\n                focuses_on_control_across_program's_internal_and_external_interfaces\n            end\n            subgraph system_level_testing\n                direction TB\n                demonstrate_all_specified_functionality_exists---\n                demonstrate_SW_is_trustworthy---\n                verifies_as_built_program's_functionality_and_performance_on_requirements---\n                addresses_functional_concerns_and_intended_uses---\n                like_Performance_issues---\n                like_Responses_to_stress_conditions---\n                like_Operation_of_internal_and_external_security_features---\n                like_Effectiveness_of_recovery_procedures---\n                like_disaster_recovery---\n                like_Usability---\n                like_Compatibility_with_other_SW---\n                like_Behavior_in_each_of_the_defined_hardware_configurations---\n                like_Accuracy_of_documentation\n            end\n            subgraph Error_Detected\n                direction TB        \n                should_be_logged---\n                should_be_classified---\n                should_be_reviewed---\n                should_be_resolved_before_SW_release\n            end\n        unit_level_testing-->integration_level_testing-->system_level_testing-->Error_Detected\n        end\nend\n\n\n\n\n\n\n\n\n\n\nUser Site Testing\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph User_Site_Testing\n            direction LR\n            subgraph Quality_System_Rregulation\n                direction TB\n                installation---\n                inspection_procedures---\n                testing_appropriateness---\n                documentation_of_inspection---\n                testing_to_demonstrate_proper_installation\n            end\n            subgraph Understand_Terminology\n                direction TB\n                beta_test---\n                site_validation---\n                user_acceptance_test---\n                installation_verification---\n                installation_testing\n            end\n            subgraph Testing\n                direction TB\n                subgraph Requirements\n                    direction TB\n                    either_actual_or_simulated_use---\n                    verification_of_intended_functionality---\n                    constant_contact_FDA_center\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n    \n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_System_Ability\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_User_Ability\n                        direction TB\n        \n                    end \n                    subgraph Evaluation_of_Operator_Ability\n                        direction TB\n        \n                    end\n                constant_contact_FDA_center-->Follow_Predefiened_Plan-->Documented_Evidence-->\n            Evaluation_of_System_Ability-->Evaluation_of_User_Ability-->Evaluation_of_Operator_Ability    \n                end \n                        \n            \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements-->User_Site_Testing_Task\n        end\n        Quality_System_Rregulation-->    Understand_Terminology-->Testing-->User_Site_Testing_Task\n        end\nend\n\n\n\n\n\n\n\n\n\n\nTesting\n\n\n\n\nflowchart TB\n            subgraph Testing\n                direction LR\n                subgraph Requirements\n                    direction LR\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n                        either_actual_or_simulated_use---\n                        verification_of_intended_functionality---\n                        constant_contact_FDA_center---\n                        formal_summary_of_testing---\n                        record_of_formal_acceptance\n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n                        testing_plan_of_full_range_of_operating_conditions---\n                        testing_plan_to_detect_any_latent_faults---\n                        all_testing_procedures---\n                        test_input_data---\n                        test_results---\n                        hardware_installation_and_configuration---\n                        software_installation_and_configuration---\n                        exercising_measure_of_all_system_components---\n                        versions_of_all_system_components           \n                    end\n                    subgraph Evaluation\n                        direction TB\n                      subgraph Evaluation_of_System_Ability\n                            direction TB\n                            high_volume_of_data---\n                            heavy_loads_or_stresses---\n                            security\n                            subgraph fault_testing\n                                direction TB\n                                avoidance---\n                                detection---\n                                tolerance---\n                                recovery\n                            end\n                        security---fault_testing---\n                        error_message---\n                        implementation_of_safety_requirements\n                        end\n                      subgraph Evaluation_of_User_Ability\n                            direction TB\n                            ability_to_understand_system---\n                            ability_to_interface_with_system\n                        end \n                        subgraph Evaluation_of_Operator_Ability\n                            direction TB\n                            ability_to_perform_intended_functions---\n                            ability_to_respond_in_alarms---\n                            ability_to_respond_in_warnings---\n                            ability_to_respond_in_error_messages\n                        end\n\n                    end\n            Follow_Predefiened_Plan-->Documented_Evidence-->\n            Evaluation_of_System_Ability-->Evaluation_of_User_Ability-->Evaluation_of_Operator_Ability    \n            end     \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements-->User_Site_Testing_Task\n        end"
  },
  {
    "objectID": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#maintenance-and-software-changes",
    "href": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#maintenance-and-software-changes",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Maintenance and Software Changes",
    "text": "Maintenance and Software Changes\n\n\n\n\nflowchart LR\n    subgraph Hardware_VS_Software\n        direction LR\n        subgraph HW_maintenance_Inclusion\n            direction TB\n            preventive_hardware_maintenance_actions--- \n            component_replacement---\n            corrective_changes\n        end\n        subgraph SW_maintenance_Inclusion\n            direction TB\n            corrective---\n            perfective---\n            adaptive_maintenance---\n            not_include_preventive_maintenance_actions---\n            not_include_software_component_replacement\n        end\n    end\n    subgraph Maintenance_Type\n        direction TB\n        Corrective_maintenance---\n        Perfective_maintenance---\n        Adaptive_maintenance---\n        Sufficient_regression_analysis---\n        Sufficient_regression_testing\n    end\n    subgraph Factors_of_Validation_for_SW_change\n        direction TB\n        type_of_change---\n        development_products_affected---\n        impact_of_those_products_on_operation\n    end\n    subgraph Factors_of_Limitting_Validation_Effort\n        direction TB\n        documentation_of_design_structure---\n        documentation_of_interrelationships_of_modules---\n        documentation_of_interrelationships_of_interfaces---\n        test_documentation---\n    test_cases---\n        results_of_previous_verification_and_validation_testing\n    end\n    subgraph Maintenance_tasks\n        direction TB\n        Software_Validation_Plan_Revision---\n        Anomaly_Evaluation---\n        Problem_Identification_and_Resolution_Tracking---\n        Proposed_Change_Assessment---\n        Task_Iteration---\n        Documentation_Updating\n    end\nHardware_VS_Software-->Maintenance_Type-->Factors_of_Validation_for_SW_change-->\nFactors_of_Limitting_Validation_Effort-->Maintenance_tasks"
  },
  {
    "objectID": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-of-quality-system-software",
    "href": "docs/blog/posts/Validation/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-of-quality-system-software",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation of Quality System Software",
    "text": "Validation of Quality System Software\n\nOverview\n\n\n\n\nflowchart LR\n    subgraph Use_of_Computers_and_automated_equipment\n        direction TB\n        medical_device_design---\n        laboratory_testing_and_analysis---\n        product_inspection_and_acceptance---\n        production_and_process_control---\n        environmental_controls---\n        packaging---\n        labeling---\n        traceability---\n        document_control---\n        complaint_management---\n        programmable_logic_controllers---\n        digital_function_controllers---\n        statistical_process_control---\n        supervisory_control_and_data_acquisition---\n        robotics---\n        human_machine_interfaces---\n        input_output_devices---\n        computer_operating_systems\n    end\n    subgraph Factors_in_Validation\n        direction TB\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System-->Validation_Supporting_Factors-->Factors_of_Validation_Evidence_Level-->\nFactors_of_Easing_Validation_Effort\n    end\n    subgraph Documented_User_Requirements\n        direction TB\n        intended_use_of_software_or_automated_equipment---\n      level_of_dependency_on_software_or_equipment\n    end\n    subgraph List_That_Must_Be_Defined_by_User\n        direction TB\n        \n    end\n    subgraph Documentation_List\n        direction TB\n        documented_protocol---\n        documented_validation_results\n        subgraph Documented_Test_Cases\n            direction TB\n        \n        end\n        documented_validation_results---Documented_Test_Cases\n    end\n\n    subgraph Manufaturer's_Responsbility\n        direction TB\n        \n    end\nUse_of_Computers_and_automated_equipment---Factors_in_Validation---Documented_User_Requirements---\nList_That_Must_Be_Defined_by_User---Documentation_List---Manufaturer's_Responsbility\n\n\n\n\n\n\n\n\n\n\nFactors in Validation\n\n\n\n\nflowchart LR\n    subgraph Factors_in_Validation\n        direction LR\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n                electronic_records_regulation---\n                electronic_signatures_regulation---\n                regulations_establishment---\n                security---\n                data_integrity---\n                validation_requirements \n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n            verifications_of_outputs_from_each_stage--- \n            verifications_of_outputs_throught_SW_life_cycle---\n            checking_for_proper_operation_in_intended_use_environment\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n            risk_posed_by_automated_operation---\n            complexity_of_process_software---\n            degree_of_dependence_on_automated_process\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System-->Validation_Supporting_Factors-->Factors_of_Validation_Evidence_Level-->\nFactors_of_Easing_Validation_Effort\n    end"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-07-anova/index.html#similar-statistical-methods",
    "href": "docs/blog/posts/statistics/2023-01-07-anova/index.html#similar-statistical-methods",
    "title": "ANOVA",
    "section": "1 Similar Statistical Methods",
    "text": "1 Similar Statistical Methods\n\n2023-01-27, ANCOVA\n2023-01-27, repeated measures ANOVA\n2023-01-28, MANOVA\n2023-01-28, MANCOVA\n\n\n\n\nKorean\n\n\n\n\nEnglish\n\n\n\n\n\n\n1.1 Description\nANOVA는 3개 이상의 모집단 사이의 평균의 동일성을 검정하는 통계 분석 방법이다.\n\n일원 분산 분석 (One-way ANOVA)\n\n그룹을 구분하는 변수가 1개\nBetween-Groups one-way ANOVA(집단간 일원분산분석): 관측치를 grouping하는 범주형 변수가 1개이며 각 관측치는 범주형 변수에 의해 구분되는 그룹들 가운데 반드시 하나에만 할당되어야한다. 즉, 어떠한 경우에도 하나의 관측치 또는 샘플이 여러 groups에 동시에 들어가면 안된다. 이 때 이렇게 그룹을 나누는 범주형 변수를 집단간 요인이라고 한다.\nWithin-groups one-way ANOVA (집단 내 일원분산분석) or repeated measures ANOVA: 시간과 같은 하나의 범주형 변수로 샘플들을 측정한다. 시간의 경과에 따라 측정된 샘플들을 범주형 변수의 여러 기간에 걸쳐 모두 할당시킨다. 즉, 하나의 샘플이 여러 그룹에 다른 측정치로 관찰될 수 있다. 예를들어, sample A가 4주, 8주, 12주, 16주 그룹에 모두 측정 된다. 이때 기간변수는 집단 내 요인이라고 부른다.\n\n이원 분산 분석 (Two-way ANOVA)\n\n집단을 구분하는 변수가 2개이며 각 집단 간 요인과 집단 내 요인을 나타낸다.\n이원 분석 부터는 main effect와 interaction effect가 존재한다.\n범주형 변수 A와 범주형 변수 B의 Main effect 계산\n범주형 변수 A와 범주형 변수 B의 상호 작용 효과 or 교호 작용 효과 (Interaction effect) 계산\ngroup을 구분하는 독립변수가 2개 일때 모집단 간 평균의 동일성 검정\n\n2개의 주효과(main effect) 검정: 각 독립 변수에 의해 만들어지는 집단 간 평균의 차이에 대한 검정\n\n먼저, 두 독립변수가 종속변수에 개별적으로 영향을 미치는지 검정\n\n\n1개의 상호작용효과(interaction effect) 검정: 두 독립 변수의 조합에 의해 만들어지는 집단 간 평균의 차이에 대한 검정\n\n두 독립변수의 조합이 종속변수와 유의한 영향관계를 갖는지 검정\n\n만약에 유의하다면 2개의 독립변수가 합쳐져서 나온 파생효과이기 때문에 1개만 골라서 분석해서 해석 할 수 없음\n\n\n\n\n\n1.2 How to conduct ANOVA?\n\n분산 분석은 F검정(F test)을 통해 수행한다.\nF 검정은 집단 간 분산 (between-groups variability)과 집단 내 분산 (within-groups variability)의 ratio로 계산된 F값 (F value or F statistic)을 토대로 가설검정을 수행한다. 이때 F value or F statistic을 통계 검정을 위한 검정통계량 (test statistic) 라고 부른다.\nF 검정 결과가 통계적으로 유의하면 집단 간 평균의 차이가 존재한다. (즉, 독립 변수가 종속변수에 영향을 미침)\nF 분포 2개의 자유도에 의해 분포의 모양이 결정되며 대체로 오른쪽으로 긴 꼬리를 갖는다\n\n첫 번째 자유도: 집단 간(between-group)의 자유도\n두 번째 자유도: 집단 내(within-group)의 자유도\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(gplots)\nlibrary(rmarkdown)\nknitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)\noptions(digits = 5)\nset.seed(20230109)\n\n\n\n\n\n\n\n종속 변수의 변동성은 다음과 같이 설명되기 때문에 아래의 식을 만족한다.\n\\[SS_{total}=SS_A+SS_B+SS_{AB}+SS_{error}\\]\n\\(SS_{total}\\)은 쉽게 구할 수 있고 \\(SS_A\\), \\(SS_B\\), \\(SS_{error}\\)를 계산하여 빼준다.\nTwo Way Anova SS 계산 공식 링크\n\\(SS_{AB}\\) 즉,\n\\[ SS_{AB}=SS_{total}-SS_A-SS_B-SS_{error} \\]\n\n\n1.3 Meaning\nANOVA는 집단 간 분산과 집단 내 분산의 비교하는 방식으로, 좀 더 구체적으로는 집단 간 분산과 집단 내 분산의 비를 계산하여, 집단 간 분산이 클수록 그리고 집단 분산이 작을 수록 집단 평균이 다를 가능성이 증가한다는 알고리즘에 기초한다.\n\n\n2 Application to Example\n\n2.1 Data Description\n\n2.1.1 Raw Data\n(…통계 컨설팅 일부 발췌…)\n\n\n\nexample data는 Day, Run, response의 변수들을 포함하고 있습니다. 공유해주신 정보에 따르면 아마도 Run은 오전과 오후를 나누는 변수인 것으로 생각 됩니다. 이 data만 보면 아마도 같은 샘플에 대해서 시약 제품이 시간에 따라 얼마나 안정적인 performance를 보여주는지 검사하는 실험으로 추측됩니다. 좀 더 분석하기 용이한 형태로 data structure를 바꾸겠습니다.\n\n\n2.1.2 Processed Data\n\n\n\n\n  \n\n\n\n재가공된 data는 120개의 샘플과 5개의 변수를 갖고있습니다. 변수 목록은 다음과 같습니다.\n\nid: 열번호, 총 20일간 하루 2회 구동(AM, PM) 구동, 오전 오후 각 각 3번씩 구동 총 120 \\((=20 \\times 3 \\times 2)\\) 샘플\n\nDay: Day1~20\n\nnoon: AM= before noon, PM= after noon\nRun: 1회 구동당 3번 반복씩1, 2, 3\n\nresponse: response variable, 낮을 수록 좋음\n\nANOVA의 Assumption\n\nresponse variable should follow normal distribution.\n\nhomoscedasticity, equality of variance: 각 집단의 분포는 모두 동일한 분산을 가짐\nANOVA의 가정들을 반드시 충족하지 않아도 되지만 충족하면 Power 가 올라감\n\n\n\n\n2.2 EDA (Explorator Data Analysis)\n이 data는 아래 처럼 1의 결측치를 갖고 있습니다.\n\n\n\n\n\nid\nDay\nnoon\nRun\nresponse\n\n\n\n\n117\n20\nAM\n3\nNA\n\n\n\n\n\nCt에 대한 Global Statistics는 다음과 같습니다.\n\n\n\n\n\ncount\nglobal_response_mean\nglobal_response_sd\nglobal_response_CV\n\n\n\n\n119\n38.727\n18.47\n47.694 %\n\n\n\n\n\nDay groups의 Statistics은 다음과 같습니다.\n\n\n\n\n\n\n\n\n\n\n\n\nDay\ncount\nDay_group_response_mean\nDay_group_response_sd\nDay_group_response_CV\n\n\n\n\n1\n6\n47.342\n18.7880\n39.686 %\n\n\n2\n6\n41.203\n14.1028\n34.227 %\n\n\n3\n6\n43.261\n27.9790\n64.674 %\n\n\n4\n6\n33.042\n12.0743\n36.542 %\n\n\n5\n6\n44.006\n7.5597\n17.179 %\n\n\n6\n6\n40.529\n17.6875\n43.641 %\n\n\n7\n6\n40.919\n20.5293\n50.17 %\n\n\n8\n6\n42.977\n24.1723\n56.244 %\n\n\n9\n6\n55.894\n25.1692\n45.031 %\n\n\n10\n6\n34.639\n10.3367\n29.841 %\n\n\n11\n6\n49.187\n28.0760\n57.08 %\n\n\n12\n6\n26.939\n9.0372\n33.547 %\n\n\n13\n6\n43.865\n11.2744\n25.703 %\n\n\n14\n6\n28.819\n18.3234\n63.58 %\n\n\n15\n6\n28.677\n25.0912\n87.495 %\n\n\n16\n6\n31.516\n13.0811\n41.506 %\n\n\n17\n6\n30.061\n16.0155\n53.276 %\n\n\n18\n6\n35.277\n14.7036\n41.68 %\n\n\n19\n6\n32.971\n18.9421\n57.451 %\n\n\n20\n5\n44.347\n10.2720\n23.163 %\n\n\n\n\n\nAM/PM groups의 Statistics은 다음과 같습니다.\n\n\n\n\n\n\n\n\n\n\n\n\nnoon\ncount\nnoon_group_response_mean\nnoon_group_response_sd\nnoon_group_response_CV\n\n\n\n\nAM\n59\n40.380\n18.701\n46.313 %\n\n\nPM\n60\n37.101\n18.250\n49.191 %\n\n\n\n\n\nDays와 AM/PM 조합 groups의 Statistics은 다음과 같습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nnoon\ncount\ncombi_group_response_mean\ncombi_group_response_sd\ncombi_group_response_CV\n\n\n\n\n1\nAM\n3\n35.987\n20.9797\n58.298 %\n\n\n1\nPM\n3\n58.697\n7.4516\n12.695 %\n\n\n2\nAM\n3\n40.032\n18.6291\n46.535 %\n\n\n2\nPM\n3\n42.374\n12.0856\n28.521 %\n\n\n3\nAM\n3\n33.148\n22.1450\n66.806 %\n\n\n3\nPM\n3\n53.374\n34.0565\n63.807 %\n\n\n4\nAM\n3\n36.697\n16.3483\n44.55 %\n\n\n4\nPM\n3\n29.387\n7.5583\n25.72 %\n\n\n5\nAM\n3\n44.432\n4.6107\n10.377 %\n\n\n5\nPM\n3\n43.581\n11.0032\n25.248 %\n\n\n6\nAM\n3\n45.781\n23.6862\n51.739 %\n\n\n6\nPM\n3\n35.277\n11.7618\n33.341 %\n\n\n7\nAM\n3\n40.458\n12.3380\n30.496 %\n\n\n7\nPM\n3\n41.381\n30.0128\n72.529 %\n\n\n8\nAM\n3\n51.103\n33.4416\n65.439 %\n\n\n8\nPM\n3\n34.852\n12.0135\n34.47 %\n\n\n9\nAM\n3\n69.981\n7.7381\n11.057 %\n\n\n9\nPM\n3\n41.806\n30.4715\n72.887 %\n\n\n10\nAM\n3\n26.619\n3.4483\n12.954 %\n\n\n10\nPM\n3\n42.658\n7.8928\n18.502 %\n\n\n11\nAM\n3\n67.923\n28.8752\n42.512 %\n\n\n11\nPM\n3\n30.452\n9.1557\n30.066 %\n\n\n12\nAM\n3\n25.981\n5.9053\n22.729 %\n\n\n12\nPM\n3\n27.897\n12.9054\n46.261 %\n\n\n13\nAM\n3\n40.600\n15.2088\n37.46 %\n\n\n13\nPM\n3\n47.129\n7.3823\n15.664 %\n\n\n14\nAM\n3\n25.839\n8.4556\n32.725 %\n\n\n14\nPM\n3\n31.800\n27.2253\n85.614 %\n\n\n15\nAM\n3\n18.458\n4.5063\n24.414 %\n\n\n15\nPM\n3\n38.897\n35.2180\n90.542 %\n\n\n16\nAM\n3\n36.910\n18.3756\n49.785 %\n\n\n16\nPM\n3\n26.123\n1.6899\n6.469 %\n\n\n17\nAM\n3\n41.594\n13.3474\n32.09 %\n\n\n17\nPM\n3\n18.529\n8.0068\n43.212 %\n\n\n18\nAM\n3\n38.471\n6.0067\n15.614 %\n\n\n18\nPM\n3\n32.084\n21.7672\n67.845 %\n\n\n19\nAM\n3\n45.923\n19.7931\n43.101 %\n\n\n19\nPM\n3\n20.019\n1.4176\n7.081 %\n\n\n20\nAM\n2\n42.303\n3.9142\n9.253 %\n\n\n20\nPM\n3\n45.710\n14.0145\n30.66 %\n\n\n\n\n\n이제 ANOVA를 수행하기 위한 basic statistics는 모두 구했습니다. ANOVA를 수행하기 위해 집단 간 분산과 집단 내 분산을 계산하도록 하겠습니다.\n\n\n2.3 집단 간 분산\n앞에서 설명 드린바로 유추해보면 예시 data의 집단 간 분산의 범주형 변수는 Day로 설정하는 것이 합리적인 것으로 보입니다.\n\n\\(g=g\\) Day의 sample size = 20, 자유도 = 20-1 = 19 입니다.\n\\(n_g=g\\) group의 sample size, \\(\\overline{X}_g=g\\) 의 sample mean은 다음과 같습니다.\n\\(\\overline{X}\\) = global sample mean = 38.72681\n집단 간 분산: \\(\\frac{집단 간 제곱합}{자유도}=\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n2.3.1 SS_Day (집단간 분산 Day)\nDay sq = \\([(\\overline{X}_g-\\overline{X})^2n_g]\\) 집단(Day)간 분산 계산, 집단(Day)간 분산 = \\(\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\ncount\nDay_group_response_mean\nDay_group_response_sd\nDay_group_response_CV\nday_sq\nsum_day_ssq\ndf\nday_mean_ssq\nday_sd\n\n\n\n\n1\n6\n47.342\n18.7880\n39.686 %\n445.322\n7026\n19\n369.79\n19.23\n\n\n2\n6\n41.203\n14.1028\n34.227 %\n36.796\n7026\n19\n369.79\n19.23\n\n\n3\n6\n43.261\n27.9790\n64.674 %\n123.369\n7026\n19\n369.79\n19.23\n\n\n4\n6\n33.042\n12.0743\n36.542 %\n193.907\n7026\n19\n369.79\n19.23\n\n\n5\n6\n44.006\n7.5597\n17.179 %\n167.248\n7026\n19\n369.79\n19.23\n\n\n6\n6\n40.529\n17.6875\n43.641 %\n19.488\n7026\n19\n369.79\n19.23\n\n\n7\n6\n40.919\n20.5293\n50.17 %\n28.844\n7026\n19\n369.79\n19.23\n\n\n8\n6\n42.977\n24.1723\n56.244 %\n108.406\n7026\n19\n369.79\n19.23\n\n\n9\n6\n55.894\n25.1692\n45.031 %\n1768.182\n7026\n19\n369.79\n19.23\n\n\n10\n6\n34.639\n10.3367\n29.841 %\n100.275\n7026\n19\n369.79\n19.23\n\n\n11\n6\n49.187\n28.0760\n57.08 %\n656.506\n7026\n19\n369.79\n19.23\n\n\n12\n6\n26.939\n9.0372\n33.547 %\n833.756\n7026\n19\n369.79\n19.23\n\n\n13\n6\n43.865\n11.2744\n25.703 %\n158.376\n7026\n19\n369.79\n19.23\n\n\n14\n6\n28.819\n18.3234\n63.58 %\n588.946\n7026\n19\n369.79\n19.23\n\n\n15\n6\n28.677\n25.0912\n87.495 %\n605.941\n7026\n19\n369.79\n19.23\n\n\n16\n6\n31.516\n13.0811\n41.506 %\n311.963\n7026\n19\n369.79\n19.23\n\n\n17\n6\n30.061\n16.0155\n53.276 %\n450.547\n7026\n19\n369.79\n19.23\n\n\n18\n6\n35.277\n14.7036\n41.68 %\n71.390\n7026\n19\n369.79\n19.23\n\n\n19\n6\n32.971\n18.9421\n57.451 %\n198.778\n7026\n19\n369.79\n19.23\n\n\n20\n5\n44.347\n10.2720\n23.163 %\n157.938\n7026\n19\n369.79\n19.23\n\n\n\n\n\nAnalysis-In program의 ANOVA결과값과 일치하는 것을 볼 수 있습니다. $SS_{day} $= 7025.97838 with \\(df=19\\).\n\n\n2.3.2 SS_noon (집단간 분산 noon)\nnoon sq = \\([(\\overline{X}_g-\\overline{X})^2n_g]\\) 집단(noon)간 분산 계산, 집단(noon)간 분산 = \\(\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnoon\ncount\nnoon_group_response_mean\nnoon_group_response_sd\nnoon_group_response_CV\nnoon_sq\nsum_noon_ssq\ndf\nnoon_mean_ssq\nnoon_sd\n\n\n\n\nAM\n59\n40.380\n18.701\n46.313 %\n161.23\n319.76\n1\n319.76\n17.882\n\n\nPM\n60\n37.101\n18.250\n49.191 %\n158.54\n319.76\n1\n319.76\n17.882\n\n\n\n\n\nAnalysis-In program의 결과에서 찾아 볼 수 없죠? 이 결과는 숨어 있습니다. 상호 작용에 대한 분산값을 구하고 나면 정체를 알 수 있습니다.\n\\(SS_{noon}\\) = 319.76458 with \\(df=1\\).\n\n\n2.3.3 SS_error (집단내 분산)\n\n집단 내 분산 (within-groups variability)\n\n\n\n# A tibble: 119 × 10\n   Day   noon    Run response count combi_group_…¹ resid…²    df resid…³ mean_…⁴\n   <fct> <fct> <int>    <dbl> <int>          <dbl>   <dbl> <dbl>   <dbl>   <dbl>\n 1 1     AM        1     60.2     3           36.0 5.86e+2    79  24704.    313.\n 2 1     AM        2     22.9     3           36.0 1.71e+2    79  24704.    313.\n 3 1     AM        3     24.8     3           36.0 1.24e+2    79  24704.    313.\n 4 1     PM        1     61.9     3           58.7 1.02e+1    79  24704.    313.\n 5 1     PM        2     50.2     3           58.7 7.25e+1    79  24704.    313.\n 6 1     PM        3     64.0     3           58.7 2.83e+1    79  24704.    313.\n 7 2     AM        1     21.4     3           40.0 3.46e+2    79  24704.    313.\n 8 2     AM        2     40.0     3           40.0 5.04e-3    79  24704.    313.\n 9 2     AM        3     58.7     3           40.0 3.48e+2    79  24704.    313.\n10 2     PM        1     38.5     3           42.4 1.52e+1    79  24704.    313.\n# … with 109 more rows, and abbreviated variable names\n#   ¹​combi_group_response_mean, ²​residual_sq, ³​residual_ssq, ⁴​mean_residual_ssq\n\n\n\\(SS_{error}\\) = 2.47041^{4}\nAnalysis-In program의 결과와 일치하는 것을 확인할 수 있습니다.\n\n\n2.3.4 SS_total\n\n\n# A tibble: 119 × 6\n      id Day   noon    Run response total_ssq\n   <int> <fct> <fct> <int>    <dbl>     <dbl>\n 1     1 1     AM        1     60.2    40256.\n 2     2 1     AM        2     22.9    40256.\n 3     3 1     AM        3     24.8    40256.\n 4     4 1     PM        1     61.9    40256.\n 5     5 1     PM        2     50.2    40256.\n 6     6 1     PM        3     64.0    40256.\n 7     7 2     AM        1     21.4    40256.\n 8     8 2     AM        2     40.0    40256.\n 9     9 2     AM        3     58.7    40256.\n10    10 2     PM        1     38.5    40256.\n# … with 109 more rows\n\n\n\\(SS_{total}\\) = 4.02557^{4}\nAnalysis-In program의 ANOVA 결과 table에 있는 SS들의 합과 일치하는 것을 확인할 수 있습니다.\n\n\n2.3.5 상호 작용 분산\n\n\n\n\\(SS_{interaction}=SS_{DayNoon}= SS_{total}-SS_{Day}-SS_{noon}-SS_{error}\\) = 4.02557{4}-2.47041{4}-319.76458-7025.97838 = 8205.93974\nAnalysis-In program의 ANOVA 결과 table과 일치하는 것을 확인할 수 있습니다.\n위의 결과들을 종합하면 아래와 같이 요약됩니다.\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   7026     370    1.18   0.29\nnoon         1    339     339    1.08   0.30\nDay:noon    19   8187     431    1.38   0.16\nResiduals   79  24704     313               \n1 observation deleted due to missingness\n\n\n\nRepeatability SD = \\(\\sqrt{V_{error}}=\\sqrt{MS_{error}}\\) = 17.6836\nRepeatability CV = \\(\\frac{repeatability \\space SD}{global \\space mean \\space response}\\) = 0.45662\n\n(생략, 다른 통계량들은 ANOVA_A repeat2.xlsx 참조)\n위의 결과를 간단히 해석해 보면\n\n집단간 범주 변수인 Day는 p-value =0.29>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, 일별로 평균 response값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, 일별로 평균 response값이 다르지 않습니다.\n\n집단간 범주 변수인 noon은 p-value =0.30>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, 오전/오후별 평균 response값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, 오전/오후별 평균 response값이 다르지 않습니다.\n\nDay와 noon두 변수의 상호작용 변수는 p-value =0.16>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, Day별 Noon(오전/오후)별 평균 response값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, Day별 Noon(오전/오후)별 평균 response값이 다르지 않습니다.\n\n\n최종 결론, 제품의 response값이 Day별 오전/오후별 안정적인 performance를 보인다고 조심스럽게 결론을 내릴 수 있습니다.\n이제 까지는 질문에 대한 답이 되는 ANOVA의 원리 및 통계량의 재현 및 해석법에 대하여 알아봤습니다. 하지만 직관적으로 어떤 의미가 있을 까요? 원래는 시각화를 통해 데이터의 패턴을 짐작하고 통계 검정 결과를 예상하는데 우리는 반대로 가고 있네요 ㅎㅎ 시각화를 통해 ANOVA 결과가 얼마나 직관적인지 알아보겠습니다.\n\n\n\n2.4 Visualization\n\n2.4.1 One-way: Day\n\n\n\n\n\n\n\n\n자세히 보면 일별로 시간의 경과에 따라 전체적으로 평균 response값이 약간 하향하는 것으로 보입니다. 하지만 좀 더 세부적으로 관찰하면 1일~8일 평균 response의 경향이 constant한 패턴을 보입니다. 9일~13일 평균 response가 진동 하향하는 패턴을 보입니다. 14일~20일 평균 response가 상향하는 패턴을 보입니다.\n\n\nTables of means\nGrand mean\n       \n38.727 \n\n Day \n        1    2     3     4     5     6     7     8     9    10    11    12\n    47.34 41.2 43.26 33.04 44.01 40.53 40.92 42.98 55.89 34.64 49.19 26.94\nrep  6.00  6.0  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00\n       13    14    15    16    17    18    19    20\n    43.86 28.82 28.68 31.52 30.06 35.28 32.97 44.35\nrep  6.00  6.00  6.00  6.00  6.00  6.00  6.00  5.00\n\n\nTables of effects\n\n Day \n        1     2     3      4    5     6     7     8     9     10    11     12\n    8.615 2.476 4.534 -5.685 5.28 1.802 2.193 4.251 17.17 -4.088 10.46 -11.79\nrep 6.000 6.000 6.000  6.000 6.00 6.000 6.000 6.000  6.00  6.000  6.00   6.00\n       13     14     15     16     17     18     19   20\n    5.138 -9.907 -10.05 -7.211 -8.666 -3.449 -5.756 5.62\nrep 6.000  6.000   6.00  6.000  6.000  6.000  6.000 5.00\n\n\n위에 첫 번째표에서 Global Sample response Mean = 38.727 과 각 집단의 평균 response를 확인할 수 있습니다. 위에 두 번째표에서 Global Sample response Mean = 37.322 과 각 집단의 평균 response의 차이를 확인할 수 있습니다.\n\nDay 9에서 차이가 가장 큰 것으로 보아 9일째 실험에서 performance가 가장 낮은 것이 관측됐습니다.\n반대로, 12일에 performance 가장 좋은 것으로 관측됐습니다.\n\n9일과 12일에 response값에 영향을 미쳤던 요인이 있었는지 복기 하는것도 도움이 되겠군요.\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   7026     370     1.1   0.36\nResiduals   99  33230     336               \n1 observation deleted due to missingness\n\n\nOne-way ANOVA의 결과값입니다. Day별 평균 response의 차이는 거의 없는 것으로 보입니다. 따라서 Day 별 평균 response의 경향이 일관되지 않고 One-way ANOVA에서 역시 통계적으로 유의하지 않아 Day 변수는 평균 response에 영향을 미치지 않는 것 같습니다.\n\n\n2.4.2 One-way: AM/PM\n\n\n\n\n\n\n\n\n오후에 시간의 경과에 따라 전체적으로 평균 response값이 약간 하향하는 것으로 보입니다.\n\n\nTables of means\nGrand mean\n       \n38.727 \n\n noon \n       AM   PM\n    40.38 37.1\nrep 59.00 60.0\n\n\nTables of effects\n\n noon \n        AM     PM\n     1.653 -1.626\nrep 59.000 60.000\n\n\n위 첫 번째 표에서 AM/PM 간의 평균 response차이는 0.15 (농도가 약 0.5배) 차이가 나는 것을 확인할 수 있습니다. 생물학적으로 의미가 있는 수치일까요? 위 두 번째 표에서 Global Sample Mean 37.322와 오전/오후 별 약 0.07씩(농도가 약 0.25배) 차이가 납니다.\n\n\n             Df Sum Sq Mean Sq F value Pr(>F)\nnoon          1    320     320    0.94   0.34\nResiduals   117  39936     341               \n1 observation deleted due to missingness\n\n\n오전 오후별 One way ANOVA를 실행한 결과가 오전/오부 평균 response값의 차이가 다르지 않다는 것을 시사하고 있습니다. 아무래도 위의 차이는 우연에 의해 발생한 현상인 것 같습니다.\n\n\n\n\n\n일별로 평균 response값이 차이를 모든 경우의 수에서 차이가 나는지 확인한 결과 신뢰구간 모두가 0을 포함하고 있으므로 Day 변수는 유의하다고 볼 수 없습니다.\n\n\n\n\n\n오전/오후별로 평균 response값이 차이를 모든 경우의 수에서 차이가 나는지 확인한 결과 신뢰구간 모두가 0을 포함하고 있으므로 오전/오후 변수는 유의하다고 볼 수 없습니다.\n여기 까지 각 변수별 평균 response로의 영향도를 통계적으로 시각적으로 관찰했습니다. 하지만 Day별 오전/오후별 영향도가 있는지 확인하겠습니다. (이미 위에서 통계적으로 없다고 검정됐습니다.)\n\n\n\n2.5 Two way Anova\n\n\nTables of means\nGrand mean\n       \n38.727 \n\n Day \n        1    2     3     4     5     6     7     8     9    10    11    12\n    47.34 41.2 43.26 33.04 44.01 40.53 40.92 42.98 55.89 34.64 49.19 26.94\nrep  6.00  6.0  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00\n       13    14    15    16    17    18    19    20\n    43.86 28.82 28.68 31.52 30.06 35.28 32.97 44.35\nrep  6.00  6.00  6.00  6.00  6.00  6.00  6.00  5.00\n\n noon \n       AM    PM\n    40.43 37.05\nrep 59.00 60.00\n\n Day:noon \n     noon\nDay   AM    PM   \n  1   35.99 58.70\n  rep  3.00  3.00\n  2   40.03 42.37\n  rep  3.00  3.00\n  3   33.15 53.37\n  rep  3.00  3.00\n  4   36.70 29.39\n  rep  3.00  3.00\n  5   44.43 43.58\n  rep  3.00  3.00\n  6   45.78 35.28\n  rep  3.00  3.00\n  7   40.46 41.38\n  rep  3.00  3.00\n  8   51.10 34.85\n  rep  3.00  3.00\n  9   69.98 41.81\n  rep  3.00  3.00\n  10  26.62 42.66\n  rep  3.00  3.00\n  11  67.92 30.45\n  rep  3.00  3.00\n  12  25.98 27.90\n  rep  3.00  3.00\n  13  40.60 47.13\n  rep  3.00  3.00\n  14  25.84 31.80\n  rep  3.00  3.00\n  15  18.46 38.90\n  rep  3.00  3.00\n  16  36.91 26.12\n  rep  3.00  3.00\n  17  41.59 18.53\n  rep  3.00  3.00\n  18  38.47 32.08\n  rep  3.00  3.00\n  19  45.92 20.02\n  rep  3.00  3.00\n  20  42.30 45.71\n  rep  2.00  3.00\n\n\nTables of effects\n\n Day \n        1     2     3      4    5     6     7     8     9     10    11     12\n    8.615 2.476 4.534 -5.685 5.28 1.802 2.193 4.251 17.17 -4.088 10.46 -11.79\nrep 6.000 6.000 6.000  6.000 6.00 6.000 6.000 6.000  6.00  6.000  6.00   6.00\n       13     14     15     16     17     18     19   20\n    5.138 -9.907 -10.05 -7.211 -8.666 -3.449 -5.756 5.62\nrep 6.000  6.000   6.00  6.000  6.000  6.000  6.000 5.00\n\n noon \n        AM     PM\n     1.701 -1.672\nrep 59.000 60.000\n\n Day:noon \n     noon\nDay   AM      PM     \n  1   -13.044  13.044\n  rep   3.000   3.000\n  2    -2.860   2.860\n  rep   3.000   3.000\n  3   -11.802  11.802\n  rep   3.000   3.000\n  4     1.966  -1.966\n  rep   3.000   3.000\n  5    -1.263   1.263\n  rep   3.000   3.000\n  6     3.562  -3.562\n  rep   3.000   3.000\n  7    -2.151   2.151\n  rep   3.000   3.000\n  8     6.437  -6.437\n  rep   3.000   3.000\n  9    12.398 -12.398\n  rep   3.000   3.000\n  10   -9.709   9.709\n  rep   3.000   3.000\n  11   17.046 -17.046\n  rep   3.000   3.000\n  12   -2.647   2.647\n  rep   3.000   3.000\n  13   -4.954   4.954\n  rep   3.000   3.000\n  14   -4.670   4.670\n  rep   3.000   3.000\n  15  -11.909  11.909\n  rep   3.000   3.000\n  16    3.704  -3.704\n  rep   3.000   3.000\n  17    9.843  -9.843\n  rep   3.000   3.000\n  18    1.504  -1.504\n  rep   3.000   3.000\n  19   11.262 -11.262\n  rep   3.000   3.000\n  20   -4.071   2.714\n  rep   2.000   3.000\n\n\none way ANOVA와 같이 해석\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   7026     370    1.18   0.29\nnoon         1    339     339    1.08   0.30\nDay:noon    19   8187     431    1.38   0.16\nResiduals   79  24704     313               \n1 observation deleted due to missingness\n\n\n위 그림을 보듯이 두 변수의 영향도가 없음, ANOVA 역시 유의하지 않음\n\n\n\n\n\n\n Missing rows: 117 \n\n\n\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = response ~ Day + noon + Day:noon, data = data)\n\n$Day\n            diff     lwr     upr   p adj\n2-1    -6.138710 -43.575 31.2979 1.00000\n3-1    -4.080645 -41.517 33.3560 1.00000\n4-1   -14.300000 -51.737 23.1366 0.99700\n5-1    -3.335484 -40.772 34.1011 1.00000\n6-1    -6.812903 -44.250 30.6237 1.00000\n7-1    -6.422581 -43.859 31.0140 1.00000\n8-1    -4.364516 -41.801 33.0721 1.00000\n9-1     8.551613 -28.885 45.9882 1.00000\n10-1  -12.703226 -50.140 24.7334 0.99934\n11-1    1.845161 -35.591 39.2818 1.00000\n12-1  -20.403226 -57.840 17.0334 0.89330\n13-1   -3.477419 -40.914 33.9592 1.00000\n14-1  -18.522581 -55.959 18.9140 0.95243\n15-1  -18.664516 -56.101 18.7721 0.94905\n16-1  -15.825806 -53.262 21.6108 0.99024\n17-1  -17.280645 -54.717 20.1560 0.97539\n18-1  -12.064516 -49.501 25.3721 0.99968\n19-1  -14.370968 -51.808 23.0657 0.99682\n20-1   -2.994839 -42.259 36.2690 1.00000\n3-2     2.058065 -35.379 39.4947 1.00000\n4-2    -8.161290 -45.598 29.2753 1.00000\n5-2     2.803226 -34.633 40.2399 1.00000\n6-2    -0.674194 -38.111 36.7624 1.00000\n7-2    -0.283871 -37.721 37.1528 1.00000\n8-2     1.774194 -35.662 39.2108 1.00000\n9-2    14.690323 -22.746 52.1270 0.99585\n10-2   -6.564516 -44.001 30.8721 1.00000\n11-2    7.983871 -29.453 45.4205 1.00000\n12-2  -14.264516 -51.701 23.1721 0.99709\n13-2    2.661290 -34.775 40.0979 1.00000\n14-2  -12.383871 -49.821 25.0528 0.99953\n15-2  -12.525806 -49.962 24.9108 0.99946\n16-2   -9.687097 -47.124 27.7495 0.99999\n17-2  -11.141935 -48.579 26.2947 0.99990\n18-2   -5.925806 -43.362 31.5108 1.00000\n19-2   -8.232258 -45.669 29.2044 1.00000\n20-2    3.143871 -36.120 42.4077 1.00000\n4-3   -10.219355 -47.656 27.2173 0.99997\n5-3     0.745161 -36.691 38.1818 1.00000\n6-3    -2.732258 -40.169 34.7044 1.00000\n7-3    -2.341935 -39.779 35.0947 1.00000\n8-3    -0.283871 -37.721 37.1528 1.00000\n9-3    12.632258 -24.804 50.0689 0.99939\n10-3   -8.622581 -46.059 28.8140 1.00000\n11-3    5.925806 -31.511 43.3624 1.00000\n12-3  -16.322581 -53.759 21.1140 0.98634\n13-3    0.603226 -36.833 38.0399 1.00000\n14-3  -14.441935 -51.879 22.9947 0.99662\n15-3  -14.583871 -52.021 22.8528 0.99620\n16-3  -11.745161 -49.182 25.6915 0.99978\n17-3  -13.200000 -50.637 24.2366 0.99891\n18-3   -7.983871 -45.421 29.4528 1.00000\n19-3  -10.290323 -47.727 27.1463 0.99997\n20-3    1.085806 -38.178 40.3497 1.00000\n5-4    10.964516 -26.472 48.4011 0.99992\n6-4     7.487097 -29.950 44.9237 1.00000\n7-4     7.877419 -29.559 45.3140 1.00000\n8-4     9.935484 -27.501 47.3721 0.99998\n9-4    22.851613 -14.585 60.2882 0.76824\n10-4    1.596774 -35.840 39.0334 1.00000\n11-4   16.145161 -21.291 53.5818 0.98785\n12-4   -6.103226 -43.540 31.3334 1.00000\n13-4   10.822581 -26.614 48.2592 0.99993\n14-4   -4.222581 -41.659 33.2140 1.00000\n15-4   -4.364516 -41.801 33.0721 1.00000\n16-4   -1.525806 -38.962 35.9108 1.00000\n17-4   -2.980645 -40.417 34.4560 1.00000\n18-4    2.235484 -35.201 39.6721 1.00000\n19-4   -0.070968 -37.508 37.3657 1.00000\n20-4   11.305161 -27.959 50.5690 0.99994\n6-5    -3.477419 -40.914 33.9592 1.00000\n7-5    -3.087097 -40.524 34.3495 1.00000\n8-5    -1.029032 -38.466 36.4076 1.00000\n9-5    11.887097 -25.550 49.3237 0.99974\n10-5   -9.367742 -46.804 28.0689 0.99999\n11-5    5.180645 -32.256 42.6173 1.00000\n12-5  -17.067742 -54.504 20.3689 0.97827\n13-5   -0.141935 -37.579 37.2947 1.00000\n14-5  -15.187097 -52.624 22.2495 0.99387\n15-5  -15.329032 -52.766 22.1076 0.99318\n16-5  -12.490323 -49.927 24.9463 0.99948\n17-5  -13.945161 -51.382 23.4915 0.99780\n18-5   -8.729032 -46.166 28.7076 1.00000\n19-5  -11.035484 -48.472 26.4011 0.99991\n20-5    0.340645 -38.923 39.6045 1.00000\n7-6     0.390323 -37.046 37.8270 1.00000\n8-6     2.448387 -34.988 39.8850 1.00000\n9-6    15.364516 -22.072 52.8011 0.99300\n10-6   -5.890323 -43.327 31.5463 1.00000\n11-6    8.658065 -28.779 46.0947 1.00000\n12-6  -13.590323 -51.027 23.8463 0.99841\n13-6    3.335484 -34.101 40.7721 1.00000\n14-6  -11.709677 -49.146 25.7270 0.99979\n15-6  -11.851613 -49.288 25.5850 0.99975\n16-6   -9.012903 -46.450 28.4237 1.00000\n17-6  -10.467742 -47.904 26.9689 0.99996\n18-6   -5.251613 -42.688 32.1850 1.00000\n19-6   -7.558065 -44.995 29.8786 1.00000\n20-6    3.818065 -35.446 43.0819 1.00000\n8-7     2.058065 -35.379 39.4947 1.00000\n9-7    14.974194 -22.462 52.4108 0.99480\n10-7   -6.280645 -43.717 31.1560 1.00000\n11-7    8.267742 -29.169 45.7044 1.00000\n12-7  -13.980645 -51.417 23.4560 0.99773\n13-7    2.945161 -34.491 40.3818 1.00000\n14-7  -12.100000 -49.537 25.3366 0.99966\n15-7  -12.241935 -49.679 25.1947 0.99960\n16-7   -9.403226 -46.840 28.0334 0.99999\n17-7  -10.858065 -48.295 26.5786 0.99993\n18-7   -5.641935 -43.079 31.7947 1.00000\n19-7   -7.948387 -45.385 29.4882 1.00000\n20-7    3.427742 -35.836 42.6916 1.00000\n9-8    12.916129 -24.521 50.3528 0.99918\n10-8   -8.338710 -45.775 29.0979 1.00000\n11-8    6.209677 -31.227 43.6463 1.00000\n12-8  -16.038710 -53.475 21.3979 0.98869\n13-8    0.887097 -36.550 38.3237 1.00000\n14-8  -14.158065 -51.595 23.2786 0.99735\n15-8  -14.300000 -51.737 23.1366 0.99700\n16-8  -11.461290 -48.898 25.9753 0.99984\n17-8  -12.916129 -50.353 24.5205 0.99918\n18-8   -7.700000 -45.137 29.7366 1.00000\n19-8  -10.006452 -47.443 27.4302 0.99998\n20-8    1.369677 -37.894 40.6335 1.00000\n10-9  -21.254839 -58.691 16.1818 0.85576\n11-9   -6.706452 -44.143 30.7302 1.00000\n12-9  -28.954839 -66.391  8.4818 0.35269\n13-9  -12.029032 -49.466 25.4076 0.99969\n14-9  -27.074194 -64.511 10.3624 0.47719\n15-9  -27.216129 -64.653 10.2205 0.46731\n16-9  -24.377419 -61.814 13.0592 0.66832\n17-9  -25.832258 -63.269 11.6044 0.56535\n18-9  -20.616129 -58.053 16.8205 0.88455\n19-9  -22.922581 -60.359 14.5140 0.76389\n20-9  -11.546452 -50.810 27.7174 0.99991\n11-10  14.548387 -22.888 51.9850 0.99631\n12-10  -7.700000 -45.137 29.7366 1.00000\n13-10   9.225806 -28.211 46.6624 0.99999\n14-10  -5.819355 -43.256 31.6173 1.00000\n15-10  -5.961290 -43.398 31.4753 1.00000\n16-10  -3.122581 -40.559 34.3140 1.00000\n17-10  -4.577419 -42.014 32.8592 1.00000\n18-10   0.638710 -36.798 38.0753 1.00000\n19-10  -1.667742 -39.104 35.7689 1.00000\n20-10   9.708387 -29.555 48.9723 0.99999\n12-11 -22.248387 -59.685 15.1882 0.80373\n13-11  -5.322581 -42.759 32.1140 1.00000\n14-11 -20.367742 -57.804 17.0689 0.89472\n15-11 -20.509677 -57.946 16.9270 0.88898\n16-11 -17.670968 -55.108 19.7657 0.96936\n17-11 -19.125806 -56.562 18.3108 0.93692\n18-11 -13.909677 -51.346 23.5270 0.99787\n19-11 -16.216129 -53.653 21.2205 0.98726\n20-11  -4.840000 -44.104 34.4239 1.00000\n13-12  16.925806 -20.511 54.3624 0.98004\n14-12   1.880645 -35.556 39.3173 1.00000\n15-12   1.738710 -35.698 39.1753 1.00000\n16-12   4.577419 -32.859 42.0140 1.00000\n17-12   3.122581 -34.314 40.5592 1.00000\n18-12   8.338710 -29.098 45.7753 1.00000\n19-12   6.032258 -31.404 43.4689 1.00000\n20-12  17.408387 -21.855 56.6723 0.98369\n14-13 -15.045161 -52.482 22.3915 0.99450\n15-13 -15.187097 -52.624 22.2495 0.99387\n16-13 -12.348387 -49.785 25.0882 0.99955\n17-13 -13.803226 -51.240 23.6334 0.99807\n18-13  -8.587097 -46.024 28.8495 1.00000\n19-13 -10.893548 -48.330 26.5431 0.99992\n20-13   0.482581 -38.781 39.7464 1.00000\n15-14  -0.141935 -37.579 37.2947 1.00000\n16-14   2.696774 -34.740 40.1334 1.00000\n17-14   1.241935 -36.195 38.6786 1.00000\n18-14   6.458065 -30.979 43.8947 1.00000\n19-14   4.151613 -33.285 41.5882 1.00000\n20-14  15.527742 -23.736 54.7916 0.99545\n16-15   2.838710 -34.598 40.2753 1.00000\n17-15   1.383871 -36.053 38.8205 1.00000\n18-15   6.600000 -30.837 44.0366 1.00000\n19-15   4.293548 -33.143 41.7302 1.00000\n20-15  15.669677 -23.594 54.9335 0.99493\n17-16  -1.454839 -38.891 35.9818 1.00000\n18-16   3.761290 -33.675 41.1979 1.00000\n19-16   1.454839 -35.982 38.8915 1.00000\n20-16  12.830968 -26.433 52.0948 0.99961\n18-17   5.216129 -32.221 42.6528 1.00000\n19-17   2.909677 -34.527 40.3463 1.00000\n20-17  14.285806 -24.978 53.5497 0.99837\n19-18  -2.306452 -39.743 35.1302 1.00000\n20-18   9.069677 -30.194 48.3335 1.00000\n20-19  11.376129 -27.888 50.6400 0.99993\n\n$noon\n         diff     lwr    upr   p adj\nPM-AM -3.3731 -9.8265 3.0804 0.30135\n\n$`Day:noon`\n                  diff      lwr     upr   p adj\n2:AM-1:AM     4.045161  -54.344 62.4344 1.00000\n3:AM-1:AM    -2.838710  -61.228 55.5505 1.00000\n4:AM-1:AM     0.709677  -57.680 59.0989 1.00000\n5:AM-1:AM     8.445161  -49.944 66.8344 1.00000\n6:AM-1:AM     9.793548  -48.596 68.1827 1.00000\n7:AM-1:AM     4.470968  -53.918 62.8602 1.00000\n8:AM-1:AM    15.116129  -43.273 73.5053 1.00000\n9:AM-1:AM    33.993548  -24.396 92.3827 0.92887\n10:AM-1:AM   -9.367742  -67.757 49.0215 1.00000\n11:AM-1:AM   31.935484  -26.454 90.3247 0.96592\n12:AM-1:AM  -10.006452  -68.396 48.3827 1.00000\n13:AM-1:AM    4.612903  -53.776 63.0021 1.00000\n14:AM-1:AM  -10.148387  -68.538 48.2408 1.00000\n15:AM-1:AM  -17.529032  -75.918 40.8602 1.00000\n16:AM-1:AM    0.922581  -57.467 59.3118 1.00000\n17:AM-1:AM    5.606452  -52.783 63.9956 1.00000\n18:AM-1:AM    2.483871  -55.905 60.8731 1.00000\n19:AM-1:AM    9.935484  -48.454 68.3247 1.00000\n20:AM-1:AM    6.316129  -58.965 71.5972 1.00000\n1:PM-1:AM    22.709677  -35.680 81.0989 0.99990\n2:PM-1:AM     6.387097  -52.002 64.7763 1.00000\n3:PM-1:AM    17.387097  -41.002 75.7763 1.00000\n4:PM-1:AM    -6.600000  -64.989 51.7892 1.00000\n5:PM-1:AM     7.593548  -50.796 65.9827 1.00000\n6:PM-1:AM    -0.709677  -59.099 57.6795 1.00000\n7:PM-1:AM     5.393548  -52.996 63.7827 1.00000\n8:PM-1:AM    -1.135484  -59.525 57.2537 1.00000\n9:PM-1:AM     5.819355  -52.570 64.2086 1.00000\n10:PM-1:AM    6.670968  -51.718 65.0602 1.00000\n11:PM-1:AM   -5.535484  -63.925 52.8537 1.00000\n12:PM-1:AM   -8.090323  -66.480 50.2989 1.00000\n13:PM-1:AM   11.141935  -47.247 69.5311 1.00000\n14:PM-1:AM   -4.187097  -62.576 54.2021 1.00000\n15:PM-1:AM    2.909677  -55.480 61.2989 1.00000\n16:PM-1:AM   -9.864516  -68.254 48.5247 1.00000\n17:PM-1:AM  -17.458065  -75.847 40.9311 1.00000\n18:PM-1:AM   -3.903226  -62.292 54.4860 1.00000\n19:PM-1:AM  -15.967742  -74.357 42.4215 1.00000\n20:PM-1:AM    9.722581  -48.667 68.1118 1.00000\n3:AM-2:AM    -6.883871  -65.273 51.5053 1.00000\n4:AM-2:AM    -3.335484  -61.725 55.0537 1.00000\n5:AM-2:AM     4.400000  -53.989 62.7892 1.00000\n6:AM-2:AM     5.748387  -52.641 64.1376 1.00000\n7:AM-2:AM     0.425806  -57.963 58.8150 1.00000\n8:AM-2:AM    11.070968  -47.318 69.4602 1.00000\n9:AM-2:AM    29.948387  -28.441 88.3376 0.98570\n10:AM-2:AM  -13.412903  -71.802 44.9763 1.00000\n11:AM-2:AM   27.890323  -30.499 86.2795 0.99521\n12:AM-2:AM  -14.051613  -72.441 44.3376 1.00000\n13:AM-2:AM    0.567742  -57.821 58.9569 1.00000\n14:AM-2:AM  -14.193548  -72.583 44.1956 1.00000\n15:AM-2:AM  -21.574194  -79.963 36.8150 0.99997\n16:AM-2:AM   -3.122581  -61.512 55.2666 1.00000\n17:AM-2:AM    1.561290  -56.828 59.9505 1.00000\n18:AM-2:AM   -1.561290  -59.950 56.8279 1.00000\n19:AM-2:AM    5.890323  -52.499 64.2795 1.00000\n20:AM-2:AM    2.270968  -63.010 67.5521 1.00000\n1:PM-2:AM    18.664516  -39.725 77.0537 1.00000\n2:PM-2:AM     2.341935  -56.047 60.7311 1.00000\n3:PM-2:AM    13.341935  -45.047 71.7311 1.00000\n4:PM-2:AM   -10.645161  -69.034 47.7440 1.00000\n5:PM-2:AM     3.548387  -54.841 61.9376 1.00000\n6:PM-2:AM    -4.754839  -63.144 53.6344 1.00000\n7:PM-2:AM     1.348387  -57.041 59.7376 1.00000\n8:PM-2:AM    -5.180645  -63.570 53.2086 1.00000\n9:PM-2:AM     1.774194  -56.615 60.1634 1.00000\n10:PM-2:AM    2.625806  -55.763 61.0150 1.00000\n11:PM-2:AM   -9.580645  -67.970 48.8086 1.00000\n12:PM-2:AM  -12.135484  -70.525 46.2537 1.00000\n13:PM-2:AM    7.096774  -51.292 65.4860 1.00000\n14:PM-2:AM   -8.232258  -66.621 50.1569 1.00000\n15:PM-2:AM   -1.135484  -59.525 57.2537 1.00000\n16:PM-2:AM  -13.909677  -72.299 44.4795 1.00000\n17:PM-2:AM  -21.503226  -79.892 36.8860 0.99997\n18:PM-2:AM   -7.948387  -66.338 50.4408 1.00000\n19:PM-2:AM  -20.012903  -78.402 38.3763 0.99999\n20:PM-2:AM    5.677419  -52.712 64.0666 1.00000\n4:AM-3:AM     3.548387  -54.841 61.9376 1.00000\n5:AM-3:AM    11.283871  -47.105 69.6731 1.00000\n6:AM-3:AM    12.632258  -45.757 71.0215 1.00000\n7:AM-3:AM     7.309677  -51.080 65.6989 1.00000\n8:AM-3:AM    17.954839  -40.434 76.3440 1.00000\n9:AM-3:AM    36.832258  -21.557 95.2215 0.84321\n10:AM-3:AM   -6.529032  -64.918 51.8602 1.00000\n11:AM-3:AM   34.774194  -23.615 93.1634 0.90946\n12:AM-3:AM   -7.167742  -65.557 51.2215 1.00000\n13:AM-3:AM    7.451613  -50.938 65.8408 1.00000\n14:AM-3:AM   -7.309677  -65.699 51.0795 1.00000\n15:AM-3:AM  -14.690323  -73.080 43.6989 1.00000\n16:AM-3:AM    3.761290  -54.628 62.1505 1.00000\n17:AM-3:AM    8.445161  -49.944 66.8344 1.00000\n18:AM-3:AM    5.322581  -53.067 63.7118 1.00000\n19:AM-3:AM   12.774194  -45.615 71.1634 1.00000\n20:AM-3:AM    9.154839  -56.126 74.4359 1.00000\n1:PM-3:AM    25.548387  -32.841 83.9376 0.99897\n2:PM-3:AM     9.225806  -49.163 67.6150 1.00000\n3:PM-3:AM    20.225806  -38.163 78.6150 0.99999\n4:PM-3:AM    -3.761290  -62.150 54.6279 1.00000\n5:PM-3:AM    10.432258  -47.957 68.8215 1.00000\n6:PM-3:AM     2.129032  -56.260 60.5182 1.00000\n7:PM-3:AM     8.232258  -50.157 66.6215 1.00000\n8:PM-3:AM     1.703226  -56.686 60.0924 1.00000\n9:PM-3:AM     8.658065  -49.731 67.0473 1.00000\n10:PM-3:AM    9.509677  -48.880 67.8989 1.00000\n11:PM-3:AM   -2.696774  -61.086 55.6924 1.00000\n12:PM-3:AM   -5.251613  -63.641 53.1376 1.00000\n13:PM-3:AM   13.980645  -44.409 72.3698 1.00000\n14:PM-3:AM   -1.348387  -59.738 57.0408 1.00000\n15:PM-3:AM    5.748387  -52.641 64.1376 1.00000\n16:PM-3:AM   -7.025806  -65.415 51.3634 1.00000\n17:PM-3:AM  -14.619355  -73.009 43.7698 1.00000\n18:PM-3:AM   -1.064516  -59.454 57.3247 1.00000\n19:PM-3:AM  -13.129032  -71.518 45.2602 1.00000\n20:PM-3:AM   12.561290  -45.828 70.9505 1.00000\n5:AM-4:AM     7.735484  -50.654 66.1247 1.00000\n6:AM-4:AM     9.083871  -49.305 67.4731 1.00000\n7:AM-4:AM     3.761290  -54.628 62.1505 1.00000\n8:AM-4:AM    14.406452  -43.983 72.7956 1.00000\n9:AM-4:AM    33.283871  -25.105 91.6731 0.94386\n10:AM-4:AM  -10.077419  -68.467 48.3118 1.00000\n11:AM-4:AM   31.225806  -27.163 89.6150 0.97452\n12:AM-4:AM  -10.716129  -69.105 47.6731 1.00000\n13:AM-4:AM    3.903226  -54.486 62.2924 1.00000\n14:AM-4:AM  -10.858065  -69.247 47.5311 1.00000\n15:AM-4:AM  -18.238710  -76.628 40.1505 1.00000\n16:AM-4:AM    0.212903  -58.176 58.6021 1.00000\n17:AM-4:AM    4.896774  -53.492 63.2860 1.00000\n18:AM-4:AM    1.774194  -56.615 60.1634 1.00000\n19:AM-4:AM    9.225806  -49.163 67.6150 1.00000\n20:AM-4:AM    5.606452  -59.675 70.8876 1.00000\n1:PM-4:AM    22.000000  -36.389 80.3892 0.99995\n2:PM-4:AM     5.677419  -52.712 64.0666 1.00000\n3:PM-4:AM    16.677419  -41.712 75.0666 1.00000\n4:PM-4:AM    -7.309677  -65.699 51.0795 1.00000\n5:PM-4:AM     6.883871  -51.505 65.2731 1.00000\n6:PM-4:AM    -1.419355  -59.809 56.9698 1.00000\n7:PM-4:AM     4.683871  -53.705 63.0731 1.00000\n8:PM-4:AM    -1.845161  -60.234 56.5440 1.00000\n9:PM-4:AM     5.109677  -53.280 63.4989 1.00000\n10:PM-4:AM    5.961290  -52.428 64.3505 1.00000\n11:PM-4:AM   -6.245161  -64.634 52.1440 1.00000\n12:PM-4:AM   -8.800000  -67.189 49.5892 1.00000\n13:PM-4:AM   10.432258  -47.957 68.8215 1.00000\n14:PM-4:AM   -4.896774  -63.286 53.4924 1.00000\n15:PM-4:AM    2.200000  -56.189 60.5892 1.00000\n16:PM-4:AM  -10.574194  -68.963 47.8150 1.00000\n17:PM-4:AM  -18.167742  -76.557 40.2215 1.00000\n18:PM-4:AM   -4.612903  -63.002 53.7763 1.00000\n19:PM-4:AM  -16.677419  -75.067 41.7118 1.00000\n20:PM-4:AM    9.012903  -49.376 67.4021 1.00000\n6:AM-5:AM     1.348387  -57.041 59.7376 1.00000\n7:AM-5:AM    -3.974194  -62.363 54.4150 1.00000\n8:AM-5:AM     6.670968  -51.718 65.0602 1.00000\n9:AM-5:AM    25.548387  -32.841 83.9376 0.99897\n10:AM-5:AM  -17.812903  -76.202 40.5763 1.00000\n11:AM-5:AM   23.490323  -34.899 81.8795 0.99980\n12:AM-5:AM  -18.451613  -76.841 39.9376 1.00000\n13:AM-5:AM   -3.832258  -62.221 54.5569 1.00000\n14:AM-5:AM  -18.593548  -76.983 39.7956 1.00000\n15:AM-5:AM  -25.974194  -84.363 32.4150 0.99860\n16:AM-5:AM   -7.522581  -65.912 50.8666 1.00000\n17:AM-5:AM   -2.838710  -61.228 55.5505 1.00000\n18:AM-5:AM   -5.961290  -64.350 52.4279 1.00000\n19:AM-5:AM    1.490323  -56.899 59.8795 1.00000\n20:AM-5:AM   -2.129032  -67.410 63.1521 1.00000\n1:PM-5:AM    14.264516  -44.125 72.6537 1.00000\n2:PM-5:AM    -2.058065  -60.447 56.3311 1.00000\n3:PM-5:AM     8.941935  -49.447 67.3311 1.00000\n4:PM-5:AM   -15.045161  -73.434 43.3440 1.00000\n5:PM-5:AM    -0.851613  -59.241 57.5376 1.00000\n6:PM-5:AM    -9.154839  -67.544 49.2344 1.00000\n7:PM-5:AM    -3.051613  -61.441 55.3376 1.00000\n8:PM-5:AM    -9.580645  -67.970 48.8086 1.00000\n9:PM-5:AM    -2.625806  -61.015 55.7634 1.00000\n10:PM-5:AM   -1.774194  -60.163 56.6150 1.00000\n11:PM-5:AM  -13.980645  -72.370 44.4086 1.00000\n12:PM-5:AM  -16.535484  -74.925 41.8537 1.00000\n13:PM-5:AM    2.696774  -55.692 61.0860 1.00000\n14:PM-5:AM  -12.632258  -71.021 45.7569 1.00000\n15:PM-5:AM   -5.535484  -63.925 52.8537 1.00000\n16:PM-5:AM  -18.309677  -76.699 40.0795 1.00000\n17:PM-5:AM  -25.903226  -84.292 32.4860 0.99867\n18:PM-5:AM  -12.348387  -70.738 46.0408 1.00000\n19:PM-5:AM  -24.412903  -82.802 33.9763 0.99957\n20:PM-5:AM    1.277419  -57.112 59.6666 1.00000\n7:AM-6:AM    -5.322581  -63.712 53.0666 1.00000\n8:AM-6:AM     5.322581  -53.067 63.7118 1.00000\n9:AM-6:AM    24.200000  -34.189 82.5892 0.99964\n10:AM-6:AM  -19.161290  -77.550 39.2279 1.00000\n11:AM-6:AM   22.141935  -36.247 80.5311 0.99994\n12:AM-6:AM  -19.800000  -78.189 38.5892 1.00000\n13:AM-6:AM   -5.180645  -63.570 53.2086 1.00000\n14:AM-6:AM  -19.941935  -78.331 38.4473 1.00000\n15:AM-6:AM  -27.322581  -85.712 31.0666 0.99660\n16:AM-6:AM   -8.870968  -67.260 49.5182 1.00000\n17:AM-6:AM   -4.187097  -62.576 54.2021 1.00000\n18:AM-6:AM   -7.309677  -65.699 51.0795 1.00000\n19:AM-6:AM    0.141935  -58.247 58.5311 1.00000\n20:AM-6:AM   -3.477419  -68.759 61.8037 1.00000\n1:PM-6:AM    12.916129  -45.473 71.3053 1.00000\n2:PM-6:AM    -3.406452  -61.796 54.9827 1.00000\n3:PM-6:AM     7.593548  -50.796 65.9827 1.00000\n4:PM-6:AM   -16.393548  -74.783 41.9956 1.00000\n5:PM-6:AM    -2.200000  -60.589 56.1892 1.00000\n6:PM-6:AM   -10.503226  -68.892 47.8860 1.00000\n7:PM-6:AM    -4.400000  -62.789 53.9892 1.00000\n8:PM-6:AM   -10.929032  -69.318 47.4602 1.00000\n9:PM-6:AM    -3.974194  -62.363 54.4150 1.00000\n10:PM-6:AM   -3.122581  -61.512 55.2666 1.00000\n11:PM-6:AM  -15.329032  -73.718 43.0602 1.00000\n12:PM-6:AM  -17.883871  -76.273 40.5053 1.00000\n13:PM-6:AM    1.348387  -57.041 59.7376 1.00000\n14:PM-6:AM  -13.980645  -72.370 44.4086 1.00000\n15:PM-6:AM   -6.883871  -65.273 51.5053 1.00000\n16:PM-6:AM  -19.658065  -78.047 38.7311 1.00000\n17:PM-6:AM  -27.251613  -85.641 31.1376 0.99674\n18:PM-6:AM  -13.696774  -72.086 44.6924 1.00000\n19:PM-6:AM  -25.761290  -84.150 32.6279 0.99880\n20:PM-6:AM   -0.070968  -58.460 58.3182 1.00000\n8:AM-7:AM    10.645161  -47.744 69.0344 1.00000\n9:AM-7:AM    29.522581  -28.867 87.9118 0.98840\n10:AM-7:AM  -13.838710  -72.228 44.5505 1.00000\n11:AM-7:AM   27.464516  -30.925 85.8537 0.99629\n12:AM-7:AM  -14.477419  -72.867 43.9118 1.00000\n13:AM-7:AM    0.141935  -58.247 58.5311 1.00000\n14:AM-7:AM  -14.619355  -73.009 43.7698 1.00000\n15:AM-7:AM  -22.000000  -80.389 36.3892 0.99995\n16:AM-7:AM   -3.548387  -61.938 54.8408 1.00000\n17:AM-7:AM    1.135484  -57.254 59.5247 1.00000\n18:AM-7:AM   -1.987097  -60.376 56.4021 1.00000\n19:AM-7:AM    5.464516  -52.925 63.8537 1.00000\n20:AM-7:AM    1.845161  -63.436 67.1263 1.00000\n1:PM-7:AM    18.238710  -40.150 76.6279 1.00000\n2:PM-7:AM     1.916129  -56.473 60.3053 1.00000\n3:PM-7:AM    12.916129  -45.473 71.3053 1.00000\n4:PM-7:AM   -11.070968  -69.460 47.3182 1.00000\n5:PM-7:AM     3.122581  -55.267 61.5118 1.00000\n6:PM-7:AM    -5.180645  -63.570 53.2086 1.00000\n7:PM-7:AM     0.922581  -57.467 59.3118 1.00000\n8:PM-7:AM    -5.606452  -63.996 52.7827 1.00000\n9:PM-7:AM     1.348387  -57.041 59.7376 1.00000\n10:PM-7:AM    2.200000  -56.189 60.5892 1.00000\n11:PM-7:AM  -10.006452  -68.396 48.3827 1.00000\n12:PM-7:AM  -12.561290  -70.950 45.8279 1.00000\n13:PM-7:AM    6.670968  -51.718 65.0602 1.00000\n14:PM-7:AM   -8.658065  -67.047 49.7311 1.00000\n15:PM-7:AM   -1.561290  -59.950 56.8279 1.00000\n16:PM-7:AM  -14.335484  -72.725 44.0537 1.00000\n17:PM-7:AM  -21.929032  -80.318 36.4602 0.99996\n18:PM-7:AM   -8.374194  -66.763 50.0150 1.00000\n19:PM-7:AM  -20.438710  -78.828 37.9505 0.99999\n20:PM-7:AM    5.251613  -53.138 63.6408 1.00000\n9:AM-8:AM    18.877419  -39.512 77.2666 1.00000\n10:AM-8:AM  -24.483871  -82.873 33.9053 0.99954\n11:AM-8:AM   16.819355  -41.570 75.2086 1.00000\n12:AM-8:AM  -25.122581  -83.512 33.2666 0.99925\n13:AM-8:AM  -10.503226  -68.892 47.8860 1.00000\n14:AM-8:AM  -25.264516  -83.654 33.1247 0.99916\n15:AM-8:AM  -32.645161  -91.034 25.7440 0.95531\n16:AM-8:AM  -14.193548  -72.583 44.1956 1.00000\n17:AM-8:AM   -9.509677  -67.899 48.8795 1.00000\n18:AM-8:AM  -12.632258  -71.021 45.7569 1.00000\n19:AM-8:AM   -5.180645  -63.570 53.2086 1.00000\n20:AM-8:AM   -8.800000  -74.081 56.4811 1.00000\n1:PM-8:AM     7.593548  -50.796 65.9827 1.00000\n2:PM-8:AM    -8.729032  -67.118 49.6602 1.00000\n3:PM-8:AM     2.270968  -56.118 60.6602 1.00000\n4:PM-8:AM   -21.716129  -80.105 36.6731 0.99996\n5:PM-8:AM    -7.522581  -65.912 50.8666 1.00000\n6:PM-8:AM   -15.825806  -74.215 42.5634 1.00000\n7:PM-8:AM    -9.722581  -68.112 48.6666 1.00000\n8:PM-8:AM   -16.251613  -74.641 42.1376 1.00000\n9:PM-8:AM    -9.296774  -67.686 49.0924 1.00000\n10:PM-8:AM   -8.445161  -66.834 49.9440 1.00000\n11:PM-8:AM  -20.651613  -79.041 37.7376 0.99999\n12:PM-8:AM  -23.206452  -81.596 35.1827 0.99985\n13:PM-8:AM   -3.974194  -62.363 54.4150 1.00000\n14:PM-8:AM  -19.303226  -77.692 39.0860 1.00000\n15:PM-8:AM  -12.206452  -70.596 46.1827 1.00000\n16:PM-8:AM  -24.980645  -83.370 33.4086 0.99933\n17:PM-8:AM  -32.574194  -90.963 25.8150 0.95647\n18:PM-8:AM  -19.019355  -77.409 39.3698 1.00000\n19:PM-8:AM  -31.083871  -89.473 27.3053 0.97602\n20:PM-8:AM   -5.393548  -63.783 52.9956 1.00000\n10:AM-9:AM  -43.361290 -101.750 15.0279 0.52759\n11:AM-9:AM   -2.058065  -60.447 56.3311 1.00000\n12:AM-9:AM  -44.000000 -102.389 14.3892 0.49403\n13:AM-9:AM  -29.380645  -87.770 29.0086 0.98920\n14:AM-9:AM  -44.141935 -102.531 14.2473 0.48663\n15:AM-9:AM  -51.522581 -109.912  6.8666 0.17781\n16:AM-9:AM  -33.070968  -91.460 25.3182 0.94789\n17:AM-9:AM  -28.387097  -86.776 30.0021 0.99364\n18:AM-9:AM  -31.509677  -89.899 26.8795 0.97131\n19:AM-9:AM  -24.058065  -82.447 34.3311 0.99968\n20:AM-9:AM  -27.677419  -92.959 37.6037 0.99943\n1:PM-9:AM   -11.283871  -69.673 47.1053 1.00000\n2:PM-9:AM   -27.606452  -85.996 30.7827 0.99595\n3:PM-9:AM   -16.606452  -74.996 41.7827 1.00000\n4:PM-9:AM   -40.593548  -98.983 17.7956 0.67322\n5:PM-9:AM   -26.400000  -84.789 31.9892 0.99812\n6:PM-9:AM   -34.703226  -93.092 23.6860 0.91136\n7:PM-9:AM   -28.600000  -86.989 29.7892 0.99284\n8:PM-9:AM   -35.129032  -93.518 23.2602 0.89961\n9:PM-9:AM   -28.174194  -86.563 30.2150 0.99436\n10:PM-9:AM  -27.322581  -85.712 31.0666 0.99660\n11:PM-9:AM  -39.529032  -97.918 18.8602 0.72631\n12:PM-9:AM  -42.083871 -100.473 16.3053 0.59536\n13:PM-9:AM  -22.851613  -81.241 35.5376 0.99989\n14:PM-9:AM  -38.180645  -96.570 20.2086 0.78850\n15:PM-9:AM  -31.083871  -89.473 27.3053 0.97602\n16:PM-9:AM  -43.858065 -102.247 14.5311 0.50145\n17:PM-9:AM  -51.451613 -109.841  6.9376 0.17988\n18:PM-9:AM  -37.896774  -96.286 20.4924 0.80070\n19:PM-9:AM  -49.961290 -108.350  8.4279 0.22767\n20:PM-9:AM  -24.270968  -82.660 34.1182 0.99962\n11:AM-10:AM  41.303226  -17.086 99.6924 0.63649\n12:AM-10:AM  -0.638710  -59.028 57.7505 1.00000\n13:AM-10:AM  13.980645  -44.409 72.3698 1.00000\n14:AM-10:AM  -0.780645  -59.170 57.6086 1.00000\n15:AM-10:AM  -8.161290  -66.550 50.2279 1.00000\n16:AM-10:AM  10.290323  -48.099 68.6795 1.00000\n17:AM-10:AM  14.974194  -43.415 73.3634 1.00000\n18:AM-10:AM  11.851613  -46.538 70.2408 1.00000\n19:AM-10:AM  19.303226  -39.086 77.6924 1.00000\n20:AM-10:AM  15.683871  -49.597 80.9650 1.00000\n1:PM-10:AM   32.077419  -26.312 90.4666 0.96397\n2:PM-10:AM   15.754839  -42.634 74.1440 1.00000\n3:PM-10:AM   26.754839  -31.634 85.1440 0.99763\n4:PM-10:AM    2.767742  -55.621 61.1569 1.00000\n5:PM-10:AM   16.961290  -41.428 75.3505 1.00000\n6:PM-10:AM    8.658065  -49.731 67.0473 1.00000\n7:PM-10:AM   14.761290  -43.628 73.1505 1.00000\n8:PM-10:AM    8.232258  -50.157 66.6215 1.00000\n9:PM-10:AM   15.187097  -43.202 73.5763 1.00000\n10:PM-10:AM  16.038710  -42.350 74.4279 1.00000\n11:PM-10:AM   3.832258  -54.557 62.2215 1.00000\n12:PM-10:AM   1.277419  -57.112 59.6666 1.00000\n13:PM-10:AM  20.509677  -37.880 78.8989 0.99999\n14:PM-10:AM   5.180645  -53.209 63.5698 1.00000\n15:PM-10:AM  12.277419  -46.112 70.6666 1.00000\n16:PM-10:AM  -0.496774  -58.886 57.8924 1.00000\n17:PM-10:AM  -8.090323  -66.480 50.2989 1.00000\n18:PM-10:AM   5.464516  -52.925 63.8537 1.00000\n19:PM-10:AM  -6.600000  -64.989 51.7892 1.00000\n20:PM-10:AM  19.090323  -39.299 77.4795 1.00000\n12:AM-11:AM -41.941935 -100.331 16.4473 0.60287\n13:AM-11:AM -27.322581  -85.712 31.0666 0.99660\n14:AM-11:AM -42.083871 -100.473 16.3053 0.59536\n15:AM-11:AM -49.464516 -107.854  8.9247 0.24541\n16:AM-11:AM -31.012903  -89.402 27.3763 0.97675\n17:AM-11:AM -26.329032  -84.718 32.0602 0.99821\n18:AM-11:AM -29.451613  -87.841 28.9376 0.98880\n19:AM-11:AM -22.000000  -80.389 36.3892 0.99995\n20:AM-11:AM -25.619355  -90.900 39.6618 0.99988\n1:PM-11:AM   -9.225806  -67.615 49.1634 1.00000\n2:PM-11:AM  -25.548387  -83.938 32.8408 0.99897\n3:PM-11:AM  -14.548387  -72.938 43.8408 1.00000\n4:PM-11:AM  -38.535484  -96.925 19.8537 0.77279\n5:PM-11:AM  -24.341935  -82.731 34.0473 0.99959\n6:PM-11:AM  -32.645161  -91.034 25.7440 0.95531\n7:PM-11:AM  -26.541935  -84.931 31.8473 0.99794\n8:PM-11:AM  -33.070968  -91.460 25.3182 0.94789\n9:PM-11:AM  -26.116129  -84.505 32.2731 0.99846\n10:PM-11:AM -25.264516  -83.654 33.1247 0.99916\n11:PM-11:AM -37.470968  -95.860 20.9182 0.81833\n12:PM-11:AM -40.025806  -98.415 18.3634 0.70190\n13:PM-11:AM -20.793548  -79.183 37.5956 0.99999\n14:PM-11:AM -36.122581  -94.512 22.2666 0.86852\n15:PM-11:AM -29.025806  -87.415 29.3634 0.99101\n16:PM-11:AM -41.800000 -100.189 16.5892 0.61038\n17:PM-11:AM -49.393548 -107.783  8.9956 0.24802\n18:PM-11:AM -35.838710  -94.228 22.5505 0.87792\n19:PM-11:AM -47.903226 -106.292 10.4860 0.30702\n20:PM-11:AM -22.212903  -80.602 36.1763 0.99994\n13:AM-12:AM  14.619355  -43.770 73.0086 1.00000\n14:AM-12:AM  -0.141935  -58.531 58.2473 1.00000\n15:AM-12:AM  -7.522581  -65.912 50.8666 1.00000\n16:AM-12:AM  10.929032  -47.460 69.3182 1.00000\n17:AM-12:AM  15.612903  -42.776 74.0021 1.00000\n18:AM-12:AM  12.490323  -45.899 70.8795 1.00000\n19:AM-12:AM  19.941935  -38.447 78.3311 1.00000\n20:AM-12:AM  16.322581  -48.959 81.6037 1.00000\n1:PM-12:AM   32.716129  -25.673 91.1053 0.95413\n2:PM-12:AM   16.393548  -41.996 74.7827 1.00000\n3:PM-12:AM   27.393548  -30.996 85.7827 0.99645\n4:PM-12:AM    3.406452  -54.983 61.7956 1.00000\n5:PM-12:AM   17.600000  -40.789 75.9892 1.00000\n6:PM-12:AM    9.296774  -49.092 67.6860 1.00000\n7:PM-12:AM   15.400000  -42.989 73.7892 1.00000\n8:PM-12:AM    8.870968  -49.518 67.2602 1.00000\n9:PM-12:AM   15.825806  -42.563 74.2150 1.00000\n10:PM-12:AM  16.677419  -41.712 75.0666 1.00000\n11:PM-12:AM   4.470968  -53.918 62.8602 1.00000\n12:PM-12:AM   1.916129  -56.473 60.3053 1.00000\n13:PM-12:AM  21.148387  -37.241 79.5376 0.99998\n14:PM-12:AM   5.819355  -52.570 64.2086 1.00000\n15:PM-12:AM  12.916129  -45.473 71.3053 1.00000\n16:PM-12:AM   0.141935  -58.247 58.5311 1.00000\n17:PM-12:AM  -7.451613  -65.841 50.9376 1.00000\n18:PM-12:AM   6.103226  -52.286 64.4924 1.00000\n19:PM-12:AM  -5.961290  -64.350 52.4279 1.00000\n20:PM-12:AM  19.729032  -38.660 78.1182 1.00000\n14:AM-13:AM -14.761290  -73.150 43.6279 1.00000\n15:AM-13:AM -22.141935  -80.531 36.2473 0.99994\n16:AM-13:AM  -3.690323  -62.080 54.6989 1.00000\n17:AM-13:AM   0.993548  -57.396 59.3827 1.00000\n18:AM-13:AM  -2.129032  -60.518 56.2602 1.00000\n19:AM-13:AM   5.322581  -53.067 63.7118 1.00000\n20:AM-13:AM   1.703226  -63.578 66.9843 1.00000\n1:PM-13:AM   18.096774  -40.292 76.4860 1.00000\n2:PM-13:AM    1.774194  -56.615 60.1634 1.00000\n3:PM-13:AM   12.774194  -45.615 71.1634 1.00000\n4:PM-13:AM  -11.212903  -69.602 47.1763 1.00000\n5:PM-13:AM    2.980645  -55.409 61.3698 1.00000\n6:PM-13:AM   -5.322581  -63.712 53.0666 1.00000\n7:PM-13:AM    0.780645  -57.609 59.1698 1.00000\n8:PM-13:AM   -5.748387  -64.138 52.6408 1.00000\n9:PM-13:AM    1.206452  -57.183 59.5956 1.00000\n10:PM-13:AM   2.058065  -56.331 60.4473 1.00000\n11:PM-13:AM -10.148387  -68.538 48.2408 1.00000\n12:PM-13:AM -12.703226  -71.092 45.6860 1.00000\n13:PM-13:AM   6.529032  -51.860 64.9182 1.00000\n14:PM-13:AM  -8.800000  -67.189 49.5892 1.00000\n15:PM-13:AM  -1.703226  -60.092 56.6860 1.00000\n16:PM-13:AM -14.477419  -72.867 43.9118 1.00000\n17:PM-13:AM -22.070968  -80.460 36.3182 0.99995\n18:PM-13:AM  -8.516129  -66.905 49.8731 1.00000\n19:PM-13:AM -20.580645  -78.970 37.8086 0.99999\n20:PM-13:AM   5.109677  -53.280 63.4989 1.00000\n15:AM-14:AM  -7.380645  -65.770 51.0086 1.00000\n16:AM-14:AM  11.070968  -47.318 69.4602 1.00000\n17:AM-14:AM  15.754839  -42.634 74.1440 1.00000\n18:AM-14:AM  12.632258  -45.757 71.0215 1.00000\n19:AM-14:AM  20.083871  -38.305 78.4731 0.99999\n20:AM-14:AM  16.464516  -48.817 81.7456 1.00000\n1:PM-14:AM   32.858065  -25.531 91.2473 0.95170\n2:PM-14:AM   16.535484  -41.854 74.9247 1.00000\n3:PM-14:AM   27.535484  -30.854 85.9247 0.99612\n4:PM-14:AM    3.548387  -54.841 61.9376 1.00000\n5:PM-14:AM   17.741935  -40.647 76.1311 1.00000\n6:PM-14:AM    9.438710  -48.950 67.8279 1.00000\n7:PM-14:AM   15.541935  -42.847 73.9311 1.00000\n8:PM-14:AM    9.012903  -49.376 67.4021 1.00000\n9:PM-14:AM   15.967742  -42.421 74.3569 1.00000\n10:PM-14:AM  16.819355  -41.570 75.2086 1.00000\n11:PM-14:AM   4.612903  -53.776 63.0021 1.00000\n12:PM-14:AM   2.058065  -56.331 60.4473 1.00000\n13:PM-14:AM  21.290323  -37.099 79.6795 0.99998\n14:PM-14:AM   5.961290  -52.428 64.3505 1.00000\n15:PM-14:AM  13.058065  -45.331 71.4473 1.00000\n16:PM-14:AM   0.283871  -58.105 58.6731 1.00000\n17:PM-14:AM  -7.309677  -65.699 51.0795 1.00000\n18:PM-14:AM   6.245161  -52.144 64.6344 1.00000\n19:PM-14:AM  -5.819355  -64.209 52.5698 1.00000\n20:PM-14:AM  19.870968  -38.518 78.2602 1.00000\n16:AM-15:AM  18.451613  -39.938 76.8408 1.00000\n17:AM-15:AM  23.135484  -35.254 81.5247 0.99986\n18:AM-15:AM  20.012903  -38.376 78.4021 0.99999\n19:AM-15:AM  27.464516  -30.925 85.8537 0.99629\n20:AM-15:AM  23.845161  -41.436 89.1263 0.99998\n1:PM-15:AM   40.238710  -18.150 98.6279 0.69123\n2:PM-15:AM   23.916129  -34.473 82.3053 0.99972\n3:PM-15:AM   34.916129  -23.473 93.3053 0.90560\n4:PM-15:AM   10.929032  -47.460 69.3182 1.00000\n5:PM-15:AM   25.122581  -33.267 83.5118 0.99925\n6:PM-15:AM   16.819355  -41.570 75.2086 1.00000\n7:PM-15:AM   22.922581  -35.467 81.3118 0.99988\n8:PM-15:AM   16.393548  -41.996 74.7827 1.00000\n9:PM-15:AM   23.348387  -35.041 81.7376 0.99983\n10:PM-15:AM  24.200000  -34.189 82.5892 0.99964\n11:PM-15:AM  11.993548  -46.396 70.3827 1.00000\n12:PM-15:AM   9.438710  -48.950 67.8279 1.00000\n13:PM-15:AM  28.670968  -29.718 87.0602 0.99256\n14:PM-15:AM  13.341935  -45.047 71.7311 1.00000\n15:PM-15:AM  20.438710  -37.950 78.8279 0.99999\n16:PM-15:AM   7.664516  -50.725 66.0537 1.00000\n17:PM-15:AM   0.070968  -58.318 58.4602 1.00000\n18:PM-15:AM  13.625806  -44.763 72.0150 1.00000\n19:PM-15:AM   1.561290  -56.828 59.9505 1.00000\n20:PM-15:AM  27.251613  -31.138 85.6408 0.99674\n17:AM-16:AM   4.683871  -53.705 63.0731 1.00000\n18:AM-16:AM   1.561290  -56.828 59.9505 1.00000\n19:AM-16:AM   9.012903  -49.376 67.4021 1.00000\n20:AM-16:AM   5.393548  -59.888 70.6747 1.00000\n1:PM-16:AM   21.787097  -36.602 80.1763 0.99996\n2:PM-16:AM    5.464516  -52.925 63.8537 1.00000\n3:PM-16:AM   16.464516  -41.925 74.8537 1.00000\n4:PM-16:AM   -7.522581  -65.912 50.8666 1.00000\n5:PM-16:AM    6.670968  -51.718 65.0602 1.00000\n6:PM-16:AM   -1.632258  -60.021 56.7569 1.00000\n7:PM-16:AM    4.470968  -53.918 62.8602 1.00000\n8:PM-16:AM   -2.058065  -60.447 56.3311 1.00000\n9:PM-16:AM    4.896774  -53.492 63.2860 1.00000\n10:PM-16:AM   5.748387  -52.641 64.1376 1.00000\n11:PM-16:AM  -6.458065  -64.847 51.9311 1.00000\n12:PM-16:AM  -9.012903  -67.402 49.3763 1.00000\n13:PM-16:AM  10.219355  -48.170 68.6086 1.00000\n14:PM-16:AM  -5.109677  -63.499 53.2795 1.00000\n15:PM-16:AM   1.987097  -56.402 60.3763 1.00000\n16:PM-16:AM -10.787097  -69.176 47.6021 1.00000\n17:PM-16:AM -18.380645  -76.770 40.0086 1.00000\n18:PM-16:AM  -4.825806  -63.215 53.5634 1.00000\n19:PM-16:AM -16.890323  -75.280 41.4989 1.00000\n20:PM-16:AM   8.800000  -49.589 67.1892 1.00000\n18:AM-17:AM  -3.122581  -61.512 55.2666 1.00000\n19:AM-17:AM   4.329032  -54.060 62.7182 1.00000\n20:AM-17:AM   0.709677  -64.571 65.9908 1.00000\n1:PM-17:AM   17.103226  -41.286 75.4924 1.00000\n2:PM-17:AM    0.780645  -57.609 59.1698 1.00000\n3:PM-17:AM   11.780645  -46.609 70.1698 1.00000\n4:PM-17:AM  -12.206452  -70.596 46.1827 1.00000\n5:PM-17:AM    1.987097  -56.402 60.3763 1.00000\n6:PM-17:AM   -6.316129  -64.705 52.0731 1.00000\n7:PM-17:AM   -0.212903  -58.602 58.1763 1.00000\n8:PM-17:AM   -6.741935  -65.131 51.6473 1.00000\n9:PM-17:AM    0.212903  -58.176 58.6021 1.00000\n10:PM-17:AM   1.064516  -57.325 59.4537 1.00000\n11:PM-17:AM -11.141935  -69.531 47.2473 1.00000\n12:PM-17:AM -13.696774  -72.086 44.6924 1.00000\n13:PM-17:AM   5.535484  -52.854 63.9247 1.00000\n14:PM-17:AM  -9.793548  -68.183 48.5956 1.00000\n15:PM-17:AM  -2.696774  -61.086 55.6924 1.00000\n16:PM-17:AM -15.470968  -73.860 42.9182 1.00000\n17:PM-17:AM -23.064516  -81.454 35.3247 0.99987\n18:PM-17:AM  -9.509677  -67.899 48.8795 1.00000\n19:PM-17:AM -21.574194  -79.963 36.8150 0.99997\n20:PM-17:AM   4.116129  -54.273 62.5053 1.00000\n19:AM-18:AM   7.451613  -50.938 65.8408 1.00000\n20:AM-18:AM   3.832258  -61.449 69.1134 1.00000\n1:PM-18:AM   20.225806  -38.163 78.6150 0.99999\n2:PM-18:AM    3.903226  -54.486 62.2924 1.00000\n3:PM-18:AM   14.903226  -43.486 73.2924 1.00000\n4:PM-18:AM   -9.083871  -67.473 49.3053 1.00000\n5:PM-18:AM    5.109677  -53.280 63.4989 1.00000\n6:PM-18:AM   -3.193548  -61.583 55.1956 1.00000\n7:PM-18:AM    2.909677  -55.480 61.2989 1.00000\n8:PM-18:AM   -3.619355  -62.009 54.7698 1.00000\n9:PM-18:AM    3.335484  -55.054 61.7247 1.00000\n10:PM-18:AM   4.187097  -54.202 62.5763 1.00000\n11:PM-18:AM  -8.019355  -66.409 50.3698 1.00000\n12:PM-18:AM -10.574194  -68.963 47.8150 1.00000\n13:PM-18:AM   8.658065  -49.731 67.0473 1.00000\n14:PM-18:AM  -6.670968  -65.060 51.7182 1.00000\n15:PM-18:AM   0.425806  -57.963 58.8150 1.00000\n16:PM-18:AM -12.348387  -70.738 46.0408 1.00000\n17:PM-18:AM -19.941935  -78.331 38.4473 1.00000\n18:PM-18:AM  -6.387097  -64.776 52.0021 1.00000\n19:PM-18:AM -18.451613  -76.841 39.9376 1.00000\n20:PM-18:AM   7.238710  -51.150 65.6279 1.00000\n20:AM-19:AM  -3.619355  -68.900 61.6618 1.00000\n1:PM-19:AM   12.774194  -45.615 71.1634 1.00000\n2:PM-19:AM   -3.548387  -61.938 54.8408 1.00000\n3:PM-19:AM    7.451613  -50.938 65.8408 1.00000\n4:PM-19:AM  -16.535484  -74.925 41.8537 1.00000\n5:PM-19:AM   -2.341935  -60.731 56.0473 1.00000\n6:PM-19:AM  -10.645161  -69.034 47.7440 1.00000\n7:PM-19:AM   -4.541935  -62.931 53.8473 1.00000\n8:PM-19:AM  -11.070968  -69.460 47.3182 1.00000\n9:PM-19:AM   -4.116129  -62.505 54.2731 1.00000\n10:PM-19:AM  -3.264516  -61.654 55.1247 1.00000\n11:PM-19:AM -15.470968  -73.860 42.9182 1.00000\n12:PM-19:AM -18.025806  -76.415 40.3634 1.00000\n13:PM-19:AM   1.206452  -57.183 59.5956 1.00000\n14:PM-19:AM -14.122581  -72.512 44.2666 1.00000\n15:PM-19:AM  -7.025806  -65.415 51.3634 1.00000\n16:PM-19:AM -19.800000  -78.189 38.5892 1.00000\n17:PM-19:AM -27.393548  -85.783 30.9956 0.99645\n18:PM-19:AM -13.838710  -72.228 44.5505 1.00000\n19:PM-19:AM -25.903226  -84.292 32.4860 0.99867\n20:PM-19:AM  -0.212903  -58.602 58.1763 1.00000\n1:PM-20:AM   16.393548  -48.888 81.6747 1.00000\n2:PM-20:AM    0.070968  -65.210 65.3521 1.00000\n3:PM-20:AM   11.070968  -54.210 76.3521 1.00000\n4:PM-20:AM  -12.916129  -78.197 52.3650 1.00000\n5:PM-20:AM    1.277419  -64.004 66.5585 1.00000\n6:PM-20:AM   -7.025806  -72.307 58.2553 1.00000\n7:PM-20:AM   -0.922581  -66.204 64.3585 1.00000\n8:PM-20:AM   -7.451613  -72.733 57.8295 1.00000\n9:PM-20:AM   -0.496774  -65.778 64.7843 1.00000\n10:PM-20:AM   0.354839  -64.926 65.6359 1.00000\n11:PM-20:AM -11.851613  -77.133 53.4295 1.00000\n12:PM-20:AM -14.406452  -79.688 50.8747 1.00000\n13:PM-20:AM   4.825806  -60.455 70.1069 1.00000\n14:PM-20:AM -10.503226  -75.784 54.7779 1.00000\n15:PM-20:AM  -3.406452  -68.688 61.8747 1.00000\n16:PM-20:AM -16.180645  -81.462 49.1005 1.00000\n17:PM-20:AM -23.774194  -89.055 41.5069 0.99998\n18:PM-20:AM -10.219355  -75.500 55.0618 1.00000\n19:PM-20:AM -22.283871  -87.565 42.9972 1.00000\n20:PM-20:AM   3.406452  -61.875 68.6876 1.00000\n2:PM-1:PM   -16.322581  -74.712 42.0666 1.00000\n3:PM-1:PM    -5.322581  -63.712 53.0666 1.00000\n4:PM-1:PM   -29.309677  -87.699 29.0795 0.98958\n5:PM-1:PM   -15.116129  -73.505 43.2731 1.00000\n6:PM-1:PM   -23.419355  -81.809 34.9698 0.99982\n7:PM-1:PM   -17.316129  -75.705 41.0731 1.00000\n8:PM-1:PM   -23.845161  -82.234 34.5440 0.99973\n9:PM-1:PM   -16.890323  -75.280 41.4989 1.00000\n10:PM-1:PM  -16.038710  -74.428 42.3505 1.00000\n11:PM-1:PM  -28.245161  -86.634 30.1440 0.99413\n12:PM-1:PM  -30.800000  -89.189 27.5892 0.97882\n13:PM-1:PM  -11.567742  -69.957 46.8215 1.00000\n14:PM-1:PM  -26.896774  -85.286 31.4924 0.99740\n15:PM-1:PM  -19.800000  -78.189 38.5892 1.00000\n16:PM-1:PM  -32.574194  -90.963 25.8150 0.95647\n17:PM-1:PM  -40.167742  -98.557 18.2215 0.69480\n18:PM-1:PM  -26.612903  -85.002 31.7763 0.99784\n19:PM-1:PM  -38.677419  -97.067 19.7118 0.76637\n20:PM-1:PM  -12.987097  -71.376 45.4021 1.00000\n3:PM-2:PM    11.000000  -47.389 69.3892 1.00000\n4:PM-2:PM   -12.987097  -71.376 45.4021 1.00000\n5:PM-2:PM     1.206452  -57.183 59.5956 1.00000\n6:PM-2:PM    -7.096774  -65.486 51.2924 1.00000\n7:PM-2:PM    -0.993548  -59.383 57.3956 1.00000\n8:PM-2:PM    -7.522581  -65.912 50.8666 1.00000\n9:PM-2:PM    -0.567742  -58.957 57.8215 1.00000\n10:PM-2:PM    0.283871  -58.105 58.6731 1.00000\n11:PM-2:PM  -11.922581  -70.312 46.4666 1.00000\n12:PM-2:PM  -14.477419  -72.867 43.9118 1.00000\n13:PM-2:PM    4.754839  -53.634 63.1440 1.00000\n14:PM-2:PM  -10.574194  -68.963 47.8150 1.00000\n15:PM-2:PM   -3.477419  -61.867 54.9118 1.00000\n16:PM-2:PM  -16.251613  -74.641 42.1376 1.00000\n17:PM-2:PM  -23.845161  -82.234 34.5440 0.99973\n18:PM-2:PM  -10.290323  -68.680 48.0989 1.00000\n19:PM-2:PM  -22.354839  -80.744 36.0344 0.99993\n20:PM-2:PM    3.335484  -55.054 61.7247 1.00000\n4:PM-3:PM   -23.987097  -82.376 34.4021 0.99970\n5:PM-3:PM    -9.793548  -68.183 48.5956 1.00000\n6:PM-3:PM   -18.096774  -76.486 40.2924 1.00000\n7:PM-3:PM   -11.993548  -70.383 46.3956 1.00000\n8:PM-3:PM   -18.522581  -76.912 39.8666 1.00000\n9:PM-3:PM   -11.567742  -69.957 46.8215 1.00000\n10:PM-3:PM  -10.716129  -69.105 47.6731 1.00000\n11:PM-3:PM  -22.922581  -81.312 35.4666 0.99988\n12:PM-3:PM  -25.477419  -83.867 32.9118 0.99902\n13:PM-3:PM   -6.245161  -64.634 52.1440 1.00000\n14:PM-3:PM  -21.574194  -79.963 36.8150 0.99997\n15:PM-3:PM  -14.477419  -72.867 43.9118 1.00000\n16:PM-3:PM  -27.251613  -85.641 31.1376 0.99674\n17:PM-3:PM  -34.845161  -93.234 23.5440 0.90755\n18:PM-3:PM  -21.290323  -79.680 37.0989 0.99998\n19:PM-3:PM  -33.354839  -91.744 25.0344 0.94247\n20:PM-3:PM   -7.664516  -66.054 50.7247 1.00000\n5:PM-4:PM    14.193548  -44.196 72.5827 1.00000\n6:PM-4:PM     5.890323  -52.499 64.2795 1.00000\n7:PM-4:PM    11.993548  -46.396 70.3827 1.00000\n8:PM-4:PM     5.464516  -52.925 63.8537 1.00000\n9:PM-4:PM    12.419355  -45.970 70.8086 1.00000\n10:PM-4:PM   13.270968  -45.118 71.6602 1.00000\n11:PM-4:PM    1.064516  -57.325 59.4537 1.00000\n12:PM-4:PM   -1.490323  -59.880 56.8989 1.00000\n13:PM-4:PM   17.741935  -40.647 76.1311 1.00000\n14:PM-4:PM    2.412903  -55.976 60.8021 1.00000\n15:PM-4:PM    9.509677  -48.880 67.8989 1.00000\n16:PM-4:PM   -3.264516  -61.654 55.1247 1.00000\n17:PM-4:PM  -10.858065  -69.247 47.5311 1.00000\n18:PM-4:PM    2.696774  -55.692 61.0860 1.00000\n19:PM-4:PM   -9.367742  -67.757 49.0215 1.00000\n20:PM-4:PM   16.322581  -42.067 74.7118 1.00000\n6:PM-5:PM    -8.303226  -66.692 50.0860 1.00000\n7:PM-5:PM    -2.200000  -60.589 56.1892 1.00000\n8:PM-5:PM    -8.729032  -67.118 49.6602 1.00000\n9:PM-5:PM    -1.774194  -60.163 56.6150 1.00000\n10:PM-5:PM   -0.922581  -59.312 57.4666 1.00000\n11:PM-5:PM  -13.129032  -71.518 45.2602 1.00000\n12:PM-5:PM  -15.683871  -74.073 42.7053 1.00000\n13:PM-5:PM    3.548387  -54.841 61.9376 1.00000\n14:PM-5:PM  -11.780645  -70.170 46.6086 1.00000\n15:PM-5:PM   -4.683871  -63.073 53.7053 1.00000\n16:PM-5:PM  -17.458065  -75.847 40.9311 1.00000\n17:PM-5:PM  -25.051613  -83.441 33.3376 0.99929\n18:PM-5:PM  -11.496774  -69.886 46.8924 1.00000\n19:PM-5:PM  -23.561290  -81.950 34.8279 0.99979\n20:PM-5:PM    2.129032  -56.260 60.5182 1.00000\n7:PM-6:PM     6.103226  -52.286 64.4924 1.00000\n8:PM-6:PM    -0.425806  -58.815 57.9634 1.00000\n9:PM-6:PM     6.529032  -51.860 64.9182 1.00000\n10:PM-6:PM    7.380645  -51.009 65.7698 1.00000\n11:PM-6:PM   -4.825806  -63.215 53.5634 1.00000\n12:PM-6:PM   -7.380645  -65.770 51.0086 1.00000\n13:PM-6:PM   11.851613  -46.538 70.2408 1.00000\n14:PM-6:PM   -3.477419  -61.867 54.9118 1.00000\n15:PM-6:PM    3.619355  -54.770 62.0086 1.00000\n16:PM-6:PM   -9.154839  -67.544 49.2344 1.00000\n17:PM-6:PM  -16.748387  -75.138 41.6408 1.00000\n18:PM-6:PM   -3.193548  -61.583 55.1956 1.00000\n19:PM-6:PM  -15.258065  -73.647 43.1311 1.00000\n20:PM-6:PM   10.432258  -47.957 68.8215 1.00000\n8:PM-7:PM    -6.529032  -64.918 51.8602 1.00000\n9:PM-7:PM     0.425806  -57.963 58.8150 1.00000\n10:PM-7:PM    1.277419  -57.112 59.6666 1.00000\n11:PM-7:PM  -10.929032  -69.318 47.4602 1.00000\n12:PM-7:PM  -13.483871  -71.873 44.9053 1.00000\n13:PM-7:PM    5.748387  -52.641 64.1376 1.00000\n14:PM-7:PM   -9.580645  -67.970 48.8086 1.00000\n15:PM-7:PM   -2.483871  -60.873 55.9053 1.00000\n16:PM-7:PM  -15.258065  -73.647 43.1311 1.00000\n17:PM-7:PM  -22.851613  -81.241 35.5376 0.99989\n18:PM-7:PM   -9.296774  -67.686 49.0924 1.00000\n19:PM-7:PM  -21.361290  -79.750 37.0279 0.99998\n20:PM-7:PM    4.329032  -54.060 62.7182 1.00000\n9:PM-8:PM     6.954839  -51.434 65.3440 1.00000\n10:PM-8:PM    7.806452  -50.583 66.1956 1.00000\n11:PM-8:PM   -4.400000  -62.789 53.9892 1.00000\n12:PM-8:PM   -6.954839  -65.344 51.4344 1.00000\n13:PM-8:PM   12.277419  -46.112 70.6666 1.00000\n14:PM-8:PM   -3.051613  -61.441 55.3376 1.00000\n15:PM-8:PM    4.045161  -54.344 62.4344 1.00000\n16:PM-8:PM   -8.729032  -67.118 49.6602 1.00000\n17:PM-8:PM  -16.322581  -74.712 42.0666 1.00000\n18:PM-8:PM   -2.767742  -61.157 55.6215 1.00000\n19:PM-8:PM  -14.832258  -73.221 43.5569 1.00000\n20:PM-8:PM   10.858065  -47.531 69.2473 1.00000\n10:PM-9:PM    0.851613  -57.538 59.2408 1.00000\n11:PM-9:PM  -11.354839  -69.744 47.0344 1.00000\n12:PM-9:PM  -13.909677  -72.299 44.4795 1.00000\n13:PM-9:PM    5.322581  -53.067 63.7118 1.00000\n14:PM-9:PM  -10.006452  -68.396 48.3827 1.00000\n15:PM-9:PM   -2.909677  -61.299 55.4795 1.00000\n16:PM-9:PM  -15.683871  -74.073 42.7053 1.00000\n17:PM-9:PM  -23.277419  -81.667 35.1118 0.99984\n18:PM-9:PM   -9.722581  -68.112 48.6666 1.00000\n19:PM-9:PM  -21.787097  -80.176 36.6021 0.99996\n20:PM-9:PM    3.903226  -54.486 62.2924 1.00000\n11:PM-10:PM -12.206452  -70.596 46.1827 1.00000\n12:PM-10:PM -14.761290  -73.150 43.6279 1.00000\n13:PM-10:PM   4.470968  -53.918 62.8602 1.00000\n14:PM-10:PM -10.858065  -69.247 47.5311 1.00000\n15:PM-10:PM  -3.761290  -62.150 54.6279 1.00000\n16:PM-10:PM -16.535484  -74.925 41.8537 1.00000\n17:PM-10:PM -24.129032  -82.518 34.2602 0.99966\n18:PM-10:PM -10.574194  -68.963 47.8150 1.00000\n19:PM-10:PM -22.638710  -81.028 35.7505 0.99991\n20:PM-10:PM   3.051613  -55.338 61.4408 1.00000\n12:PM-11:PM  -2.554839  -60.944 55.8344 1.00000\n13:PM-11:PM  16.677419  -41.712 75.0666 1.00000\n14:PM-11:PM   1.348387  -57.041 59.7376 1.00000\n15:PM-11:PM   8.445161  -49.944 66.8344 1.00000\n16:PM-11:PM  -4.329032  -62.718 54.0602 1.00000\n17:PM-11:PM -11.922581  -70.312 46.4666 1.00000\n18:PM-11:PM   1.632258  -56.757 60.0215 1.00000\n19:PM-11:PM -10.432258  -68.821 47.9569 1.00000\n20:PM-11:PM  15.258065  -43.131 73.6473 1.00000\n13:PM-12:PM  19.232258  -39.157 77.6215 1.00000\n14:PM-12:PM   3.903226  -54.486 62.2924 1.00000\n15:PM-12:PM  11.000000  -47.389 69.3892 1.00000\n16:PM-12:PM  -1.774194  -60.163 56.6150 1.00000\n17:PM-12:PM  -9.367742  -67.757 49.0215 1.00000\n18:PM-12:PM   4.187097  -54.202 62.5763 1.00000\n19:PM-12:PM  -7.877419  -66.267 50.5118 1.00000\n20:PM-12:PM  17.812903  -40.576 76.2021 1.00000\n14:PM-13:PM -15.329032  -73.718 43.0602 1.00000\n15:PM-13:PM  -8.232258  -66.621 50.1569 1.00000\n16:PM-13:PM -21.006452  -79.396 37.3827 0.99998\n17:PM-13:PM -28.600000  -86.989 29.7892 0.99284\n18:PM-13:PM -15.045161  -73.434 43.3440 1.00000\n19:PM-13:PM -27.109677  -85.499 31.2795 0.99702\n20:PM-13:PM  -1.419355  -59.809 56.9698 1.00000\n15:PM-14:PM   7.096774  -51.292 65.4860 1.00000\n16:PM-14:PM  -5.677419  -64.067 52.7118 1.00000\n17:PM-14:PM -13.270968  -71.660 45.1182 1.00000\n18:PM-14:PM   0.283871  -58.105 58.6731 1.00000\n19:PM-14:PM -11.780645  -70.170 46.6086 1.00000\n20:PM-14:PM  13.909677  -44.480 72.2989 1.00000\n16:PM-15:PM -12.774194  -71.163 45.6150 1.00000\n17:PM-15:PM -20.367742  -78.757 38.0215 0.99999\n18:PM-15:PM  -6.812903  -65.202 51.5763 1.00000\n19:PM-15:PM -18.877419  -77.267 39.5118 1.00000\n20:PM-15:PM   6.812903  -51.576 65.2021 1.00000\n17:PM-16:PM  -7.593548  -65.983 50.7956 1.00000\n18:PM-16:PM   5.961290  -52.428 64.3505 1.00000\n19:PM-16:PM  -6.103226  -64.492 52.2860 1.00000\n20:PM-16:PM  19.587097  -38.802 77.9763 1.00000\n18:PM-17:PM  13.554839  -44.834 71.9440 1.00000\n19:PM-17:PM   1.490323  -56.899 59.8795 1.00000\n20:PM-17:PM  27.180645  -31.209 85.5698 0.99689\n19:PM-18:PM -12.064516  -70.454 46.3247 1.00000\n20:PM-18:PM  13.625806  -44.763 72.0150 1.00000\n20:PM-19:PM  25.690323  -32.699 84.0795 0.99886\n\n\n  Tukey multiple comparisons of means\n    99% family-wise confidence level\n\nFit: aov(formula = response ~ Day + noon + Day:noon, data = data)\n\n$Day\n            diff     lwr    upr   p adj\n2-1    -6.138710 -48.905 36.628 1.00000\n3-1    -4.080645 -46.847 38.686 1.00000\n4-1   -14.300000 -57.066 28.466 0.99700\n5-1    -3.335484 -46.102 39.431 1.00000\n6-1    -6.812903 -49.579 35.953 1.00000\n7-1    -6.422581 -49.189 36.344 1.00000\n8-1    -4.364516 -47.131 38.402 1.00000\n9-1     8.551613 -34.215 51.318 1.00000\n10-1  -12.703226 -55.470 30.063 0.99934\n11-1    1.845161 -40.921 44.611 1.00000\n12-1  -20.403226 -63.170 22.363 0.89330\n13-1   -3.477419 -46.244 39.289 1.00000\n14-1  -18.522581 -61.289 24.244 0.95243\n15-1  -18.664516 -61.431 24.102 0.94905\n16-1  -15.825806 -58.592 26.941 0.99024\n17-1  -17.280645 -60.047 25.486 0.97539\n18-1  -12.064516 -54.831 30.702 0.99968\n19-1  -14.370968 -57.137 28.395 0.99682\n20-1   -2.994839 -47.849 41.859 1.00000\n3-2     2.058065 -40.708 44.824 1.00000\n4-2    -8.161290 -50.928 34.605 1.00000\n5-2     2.803226 -39.963 45.570 1.00000\n6-2    -0.674194 -43.441 42.092 1.00000\n7-2    -0.283871 -43.050 42.482 1.00000\n8-2     1.774194 -40.992 44.541 1.00000\n9-2    14.690323 -28.076 57.457 0.99585\n10-2   -6.564516 -49.331 36.202 1.00000\n11-2    7.983871 -34.782 50.750 1.00000\n12-2  -14.264516 -57.031 28.502 0.99709\n13-2    2.661290 -40.105 45.428 1.00000\n14-2  -12.383871 -55.150 30.382 0.99953\n15-2  -12.525806 -55.292 30.241 0.99946\n16-2   -9.687097 -52.453 33.079 0.99999\n17-2  -11.141935 -53.908 31.624 0.99990\n18-2   -5.925806 -48.692 36.841 1.00000\n19-2   -8.232258 -50.999 34.534 1.00000\n20-2    3.143871 -41.710 47.998 1.00000\n4-3   -10.219355 -52.986 32.547 0.99997\n5-3     0.745161 -42.021 43.511 1.00000\n6-3    -2.732258 -45.499 40.034 1.00000\n7-3    -2.341935 -45.108 40.424 1.00000\n8-3    -0.283871 -43.050 42.482 1.00000\n9-3    12.632258 -30.134 55.399 0.99939\n10-3   -8.622581 -51.389 34.144 1.00000\n11-3    5.925806 -36.841 48.692 1.00000\n12-3  -16.322581 -59.089 26.444 0.98634\n13-3    0.603226 -42.163 43.370 1.00000\n14-3  -14.441935 -57.208 28.324 0.99662\n15-3  -14.583871 -57.350 28.182 0.99620\n16-3  -11.745161 -54.511 31.021 0.99978\n17-3  -13.200000 -55.966 29.566 0.99891\n18-3   -7.983871 -50.750 34.782 1.00000\n19-3  -10.290323 -53.057 32.476 0.99997\n20-3    1.085806 -43.768 45.939 1.00000\n5-4    10.964516 -31.802 53.731 0.99992\n6-4     7.487097 -35.279 50.253 1.00000\n7-4     7.877419 -34.889 50.644 1.00000\n8-4     9.935484 -32.831 52.702 0.99998\n9-4    22.851613 -19.915 65.618 0.76824\n10-4    1.596774 -41.170 44.363 1.00000\n11-4   16.145161 -26.621 58.911 0.98785\n12-4   -6.103226 -48.870 36.663 1.00000\n13-4   10.822581 -31.944 53.589 0.99993\n14-4   -4.222581 -46.989 38.544 1.00000\n15-4   -4.364516 -47.131 38.402 1.00000\n16-4   -1.525806 -44.292 41.241 1.00000\n17-4   -2.980645 -45.747 39.786 1.00000\n18-4    2.235484 -40.531 45.002 1.00000\n19-4   -0.070968 -42.837 42.695 1.00000\n20-4   11.305161 -33.549 56.159 0.99994\n6-5    -3.477419 -46.244 39.289 1.00000\n7-5    -3.087097 -45.853 39.679 1.00000\n8-5    -1.029032 -43.795 41.737 1.00000\n9-5    11.887097 -30.879 54.653 0.99974\n10-5   -9.367742 -52.134 33.399 0.99999\n11-5    5.180645 -37.586 47.947 1.00000\n12-5  -17.067742 -59.834 25.699 0.97827\n13-5   -0.141935 -42.908 42.624 1.00000\n14-5  -15.187097 -57.953 27.579 0.99387\n15-5  -15.329032 -58.095 27.437 0.99318\n16-5  -12.490323 -55.257 30.276 0.99948\n17-5  -13.945161 -56.711 28.821 0.99780\n18-5   -8.729032 -51.495 34.037 1.00000\n19-5  -11.035484 -53.802 31.731 0.99991\n20-5    0.340645 -44.513 45.194 1.00000\n7-6     0.390323 -42.376 43.157 1.00000\n8-6     2.448387 -40.318 45.215 1.00000\n9-6    15.364516 -27.402 58.131 0.99300\n10-6   -5.890323 -48.657 36.876 1.00000\n11-6    8.658065 -34.108 51.424 1.00000\n12-6  -13.590323 -56.357 29.176 0.99841\n13-6    3.335484 -39.431 46.102 1.00000\n14-6  -11.709677 -54.476 31.057 0.99979\n15-6  -11.851613 -54.618 30.915 0.99975\n16-6   -9.012903 -51.779 33.753 1.00000\n17-6  -10.467742 -53.234 32.299 0.99996\n18-6   -5.251613 -48.018 37.515 1.00000\n19-6   -7.558065 -50.324 35.208 1.00000\n20-6    3.818065 -41.036 48.672 1.00000\n8-7     2.058065 -40.708 44.824 1.00000\n9-7    14.974194 -27.792 57.741 0.99480\n10-7   -6.280645 -49.047 36.486 1.00000\n11-7    8.267742 -34.499 51.034 1.00000\n12-7  -13.980645 -56.747 28.786 0.99773\n13-7    2.945161 -39.821 45.711 1.00000\n14-7  -12.100000 -54.866 30.666 0.99966\n15-7  -12.241935 -55.008 30.524 0.99960\n16-7   -9.403226 -52.170 33.363 0.99999\n17-7  -10.858065 -53.624 31.908 0.99993\n18-7   -5.641935 -48.408 37.124 1.00000\n19-7   -7.948387 -50.715 34.818 1.00000\n20-7    3.427742 -41.426 48.281 1.00000\n9-8    12.916129 -29.850 55.682 0.99918\n10-8   -8.338710 -51.105 34.428 1.00000\n11-8    6.209677 -36.557 48.976 1.00000\n12-8  -16.038710 -58.805 26.728 0.98869\n13-8    0.887097 -41.879 43.653 1.00000\n14-8  -14.158065 -56.924 28.608 0.99735\n15-8  -14.300000 -57.066 28.466 0.99700\n16-8  -11.461290 -54.228 31.305 0.99984\n17-8  -12.916129 -55.682 29.850 0.99918\n18-8   -7.700000 -50.466 35.066 1.00000\n19-8  -10.006452 -52.773 32.760 0.99998\n20-8    1.369677 -43.484 46.223 1.00000\n10-9  -21.254839 -64.021 21.511 0.85576\n11-9   -6.706452 -49.473 36.060 1.00000\n12-9  -28.954839 -71.721 13.811 0.35269\n13-9  -12.029032 -54.795 30.737 0.99969\n14-9  -27.074194 -69.841 15.692 0.47719\n15-9  -27.216129 -69.982 15.550 0.46731\n16-9  -24.377419 -67.144 18.389 0.66832\n17-9  -25.832258 -68.599 16.934 0.56535\n18-9  -20.616129 -63.382 22.150 0.88455\n19-9  -22.922581 -65.689 19.844 0.76389\n20-9  -11.546452 -56.400 33.307 0.99991\n11-10  14.548387 -28.218 57.315 0.99631\n12-10  -7.700000 -50.466 35.066 1.00000\n13-10   9.225806 -33.541 51.992 0.99999\n14-10  -5.819355 -48.586 36.947 1.00000\n15-10  -5.961290 -48.728 36.805 1.00000\n16-10  -3.122581 -45.889 39.644 1.00000\n17-10  -4.577419 -47.344 38.189 1.00000\n18-10   0.638710 -42.128 43.405 1.00000\n19-10  -1.667742 -44.434 41.099 1.00000\n20-10   9.708387 -35.145 54.562 0.99999\n12-11 -22.248387 -65.015 20.518 0.80373\n13-11  -5.322581 -48.089 37.444 1.00000\n14-11 -20.367742 -63.134 22.399 0.89472\n15-11 -20.509677 -63.276 22.257 0.88898\n16-11 -17.670968 -60.437 25.095 0.96936\n17-11 -19.125806 -61.892 23.641 0.93692\n18-11 -13.909677 -56.676 28.857 0.99787\n19-11 -16.216129 -58.982 26.550 0.98726\n20-11  -4.840000 -49.694 40.014 1.00000\n13-12  16.925806 -25.841 59.692 0.98004\n14-12   1.880645 -40.886 44.647 1.00000\n15-12   1.738710 -41.028 44.505 1.00000\n16-12   4.577419 -38.189 47.344 1.00000\n17-12   3.122581 -39.644 45.889 1.00000\n18-12   8.338710 -34.428 51.105 1.00000\n19-12   6.032258 -36.734 48.799 1.00000\n20-12  17.408387 -27.445 62.262 0.98369\n14-13 -15.045161 -57.811 27.721 0.99450\n15-13 -15.187097 -57.953 27.579 0.99387\n16-13 -12.348387 -55.115 30.418 0.99955\n17-13 -13.803226 -56.570 28.963 0.99807\n18-13  -8.587097 -51.353 34.179 1.00000\n19-13 -10.893548 -53.660 31.873 0.99992\n20-13   0.482581 -44.371 45.336 1.00000\n15-14  -0.141935 -42.908 42.624 1.00000\n16-14   2.696774 -40.070 45.463 1.00000\n17-14   1.241935 -41.524 44.008 1.00000\n18-14   6.458065 -36.308 49.224 1.00000\n19-14   4.151613 -38.615 46.918 1.00000\n20-14  15.527742 -29.326 60.381 0.99545\n16-15   2.838710 -39.928 45.605 1.00000\n17-15   1.383871 -41.382 44.150 1.00000\n18-15   6.600000 -36.166 49.366 1.00000\n19-15   4.293548 -38.473 47.060 1.00000\n20-15  15.669677 -29.184 60.523 0.99493\n17-16  -1.454839 -44.221 41.311 1.00000\n18-16   3.761290 -39.005 46.528 1.00000\n19-16   1.454839 -41.311 44.221 1.00000\n20-16  12.830968 -32.023 57.685 0.99961\n18-17   5.216129 -37.550 47.982 1.00000\n19-17   2.909677 -39.857 45.676 1.00000\n20-17  14.285806 -30.568 59.139 0.99837\n19-18  -2.306452 -45.073 40.460 1.00000\n20-18   9.069677 -35.784 53.923 1.00000\n20-19  11.376129 -33.478 56.230 0.99993\n\n\n유의한 패턴 없음 Tukey 검정 결과 (보통 유의할때 함) 테이블 도 유의한 수치가 없음"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-07-anova/index.html#statistical-methods-similar-to-anova",
    "href": "docs/blog/posts/statistics/2023-01-07-anova/index.html#statistical-methods-similar-to-anova",
    "title": "ANOVA",
    "section": "1 Statistical Methods Similar to ANOVA",
    "text": "1 Statistical Methods Similar to ANOVA\n\n2023-01-27, repeated measures ANOVA\n2023-01-27, ANCOVA\n2023-01-28, MANOVA\n\n\n\n\nKorean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n1.1 Description\nANOVA는 3개 이상의 모집단 사이의 평균의 동일성을 검정하는 통계 분석 방법이다.\n\n일원 분산 분석 (One-way ANOVA)\n\n그룹을 구분하는 변수가 1개\nBetween-Groups one-way ANOVA(집단간 일원분산분석): 관측치를 grouping하는 범주형 변수가 1개이며 각 관측치는 범주형 변수에 의해 구분되는 그룹들 가운데 반드시 하나에만 할당되어야한다. 즉, 어떠한 경우에도 하나의 관측치 또는 샘플이 여러 groups에 동시에 들어가면 안된다. 이 때 이렇게 그룹을 나누는 범주형 변수를 집단간 요인이라고 한다.\nWithin-groups one-way ANOVA (집단 내 일원분산분석) or repeated measures ANOVA: 시간과 같은 하나의 범주형 변수로 샘플들을 측정한다. 시간의 경과에 따라 측정된 샘플들을 범주형 변수의 여러 기간에 걸쳐 모두 할당시킨다. 즉, 하나의 샘플이 여러 그룹에 다른 측정치로 관찰될 수 있다. 예를들어, sample A가 4주, 8주, 12주, 16주 그룹에 모두 측정 된다. 이때 기간변수는 집단 내 요인이라고 부른다.\n\n이원 분산 분석 (Two-way ANOVA)\n\n집단을 구분하는 변수가 2개이며 각 집단 간 요인과 집단 내 요인을 나타낸다.\n이원 분석 부터는 main effect와 interaction effect가 존재한다.\n범주형 변수 A와 범주형 변수 B의 Main effect 계산\n범주형 변수 A와 범주형 변수 B의 상호 작용 효과 or 교호 작용 효과 (Interaction effect) 계산\ngroup을 구분하는 독립변수가 2개 일때 모집단 간 평균의 동일성 검정\n\n2개의 주효과(main effect) 검정: 각 독립 변수에 의해 만들어지는 집단 간 평균의 차이에 대한 검정\n\n먼저, 두 독립변수가 종속변수에 개별적으로 영향을 미치는지 검정\n\n\n1개의 상호작용효과(interaction effect) 검정: 두 독립 변수의 조합에 의해 만들어지는 집단 간 평균의 차이에 대한 검정\n\n두 독립변수의 조합이 종속변수와 유의한 영향관계를 갖는지 검정\n\n만약에 유의하다면 2개의 독립변수가 합쳐져서 나온 파생효과이기 때문에 1개만 골라서 분석해서 해석 할 수 없음\n\n\n\n\n\n1.2 How to conduct ANOVA?\n\n분산 분석은 F검정(F test)을 통해 수행한다.\nF 검정은 집단 간 분산 (between-groups variability)과 집단 내 분산 (within-groups variability)의 ratio로 계산된 F값 (F value or F statistic)을 토대로 가설검정을 수행한다. 이때 F value or F statistic을 통계 검정을 위한 검정통계량 (test statistic) 라고 부른다.\nF 검정 결과가 통계적으로 유의하면 집단 간 평균의 차이가 존재한다. (즉, 독립 변수가 종속변수에 영향을 미침)\nF 분포 2개의 자유도에 의해 분포의 모양이 결정되며 대체로 오른쪽으로 긴 꼬리를 갖는다\n\n첫 번째 자유도: 집단 간(between-group)의 자유도\n두 번째 자유도: 집단 내(within-group)의 자유도\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(gplots)\nlibrary(rmarkdown)\nknitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)\noptions(digits = 5)\nset.seed(20230109)\n\n\n\n\n\n\n\n종속 변수의 변동성은 다음과 같이 설명되기 때문에 아래의 식을 만족한다.\n\\[SS_{total}=SS_A+SS_B+SS_{AB}+SS_{error}\\]\n\\(SS_{total}\\)은 쉽게 구할 수 있고 \\(SS_A\\), \\(SS_B\\), \\(SS_{error}\\)를 계산하여 빼준다.\nTwo Way Anova SS 계산 공식 링크\n\\(SS_{AB}\\) 즉,\n\\[ SS_{AB}=SS_{total}-SS_A-SS_B-SS_{error} \\]\n\n\n1.3 Meaning\nANOVA는 집단 간 분산과 집단 내 분산의 비교하는 방식으로, 좀 더 구체적으로는 집단 간 분산과 집단 내 분산의 비를 계산하여, 집단 간 분산이 클수록 그리고 집단 분산이 작을 수록 집단 평균이 다를 가능성이 증가한다는 알고리즘에 기초한다.\n\n\n2 Application to Example\n\n2.1 Data Description\n\n2.1.1 Raw Data\n(…민감 정보 제거 및 데이터 변환 후 컨설팅 내용 일부 발췌…)\n\n\n\nexample data는 Day, Run, response의 변수들을 포함하고 있습니다. 공유해주신 정보에 따르면 아마도 Run은 오전과 오후를 나누는 변수인 것으로 생각 됩니다. 이 data만 보면 아마도 같은 샘플에 대해서 시약 제품이 시간에 따라 얼마나 안정적인 performance를 보여주는지 검사하는 실험으로 추측됩니다. 좀 더 분석하기 용이한 형태로 data structure를 바꾸겠습니다.\n\n\n2.1.2 Processed Data\n\n\n\n\n  \n\n\n\n재가공된 data는 120개의 샘플과 5개의 변수를 갖고있습니다. 변수 목록은 다음과 같습니다.\n\nid: 열번호, 총 20일간 하루 2회 구동(AM, PM) 구동, 오전 오후 각 각 3번씩 구동 총 120 \\((=20 \\times 3 \\times 2)\\) 샘플\n\nDay: Day1~20\n\nnoon: AM= before noon, PM= after noon\nRun: 1회 구동당 3번 반복씩1, 2, 3\n\nresponse: response variable, 낮을 수록 좋음\n\nANOVA의 Assumption\n\nresponse variable should follow normal distribution.\n\nhomoscedasticity, equality of variance: 각 집단의 분포는 모두 동일한 분산을 가짐\nANOVA의 가정들을 반드시 충족하지 않아도 되지만 충족하면 Power 가 올라감\n\n\n\n\n2.2 EDA (Explorator Data Analysis)\n이 data는 아래 처럼 1의 결측치를 갖고 있습니다.\n\n\n\n\n\nid\nDay\nnoon\nRun\nresponse\n\n\n\n\n117\n20\nAM\n3\nNA\n\n\n\n\n\nCt에 대한 Global Statistics는 다음과 같습니다.\n\n\n\n\n\ncount\nglobal_response_mean\nglobal_response_sd\nglobal_response_CV\n\n\n\n\n119\n38.727\n18.47\n47.694 %\n\n\n\n\n\nDay groups의 Statistics은 다음과 같습니다.\n\n\n\n\n  \n\n\n\nAM/PM groups의 Statistics은 다음과 같습니다.\n\n\n\n\n  \n\n\n\nDays와 AM/PM 조합 groups의 Statistics은 다음과 같습니다.\n\n\n\n\n  \n\n\n\n이제 ANOVA를 수행하기 위한 basic statistics는 모두 구했습니다. ANOVA를 수행하기 위해 집단 간 분산과 집단 내 분산을 계산하도록 하겠습니다.\n\n\n2.3 집단 간 분산\n앞에서 설명 드린바로 유추해보면 예시 data의 집단 간 분산의 범주형 변수는 Day로 설정하는 것이 합리적인 것으로 보입니다.\n\n\\(g=g\\) Day의 sample size = 20, 자유도 = 20-1 = 19 입니다.\n\\(n_g=g\\) group의 sample size, \\(\\overline{X}_g=g\\) 의 sample mean은 다음과 같습니다.\n\\(\\overline{X}\\) = global sample mean = 38.72681\n집단 간 분산: \\(\\frac{집단 간 제곱합}{자유도}=\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n2.3.1 SS_Day (집단간 분산 Day)\nDay sq = \\([(\\overline{X}_g-\\overline{X})^2n_g]\\) 집단(Day)간 분산 계산, 집단(Day)간 분산 = \\(\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n\n\n  \n\n\n\nAnalysis-In program의 ANOVA결과값과 일치하는 것을 볼 수 있습니다. $SS_{day} $= 7025.97838 with \\(df=19\\).\n\n\n2.3.2 SS_noon (집단간 분산 noon)\nnoon sq = \\([(\\overline{X}_g-\\overline{X})^2n_g]\\) 집단(noon)간 분산 계산, 집단(noon)간 분산 = \\(\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n\n\n  \n\n\n\nAnalysis-In program의 결과에서 찾아 볼 수 없죠? 이 결과는 숨어 있습니다. 상호 작용에 대한 분산값을 구하고 나면 정체를 알 수 있습니다.\n\\(SS_{noon}\\) = 319.76458 with \\(df=1\\).\n\n\n2.3.3 SS_error (집단내 분산)\n\n집단 내 분산 (within-groups variability)\n\n\n\n\n\n  \n\n\n\n\\(SS_{error}\\) = 2.47041^{4}\nAnalysis-In program의 결과와 일치하는 것을 확인할 수 있습니다.\n\n\n2.3.4 SS_total\n\n\n\n\n  \n\n\n\n\\(SS_{total}\\) = 4.02557^{4}\nAnalysis-In program의 ANOVA 결과 table에 있는 SS들의 합과 일치하는 것을 확인할 수 있습니다.\n\n\n2.3.5 상호 작용 분산\n\n\n\n\\(SS_{interaction}=SS_{DayNoon}= SS_{total}-SS_{Day}-SS_{noon}-SS_{error}\\)\n= 4.02557{4}-2.47041{4}-319.76458-7025.97838 = 8205.93974\nAnalysis-In program의 ANOVA 결과 table과 일치하는 것을 확인할 수 있습니다.\n위의 결과들을 종합하면 아래와 같이 요약됩니다.\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   7026     370    1.18   0.29\nnoon         1    339     339    1.08   0.30\nDay:noon    19   8187     431    1.38   0.16\nResiduals   79  24704     313               \n1 observation deleted due to missingness\n\n\n\nRepeatability SD = \\(\\sqrt{V_{error}}=\\sqrt{MS_{error}}\\) = 17.6836\nRepeatability CV = \\(\\frac{repeatability \\space SD}{global \\space mean \\space response}\\) = 0.45662\n\n위의 결과를 간단히 해석해 보면\n\n집단간 범주 변수인 Day는 p-value =0.29>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, 일별로 평균 response값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, 일별로 평균 response값이 다르지 않습니다.\n\n집단간 범주 변수인 noon은 p-value =0.30>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, 오전/오후별 평균 response값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, 오전/오후별 평균 response값이 다르지 않습니다.\n\nDay와 noon두 변수의 상호작용 변수는 p-value =0.16>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, Day별 Noon(오전/오후)별 평균 response값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, Day별 Noon(오전/오후)별 평균 response값이 다르지 않습니다.\n\n\n최종 결론, 제품의 response값이 Day별 오전/오후별 안정적인 performance를 보인다고 조심스럽게 결론을 내릴 수 있습니다.\n이제 까지는 질문에 대한 답이 되는 ANOVA의 원리 및 통계량의 재현 및 해석법에 대하여 알아봤습니다. 하지만 직관적으로 어떤 의미가 있을 까요? 원래는 시각화를 통해 데이터의 패턴을 짐작하고 통계 검정 결과를 예상하는데 우리는 반대로 가고 있네요 ㅎㅎ 시각화를 통해 ANOVA 결과가 얼마나 직관적인지 알아보겠습니다.\n\n\n\n2.4 Visualization\n\n2.4.1 One-way: Day\n\n\n\n\n\n\n\n\n자세히 보면 일별로 시간의 경과에 따라 전체적으로 평균 response값이 약간 하향하는 것으로 보입니다. 하지만 좀 더 세부적으로 관찰하면 1일~8일 평균 response의 경향이 constant한 패턴을 보입니다. 9일~13일 평균 response가 진동 하향하는 패턴을 보입니다. 14일~20일 평균 response가 상향하는 패턴을 보입니다.\n\n\nTables of means\nGrand mean\n       \n38.727 \n\n Day \n        1    2     3     4     5     6     7     8     9    10    11    12\n    47.34 41.2 43.26 33.04 44.01 40.53 40.92 42.98 55.89 34.64 49.19 26.94\nrep  6.00  6.0  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00\n       13    14    15    16    17    18    19    20\n    43.86 28.82 28.68 31.52 30.06 35.28 32.97 44.35\nrep  6.00  6.00  6.00  6.00  6.00  6.00  6.00  5.00\n\n\nTables of effects\n\n Day \n        1     2     3      4    5     6     7     8     9     10    11     12\n    8.615 2.476 4.534 -5.685 5.28 1.802 2.193 4.251 17.17 -4.088 10.46 -11.79\nrep 6.000 6.000 6.000  6.000 6.00 6.000 6.000 6.000  6.00  6.000  6.00   6.00\n       13     14     15     16     17     18     19   20\n    5.138 -9.907 -10.05 -7.211 -8.666 -3.449 -5.756 5.62\nrep 6.000  6.000   6.00  6.000  6.000  6.000  6.000 5.00\n\n\n위에 첫 번째표에서 Global Sample response Mean = 38.727 과 각 집단의 평균 response를 확인할 수 있습니다. 위에 두 번째표에서 Global Sample response Mean = 37.322 과 각 집단의 평균 response의 차이를 확인할 수 있습니다.\n\nDay 9에서 차이가 가장 큰 것으로 보아 9일째 실험에서 performance가 가장 낮은 것이 관측됐습니다.\n반대로, 12일에 performance 가장 좋은 것으로 관측됐습니다.\n\n9일과 12일에 response값에 영향을 미쳤던 요인이 있었는지 복기 하는것도 도움이 되겠군요.\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   7026     370     1.1   0.36\nResiduals   99  33230     336               \n1 observation deleted due to missingness\n\n\nOne-way ANOVA의 결과값입니다. Day별 평균 response의 차이는 거의 없는 것으로 보입니다. 따라서 Day 별 평균 response의 경향이 일관되지 않고 One-way ANOVA에서 역시 통계적으로 유의하지 않아 Day 변수는 평균 response에 영향을 미치지 않는 것 같습니다.\n\n\n2.4.2 One-way: AM/PM\n\n\n\n\n\n\n\n\n오후에 시간의 경과에 따라 전체적으로 평균 response값이 약간 하향하는 것으로 보입니다.\n\n\nTables of means\nGrand mean\n       \n38.727 \n\n noon \n       AM   PM\n    40.38 37.1\nrep 59.00 60.0\n\n\nTables of effects\n\n noon \n        AM     PM\n     1.653 -1.626\nrep 59.000 60.000\n\n\n위 첫 번째 표에서 AM/PM 간의 평균 response차이는 0.15 (농도가 약 0.5배) 차이가 나는 것을 확인할 수 있습니다. 생물학적으로 의미가 있는 수치일까요? 위 두 번째 표에서 Global Sample Mean 37.322와 오전/오후 별 약 0.07씩(농도가 약 0.25배) 차이가 납니다.\n\n\n             Df Sum Sq Mean Sq F value Pr(>F)\nnoon          1    320     320    0.94   0.34\nResiduals   117  39936     341               \n1 observation deleted due to missingness\n\n\n오전 오후별 One way ANOVA를 실행한 결과가 오전/오부 평균 response값의 차이가 다르지 않다는 것을 시사하고 있습니다. 아무래도 위의 차이는 우연에 의해 발생한 현상인 것 같습니다.\n\n\n\n\n\n일별로 평균 response값이 차이를 모든 경우의 수에서 차이가 나는지 확인한 결과 신뢰구간 모두가 0을 포함하고 있으므로 Day 변수는 유의하다고 볼 수 없습니다.\n\n\n\n\n\n오전/오후별로 평균 response값이 차이를 모든 경우의 수에서 차이가 나는지 확인한 결과 신뢰구간 모두가 0을 포함하고 있으므로 오전/오후 변수는 유의하다고 볼 수 없습니다.\n여기 까지 각 변수별 평균 response로의 영향도를 통계적으로 시각적으로 관찰했습니다. 하지만 Day별 오전/오후별 영향도가 있는지 확인하겠습니다. (이미 위에서 통계적으로 없다고 검정됐습니다.)\n\n\n\n2.5 Two way Anova\n\n\n       AM     PM\n1  35.987 58.697\n2  40.032 42.374\n3  33.148 53.374\n4  36.697 29.387\n5  44.432 43.581\n6  45.781 35.277\n7  40.458 41.381\n8  51.103 34.852\n9  69.981 41.806\n10 26.619 42.658\n11 67.923 30.452\n12 25.981 27.897\n13 40.600 47.129\n14 25.839 31.800\n15 18.458 38.897\n16 36.910 26.123\n17 41.594 18.529\n18 38.471 32.084\n19 45.923 20.019\n20 42.303 45.710\n\n\n        AM      PM\n1  20.9797  7.4516\n2  18.6291 12.0856\n3  22.1450 34.0565\n4  16.3483  7.5583\n5   4.6107 11.0032\n6  23.6862 11.7618\n7  12.3380 30.0128\n8  33.4416 12.0135\n9   7.7381 30.4715\n10  3.4483  7.8927\n11 28.8752  9.1557\n12  5.9053 12.9054\n13 15.2088  7.3824\n14  8.4556 27.2253\n15  4.5063 35.2180\n16 18.3756  1.6899\n17 13.3474  8.0068\n18  6.0067 21.7672\n19 19.7931  1.4176\n20  3.9142 14.0145\n\n\n\n\nTables of means\nGrand mean\n       \n38.727 \n\n Day \n        1    2     3     4     5     6     7     8     9    10    11    12\n    47.34 41.2 43.26 33.04 44.01 40.53 40.92 42.98 55.89 34.64 49.19 26.94\nrep  6.00  6.0  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00\n       13    14    15    16    17    18    19    20\n    43.86 28.82 28.68 31.52 30.06 35.28 32.97 44.35\nrep  6.00  6.00  6.00  6.00  6.00  6.00  6.00  5.00\n\n noon \n       AM    PM\n    40.43 37.05\nrep 59.00 60.00\n\n Day:noon \n     noon\nDay   AM    PM   \n  1   35.99 58.70\n  rep  3.00  3.00\n  2   40.03 42.37\n  rep  3.00  3.00\n  3   33.15 53.37\n  rep  3.00  3.00\n  4   36.70 29.39\n  rep  3.00  3.00\n  5   44.43 43.58\n  rep  3.00  3.00\n  6   45.78 35.28\n  rep  3.00  3.00\n  7   40.46 41.38\n  rep  3.00  3.00\n  8   51.10 34.85\n  rep  3.00  3.00\n  9   69.98 41.81\n  rep  3.00  3.00\n  10  26.62 42.66\n  rep  3.00  3.00\n  11  67.92 30.45\n  rep  3.00  3.00\n  12  25.98 27.90\n  rep  3.00  3.00\n  13  40.60 47.13\n  rep  3.00  3.00\n  14  25.84 31.80\n  rep  3.00  3.00\n  15  18.46 38.90\n  rep  3.00  3.00\n  16  36.91 26.12\n  rep  3.00  3.00\n  17  41.59 18.53\n  rep  3.00  3.00\n  18  38.47 32.08\n  rep  3.00  3.00\n  19  45.92 20.02\n  rep  3.00  3.00\n  20  42.30 45.71\n  rep  2.00  3.00\n\n\nTables of effects\n\n Day \n        1     2     3      4    5     6     7     8     9     10    11     12\n    8.615 2.476 4.534 -5.685 5.28 1.802 2.193 4.251 17.17 -4.088 10.46 -11.79\nrep 6.000 6.000 6.000  6.000 6.00 6.000 6.000 6.000  6.00  6.000  6.00   6.00\n       13     14     15     16     17     18     19   20\n    5.138 -9.907 -10.05 -7.211 -8.666 -3.449 -5.756 5.62\nrep 6.000  6.000   6.00  6.000  6.000  6.000  6.000 5.00\n\n noon \n        AM     PM\n     1.701 -1.672\nrep 59.000 60.000\n\n Day:noon \n     noon\nDay   AM      PM     \n  1   -13.044  13.044\n  rep   3.000   3.000\n  2    -2.860   2.860\n  rep   3.000   3.000\n  3   -11.802  11.802\n  rep   3.000   3.000\n  4     1.966  -1.966\n  rep   3.000   3.000\n  5    -1.263   1.263\n  rep   3.000   3.000\n  6     3.562  -3.562\n  rep   3.000   3.000\n  7    -2.151   2.151\n  rep   3.000   3.000\n  8     6.437  -6.437\n  rep   3.000   3.000\n  9    12.398 -12.398\n  rep   3.000   3.000\n  10   -9.709   9.709\n  rep   3.000   3.000\n  11   17.046 -17.046\n  rep   3.000   3.000\n  12   -2.647   2.647\n  rep   3.000   3.000\n  13   -4.954   4.954\n  rep   3.000   3.000\n  14   -4.670   4.670\n  rep   3.000   3.000\n  15  -11.909  11.909\n  rep   3.000   3.000\n  16    3.704  -3.704\n  rep   3.000   3.000\n  17    9.843  -9.843\n  rep   3.000   3.000\n  18    1.504  -1.504\n  rep   3.000   3.000\n  19   11.262 -11.262\n  rep   3.000   3.000\n  20   -4.071   2.714\n  rep   2.000   3.000\n\n\none way ANOVA와 같이 해석\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   7026     370    1.18   0.29\nnoon         1    339     339    1.08   0.30\nDay:noon    19   8187     431    1.38   0.16\nResiduals   79  24704     313               \n1 observation deleted due to missingness\n\n\n위 그림을 보듯이 두 변수의 영향도가 없음, ANOVA 역시 유의하지 않음\n\n\n\n\n\n\n Missing rows: 117 \n\n\n\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = response ~ Day + noon + Day:noon, data = data)\n\n$Day\n            diff     lwr     upr   p adj\n2-1    -6.138710 -43.575 31.2979 1.00000\n3-1    -4.080645 -41.517 33.3560 1.00000\n4-1   -14.300000 -51.737 23.1366 0.99700\n5-1    -3.335484 -40.772 34.1011 1.00000\n6-1    -6.812903 -44.250 30.6237 1.00000\n7-1    -6.422581 -43.859 31.0140 1.00000\n8-1    -4.364516 -41.801 33.0721 1.00000\n9-1     8.551613 -28.885 45.9882 1.00000\n10-1  -12.703226 -50.140 24.7334 0.99934\n11-1    1.845161 -35.591 39.2818 1.00000\n12-1  -20.403226 -57.840 17.0334 0.89330\n13-1   -3.477419 -40.914 33.9592 1.00000\n14-1  -18.522581 -55.959 18.9140 0.95243\n15-1  -18.664516 -56.101 18.7721 0.94905\n16-1  -15.825806 -53.262 21.6108 0.99024\n17-1  -17.280645 -54.717 20.1560 0.97539\n18-1  -12.064516 -49.501 25.3721 0.99968\n19-1  -14.370968 -51.808 23.0657 0.99682\n20-1   -2.994839 -42.259 36.2690 1.00000\n3-2     2.058065 -35.379 39.4947 1.00000\n4-2    -8.161290 -45.598 29.2753 1.00000\n5-2     2.803226 -34.633 40.2399 1.00000\n6-2    -0.674194 -38.111 36.7624 1.00000\n7-2    -0.283871 -37.721 37.1528 1.00000\n8-2     1.774194 -35.662 39.2108 1.00000\n9-2    14.690323 -22.746 52.1270 0.99585\n10-2   -6.564516 -44.001 30.8721 1.00000\n11-2    7.983871 -29.453 45.4205 1.00000\n12-2  -14.264516 -51.701 23.1721 0.99709\n13-2    2.661290 -34.775 40.0979 1.00000\n14-2  -12.383871 -49.821 25.0528 0.99953\n15-2  -12.525806 -49.962 24.9108 0.99946\n16-2   -9.687097 -47.124 27.7495 0.99999\n17-2  -11.141935 -48.579 26.2947 0.99990\n18-2   -5.925806 -43.362 31.5108 1.00000\n19-2   -8.232258 -45.669 29.2044 1.00000\n20-2    3.143871 -36.120 42.4077 1.00000\n4-3   -10.219355 -47.656 27.2173 0.99997\n5-3     0.745161 -36.691 38.1818 1.00000\n6-3    -2.732258 -40.169 34.7044 1.00000\n7-3    -2.341935 -39.779 35.0947 1.00000\n8-3    -0.283871 -37.721 37.1528 1.00000\n9-3    12.632258 -24.804 50.0689 0.99939\n10-3   -8.622581 -46.059 28.8140 1.00000\n11-3    5.925806 -31.511 43.3624 1.00000\n12-3  -16.322581 -53.759 21.1140 0.98634\n13-3    0.603226 -36.833 38.0399 1.00000\n14-3  -14.441935 -51.879 22.9947 0.99662\n15-3  -14.583871 -52.021 22.8528 0.99620\n16-3  -11.745161 -49.182 25.6915 0.99978\n17-3  -13.200000 -50.637 24.2366 0.99891\n18-3   -7.983871 -45.421 29.4528 1.00000\n19-3  -10.290323 -47.727 27.1463 0.99997\n20-3    1.085806 -38.178 40.3497 1.00000\n5-4    10.964516 -26.472 48.4011 0.99992\n6-4     7.487097 -29.950 44.9237 1.00000\n7-4     7.877419 -29.559 45.3140 1.00000\n8-4     9.935484 -27.501 47.3721 0.99998\n9-4    22.851613 -14.585 60.2882 0.76824\n10-4    1.596774 -35.840 39.0334 1.00000\n11-4   16.145161 -21.291 53.5818 0.98785\n12-4   -6.103226 -43.540 31.3334 1.00000\n13-4   10.822581 -26.614 48.2592 0.99993\n14-4   -4.222581 -41.659 33.2140 1.00000\n15-4   -4.364516 -41.801 33.0721 1.00000\n16-4   -1.525806 -38.962 35.9108 1.00000\n17-4   -2.980645 -40.417 34.4560 1.00000\n18-4    2.235484 -35.201 39.6721 1.00000\n19-4   -0.070968 -37.508 37.3657 1.00000\n20-4   11.305161 -27.959 50.5690 0.99994\n6-5    -3.477419 -40.914 33.9592 1.00000\n7-5    -3.087097 -40.524 34.3495 1.00000\n8-5    -1.029032 -38.466 36.4076 1.00000\n9-5    11.887097 -25.550 49.3237 0.99974\n10-5   -9.367742 -46.804 28.0689 0.99999\n11-5    5.180645 -32.256 42.6173 1.00000\n12-5  -17.067742 -54.504 20.3689 0.97827\n13-5   -0.141935 -37.579 37.2947 1.00000\n14-5  -15.187097 -52.624 22.2495 0.99387\n15-5  -15.329032 -52.766 22.1076 0.99318\n16-5  -12.490323 -49.927 24.9463 0.99948\n17-5  -13.945161 -51.382 23.4915 0.99780\n18-5   -8.729032 -46.166 28.7076 1.00000\n19-5  -11.035484 -48.472 26.4011 0.99991\n20-5    0.340645 -38.923 39.6045 1.00000\n7-6     0.390323 -37.046 37.8270 1.00000\n8-6     2.448387 -34.988 39.8850 1.00000\n9-6    15.364516 -22.072 52.8011 0.99300\n10-6   -5.890323 -43.327 31.5463 1.00000\n11-6    8.658065 -28.779 46.0947 1.00000\n12-6  -13.590323 -51.027 23.8463 0.99841\n13-6    3.335484 -34.101 40.7721 1.00000\n14-6  -11.709677 -49.146 25.7270 0.99979\n15-6  -11.851613 -49.288 25.5850 0.99975\n16-6   -9.012903 -46.450 28.4237 1.00000\n17-6  -10.467742 -47.904 26.9689 0.99996\n18-6   -5.251613 -42.688 32.1850 1.00000\n19-6   -7.558065 -44.995 29.8786 1.00000\n20-6    3.818065 -35.446 43.0819 1.00000\n8-7     2.058065 -35.379 39.4947 1.00000\n9-7    14.974194 -22.462 52.4108 0.99480\n10-7   -6.280645 -43.717 31.1560 1.00000\n11-7    8.267742 -29.169 45.7044 1.00000\n12-7  -13.980645 -51.417 23.4560 0.99773\n13-7    2.945161 -34.491 40.3818 1.00000\n14-7  -12.100000 -49.537 25.3366 0.99966\n15-7  -12.241935 -49.679 25.1947 0.99960\n16-7   -9.403226 -46.840 28.0334 0.99999\n17-7  -10.858065 -48.295 26.5786 0.99993\n18-7   -5.641935 -43.079 31.7947 1.00000\n19-7   -7.948387 -45.385 29.4882 1.00000\n20-7    3.427742 -35.836 42.6916 1.00000\n9-8    12.916129 -24.521 50.3528 0.99918\n10-8   -8.338710 -45.775 29.0979 1.00000\n11-8    6.209677 -31.227 43.6463 1.00000\n12-8  -16.038710 -53.475 21.3979 0.98869\n13-8    0.887097 -36.550 38.3237 1.00000\n14-8  -14.158065 -51.595 23.2786 0.99735\n15-8  -14.300000 -51.737 23.1366 0.99700\n16-8  -11.461290 -48.898 25.9753 0.99984\n17-8  -12.916129 -50.353 24.5205 0.99918\n18-8   -7.700000 -45.137 29.7366 1.00000\n19-8  -10.006452 -47.443 27.4302 0.99998\n20-8    1.369677 -37.894 40.6335 1.00000\n10-9  -21.254839 -58.691 16.1818 0.85576\n11-9   -6.706452 -44.143 30.7302 1.00000\n12-9  -28.954839 -66.391  8.4818 0.35269\n13-9  -12.029032 -49.466 25.4076 0.99969\n14-9  -27.074194 -64.511 10.3624 0.47719\n15-9  -27.216129 -64.653 10.2205 0.46731\n16-9  -24.377419 -61.814 13.0592 0.66832\n17-9  -25.832258 -63.269 11.6044 0.56535\n18-9  -20.616129 -58.053 16.8205 0.88455\n19-9  -22.922581 -60.359 14.5140 0.76389\n20-9  -11.546452 -50.810 27.7174 0.99991\n11-10  14.548387 -22.888 51.9850 0.99631\n12-10  -7.700000 -45.137 29.7366 1.00000\n13-10   9.225806 -28.211 46.6624 0.99999\n14-10  -5.819355 -43.256 31.6173 1.00000\n15-10  -5.961290 -43.398 31.4753 1.00000\n16-10  -3.122581 -40.559 34.3140 1.00000\n17-10  -4.577419 -42.014 32.8592 1.00000\n18-10   0.638710 -36.798 38.0753 1.00000\n19-10  -1.667742 -39.104 35.7689 1.00000\n20-10   9.708387 -29.555 48.9723 0.99999\n12-11 -22.248387 -59.685 15.1882 0.80373\n13-11  -5.322581 -42.759 32.1140 1.00000\n14-11 -20.367742 -57.804 17.0689 0.89472\n15-11 -20.509677 -57.946 16.9270 0.88898\n16-11 -17.670968 -55.108 19.7657 0.96936\n17-11 -19.125806 -56.562 18.3108 0.93692\n18-11 -13.909677 -51.346 23.5270 0.99787\n19-11 -16.216129 -53.653 21.2205 0.98726\n20-11  -4.840000 -44.104 34.4239 1.00000\n13-12  16.925806 -20.511 54.3624 0.98004\n14-12   1.880645 -35.556 39.3173 1.00000\n15-12   1.738710 -35.698 39.1753 1.00000\n16-12   4.577419 -32.859 42.0140 1.00000\n17-12   3.122581 -34.314 40.5592 1.00000\n18-12   8.338710 -29.098 45.7753 1.00000\n19-12   6.032258 -31.404 43.4689 1.00000\n20-12  17.408387 -21.855 56.6723 0.98369\n14-13 -15.045161 -52.482 22.3915 0.99450\n15-13 -15.187097 -52.624 22.2495 0.99387\n16-13 -12.348387 -49.785 25.0882 0.99955\n17-13 -13.803226 -51.240 23.6334 0.99807\n18-13  -8.587097 -46.024 28.8495 1.00000\n19-13 -10.893548 -48.330 26.5431 0.99992\n20-13   0.482581 -38.781 39.7464 1.00000\n15-14  -0.141935 -37.579 37.2947 1.00000\n16-14   2.696774 -34.740 40.1334 1.00000\n17-14   1.241935 -36.195 38.6786 1.00000\n18-14   6.458065 -30.979 43.8947 1.00000\n19-14   4.151613 -33.285 41.5882 1.00000\n20-14  15.527742 -23.736 54.7916 0.99545\n16-15   2.838710 -34.598 40.2753 1.00000\n17-15   1.383871 -36.053 38.8205 1.00000\n18-15   6.600000 -30.837 44.0366 1.00000\n19-15   4.293548 -33.143 41.7302 1.00000\n20-15  15.669677 -23.594 54.9335 0.99493\n17-16  -1.454839 -38.891 35.9818 1.00000\n18-16   3.761290 -33.675 41.1979 1.00000\n19-16   1.454839 -35.982 38.8915 1.00000\n20-16  12.830968 -26.433 52.0948 0.99961\n18-17   5.216129 -32.221 42.6528 1.00000\n19-17   2.909677 -34.527 40.3463 1.00000\n20-17  14.285806 -24.978 53.5497 0.99837\n19-18  -2.306452 -39.743 35.1302 1.00000\n20-18   9.069677 -30.194 48.3335 1.00000\n20-19  11.376129 -27.888 50.6400 0.99993\n\n$noon\n         diff     lwr    upr   p adj\nPM-AM -3.3731 -9.8265 3.0804 0.30135\n\n$`Day:noon`\n                  diff      lwr     upr   p adj\n2:AM-1:AM     4.045161  -54.344 62.4344 1.00000\n3:AM-1:AM    -2.838710  -61.228 55.5505 1.00000\n4:AM-1:AM     0.709677  -57.680 59.0989 1.00000\n5:AM-1:AM     8.445161  -49.944 66.8344 1.00000\n6:AM-1:AM     9.793548  -48.596 68.1827 1.00000\n7:AM-1:AM     4.470968  -53.918 62.8602 1.00000\n8:AM-1:AM    15.116129  -43.273 73.5053 1.00000\n9:AM-1:AM    33.993548  -24.396 92.3827 0.92887\n10:AM-1:AM   -9.367742  -67.757 49.0215 1.00000\n11:AM-1:AM   31.935484  -26.454 90.3247 0.96592\n12:AM-1:AM  -10.006452  -68.396 48.3827 1.00000\n13:AM-1:AM    4.612903  -53.776 63.0021 1.00000\n14:AM-1:AM  -10.148387  -68.538 48.2408 1.00000\n15:AM-1:AM  -17.529032  -75.918 40.8602 1.00000\n16:AM-1:AM    0.922581  -57.467 59.3118 1.00000\n17:AM-1:AM    5.606452  -52.783 63.9956 1.00000\n18:AM-1:AM    2.483871  -55.905 60.8731 1.00000\n19:AM-1:AM    9.935484  -48.454 68.3247 1.00000\n20:AM-1:AM    6.316129  -58.965 71.5972 1.00000\n1:PM-1:AM    22.709677  -35.680 81.0989 0.99990\n2:PM-1:AM     6.387097  -52.002 64.7763 1.00000\n3:PM-1:AM    17.387097  -41.002 75.7763 1.00000\n4:PM-1:AM    -6.600000  -64.989 51.7892 1.00000\n5:PM-1:AM     7.593548  -50.796 65.9827 1.00000\n6:PM-1:AM    -0.709677  -59.099 57.6795 1.00000\n7:PM-1:AM     5.393548  -52.996 63.7827 1.00000\n8:PM-1:AM    -1.135484  -59.525 57.2537 1.00000\n9:PM-1:AM     5.819355  -52.570 64.2086 1.00000\n10:PM-1:AM    6.670968  -51.718 65.0602 1.00000\n11:PM-1:AM   -5.535484  -63.925 52.8537 1.00000\n12:PM-1:AM   -8.090323  -66.480 50.2989 1.00000\n13:PM-1:AM   11.141935  -47.247 69.5311 1.00000\n14:PM-1:AM   -4.187097  -62.576 54.2021 1.00000\n15:PM-1:AM    2.909677  -55.480 61.2989 1.00000\n16:PM-1:AM   -9.864516  -68.254 48.5247 1.00000\n17:PM-1:AM  -17.458065  -75.847 40.9311 1.00000\n18:PM-1:AM   -3.903226  -62.292 54.4860 1.00000\n19:PM-1:AM  -15.967742  -74.357 42.4215 1.00000\n20:PM-1:AM    9.722581  -48.667 68.1118 1.00000\n3:AM-2:AM    -6.883871  -65.273 51.5053 1.00000\n4:AM-2:AM    -3.335484  -61.725 55.0537 1.00000\n5:AM-2:AM     4.400000  -53.989 62.7892 1.00000\n6:AM-2:AM     5.748387  -52.641 64.1376 1.00000\n7:AM-2:AM     0.425806  -57.963 58.8150 1.00000\n8:AM-2:AM    11.070968  -47.318 69.4602 1.00000\n9:AM-2:AM    29.948387  -28.441 88.3376 0.98570\n10:AM-2:AM  -13.412903  -71.802 44.9763 1.00000\n11:AM-2:AM   27.890323  -30.499 86.2795 0.99521\n12:AM-2:AM  -14.051613  -72.441 44.3376 1.00000\n13:AM-2:AM    0.567742  -57.821 58.9569 1.00000\n14:AM-2:AM  -14.193548  -72.583 44.1956 1.00000\n15:AM-2:AM  -21.574194  -79.963 36.8150 0.99997\n16:AM-2:AM   -3.122581  -61.512 55.2666 1.00000\n17:AM-2:AM    1.561290  -56.828 59.9505 1.00000\n18:AM-2:AM   -1.561290  -59.950 56.8279 1.00000\n19:AM-2:AM    5.890323  -52.499 64.2795 1.00000\n20:AM-2:AM    2.270968  -63.010 67.5521 1.00000\n1:PM-2:AM    18.664516  -39.725 77.0537 1.00000\n2:PM-2:AM     2.341935  -56.047 60.7311 1.00000\n3:PM-2:AM    13.341935  -45.047 71.7311 1.00000\n4:PM-2:AM   -10.645161  -69.034 47.7440 1.00000\n5:PM-2:AM     3.548387  -54.841 61.9376 1.00000\n6:PM-2:AM    -4.754839  -63.144 53.6344 1.00000\n7:PM-2:AM     1.348387  -57.041 59.7376 1.00000\n8:PM-2:AM    -5.180645  -63.570 53.2086 1.00000\n9:PM-2:AM     1.774194  -56.615 60.1634 1.00000\n10:PM-2:AM    2.625806  -55.763 61.0150 1.00000\n11:PM-2:AM   -9.580645  -67.970 48.8086 1.00000\n12:PM-2:AM  -12.135484  -70.525 46.2537 1.00000\n13:PM-2:AM    7.096774  -51.292 65.4860 1.00000\n14:PM-2:AM   -8.232258  -66.621 50.1569 1.00000\n15:PM-2:AM   -1.135484  -59.525 57.2537 1.00000\n16:PM-2:AM  -13.909677  -72.299 44.4795 1.00000\n17:PM-2:AM  -21.503226  -79.892 36.8860 0.99997\n18:PM-2:AM   -7.948387  -66.338 50.4408 1.00000\n19:PM-2:AM  -20.012903  -78.402 38.3763 0.99999\n20:PM-2:AM    5.677419  -52.712 64.0666 1.00000\n4:AM-3:AM     3.548387  -54.841 61.9376 1.00000\n5:AM-3:AM    11.283871  -47.105 69.6731 1.00000\n6:AM-3:AM    12.632258  -45.757 71.0215 1.00000\n7:AM-3:AM     7.309677  -51.080 65.6989 1.00000\n8:AM-3:AM    17.954839  -40.434 76.3440 1.00000\n9:AM-3:AM    36.832258  -21.557 95.2215 0.84321\n10:AM-3:AM   -6.529032  -64.918 51.8602 1.00000\n11:AM-3:AM   34.774194  -23.615 93.1634 0.90946\n12:AM-3:AM   -7.167742  -65.557 51.2215 1.00000\n13:AM-3:AM    7.451613  -50.938 65.8408 1.00000\n14:AM-3:AM   -7.309677  -65.699 51.0795 1.00000\n15:AM-3:AM  -14.690323  -73.080 43.6989 1.00000\n16:AM-3:AM    3.761290  -54.628 62.1505 1.00000\n17:AM-3:AM    8.445161  -49.944 66.8344 1.00000\n18:AM-3:AM    5.322581  -53.067 63.7118 1.00000\n19:AM-3:AM   12.774194  -45.615 71.1634 1.00000\n20:AM-3:AM    9.154839  -56.126 74.4359 1.00000\n1:PM-3:AM    25.548387  -32.841 83.9376 0.99897\n2:PM-3:AM     9.225806  -49.163 67.6150 1.00000\n3:PM-3:AM    20.225806  -38.163 78.6150 0.99999\n4:PM-3:AM    -3.761290  -62.150 54.6279 1.00000\n5:PM-3:AM    10.432258  -47.957 68.8215 1.00000\n6:PM-3:AM     2.129032  -56.260 60.5182 1.00000\n7:PM-3:AM     8.232258  -50.157 66.6215 1.00000\n8:PM-3:AM     1.703226  -56.686 60.0924 1.00000\n9:PM-3:AM     8.658065  -49.731 67.0473 1.00000\n10:PM-3:AM    9.509677  -48.880 67.8989 1.00000\n11:PM-3:AM   -2.696774  -61.086 55.6924 1.00000\n12:PM-3:AM   -5.251613  -63.641 53.1376 1.00000\n13:PM-3:AM   13.980645  -44.409 72.3698 1.00000\n14:PM-3:AM   -1.348387  -59.738 57.0408 1.00000\n15:PM-3:AM    5.748387  -52.641 64.1376 1.00000\n16:PM-3:AM   -7.025806  -65.415 51.3634 1.00000\n17:PM-3:AM  -14.619355  -73.009 43.7698 1.00000\n18:PM-3:AM   -1.064516  -59.454 57.3247 1.00000\n19:PM-3:AM  -13.129032  -71.518 45.2602 1.00000\n20:PM-3:AM   12.561290  -45.828 70.9505 1.00000\n5:AM-4:AM     7.735484  -50.654 66.1247 1.00000\n6:AM-4:AM     9.083871  -49.305 67.4731 1.00000\n7:AM-4:AM     3.761290  -54.628 62.1505 1.00000\n8:AM-4:AM    14.406452  -43.983 72.7956 1.00000\n9:AM-4:AM    33.283871  -25.105 91.6731 0.94386\n10:AM-4:AM  -10.077419  -68.467 48.3118 1.00000\n11:AM-4:AM   31.225806  -27.163 89.6150 0.97452\n12:AM-4:AM  -10.716129  -69.105 47.6731 1.00000\n13:AM-4:AM    3.903226  -54.486 62.2924 1.00000\n14:AM-4:AM  -10.858065  -69.247 47.5311 1.00000\n15:AM-4:AM  -18.238710  -76.628 40.1505 1.00000\n16:AM-4:AM    0.212903  -58.176 58.6021 1.00000\n17:AM-4:AM    4.896774  -53.492 63.2860 1.00000\n18:AM-4:AM    1.774194  -56.615 60.1634 1.00000\n19:AM-4:AM    9.225806  -49.163 67.6150 1.00000\n20:AM-4:AM    5.606452  -59.675 70.8876 1.00000\n1:PM-4:AM    22.000000  -36.389 80.3892 0.99995\n2:PM-4:AM     5.677419  -52.712 64.0666 1.00000\n3:PM-4:AM    16.677419  -41.712 75.0666 1.00000\n4:PM-4:AM    -7.309677  -65.699 51.0795 1.00000\n5:PM-4:AM     6.883871  -51.505 65.2731 1.00000\n6:PM-4:AM    -1.419355  -59.809 56.9698 1.00000\n7:PM-4:AM     4.683871  -53.705 63.0731 1.00000\n8:PM-4:AM    -1.845161  -60.234 56.5440 1.00000\n9:PM-4:AM     5.109677  -53.280 63.4989 1.00000\n10:PM-4:AM    5.961290  -52.428 64.3505 1.00000\n11:PM-4:AM   -6.245161  -64.634 52.1440 1.00000\n12:PM-4:AM   -8.800000  -67.189 49.5892 1.00000\n13:PM-4:AM   10.432258  -47.957 68.8215 1.00000\n14:PM-4:AM   -4.896774  -63.286 53.4924 1.00000\n15:PM-4:AM    2.200000  -56.189 60.5892 1.00000\n16:PM-4:AM  -10.574194  -68.963 47.8150 1.00000\n17:PM-4:AM  -18.167742  -76.557 40.2215 1.00000\n18:PM-4:AM   -4.612903  -63.002 53.7763 1.00000\n19:PM-4:AM  -16.677419  -75.067 41.7118 1.00000\n20:PM-4:AM    9.012903  -49.376 67.4021 1.00000\n6:AM-5:AM     1.348387  -57.041 59.7376 1.00000\n7:AM-5:AM    -3.974194  -62.363 54.4150 1.00000\n8:AM-5:AM     6.670968  -51.718 65.0602 1.00000\n9:AM-5:AM    25.548387  -32.841 83.9376 0.99897\n10:AM-5:AM  -17.812903  -76.202 40.5763 1.00000\n11:AM-5:AM   23.490323  -34.899 81.8795 0.99980\n12:AM-5:AM  -18.451613  -76.841 39.9376 1.00000\n13:AM-5:AM   -3.832258  -62.221 54.5569 1.00000\n14:AM-5:AM  -18.593548  -76.983 39.7956 1.00000\n15:AM-5:AM  -25.974194  -84.363 32.4150 0.99860\n16:AM-5:AM   -7.522581  -65.912 50.8666 1.00000\n17:AM-5:AM   -2.838710  -61.228 55.5505 1.00000\n18:AM-5:AM   -5.961290  -64.350 52.4279 1.00000\n19:AM-5:AM    1.490323  -56.899 59.8795 1.00000\n20:AM-5:AM   -2.129032  -67.410 63.1521 1.00000\n1:PM-5:AM    14.264516  -44.125 72.6537 1.00000\n2:PM-5:AM    -2.058065  -60.447 56.3311 1.00000\n3:PM-5:AM     8.941935  -49.447 67.3311 1.00000\n4:PM-5:AM   -15.045161  -73.434 43.3440 1.00000\n5:PM-5:AM    -0.851613  -59.241 57.5376 1.00000\n6:PM-5:AM    -9.154839  -67.544 49.2344 1.00000\n7:PM-5:AM    -3.051613  -61.441 55.3376 1.00000\n8:PM-5:AM    -9.580645  -67.970 48.8086 1.00000\n9:PM-5:AM    -2.625806  -61.015 55.7634 1.00000\n10:PM-5:AM   -1.774194  -60.163 56.6150 1.00000\n11:PM-5:AM  -13.980645  -72.370 44.4086 1.00000\n12:PM-5:AM  -16.535484  -74.925 41.8537 1.00000\n13:PM-5:AM    2.696774  -55.692 61.0860 1.00000\n14:PM-5:AM  -12.632258  -71.021 45.7569 1.00000\n15:PM-5:AM   -5.535484  -63.925 52.8537 1.00000\n16:PM-5:AM  -18.309677  -76.699 40.0795 1.00000\n17:PM-5:AM  -25.903226  -84.292 32.4860 0.99867\n18:PM-5:AM  -12.348387  -70.738 46.0408 1.00000\n19:PM-5:AM  -24.412903  -82.802 33.9763 0.99957\n20:PM-5:AM    1.277419  -57.112 59.6666 1.00000\n7:AM-6:AM    -5.322581  -63.712 53.0666 1.00000\n8:AM-6:AM     5.322581  -53.067 63.7118 1.00000\n9:AM-6:AM    24.200000  -34.189 82.5892 0.99964\n10:AM-6:AM  -19.161290  -77.550 39.2279 1.00000\n11:AM-6:AM   22.141935  -36.247 80.5311 0.99994\n12:AM-6:AM  -19.800000  -78.189 38.5892 1.00000\n13:AM-6:AM   -5.180645  -63.570 53.2086 1.00000\n14:AM-6:AM  -19.941935  -78.331 38.4473 1.00000\n15:AM-6:AM  -27.322581  -85.712 31.0666 0.99660\n16:AM-6:AM   -8.870968  -67.260 49.5182 1.00000\n17:AM-6:AM   -4.187097  -62.576 54.2021 1.00000\n18:AM-6:AM   -7.309677  -65.699 51.0795 1.00000\n19:AM-6:AM    0.141935  -58.247 58.5311 1.00000\n20:AM-6:AM   -3.477419  -68.759 61.8037 1.00000\n1:PM-6:AM    12.916129  -45.473 71.3053 1.00000\n2:PM-6:AM    -3.406452  -61.796 54.9827 1.00000\n3:PM-6:AM     7.593548  -50.796 65.9827 1.00000\n4:PM-6:AM   -16.393548  -74.783 41.9956 1.00000\n5:PM-6:AM    -2.200000  -60.589 56.1892 1.00000\n6:PM-6:AM   -10.503226  -68.892 47.8860 1.00000\n7:PM-6:AM    -4.400000  -62.789 53.9892 1.00000\n8:PM-6:AM   -10.929032  -69.318 47.4602 1.00000\n9:PM-6:AM    -3.974194  -62.363 54.4150 1.00000\n10:PM-6:AM   -3.122581  -61.512 55.2666 1.00000\n11:PM-6:AM  -15.329032  -73.718 43.0602 1.00000\n12:PM-6:AM  -17.883871  -76.273 40.5053 1.00000\n13:PM-6:AM    1.348387  -57.041 59.7376 1.00000\n14:PM-6:AM  -13.980645  -72.370 44.4086 1.00000\n15:PM-6:AM   -6.883871  -65.273 51.5053 1.00000\n16:PM-6:AM  -19.658065  -78.047 38.7311 1.00000\n17:PM-6:AM  -27.251613  -85.641 31.1376 0.99674\n18:PM-6:AM  -13.696774  -72.086 44.6924 1.00000\n19:PM-6:AM  -25.761290  -84.150 32.6279 0.99880\n20:PM-6:AM   -0.070968  -58.460 58.3182 1.00000\n8:AM-7:AM    10.645161  -47.744 69.0344 1.00000\n9:AM-7:AM    29.522581  -28.867 87.9118 0.98840\n10:AM-7:AM  -13.838710  -72.228 44.5505 1.00000\n11:AM-7:AM   27.464516  -30.925 85.8537 0.99629\n12:AM-7:AM  -14.477419  -72.867 43.9118 1.00000\n13:AM-7:AM    0.141935  -58.247 58.5311 1.00000\n14:AM-7:AM  -14.619355  -73.009 43.7698 1.00000\n15:AM-7:AM  -22.000000  -80.389 36.3892 0.99995\n16:AM-7:AM   -3.548387  -61.938 54.8408 1.00000\n17:AM-7:AM    1.135484  -57.254 59.5247 1.00000\n18:AM-7:AM   -1.987097  -60.376 56.4021 1.00000\n19:AM-7:AM    5.464516  -52.925 63.8537 1.00000\n20:AM-7:AM    1.845161  -63.436 67.1263 1.00000\n1:PM-7:AM    18.238710  -40.150 76.6279 1.00000\n2:PM-7:AM     1.916129  -56.473 60.3053 1.00000\n3:PM-7:AM    12.916129  -45.473 71.3053 1.00000\n4:PM-7:AM   -11.070968  -69.460 47.3182 1.00000\n5:PM-7:AM     3.122581  -55.267 61.5118 1.00000\n6:PM-7:AM    -5.180645  -63.570 53.2086 1.00000\n7:PM-7:AM     0.922581  -57.467 59.3118 1.00000\n8:PM-7:AM    -5.606452  -63.996 52.7827 1.00000\n9:PM-7:AM     1.348387  -57.041 59.7376 1.00000\n10:PM-7:AM    2.200000  -56.189 60.5892 1.00000\n11:PM-7:AM  -10.006452  -68.396 48.3827 1.00000\n12:PM-7:AM  -12.561290  -70.950 45.8279 1.00000\n13:PM-7:AM    6.670968  -51.718 65.0602 1.00000\n14:PM-7:AM   -8.658065  -67.047 49.7311 1.00000\n15:PM-7:AM   -1.561290  -59.950 56.8279 1.00000\n16:PM-7:AM  -14.335484  -72.725 44.0537 1.00000\n17:PM-7:AM  -21.929032  -80.318 36.4602 0.99996\n18:PM-7:AM   -8.374194  -66.763 50.0150 1.00000\n19:PM-7:AM  -20.438710  -78.828 37.9505 0.99999\n20:PM-7:AM    5.251613  -53.138 63.6408 1.00000\n9:AM-8:AM    18.877419  -39.512 77.2666 1.00000\n10:AM-8:AM  -24.483871  -82.873 33.9053 0.99954\n11:AM-8:AM   16.819355  -41.570 75.2086 1.00000\n12:AM-8:AM  -25.122581  -83.512 33.2666 0.99925\n13:AM-8:AM  -10.503226  -68.892 47.8860 1.00000\n14:AM-8:AM  -25.264516  -83.654 33.1247 0.99916\n15:AM-8:AM  -32.645161  -91.034 25.7440 0.95531\n16:AM-8:AM  -14.193548  -72.583 44.1956 1.00000\n17:AM-8:AM   -9.509677  -67.899 48.8795 1.00000\n18:AM-8:AM  -12.632258  -71.021 45.7569 1.00000\n19:AM-8:AM   -5.180645  -63.570 53.2086 1.00000\n20:AM-8:AM   -8.800000  -74.081 56.4811 1.00000\n1:PM-8:AM     7.593548  -50.796 65.9827 1.00000\n2:PM-8:AM    -8.729032  -67.118 49.6602 1.00000\n3:PM-8:AM     2.270968  -56.118 60.6602 1.00000\n4:PM-8:AM   -21.716129  -80.105 36.6731 0.99996\n5:PM-8:AM    -7.522581  -65.912 50.8666 1.00000\n6:PM-8:AM   -15.825806  -74.215 42.5634 1.00000\n7:PM-8:AM    -9.722581  -68.112 48.6666 1.00000\n8:PM-8:AM   -16.251613  -74.641 42.1376 1.00000\n9:PM-8:AM    -9.296774  -67.686 49.0924 1.00000\n10:PM-8:AM   -8.445161  -66.834 49.9440 1.00000\n11:PM-8:AM  -20.651613  -79.041 37.7376 0.99999\n12:PM-8:AM  -23.206452  -81.596 35.1827 0.99985\n13:PM-8:AM   -3.974194  -62.363 54.4150 1.00000\n14:PM-8:AM  -19.303226  -77.692 39.0860 1.00000\n15:PM-8:AM  -12.206452  -70.596 46.1827 1.00000\n16:PM-8:AM  -24.980645  -83.370 33.4086 0.99933\n17:PM-8:AM  -32.574194  -90.963 25.8150 0.95647\n18:PM-8:AM  -19.019355  -77.409 39.3698 1.00000\n19:PM-8:AM  -31.083871  -89.473 27.3053 0.97602\n20:PM-8:AM   -5.393548  -63.783 52.9956 1.00000\n10:AM-9:AM  -43.361290 -101.750 15.0279 0.52759\n11:AM-9:AM   -2.058065  -60.447 56.3311 1.00000\n12:AM-9:AM  -44.000000 -102.389 14.3892 0.49403\n13:AM-9:AM  -29.380645  -87.770 29.0086 0.98920\n14:AM-9:AM  -44.141935 -102.531 14.2473 0.48663\n15:AM-9:AM  -51.522581 -109.912  6.8666 0.17781\n16:AM-9:AM  -33.070968  -91.460 25.3182 0.94789\n17:AM-9:AM  -28.387097  -86.776 30.0021 0.99364\n18:AM-9:AM  -31.509677  -89.899 26.8795 0.97131\n19:AM-9:AM  -24.058065  -82.447 34.3311 0.99968\n20:AM-9:AM  -27.677419  -92.959 37.6037 0.99943\n1:PM-9:AM   -11.283871  -69.673 47.1053 1.00000\n2:PM-9:AM   -27.606452  -85.996 30.7827 0.99595\n3:PM-9:AM   -16.606452  -74.996 41.7827 1.00000\n4:PM-9:AM   -40.593548  -98.983 17.7956 0.67322\n5:PM-9:AM   -26.400000  -84.789 31.9892 0.99812\n6:PM-9:AM   -34.703226  -93.092 23.6860 0.91136\n7:PM-9:AM   -28.600000  -86.989 29.7892 0.99284\n8:PM-9:AM   -35.129032  -93.518 23.2602 0.89961\n9:PM-9:AM   -28.174194  -86.563 30.2150 0.99436\n10:PM-9:AM  -27.322581  -85.712 31.0666 0.99660\n11:PM-9:AM  -39.529032  -97.918 18.8602 0.72631\n12:PM-9:AM  -42.083871 -100.473 16.3053 0.59536\n13:PM-9:AM  -22.851613  -81.241 35.5376 0.99989\n14:PM-9:AM  -38.180645  -96.570 20.2086 0.78850\n15:PM-9:AM  -31.083871  -89.473 27.3053 0.97602\n16:PM-9:AM  -43.858065 -102.247 14.5311 0.50145\n17:PM-9:AM  -51.451613 -109.841  6.9376 0.17988\n18:PM-9:AM  -37.896774  -96.286 20.4924 0.80070\n19:PM-9:AM  -49.961290 -108.350  8.4279 0.22767\n20:PM-9:AM  -24.270968  -82.660 34.1182 0.99962\n11:AM-10:AM  41.303226  -17.086 99.6924 0.63649\n12:AM-10:AM  -0.638710  -59.028 57.7505 1.00000\n13:AM-10:AM  13.980645  -44.409 72.3698 1.00000\n14:AM-10:AM  -0.780645  -59.170 57.6086 1.00000\n15:AM-10:AM  -8.161290  -66.550 50.2279 1.00000\n16:AM-10:AM  10.290323  -48.099 68.6795 1.00000\n17:AM-10:AM  14.974194  -43.415 73.3634 1.00000\n18:AM-10:AM  11.851613  -46.538 70.2408 1.00000\n19:AM-10:AM  19.303226  -39.086 77.6924 1.00000\n20:AM-10:AM  15.683871  -49.597 80.9650 1.00000\n1:PM-10:AM   32.077419  -26.312 90.4666 0.96397\n2:PM-10:AM   15.754839  -42.634 74.1440 1.00000\n3:PM-10:AM   26.754839  -31.634 85.1440 0.99763\n4:PM-10:AM    2.767742  -55.621 61.1569 1.00000\n5:PM-10:AM   16.961290  -41.428 75.3505 1.00000\n6:PM-10:AM    8.658065  -49.731 67.0473 1.00000\n7:PM-10:AM   14.761290  -43.628 73.1505 1.00000\n8:PM-10:AM    8.232258  -50.157 66.6215 1.00000\n9:PM-10:AM   15.187097  -43.202 73.5763 1.00000\n10:PM-10:AM  16.038710  -42.350 74.4279 1.00000\n11:PM-10:AM   3.832258  -54.557 62.2215 1.00000\n12:PM-10:AM   1.277419  -57.112 59.6666 1.00000\n13:PM-10:AM  20.509677  -37.880 78.8989 0.99999\n14:PM-10:AM   5.180645  -53.209 63.5698 1.00000\n15:PM-10:AM  12.277419  -46.112 70.6666 1.00000\n16:PM-10:AM  -0.496774  -58.886 57.8924 1.00000\n17:PM-10:AM  -8.090323  -66.480 50.2989 1.00000\n18:PM-10:AM   5.464516  -52.925 63.8537 1.00000\n19:PM-10:AM  -6.600000  -64.989 51.7892 1.00000\n20:PM-10:AM  19.090323  -39.299 77.4795 1.00000\n12:AM-11:AM -41.941935 -100.331 16.4473 0.60287\n13:AM-11:AM -27.322581  -85.712 31.0666 0.99660\n14:AM-11:AM -42.083871 -100.473 16.3053 0.59536\n15:AM-11:AM -49.464516 -107.854  8.9247 0.24541\n16:AM-11:AM -31.012903  -89.402 27.3763 0.97675\n17:AM-11:AM -26.329032  -84.718 32.0602 0.99821\n18:AM-11:AM -29.451613  -87.841 28.9376 0.98880\n19:AM-11:AM -22.000000  -80.389 36.3892 0.99995\n20:AM-11:AM -25.619355  -90.900 39.6618 0.99988\n1:PM-11:AM   -9.225806  -67.615 49.1634 1.00000\n2:PM-11:AM  -25.548387  -83.938 32.8408 0.99897\n3:PM-11:AM  -14.548387  -72.938 43.8408 1.00000\n4:PM-11:AM  -38.535484  -96.925 19.8537 0.77279\n5:PM-11:AM  -24.341935  -82.731 34.0473 0.99959\n6:PM-11:AM  -32.645161  -91.034 25.7440 0.95531\n7:PM-11:AM  -26.541935  -84.931 31.8473 0.99794\n8:PM-11:AM  -33.070968  -91.460 25.3182 0.94789\n9:PM-11:AM  -26.116129  -84.505 32.2731 0.99846\n10:PM-11:AM -25.264516  -83.654 33.1247 0.99916\n11:PM-11:AM -37.470968  -95.860 20.9182 0.81833\n12:PM-11:AM -40.025806  -98.415 18.3634 0.70190\n13:PM-11:AM -20.793548  -79.183 37.5956 0.99999\n14:PM-11:AM -36.122581  -94.512 22.2666 0.86852\n15:PM-11:AM -29.025806  -87.415 29.3634 0.99101\n16:PM-11:AM -41.800000 -100.189 16.5892 0.61038\n17:PM-11:AM -49.393548 -107.783  8.9956 0.24802\n18:PM-11:AM -35.838710  -94.228 22.5505 0.87792\n19:PM-11:AM -47.903226 -106.292 10.4860 0.30702\n20:PM-11:AM -22.212903  -80.602 36.1763 0.99994\n13:AM-12:AM  14.619355  -43.770 73.0086 1.00000\n14:AM-12:AM  -0.141935  -58.531 58.2473 1.00000\n15:AM-12:AM  -7.522581  -65.912 50.8666 1.00000\n16:AM-12:AM  10.929032  -47.460 69.3182 1.00000\n17:AM-12:AM  15.612903  -42.776 74.0021 1.00000\n18:AM-12:AM  12.490323  -45.899 70.8795 1.00000\n19:AM-12:AM  19.941935  -38.447 78.3311 1.00000\n20:AM-12:AM  16.322581  -48.959 81.6037 1.00000\n1:PM-12:AM   32.716129  -25.673 91.1053 0.95413\n2:PM-12:AM   16.393548  -41.996 74.7827 1.00000\n3:PM-12:AM   27.393548  -30.996 85.7827 0.99645\n4:PM-12:AM    3.406452  -54.983 61.7956 1.00000\n5:PM-12:AM   17.600000  -40.789 75.9892 1.00000\n6:PM-12:AM    9.296774  -49.092 67.6860 1.00000\n7:PM-12:AM   15.400000  -42.989 73.7892 1.00000\n8:PM-12:AM    8.870968  -49.518 67.2602 1.00000\n9:PM-12:AM   15.825806  -42.563 74.2150 1.00000\n10:PM-12:AM  16.677419  -41.712 75.0666 1.00000\n11:PM-12:AM   4.470968  -53.918 62.8602 1.00000\n12:PM-12:AM   1.916129  -56.473 60.3053 1.00000\n13:PM-12:AM  21.148387  -37.241 79.5376 0.99998\n14:PM-12:AM   5.819355  -52.570 64.2086 1.00000\n15:PM-12:AM  12.916129  -45.473 71.3053 1.00000\n16:PM-12:AM   0.141935  -58.247 58.5311 1.00000\n17:PM-12:AM  -7.451613  -65.841 50.9376 1.00000\n18:PM-12:AM   6.103226  -52.286 64.4924 1.00000\n19:PM-12:AM  -5.961290  -64.350 52.4279 1.00000\n20:PM-12:AM  19.729032  -38.660 78.1182 1.00000\n14:AM-13:AM -14.761290  -73.150 43.6279 1.00000\n15:AM-13:AM -22.141935  -80.531 36.2473 0.99994\n16:AM-13:AM  -3.690323  -62.080 54.6989 1.00000\n17:AM-13:AM   0.993548  -57.396 59.3827 1.00000\n18:AM-13:AM  -2.129032  -60.518 56.2602 1.00000\n19:AM-13:AM   5.322581  -53.067 63.7118 1.00000\n20:AM-13:AM   1.703226  -63.578 66.9843 1.00000\n1:PM-13:AM   18.096774  -40.292 76.4860 1.00000\n2:PM-13:AM    1.774194  -56.615 60.1634 1.00000\n3:PM-13:AM   12.774194  -45.615 71.1634 1.00000\n4:PM-13:AM  -11.212903  -69.602 47.1763 1.00000\n5:PM-13:AM    2.980645  -55.409 61.3698 1.00000\n6:PM-13:AM   -5.322581  -63.712 53.0666 1.00000\n7:PM-13:AM    0.780645  -57.609 59.1698 1.00000\n8:PM-13:AM   -5.748387  -64.138 52.6408 1.00000\n9:PM-13:AM    1.206452  -57.183 59.5956 1.00000\n10:PM-13:AM   2.058065  -56.331 60.4473 1.00000\n11:PM-13:AM -10.148387  -68.538 48.2408 1.00000\n12:PM-13:AM -12.703226  -71.092 45.6860 1.00000\n13:PM-13:AM   6.529032  -51.860 64.9182 1.00000\n14:PM-13:AM  -8.800000  -67.189 49.5892 1.00000\n15:PM-13:AM  -1.703226  -60.092 56.6860 1.00000\n16:PM-13:AM -14.477419  -72.867 43.9118 1.00000\n17:PM-13:AM -22.070968  -80.460 36.3182 0.99995\n18:PM-13:AM  -8.516129  -66.905 49.8731 1.00000\n19:PM-13:AM -20.580645  -78.970 37.8086 0.99999\n20:PM-13:AM   5.109677  -53.280 63.4989 1.00000\n15:AM-14:AM  -7.380645  -65.770 51.0086 1.00000\n16:AM-14:AM  11.070968  -47.318 69.4602 1.00000\n17:AM-14:AM  15.754839  -42.634 74.1440 1.00000\n18:AM-14:AM  12.632258  -45.757 71.0215 1.00000\n19:AM-14:AM  20.083871  -38.305 78.4731 0.99999\n20:AM-14:AM  16.464516  -48.817 81.7456 1.00000\n1:PM-14:AM   32.858065  -25.531 91.2473 0.95170\n2:PM-14:AM   16.535484  -41.854 74.9247 1.00000\n3:PM-14:AM   27.535484  -30.854 85.9247 0.99612\n4:PM-14:AM    3.548387  -54.841 61.9376 1.00000\n5:PM-14:AM   17.741935  -40.647 76.1311 1.00000\n6:PM-14:AM    9.438710  -48.950 67.8279 1.00000\n7:PM-14:AM   15.541935  -42.847 73.9311 1.00000\n8:PM-14:AM    9.012903  -49.376 67.4021 1.00000\n9:PM-14:AM   15.967742  -42.421 74.3569 1.00000\n10:PM-14:AM  16.819355  -41.570 75.2086 1.00000\n11:PM-14:AM   4.612903  -53.776 63.0021 1.00000\n12:PM-14:AM   2.058065  -56.331 60.4473 1.00000\n13:PM-14:AM  21.290323  -37.099 79.6795 0.99998\n14:PM-14:AM   5.961290  -52.428 64.3505 1.00000\n15:PM-14:AM  13.058065  -45.331 71.4473 1.00000\n16:PM-14:AM   0.283871  -58.105 58.6731 1.00000\n17:PM-14:AM  -7.309677  -65.699 51.0795 1.00000\n18:PM-14:AM   6.245161  -52.144 64.6344 1.00000\n19:PM-14:AM  -5.819355  -64.209 52.5698 1.00000\n20:PM-14:AM  19.870968  -38.518 78.2602 1.00000\n16:AM-15:AM  18.451613  -39.938 76.8408 1.00000\n17:AM-15:AM  23.135484  -35.254 81.5247 0.99986\n18:AM-15:AM  20.012903  -38.376 78.4021 0.99999\n19:AM-15:AM  27.464516  -30.925 85.8537 0.99629\n20:AM-15:AM  23.845161  -41.436 89.1263 0.99998\n1:PM-15:AM   40.238710  -18.150 98.6279 0.69123\n2:PM-15:AM   23.916129  -34.473 82.3053 0.99972\n3:PM-15:AM   34.916129  -23.473 93.3053 0.90560\n4:PM-15:AM   10.929032  -47.460 69.3182 1.00000\n5:PM-15:AM   25.122581  -33.267 83.5118 0.99925\n6:PM-15:AM   16.819355  -41.570 75.2086 1.00000\n7:PM-15:AM   22.922581  -35.467 81.3118 0.99988\n8:PM-15:AM   16.393548  -41.996 74.7827 1.00000\n9:PM-15:AM   23.348387  -35.041 81.7376 0.99983\n10:PM-15:AM  24.200000  -34.189 82.5892 0.99964\n11:PM-15:AM  11.993548  -46.396 70.3827 1.00000\n12:PM-15:AM   9.438710  -48.950 67.8279 1.00000\n13:PM-15:AM  28.670968  -29.718 87.0602 0.99256\n14:PM-15:AM  13.341935  -45.047 71.7311 1.00000\n15:PM-15:AM  20.438710  -37.950 78.8279 0.99999\n16:PM-15:AM   7.664516  -50.725 66.0537 1.00000\n17:PM-15:AM   0.070968  -58.318 58.4602 1.00000\n18:PM-15:AM  13.625806  -44.763 72.0150 1.00000\n19:PM-15:AM   1.561290  -56.828 59.9505 1.00000\n20:PM-15:AM  27.251613  -31.138 85.6408 0.99674\n17:AM-16:AM   4.683871  -53.705 63.0731 1.00000\n18:AM-16:AM   1.561290  -56.828 59.9505 1.00000\n19:AM-16:AM   9.012903  -49.376 67.4021 1.00000\n20:AM-16:AM   5.393548  -59.888 70.6747 1.00000\n1:PM-16:AM   21.787097  -36.602 80.1763 0.99996\n2:PM-16:AM    5.464516  -52.925 63.8537 1.00000\n3:PM-16:AM   16.464516  -41.925 74.8537 1.00000\n4:PM-16:AM   -7.522581  -65.912 50.8666 1.00000\n5:PM-16:AM    6.670968  -51.718 65.0602 1.00000\n6:PM-16:AM   -1.632258  -60.021 56.7569 1.00000\n7:PM-16:AM    4.470968  -53.918 62.8602 1.00000\n8:PM-16:AM   -2.058065  -60.447 56.3311 1.00000\n9:PM-16:AM    4.896774  -53.492 63.2860 1.00000\n10:PM-16:AM   5.748387  -52.641 64.1376 1.00000\n11:PM-16:AM  -6.458065  -64.847 51.9311 1.00000\n12:PM-16:AM  -9.012903  -67.402 49.3763 1.00000\n13:PM-16:AM  10.219355  -48.170 68.6086 1.00000\n14:PM-16:AM  -5.109677  -63.499 53.2795 1.00000\n15:PM-16:AM   1.987097  -56.402 60.3763 1.00000\n16:PM-16:AM -10.787097  -69.176 47.6021 1.00000\n17:PM-16:AM -18.380645  -76.770 40.0086 1.00000\n18:PM-16:AM  -4.825806  -63.215 53.5634 1.00000\n19:PM-16:AM -16.890323  -75.280 41.4989 1.00000\n20:PM-16:AM   8.800000  -49.589 67.1892 1.00000\n18:AM-17:AM  -3.122581  -61.512 55.2666 1.00000\n19:AM-17:AM   4.329032  -54.060 62.7182 1.00000\n20:AM-17:AM   0.709677  -64.571 65.9908 1.00000\n1:PM-17:AM   17.103226  -41.286 75.4924 1.00000\n2:PM-17:AM    0.780645  -57.609 59.1698 1.00000\n3:PM-17:AM   11.780645  -46.609 70.1698 1.00000\n4:PM-17:AM  -12.206452  -70.596 46.1827 1.00000\n5:PM-17:AM    1.987097  -56.402 60.3763 1.00000\n6:PM-17:AM   -6.316129  -64.705 52.0731 1.00000\n7:PM-17:AM   -0.212903  -58.602 58.1763 1.00000\n8:PM-17:AM   -6.741935  -65.131 51.6473 1.00000\n9:PM-17:AM    0.212903  -58.176 58.6021 1.00000\n10:PM-17:AM   1.064516  -57.325 59.4537 1.00000\n11:PM-17:AM -11.141935  -69.531 47.2473 1.00000\n12:PM-17:AM -13.696774  -72.086 44.6924 1.00000\n13:PM-17:AM   5.535484  -52.854 63.9247 1.00000\n14:PM-17:AM  -9.793548  -68.183 48.5956 1.00000\n15:PM-17:AM  -2.696774  -61.086 55.6924 1.00000\n16:PM-17:AM -15.470968  -73.860 42.9182 1.00000\n17:PM-17:AM -23.064516  -81.454 35.3247 0.99987\n18:PM-17:AM  -9.509677  -67.899 48.8795 1.00000\n19:PM-17:AM -21.574194  -79.963 36.8150 0.99997\n20:PM-17:AM   4.116129  -54.273 62.5053 1.00000\n19:AM-18:AM   7.451613  -50.938 65.8408 1.00000\n20:AM-18:AM   3.832258  -61.449 69.1134 1.00000\n1:PM-18:AM   20.225806  -38.163 78.6150 0.99999\n2:PM-18:AM    3.903226  -54.486 62.2924 1.00000\n3:PM-18:AM   14.903226  -43.486 73.2924 1.00000\n4:PM-18:AM   -9.083871  -67.473 49.3053 1.00000\n5:PM-18:AM    5.109677  -53.280 63.4989 1.00000\n6:PM-18:AM   -3.193548  -61.583 55.1956 1.00000\n7:PM-18:AM    2.909677  -55.480 61.2989 1.00000\n8:PM-18:AM   -3.619355  -62.009 54.7698 1.00000\n9:PM-18:AM    3.335484  -55.054 61.7247 1.00000\n10:PM-18:AM   4.187097  -54.202 62.5763 1.00000\n11:PM-18:AM  -8.019355  -66.409 50.3698 1.00000\n12:PM-18:AM -10.574194  -68.963 47.8150 1.00000\n13:PM-18:AM   8.658065  -49.731 67.0473 1.00000\n14:PM-18:AM  -6.670968  -65.060 51.7182 1.00000\n15:PM-18:AM   0.425806  -57.963 58.8150 1.00000\n16:PM-18:AM -12.348387  -70.738 46.0408 1.00000\n17:PM-18:AM -19.941935  -78.331 38.4473 1.00000\n18:PM-18:AM  -6.387097  -64.776 52.0021 1.00000\n19:PM-18:AM -18.451613  -76.841 39.9376 1.00000\n20:PM-18:AM   7.238710  -51.150 65.6279 1.00000\n20:AM-19:AM  -3.619355  -68.900 61.6618 1.00000\n1:PM-19:AM   12.774194  -45.615 71.1634 1.00000\n2:PM-19:AM   -3.548387  -61.938 54.8408 1.00000\n3:PM-19:AM    7.451613  -50.938 65.8408 1.00000\n4:PM-19:AM  -16.535484  -74.925 41.8537 1.00000\n5:PM-19:AM   -2.341935  -60.731 56.0473 1.00000\n6:PM-19:AM  -10.645161  -69.034 47.7440 1.00000\n7:PM-19:AM   -4.541935  -62.931 53.8473 1.00000\n8:PM-19:AM  -11.070968  -69.460 47.3182 1.00000\n9:PM-19:AM   -4.116129  -62.505 54.2731 1.00000\n10:PM-19:AM  -3.264516  -61.654 55.1247 1.00000\n11:PM-19:AM -15.470968  -73.860 42.9182 1.00000\n12:PM-19:AM -18.025806  -76.415 40.3634 1.00000\n13:PM-19:AM   1.206452  -57.183 59.5956 1.00000\n14:PM-19:AM -14.122581  -72.512 44.2666 1.00000\n15:PM-19:AM  -7.025806  -65.415 51.3634 1.00000\n16:PM-19:AM -19.800000  -78.189 38.5892 1.00000\n17:PM-19:AM -27.393548  -85.783 30.9956 0.99645\n18:PM-19:AM -13.838710  -72.228 44.5505 1.00000\n19:PM-19:AM -25.903226  -84.292 32.4860 0.99867\n20:PM-19:AM  -0.212903  -58.602 58.1763 1.00000\n1:PM-20:AM   16.393548  -48.888 81.6747 1.00000\n2:PM-20:AM    0.070968  -65.210 65.3521 1.00000\n3:PM-20:AM   11.070968  -54.210 76.3521 1.00000\n4:PM-20:AM  -12.916129  -78.197 52.3650 1.00000\n5:PM-20:AM    1.277419  -64.004 66.5585 1.00000\n6:PM-20:AM   -7.025806  -72.307 58.2553 1.00000\n7:PM-20:AM   -0.922581  -66.204 64.3585 1.00000\n8:PM-20:AM   -7.451613  -72.733 57.8295 1.00000\n9:PM-20:AM   -0.496774  -65.778 64.7843 1.00000\n10:PM-20:AM   0.354839  -64.926 65.6359 1.00000\n11:PM-20:AM -11.851613  -77.133 53.4295 1.00000\n12:PM-20:AM -14.406452  -79.688 50.8747 1.00000\n13:PM-20:AM   4.825806  -60.455 70.1069 1.00000\n14:PM-20:AM -10.503226  -75.784 54.7779 1.00000\n15:PM-20:AM  -3.406452  -68.688 61.8747 1.00000\n16:PM-20:AM -16.180645  -81.462 49.1005 1.00000\n17:PM-20:AM -23.774194  -89.055 41.5069 0.99998\n18:PM-20:AM -10.219355  -75.500 55.0618 1.00000\n19:PM-20:AM -22.283871  -87.565 42.9972 1.00000\n20:PM-20:AM   3.406452  -61.875 68.6876 1.00000\n2:PM-1:PM   -16.322581  -74.712 42.0666 1.00000\n3:PM-1:PM    -5.322581  -63.712 53.0666 1.00000\n4:PM-1:PM   -29.309677  -87.699 29.0795 0.98958\n5:PM-1:PM   -15.116129  -73.505 43.2731 1.00000\n6:PM-1:PM   -23.419355  -81.809 34.9698 0.99982\n7:PM-1:PM   -17.316129  -75.705 41.0731 1.00000\n8:PM-1:PM   -23.845161  -82.234 34.5440 0.99973\n9:PM-1:PM   -16.890323  -75.280 41.4989 1.00000\n10:PM-1:PM  -16.038710  -74.428 42.3505 1.00000\n11:PM-1:PM  -28.245161  -86.634 30.1440 0.99413\n12:PM-1:PM  -30.800000  -89.189 27.5892 0.97882\n13:PM-1:PM  -11.567742  -69.957 46.8215 1.00000\n14:PM-1:PM  -26.896774  -85.286 31.4924 0.99740\n15:PM-1:PM  -19.800000  -78.189 38.5892 1.00000\n16:PM-1:PM  -32.574194  -90.963 25.8150 0.95647\n17:PM-1:PM  -40.167742  -98.557 18.2215 0.69480\n18:PM-1:PM  -26.612903  -85.002 31.7763 0.99784\n19:PM-1:PM  -38.677419  -97.067 19.7118 0.76637\n20:PM-1:PM  -12.987097  -71.376 45.4021 1.00000\n3:PM-2:PM    11.000000  -47.389 69.3892 1.00000\n4:PM-2:PM   -12.987097  -71.376 45.4021 1.00000\n5:PM-2:PM     1.206452  -57.183 59.5956 1.00000\n6:PM-2:PM    -7.096774  -65.486 51.2924 1.00000\n7:PM-2:PM    -0.993548  -59.383 57.3956 1.00000\n8:PM-2:PM    -7.522581  -65.912 50.8666 1.00000\n9:PM-2:PM    -0.567742  -58.957 57.8215 1.00000\n10:PM-2:PM    0.283871  -58.105 58.6731 1.00000\n11:PM-2:PM  -11.922581  -70.312 46.4666 1.00000\n12:PM-2:PM  -14.477419  -72.867 43.9118 1.00000\n13:PM-2:PM    4.754839  -53.634 63.1440 1.00000\n14:PM-2:PM  -10.574194  -68.963 47.8150 1.00000\n15:PM-2:PM   -3.477419  -61.867 54.9118 1.00000\n16:PM-2:PM  -16.251613  -74.641 42.1376 1.00000\n17:PM-2:PM  -23.845161  -82.234 34.5440 0.99973\n18:PM-2:PM  -10.290323  -68.680 48.0989 1.00000\n19:PM-2:PM  -22.354839  -80.744 36.0344 0.99993\n20:PM-2:PM    3.335484  -55.054 61.7247 1.00000\n4:PM-3:PM   -23.987097  -82.376 34.4021 0.99970\n5:PM-3:PM    -9.793548  -68.183 48.5956 1.00000\n6:PM-3:PM   -18.096774  -76.486 40.2924 1.00000\n7:PM-3:PM   -11.993548  -70.383 46.3956 1.00000\n8:PM-3:PM   -18.522581  -76.912 39.8666 1.00000\n9:PM-3:PM   -11.567742  -69.957 46.8215 1.00000\n10:PM-3:PM  -10.716129  -69.105 47.6731 1.00000\n11:PM-3:PM  -22.922581  -81.312 35.4666 0.99988\n12:PM-3:PM  -25.477419  -83.867 32.9118 0.99902\n13:PM-3:PM   -6.245161  -64.634 52.1440 1.00000\n14:PM-3:PM  -21.574194  -79.963 36.8150 0.99997\n15:PM-3:PM  -14.477419  -72.867 43.9118 1.00000\n16:PM-3:PM  -27.251613  -85.641 31.1376 0.99674\n17:PM-3:PM  -34.845161  -93.234 23.5440 0.90755\n18:PM-3:PM  -21.290323  -79.680 37.0989 0.99998\n19:PM-3:PM  -33.354839  -91.744 25.0344 0.94247\n20:PM-3:PM   -7.664516  -66.054 50.7247 1.00000\n5:PM-4:PM    14.193548  -44.196 72.5827 1.00000\n6:PM-4:PM     5.890323  -52.499 64.2795 1.00000\n7:PM-4:PM    11.993548  -46.396 70.3827 1.00000\n8:PM-4:PM     5.464516  -52.925 63.8537 1.00000\n9:PM-4:PM    12.419355  -45.970 70.8086 1.00000\n10:PM-4:PM   13.270968  -45.118 71.6602 1.00000\n11:PM-4:PM    1.064516  -57.325 59.4537 1.00000\n12:PM-4:PM   -1.490323  -59.880 56.8989 1.00000\n13:PM-4:PM   17.741935  -40.647 76.1311 1.00000\n14:PM-4:PM    2.412903  -55.976 60.8021 1.00000\n15:PM-4:PM    9.509677  -48.880 67.8989 1.00000\n16:PM-4:PM   -3.264516  -61.654 55.1247 1.00000\n17:PM-4:PM  -10.858065  -69.247 47.5311 1.00000\n18:PM-4:PM    2.696774  -55.692 61.0860 1.00000\n19:PM-4:PM   -9.367742  -67.757 49.0215 1.00000\n20:PM-4:PM   16.322581  -42.067 74.7118 1.00000\n6:PM-5:PM    -8.303226  -66.692 50.0860 1.00000\n7:PM-5:PM    -2.200000  -60.589 56.1892 1.00000\n8:PM-5:PM    -8.729032  -67.118 49.6602 1.00000\n9:PM-5:PM    -1.774194  -60.163 56.6150 1.00000\n10:PM-5:PM   -0.922581  -59.312 57.4666 1.00000\n11:PM-5:PM  -13.129032  -71.518 45.2602 1.00000\n12:PM-5:PM  -15.683871  -74.073 42.7053 1.00000\n13:PM-5:PM    3.548387  -54.841 61.9376 1.00000\n14:PM-5:PM  -11.780645  -70.170 46.6086 1.00000\n15:PM-5:PM   -4.683871  -63.073 53.7053 1.00000\n16:PM-5:PM  -17.458065  -75.847 40.9311 1.00000\n17:PM-5:PM  -25.051613  -83.441 33.3376 0.99929\n18:PM-5:PM  -11.496774  -69.886 46.8924 1.00000\n19:PM-5:PM  -23.561290  -81.950 34.8279 0.99979\n20:PM-5:PM    2.129032  -56.260 60.5182 1.00000\n7:PM-6:PM     6.103226  -52.286 64.4924 1.00000\n8:PM-6:PM    -0.425806  -58.815 57.9634 1.00000\n9:PM-6:PM     6.529032  -51.860 64.9182 1.00000\n10:PM-6:PM    7.380645  -51.009 65.7698 1.00000\n11:PM-6:PM   -4.825806  -63.215 53.5634 1.00000\n12:PM-6:PM   -7.380645  -65.770 51.0086 1.00000\n13:PM-6:PM   11.851613  -46.538 70.2408 1.00000\n14:PM-6:PM   -3.477419  -61.867 54.9118 1.00000\n15:PM-6:PM    3.619355  -54.770 62.0086 1.00000\n16:PM-6:PM   -9.154839  -67.544 49.2344 1.00000\n17:PM-6:PM  -16.748387  -75.138 41.6408 1.00000\n18:PM-6:PM   -3.193548  -61.583 55.1956 1.00000\n19:PM-6:PM  -15.258065  -73.647 43.1311 1.00000\n20:PM-6:PM   10.432258  -47.957 68.8215 1.00000\n8:PM-7:PM    -6.529032  -64.918 51.8602 1.00000\n9:PM-7:PM     0.425806  -57.963 58.8150 1.00000\n10:PM-7:PM    1.277419  -57.112 59.6666 1.00000\n11:PM-7:PM  -10.929032  -69.318 47.4602 1.00000\n12:PM-7:PM  -13.483871  -71.873 44.9053 1.00000\n13:PM-7:PM    5.748387  -52.641 64.1376 1.00000\n14:PM-7:PM   -9.580645  -67.970 48.8086 1.00000\n15:PM-7:PM   -2.483871  -60.873 55.9053 1.00000\n16:PM-7:PM  -15.258065  -73.647 43.1311 1.00000\n17:PM-7:PM  -22.851613  -81.241 35.5376 0.99989\n18:PM-7:PM   -9.296774  -67.686 49.0924 1.00000\n19:PM-7:PM  -21.361290  -79.750 37.0279 0.99998\n20:PM-7:PM    4.329032  -54.060 62.7182 1.00000\n9:PM-8:PM     6.954839  -51.434 65.3440 1.00000\n10:PM-8:PM    7.806452  -50.583 66.1956 1.00000\n11:PM-8:PM   -4.400000  -62.789 53.9892 1.00000\n12:PM-8:PM   -6.954839  -65.344 51.4344 1.00000\n13:PM-8:PM   12.277419  -46.112 70.6666 1.00000\n14:PM-8:PM   -3.051613  -61.441 55.3376 1.00000\n15:PM-8:PM    4.045161  -54.344 62.4344 1.00000\n16:PM-8:PM   -8.729032  -67.118 49.6602 1.00000\n17:PM-8:PM  -16.322581  -74.712 42.0666 1.00000\n18:PM-8:PM   -2.767742  -61.157 55.6215 1.00000\n19:PM-8:PM  -14.832258  -73.221 43.5569 1.00000\n20:PM-8:PM   10.858065  -47.531 69.2473 1.00000\n10:PM-9:PM    0.851613  -57.538 59.2408 1.00000\n11:PM-9:PM  -11.354839  -69.744 47.0344 1.00000\n12:PM-9:PM  -13.909677  -72.299 44.4795 1.00000\n13:PM-9:PM    5.322581  -53.067 63.7118 1.00000\n14:PM-9:PM  -10.006452  -68.396 48.3827 1.00000\n15:PM-9:PM   -2.909677  -61.299 55.4795 1.00000\n16:PM-9:PM  -15.683871  -74.073 42.7053 1.00000\n17:PM-9:PM  -23.277419  -81.667 35.1118 0.99984\n18:PM-9:PM   -9.722581  -68.112 48.6666 1.00000\n19:PM-9:PM  -21.787097  -80.176 36.6021 0.99996\n20:PM-9:PM    3.903226  -54.486 62.2924 1.00000\n11:PM-10:PM -12.206452  -70.596 46.1827 1.00000\n12:PM-10:PM -14.761290  -73.150 43.6279 1.00000\n13:PM-10:PM   4.470968  -53.918 62.8602 1.00000\n14:PM-10:PM -10.858065  -69.247 47.5311 1.00000\n15:PM-10:PM  -3.761290  -62.150 54.6279 1.00000\n16:PM-10:PM -16.535484  -74.925 41.8537 1.00000\n17:PM-10:PM -24.129032  -82.518 34.2602 0.99966\n18:PM-10:PM -10.574194  -68.963 47.8150 1.00000\n19:PM-10:PM -22.638710  -81.028 35.7505 0.99991\n20:PM-10:PM   3.051613  -55.338 61.4408 1.00000\n12:PM-11:PM  -2.554839  -60.944 55.8344 1.00000\n13:PM-11:PM  16.677419  -41.712 75.0666 1.00000\n14:PM-11:PM   1.348387  -57.041 59.7376 1.00000\n15:PM-11:PM   8.445161  -49.944 66.8344 1.00000\n16:PM-11:PM  -4.329032  -62.718 54.0602 1.00000\n17:PM-11:PM -11.922581  -70.312 46.4666 1.00000\n18:PM-11:PM   1.632258  -56.757 60.0215 1.00000\n19:PM-11:PM -10.432258  -68.821 47.9569 1.00000\n20:PM-11:PM  15.258065  -43.131 73.6473 1.00000\n13:PM-12:PM  19.232258  -39.157 77.6215 1.00000\n14:PM-12:PM   3.903226  -54.486 62.2924 1.00000\n15:PM-12:PM  11.000000  -47.389 69.3892 1.00000\n16:PM-12:PM  -1.774194  -60.163 56.6150 1.00000\n17:PM-12:PM  -9.367742  -67.757 49.0215 1.00000\n18:PM-12:PM   4.187097  -54.202 62.5763 1.00000\n19:PM-12:PM  -7.877419  -66.267 50.5118 1.00000\n20:PM-12:PM  17.812903  -40.576 76.2021 1.00000\n14:PM-13:PM -15.329032  -73.718 43.0602 1.00000\n15:PM-13:PM  -8.232258  -66.621 50.1569 1.00000\n16:PM-13:PM -21.006452  -79.396 37.3827 0.99998\n17:PM-13:PM -28.600000  -86.989 29.7892 0.99284\n18:PM-13:PM -15.045161  -73.434 43.3440 1.00000\n19:PM-13:PM -27.109677  -85.499 31.2795 0.99702\n20:PM-13:PM  -1.419355  -59.809 56.9698 1.00000\n15:PM-14:PM   7.096774  -51.292 65.4860 1.00000\n16:PM-14:PM  -5.677419  -64.067 52.7118 1.00000\n17:PM-14:PM -13.270968  -71.660 45.1182 1.00000\n18:PM-14:PM   0.283871  -58.105 58.6731 1.00000\n19:PM-14:PM -11.780645  -70.170 46.6086 1.00000\n20:PM-14:PM  13.909677  -44.480 72.2989 1.00000\n16:PM-15:PM -12.774194  -71.163 45.6150 1.00000\n17:PM-15:PM -20.367742  -78.757 38.0215 0.99999\n18:PM-15:PM  -6.812903  -65.202 51.5763 1.00000\n19:PM-15:PM -18.877419  -77.267 39.5118 1.00000\n20:PM-15:PM   6.812903  -51.576 65.2021 1.00000\n17:PM-16:PM  -7.593548  -65.983 50.7956 1.00000\n18:PM-16:PM   5.961290  -52.428 64.3505 1.00000\n19:PM-16:PM  -6.103226  -64.492 52.2860 1.00000\n20:PM-16:PM  19.587097  -38.802 77.9763 1.00000\n18:PM-17:PM  13.554839  -44.834 71.9440 1.00000\n19:PM-17:PM   1.490323  -56.899 59.8795 1.00000\n20:PM-17:PM  27.180645  -31.209 85.5698 0.99689\n19:PM-18:PM -12.064516  -70.454 46.3247 1.00000\n20:PM-18:PM  13.625806  -44.763 72.0150 1.00000\n20:PM-19:PM  25.690323  -32.699 84.0795 0.99886\n\n\n  Tukey multiple comparisons of means\n    99% family-wise confidence level\n\nFit: aov(formula = response ~ Day + noon + Day:noon, data = data)\n\n$Day\n            diff     lwr    upr   p adj\n2-1    -6.138710 -48.905 36.628 1.00000\n3-1    -4.080645 -46.847 38.686 1.00000\n4-1   -14.300000 -57.066 28.466 0.99700\n5-1    -3.335484 -46.102 39.431 1.00000\n6-1    -6.812903 -49.579 35.953 1.00000\n7-1    -6.422581 -49.189 36.344 1.00000\n8-1    -4.364516 -47.131 38.402 1.00000\n9-1     8.551613 -34.215 51.318 1.00000\n10-1  -12.703226 -55.470 30.063 0.99934\n11-1    1.845161 -40.921 44.611 1.00000\n12-1  -20.403226 -63.170 22.363 0.89330\n13-1   -3.477419 -46.244 39.289 1.00000\n14-1  -18.522581 -61.289 24.244 0.95243\n15-1  -18.664516 -61.431 24.102 0.94905\n16-1  -15.825806 -58.592 26.941 0.99024\n17-1  -17.280645 -60.047 25.486 0.97539\n18-1  -12.064516 -54.831 30.702 0.99968\n19-1  -14.370968 -57.137 28.395 0.99682\n20-1   -2.994839 -47.849 41.859 1.00000\n3-2     2.058065 -40.708 44.824 1.00000\n4-2    -8.161290 -50.928 34.605 1.00000\n5-2     2.803226 -39.963 45.570 1.00000\n6-2    -0.674194 -43.441 42.092 1.00000\n7-2    -0.283871 -43.050 42.482 1.00000\n8-2     1.774194 -40.992 44.541 1.00000\n9-2    14.690323 -28.076 57.457 0.99585\n10-2   -6.564516 -49.331 36.202 1.00000\n11-2    7.983871 -34.782 50.750 1.00000\n12-2  -14.264516 -57.031 28.502 0.99709\n13-2    2.661290 -40.105 45.428 1.00000\n14-2  -12.383871 -55.150 30.382 0.99953\n15-2  -12.525806 -55.292 30.241 0.99946\n16-2   -9.687097 -52.453 33.079 0.99999\n17-2  -11.141935 -53.908 31.624 0.99990\n18-2   -5.925806 -48.692 36.841 1.00000\n19-2   -8.232258 -50.999 34.534 1.00000\n20-2    3.143871 -41.710 47.998 1.00000\n4-3   -10.219355 -52.986 32.547 0.99997\n5-3     0.745161 -42.021 43.511 1.00000\n6-3    -2.732258 -45.499 40.034 1.00000\n7-3    -2.341935 -45.108 40.424 1.00000\n8-3    -0.283871 -43.050 42.482 1.00000\n9-3    12.632258 -30.134 55.399 0.99939\n10-3   -8.622581 -51.389 34.144 1.00000\n11-3    5.925806 -36.841 48.692 1.00000\n12-3  -16.322581 -59.089 26.444 0.98634\n13-3    0.603226 -42.163 43.370 1.00000\n14-3  -14.441935 -57.208 28.324 0.99662\n15-3  -14.583871 -57.350 28.182 0.99620\n16-3  -11.745161 -54.511 31.021 0.99978\n17-3  -13.200000 -55.966 29.566 0.99891\n18-3   -7.983871 -50.750 34.782 1.00000\n19-3  -10.290323 -53.057 32.476 0.99997\n20-3    1.085806 -43.768 45.939 1.00000\n5-4    10.964516 -31.802 53.731 0.99992\n6-4     7.487097 -35.279 50.253 1.00000\n7-4     7.877419 -34.889 50.644 1.00000\n8-4     9.935484 -32.831 52.702 0.99998\n9-4    22.851613 -19.915 65.618 0.76824\n10-4    1.596774 -41.170 44.363 1.00000\n11-4   16.145161 -26.621 58.911 0.98785\n12-4   -6.103226 -48.870 36.663 1.00000\n13-4   10.822581 -31.944 53.589 0.99993\n14-4   -4.222581 -46.989 38.544 1.00000\n15-4   -4.364516 -47.131 38.402 1.00000\n16-4   -1.525806 -44.292 41.241 1.00000\n17-4   -2.980645 -45.747 39.786 1.00000\n18-4    2.235484 -40.531 45.002 1.00000\n19-4   -0.070968 -42.837 42.695 1.00000\n20-4   11.305161 -33.549 56.159 0.99994\n6-5    -3.477419 -46.244 39.289 1.00000\n7-5    -3.087097 -45.853 39.679 1.00000\n8-5    -1.029032 -43.795 41.737 1.00000\n9-5    11.887097 -30.879 54.653 0.99974\n10-5   -9.367742 -52.134 33.399 0.99999\n11-5    5.180645 -37.586 47.947 1.00000\n12-5  -17.067742 -59.834 25.699 0.97827\n13-5   -0.141935 -42.908 42.624 1.00000\n14-5  -15.187097 -57.953 27.579 0.99387\n15-5  -15.329032 -58.095 27.437 0.99318\n16-5  -12.490323 -55.257 30.276 0.99948\n17-5  -13.945161 -56.711 28.821 0.99780\n18-5   -8.729032 -51.495 34.037 1.00000\n19-5  -11.035484 -53.802 31.731 0.99991\n20-5    0.340645 -44.513 45.194 1.00000\n7-6     0.390323 -42.376 43.157 1.00000\n8-6     2.448387 -40.318 45.215 1.00000\n9-6    15.364516 -27.402 58.131 0.99300\n10-6   -5.890323 -48.657 36.876 1.00000\n11-6    8.658065 -34.108 51.424 1.00000\n12-6  -13.590323 -56.357 29.176 0.99841\n13-6    3.335484 -39.431 46.102 1.00000\n14-6  -11.709677 -54.476 31.057 0.99979\n15-6  -11.851613 -54.618 30.915 0.99975\n16-6   -9.012903 -51.779 33.753 1.00000\n17-6  -10.467742 -53.234 32.299 0.99996\n18-6   -5.251613 -48.018 37.515 1.00000\n19-6   -7.558065 -50.324 35.208 1.00000\n20-6    3.818065 -41.036 48.672 1.00000\n8-7     2.058065 -40.708 44.824 1.00000\n9-7    14.974194 -27.792 57.741 0.99480\n10-7   -6.280645 -49.047 36.486 1.00000\n11-7    8.267742 -34.499 51.034 1.00000\n12-7  -13.980645 -56.747 28.786 0.99773\n13-7    2.945161 -39.821 45.711 1.00000\n14-7  -12.100000 -54.866 30.666 0.99966\n15-7  -12.241935 -55.008 30.524 0.99960\n16-7   -9.403226 -52.170 33.363 0.99999\n17-7  -10.858065 -53.624 31.908 0.99993\n18-7   -5.641935 -48.408 37.124 1.00000\n19-7   -7.948387 -50.715 34.818 1.00000\n20-7    3.427742 -41.426 48.281 1.00000\n9-8    12.916129 -29.850 55.682 0.99918\n10-8   -8.338710 -51.105 34.428 1.00000\n11-8    6.209677 -36.557 48.976 1.00000\n12-8  -16.038710 -58.805 26.728 0.98869\n13-8    0.887097 -41.879 43.653 1.00000\n14-8  -14.158065 -56.924 28.608 0.99735\n15-8  -14.300000 -57.066 28.466 0.99700\n16-8  -11.461290 -54.228 31.305 0.99984\n17-8  -12.916129 -55.682 29.850 0.99918\n18-8   -7.700000 -50.466 35.066 1.00000\n19-8  -10.006452 -52.773 32.760 0.99998\n20-8    1.369677 -43.484 46.223 1.00000\n10-9  -21.254839 -64.021 21.511 0.85576\n11-9   -6.706452 -49.473 36.060 1.00000\n12-9  -28.954839 -71.721 13.811 0.35269\n13-9  -12.029032 -54.795 30.737 0.99969\n14-9  -27.074194 -69.841 15.692 0.47719\n15-9  -27.216129 -69.982 15.550 0.46731\n16-9  -24.377419 -67.144 18.389 0.66832\n17-9  -25.832258 -68.599 16.934 0.56535\n18-9  -20.616129 -63.382 22.150 0.88455\n19-9  -22.922581 -65.689 19.844 0.76389\n20-9  -11.546452 -56.400 33.307 0.99991\n11-10  14.548387 -28.218 57.315 0.99631\n12-10  -7.700000 -50.466 35.066 1.00000\n13-10   9.225806 -33.541 51.992 0.99999\n14-10  -5.819355 -48.586 36.947 1.00000\n15-10  -5.961290 -48.728 36.805 1.00000\n16-10  -3.122581 -45.889 39.644 1.00000\n17-10  -4.577419 -47.344 38.189 1.00000\n18-10   0.638710 -42.128 43.405 1.00000\n19-10  -1.667742 -44.434 41.099 1.00000\n20-10   9.708387 -35.145 54.562 0.99999\n12-11 -22.248387 -65.015 20.518 0.80373\n13-11  -5.322581 -48.089 37.444 1.00000\n14-11 -20.367742 -63.134 22.399 0.89472\n15-11 -20.509677 -63.276 22.257 0.88898\n16-11 -17.670968 -60.437 25.095 0.96936\n17-11 -19.125806 -61.892 23.641 0.93692\n18-11 -13.909677 -56.676 28.857 0.99787\n19-11 -16.216129 -58.982 26.550 0.98726\n20-11  -4.840000 -49.694 40.014 1.00000\n13-12  16.925806 -25.841 59.692 0.98004\n14-12   1.880645 -40.886 44.647 1.00000\n15-12   1.738710 -41.028 44.505 1.00000\n16-12   4.577419 -38.189 47.344 1.00000\n17-12   3.122581 -39.644 45.889 1.00000\n18-12   8.338710 -34.428 51.105 1.00000\n19-12   6.032258 -36.734 48.799 1.00000\n20-12  17.408387 -27.445 62.262 0.98369\n14-13 -15.045161 -57.811 27.721 0.99450\n15-13 -15.187097 -57.953 27.579 0.99387\n16-13 -12.348387 -55.115 30.418 0.99955\n17-13 -13.803226 -56.570 28.963 0.99807\n18-13  -8.587097 -51.353 34.179 1.00000\n19-13 -10.893548 -53.660 31.873 0.99992\n20-13   0.482581 -44.371 45.336 1.00000\n15-14  -0.141935 -42.908 42.624 1.00000\n16-14   2.696774 -40.070 45.463 1.00000\n17-14   1.241935 -41.524 44.008 1.00000\n18-14   6.458065 -36.308 49.224 1.00000\n19-14   4.151613 -38.615 46.918 1.00000\n20-14  15.527742 -29.326 60.381 0.99545\n16-15   2.838710 -39.928 45.605 1.00000\n17-15   1.383871 -41.382 44.150 1.00000\n18-15   6.600000 -36.166 49.366 1.00000\n19-15   4.293548 -38.473 47.060 1.00000\n20-15  15.669677 -29.184 60.523 0.99493\n17-16  -1.454839 -44.221 41.311 1.00000\n18-16   3.761290 -39.005 46.528 1.00000\n19-16   1.454839 -41.311 44.221 1.00000\n20-16  12.830968 -32.023 57.685 0.99961\n18-17   5.216129 -37.550 47.982 1.00000\n19-17   2.909677 -39.857 45.676 1.00000\n20-17  14.285806 -30.568 59.139 0.99837\n19-18  -2.306452 -45.073 40.460 1.00000\n20-18   9.069677 -35.784 53.923 1.00000\n20-19  11.376129 -33.478 56.230 0.99993\n\n\n유의한 패턴 없음 Tukey 검정 결과 (보통 유의할때 함) 테이블 도 유의한 수치가 없음"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html",
    "title": "ANCOVA",
    "section": "",
    "text": "(Draft, 바쁘니까 일단 대충이라도 적어놓음 ㅠ)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#description",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#description",
    "title": "ANCOVA",
    "section": "1 Description",
    "text": "1 Description\nANCOVA (Analysis of Covariance, ANCOVA)\n\nANOVA에 공변량 (covariate)을 추가하여 분석 수행\n공변량을 조정하여 독립변수의 순수한 영향을 검정\n공변량: 연속형 변수로 한정"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#example",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#example",
    "title": "ANCOVA",
    "section": "2 Example",
    "text": "2 Example\n\n2.1 Load Libraries and Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(faraway)\nlibrary(markdown)\nlibrary(effects)\nlibrary(HH)\nlibrary(psych)\n\n\n\n\n2.2 Data Description\n\n\nCode\nstr(sexab)\n\n\n'data.frame':   76 obs. of  3 variables:\n $ cpa : num  2.048 0.839 -0.241 -1.115 2.015 ...\n $ ptsd: num  9.71 6.17 15.16 11.31 9.95 ...\n $ csa : Factor w/ 2 levels \"Abused\",\"NotAbused\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nR console에 ?sexab를 입력하면 다음과 같은 설명이 나온다.\nPost traumatic stress disorder in abused adult females\nThe data for this example come from a study of the effects of childhood sexual abuse on adult females. 45 women being treated at a clinic, who reported childhood sexual abuse, were measured for post traumatic stress disorder and childhood physical abuse both on standardized scales. 31 women also being treated at the same clinic, who did not report childhood sexual abuse were also measured. The full study was more complex than reported here and so readers interested in the subject matter should refer to the original article.\n즉, 요약하면 아동기에 성폭력을 겸험한 성인들의 정신 건강을 측정한 데이터로서, 아동기의 성폭력 경험과 학대 경험이 성인기의 정신건강에 유의한 영향을 미치는지에 대한 실험을 한 것이다.\n이 data는 3개의 변수와 76개의 samples을 포함한다.\n\ncpa : Childhood physical abuse on standard scale, covariate\nptsd : post-traumatic stress disorder on standard scale, response variable\ncsa : Childhood sexual abuse - abused or not abused, independent variable\n\n친절하게 response variable, independent variable 및 covariate을 규명해놓았다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANCOVA/index.html",
    "href": "docs/blog/posts/statistics/2023-01-28_MANCOVA/index.html",
    "title": "MANOVA",
    "section": "",
    "text": "Code\nlibrary(heplots)\n#skulls 고대 이집트 왕조 부터 로마시대까지 이집트 지역에서 발군된 두개골의 크기를 측정한 데이터\n# 이집트 역사를 5개의 시대로 구분하고 각 시대별로 30개씩의 두개골을 4개의 지표로 측정\n# epoch: 이집트의 시대를 5개로 구분, 독립변수\n# mb : 두개골의 폭, 종속 변수\n# bh : 두개골의 높이, 종속 변수\n# bl : 두개골의 길이, 종속 변수\n# nh : 코의 높이, 종속 변수\n\nlibrary(dplyr)\nsample_n(Skulls,10)\n\n\n      epoch  mb  bh  bl nh\n74  c1850BC 137 133  90 49\n29  c4000BC 131 136 114 54\n13  c4000BC 126 129 109 51\n79  c1850BC 138 134  96 51\n5   c4000BC 136 143 100 54\n150  cAD150 136 133  97 51\n145  cAD150 132 127  97 52\n42  c3300BC 131 139  98 51\n123  cAD150 128 126  91 57\n69  c1850BC 136 131  92 46\n\n\nCode\nattach(Skulls)# Skulls를 작업 경로에 포함시키기\nsearch() # 작업 경로 확인인\n\n\n [1] \".GlobalEnv\"        \"Skulls\"            \"package:dplyr\"    \n [4] \"package:heplots\"   \"package:broom\"     \"package:car\"      \n [7] \"package:carData\"   \"tools:quarto\"      \"tools:quarto\"     \n[10] \"package:stats\"     \"package:graphics\"  \"package:grDevices\"\n[13] \"package:utils\"     \"package:datasets\"  \"package:methods\"  \n[16] \"Autoloads\"         \"package:base\"     \n\n\nCode\n# 종속 변수를 결합시켜 하나의 행렬로 만들기\ny<-cbind(mb,bh,bl,nh)\n# 시대별 두개골  길이의 평균 보기\naggregate(y,by=list(epoch),mean) # 언뜻 보기에 차이가 있는 것 처럼 보임\n\n\n  Group.1       mb       bh       bl       nh\n1 c4000BC 131.3667 133.6000 99.16667 50.53333\n2 c3300BC 132.3667 132.7000 99.06667 50.23333\n3 c1850BC 134.4667 133.8000 96.03333 50.56667\n4  c200BC 135.5000 132.3000 94.53333 51.96667\n5  cAD150 136.1667 130.3333 93.50000 51.36667\n\n\nCode\n# 모집단으로 일반화하기 위해 통계적 검정 시행\nskulls_manova<-manova(y~epoch)\nsummary(skulls_manova)\n\n\n           Df  Pillai approx F num Df den Df    Pr(>F)    \nepoch       4 0.35331    3.512     16    580 4.675e-06 ***\nResiduals 145                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# 시대별 두개골 측정값이 차이가 있는 것으로 보임\n\n# 구체적으로 어느 두개 골 측정값에서 차이가 나는지 확인\nsummary.aov(skulls_manova)\n\n\n Response mb :\n             Df  Sum Sq Mean Sq F value    Pr(>F)    \nepoch         4  502.83 125.707  5.9546 0.0001826 ***\nResiduals   145 3061.07  21.111                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response bh :\n             Df Sum Sq Mean Sq F value  Pr(>F)  \nepoch         4  229.9  57.477  2.4474 0.04897 *\nResiduals   145 3405.3  23.485                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response bl :\n             Df Sum Sq Mean Sq F value    Pr(>F)    \nepoch         4  803.3 200.823  8.3057 4.636e-06 ***\nResiduals   145 3506.0  24.179                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response nh :\n             Df Sum Sq Mean Sq F value Pr(>F)\nepoch         4   61.2  15.300   1.507 0.2032\nResiduals   145 1472.1  10.153               \n\n\nCode\n# nh는 차이가 없는 것으로 보임\n\n## 시간에 따라 두개골 측정이 다르다는 것은 이민족 유입의 혼혈 가능성이 있음\n\ndetach(Skulls)# 작업경로에서 삭제제"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANCOVA/index.html#시간에-따라-두개골-측정이-다르다는-것은-이민족-유입의-혼혈-가능성이-있음",
    "href": "docs/blog/posts/statistics/2023-01-28_MANCOVA/index.html#시간에-따라-두개골-측정이-다르다는-것은-이민족-유입의-혼혈-가능성이-있음",
    "title": "MANOVA",
    "section": "1.1 시간에 따라 두개골 측정이 다르다는 것은 이민족 유입의 혼혈 가능성이 있음",
    "text": "1.1 시간에 따라 두개골 측정이 다르다는 것은 이민족 유입의 혼혈 가능성이 있음\ndetach(Skulls)# 작업경로에서 삭제제\n`"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#eda",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#eda",
    "title": "ANCOVA",
    "section": "3 EDA",
    "text": "3 EDA\n\n3.1 Descriptive Statistics\n\n\nCode\ntemp<-describeBy(ptsd~csa,data=sexab)\ntemp<-rbind('abused'=temp$Abused,'notAbused'=temp$NotAbused)%>%\nas.data.frame()\ntemp%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n\nabused\n1\n45\n11.941093\n3.440151\n11.31277\n11.883422\n3.857355\n5.98491\n18.99251\n13.00760\n0.1556159\n-0.9124483\n0.5128275\n\n\nnotAbused\n1\n31\n4.695874\n3.519743\n5.79447\n4.903441\n1.978841\n-3.34921\n10.91447\n14.26368\n-0.6589170\n-0.2008051\n0.6321645\n\n\n\n\n\n위의 요약된 기술 통계량들 중 표준 편차는 유사하지만 평균 ptsd가 약 7.245219의 차이를 보여준다. 아래의 histogram역시 성폭력을 경험한 그룹과 경험하지 않은 그룹간의 PTSD 수치가 다른것을 볼 수 있다.\n\n\nCode\nggplot(data=sexab,aes(x=ptsd,color=csa,fill=csa))+\ngeom_histogram(aes(y=..density..),position=\"identity\",fill='white')+\ngeom_density(alpha=0.5)+\nlabs(title=\"Histogram, PTSD Grouped by Childhood Sexual Abuse Experience\", x=\"PTSD\", y=\"Desnsity\")\n\n\n\n\n\n\n\n3.2 One-Way ANOVA\n성폭력 경험 유무에 따른 PTSD 평균 차이가 통계적으로 유의한지 확인하기 위해 ANOVA를 수행한다.\n\n\nCode\nsexab_aov<-aov(ptsd~csa, data=sexab)\nsummary(sexab_aov)\n\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncsa          1  963.5   963.5    79.9 2.17e-13 ***\nResiduals   74  892.4    12.1                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n집단간 변수 csa p value가 <0.05 인 것을 확인할 수 있다. csa는 5% 유의수준에서 유의하다.\n하지만 PTSD의 변동량은 아동 학대에 의해 설명될 수도 있기 때문에 ptsd의 평균은 csa뿐만 아니라 cpa에 또한 고려되어야한다.\n\n\nCode\nggplot(data=sexab,aes(x=cpa,y=ptsd))+geom_point()+geom_smooth(method=\"lm\")+\nlabs(title=\"Scatter Plot, PTSD vs CPA\", x=\"CPA\", y=\"PTSD\")\n\n\n\n\n\nCode\ncorrelation<-cor.test(sexab$cpa,sexab$ptsd, method='pearson')\n\n\n그림과 같이 CPA가 증가하면서 PTSD또한 선형적으로 증가하는 패턴을 관찰할 수 있다. 두 변수간의 상관계수 = 0.49이고 p value= 6.2715909^{-6}으로 보아 두 변수 사이에 선형적인 상관관계가 있는 것으로 보인다.\n\n\nCode\nggplot(data=sexab,aes(x=cpa,y=ptsd))+geom_point()+geom_smooth(method=\"lm\")+\nfacet_wrap(.~csa)+\nlabs(title=\"Scatter Plot, PTSD vs CPA Grouped By CSA\", x=\"CPA\", y=\"PTSD\")\n\n\n\n\n\n아동기 성폭력 경험 유/무에도 PTSD와 CPA와 선형적인 관계가 있는 것으로 보이기 때문에 CSA의 PTSD로의 효과를 검정하기 위해선 CPA를 조정할 필요가 있는것으로 보인다.\n\n\nCode\n# ptsd로의 순수한 성폭력 경험의 영향도를 얻기 위해서는 아동기 신체적 학대(공변량)에 대해서 고려를 해줘야함\n\nsexab_aov<-aov(ptsd~cpa+csa, data=sexab) \nsummary(sexab_aov)\n\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncpa          1  449.8   449.8   41.98 9.46e-09 ***\ncsa          1  624.0   624.0   58.25 6.91e-11 ***\nResiduals   73  782.1    10.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n아동기의 신체적 학대가 일정하다는 가정하에서 PTSD와 성폭력의 순수한 관계는 5% 유의수준에서 유의하고 공변량, CPA를 조정하기전과 그 유의성이 차이가 있음을 관찰할 수 있다.\n\n\nCode\n# CPA가 제거 된 후에 CSA의 순수한 효과를 알아보기\n\nancova(ptsd~cpa+csa, data=sexab) \n\n\nAnalysis of Variance Table\n\nResponse: ptsd\n          Df Sum Sq Mean Sq F value    Pr(>F)    \ncpa        1 449.80  449.80  41.984 9.462e-09 ***\ncsa        1 624.03  624.03  58.247 6.907e-11 ***\nResiduals 73 782.08   10.71                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n두 csa집단에서 두 회귀선의 기울기 같고 절편이 다르게 나타나는 것을 관찰 할 수있다. 기울기가 같은 이유는 cpa가 ptsd에 영향을 미치는 정도가 두집단에서 일정하도록 공변량으로서 통제 했기 때문이다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html",
    "href": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html",
    "title": "Repeated Meausres ANOVA",
    "section": "",
    "text": "(Draft, 바쁘니까 일단 대충이라도 적어놓음 ㅠ)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html#description",
    "href": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html#description",
    "title": "Repeated Meausres ANOVA",
    "section": "1 Description",
    "text": "1 Description\nRepeated Meausres ANOVA (반복 측정 분산 분석)\n\n동일한 대상에 대해 여러 번 반복측정하여 반복측정(3번 이상) 집단 간에 차이가 존재하는지 검정\n\n대응 표본 검정은 동일한 대상에 2번 반복 측정함 2개의 대응표본에 대한 검정\n\n이 때, 반복 측정 기간은 집단 내 요인이라 부르고 이를 반복측정 일원분산분석-집단내요인이라고 부름\n반복측정 기간외에 대상을 구분하는 집단 변수가 포함되면 반복측정 이원분산분석- 집단내요인 & 집단간 요인"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html#example",
    "href": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html#example",
    "title": "Repeated Meausres ANOVA",
    "section": "2 Example",
    "text": "2 Example\n\n\nCode\nstr(CO2)\n\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  84 obs. of  5 variables:\n $ Plant    : Ord.factor w/ 12 levels \"Qn1\"<\"Qn2\"<\"Qn3\"<..: 1 1 1 1 1 1 1 2 2 2 ...\n $ Type     : Factor w/ 2 levels \"Quebec\",\"Mississippi\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Treatment: Factor w/ 2 levels \"nonchilled\",\"chilled\": 1 1 1 1 1 1 1 1 1 1 ...\n $ conc     : num  95 175 250 350 500 675 1000 95 175 250 ...\n $ uptake   : num  16 30.4 34.8 37.2 35.3 39.2 39.7 13.6 27.3 37.1 ...\n - attr(*, \"formula\")=Class 'formula'  language uptake ~ conc | Plant\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n - attr(*, \"outer\")=Class 'formula'  language ~Treatment * Type\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n - attr(*, \"labels\")=List of 2\n  ..$ x: chr \"Ambient carbon dioxide concentration\"\n  ..$ y: chr \"CO2 uptake rate\"\n - attr(*, \"units\")=List of 2\n  ..$ x: chr \"(uL/L)\"\n  ..$ y: chr \"(umol/m^2 s)\"\n\n\nCode\nnames(CO2)\n\n\n[1] \"Plant\"     \"Type\"      \"Treatment\" \"conc\"      \"uptake\"   \n\n\nR console에 ?CO2를 입력하면 다음과 같은 설명이 나온다.\nThe CO2 data frame has 84 rows and 5 columns of data from an experiment on the cold tolerance of the grass species Echinochloa crus-galli.The \\(CO_2\\) uptake of six plants from Quebec and six plants from Mississippi was measured at several levels of ambient \\(CO_2\\) concentration. Half the plants of each type were chilled overnight before the experiment was conducted.\n즉, 식물이 저온의 환경에서 견디는 정도를 실험한 데이터로 퀘벡 지역의 6개의 나무와 미시시피 지역의 6개 나무의 이산화 탄소 흡수율을 7개의 서로 다른 이산화 탄소 농도 하에서 반복적으로 측정했다.\n\n2.1 Goals\n분석의 편의를 위해 저온 처리된 나무에 한정하여 분석\n\n두 지역간의 CO2흡수율의 차이를 검정\n7개의 서로 다른 이산화탄소 농도에 따라서 이산화 탄소의 흡수율 차이를 검정\n나무의 출신 지역과 이산화 탄소 흡수율 간의 관계가 이산화탄소 농도에 따라 달라지는 지도 검정\n\n\n\n2.2 Data Description\n\nPlant: plant id\nType: 나무의 출신 지역, 2개의 범주, 집단간 요인\nTreatment: 퀘벡 지역 나무와 미시시피 지역 나무dp 각 각 절반씩 실험 전에 저온 처리 했음, 저온 처리 여부가 treatment 변수에 저장됨\nconc: co2농도, 7개의 범주, 집단 내 요인\nuptake: 종속 변수, 이산화 탄소 흡수율\n\n\n\n2.3 Load Libraries and Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(faraway)\nlibrary(markdown)\nlibrary(effects)\nlibrary(HH)\nlibrary(psych)\n\n\n\n\n2.4 EDA\n\n\nCode\ndata=CO2%>%\nfilter(Treatment=='chilled')%>%\nmutate(conc=factor(conc),\ntype=factor(conc))\n\n\n\n\n2.5 Descriptive Statistics\n\n\nCode\ndata%>%\ngroup_by(Type)%>%\nsummarise(count=n(),\nmean_uptake=mean(uptake),\nsd_uptake=sd(uptake)\n)%>%knitr::kable()\n\n\n\n\n\nType\ncount\nmean_uptake\nsd_uptake\n\n\n\n\nQuebec\n21\n31.75238\n9.644823\n\n\nMississippi\n21\n15.81429\n4.058976\n\n\n\n\n\n두 지역의 분산과 평균 흡수율이 차이가 남\n\n\nCode\ndata%>%\ngroup_by(conc)%>%\nsummarise(count=n(),\nmean_uptake=mean(uptake),\nsd_uptake=sd(uptake)\n)%>%knitr::kable()\n\n\n\n\n\nconc\ncount\nmean_uptake\nsd_uptake\n\n\n\n\n95\n6\n11.23333\n2.860536\n\n\n175\n6\n19.45000\n5.886510\n\n\n250\n6\n25.28333\n10.569090\n\n\n350\n6\n26.20000\n10.831251\n\n\n500\n6\n26.65000\n11.445479\n\n\n675\n6\n27.88333\n10.958361\n\n\n1000\n6\n29.78333\n12.410547\n\n\n\n\n\n농도에 따라서 평균 흡수율과 분산이 약간 차이가 나는 것 같음. 다만, 고농도가 될수록 변화량이 줄어드는 것으로 보임\n\n\nCode\ndata%>%\ngroup_by(Type,conc)%>%\nsummarise(count=n(),\nmean_uptake=mean(uptake),\nsd_uptake=sd(uptake)\n)%>%knitr::kable()\n\n\n\n\n\nType\nconc\ncount\nmean_uptake\nsd_uptake\n\n\n\n\nQuebec\n95\n3\n12.86667\n3.121431\n\n\nQuebec\n175\n3\n24.13333\n3.150132\n\n\nQuebec\n250\n3\n34.46667\n3.927255\n\n\nQuebec\n350\n3\n35.80000\n2.615339\n\n\nQuebec\n500\n3\n36.66667\n3.611556\n\n\nQuebec\n675\n3\n37.50000\n2.100000\n\n\nQuebec\n1000\n3\n40.83333\n1.913984\n\n\nMississippi\n95\n3\n9.60000\n1.646208\n\n\nMississippi\n175\n3\n14.76667\n3.302020\n\n\nMississippi\n250\n3\n16.10000\n3.292416\n\n\nMississippi\n350\n3\n16.60000\n3.157531\n\n\nMississippi\n500\n3\n16.63333\n3.667879\n\n\nMississippi\n675\n3\n18.26667\n4.285246\n\n\nMississippi\n1000\n3\n18.73333\n3.883727\n\n\n\n\n\n농도가 증가함에 따라 지역간 평균 흡수율의 차이가 커지는 것을 관찰할 수 있음\n\n\nCode\nboxplot(uptake~Type*conc,data=data,col=c(\"darkblue\",\"darkred\"),\n        las=2,cex.axis=0.7,xlab=\"\",ylab=\"Carbon Dioxide Uptake Rate\",\n        main=\"Effects of Plant Ype and CO2 on Carbon Dioxide Uptake\")\nlegend(\"topleft\",inset=0.025, legend=c(\"Quebec\",\"Mississippi\"),\n       fill=c(\"darkblue\",\"darkred\"))\n\n\n\n\n\n\n\n2.6 One-Wway ANOVA\n반복 측정 일원분산분석: y~W+Error(subject) where W= a within grouping variable, subject= a sample identifier\n\n\nCode\naov(uptake~Type+Error(Plant),data=data)%>%summary()\n\n\n\nError: Plant\n          Df Sum Sq Mean Sq F value  Pr(>F)   \nType       1 2667.2  2667.2   60.41 0.00148 **\nResiduals  4  176.6    44.1                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals 36   2013   55.93               \n\n\nCode\naov(uptake~conc+Error(Plant),data=data)%>%summary()\n\n\n\nError: Plant\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals  5   2844   568.8               \n\nError: Within\n          Df Sum Sq Mean Sq F value   Pr(>F)    \nconc       6   1472  245.40   13.61 2.09e-07 ***\nResiduals 30    541   18.03                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n지역간 농도간 평균 흡수율이 차이가 5% 유의수준에서 유의하다.\n\n\n2.7 Two-Wway ANOVA\n반복측정 이원분산 분석: y~B*W+Error(subject/W) where W= within grouping variable, B =Between Group Variable, and subject= a sample identifier\n\n\nCode\naov(uptake~Type*conc+Error(Plant/conc),data=data)%>%summary()\n\n\n\nError: Plant\n          Df Sum Sq Mean Sq F value  Pr(>F)   \nType       1 2667.2  2667.2   60.41 0.00148 **\nResiduals  4  176.6    44.1                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Plant:conc\n          Df Sum Sq Mean Sq F value   Pr(>F)    \nconc       6 1472.4  245.40   52.52 1.26e-12 ***\nType:conc  6  428.8   71.47   15.30 3.75e-07 ***\nResiduals 24  112.1    4.67                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n지역간 농도간 집단간에서 흡수율 차이가 유의한것으로 나타나고 둘의 상호작용 또한 5% 유의수준에서 유의하다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html#blog-guide-map-link",
    "title": "Repeated Meausres ANOVA",
    "section": "3 Blog Guide Map Link",
    "text": "3 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/DL/guide_map/index.html",
    "href": "docs/blog/posts/DL/guide_map/index.html",
    "title": "Content List, Deep Learning",
    "section": "",
    "text": "(Draft)"
  },
  {
    "objectID": "docs/blog/posts/DL/guide_map/index.html#basic",
    "href": "docs/blog/posts/DL/guide_map/index.html#basic",
    "title": "Content List, Deep Learning",
    "section": "Basic",
    "text": "Basic"
  },
  {
    "objectID": "docs/blog/posts/DL/guide_map/index.html#inferencce",
    "href": "docs/blog/posts/DL/guide_map/index.html#inferencce",
    "title": "Blog Guide Map, Deep Learning",
    "section": "Inferencce",
    "text": "Inferencce\n\n1111-11-11, Hypothesis Testing\n2022-12-28, p-values\n1111-11-11, Permutation Test\n1111-11-11, Power\n1111-11-11, Multiple Testing\n1111-11-11, Sample Size Calculation\n1111-11-11, A/B Testing\n2023-01-07, ANOVA\n\n2023-01-27, ANCOVA\n2023-01-27, repeated measures ANOVA\n2023-01-28, MANOVA\n2023-01-28, MANCOVA"
  },
  {
    "objectID": "docs/blog/posts/DL/guide_map/index.html#regression",
    "href": "docs/blog/posts/DL/guide_map/index.html#regression",
    "title": "Blog Guide Map, Deep Learning",
    "section": "Regression",
    "text": "Regression\n\n1111-11-11, Least Square and Simple Linear Regression\n1111-11-11, Multiple Linear Regression\n\n\nGeneralized Linear Models\n\n1111-11-11, Logistic Regression\n1111-11-11, Multinomial Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression\n\n\n\nMixed Models\n\n1111-11-11, Linear Mixed Models"
  },
  {
    "objectID": "docs/blog/posts/DL/guide_map/index.html#generalized-additive-models",
    "href": "docs/blog/posts/DL/guide_map/index.html#generalized-additive-models",
    "title": "Blog Guide Map, Deep Learning",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models"
  },
  {
    "objectID": "docs/blog/posts/DL/guide_map/index.html#survival-analysis",
    "href": "docs/blog/posts/DL/guide_map/index.html#survival-analysis",
    "title": "Blog Guide Map, Deep Learning",
    "section": "Survival Analysis",
    "text": "Survival Analysis\n\n1111-11-11, Cox-Hazard Model"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html",
    "title": "Content List, Mathematics",
    "section": "",
    "text": "2023-03-24, Variable types\n1111-11-11, Function\n\n2023-01-31, Function (1) - Univariable Scalar Function (One to One)\n2023-01-31, Function (2) - Multi-variable Scalar Function (Many to One)\n2023-01-31, Function (3) - Univariable Vector Function (One to Many)\n2023-01-31, Function (4) - Multi-variable Vector Function (Many to Many)\n2023-02-18, Function (5) - Composite Function\n\n2023-02-18, Transformations of Functions\n1111-11-11, Vector & Matrix\n2023-03-15, Limit, \\(\\epsilon-\\delta\\) Method\nDifferentiation\n\n2023-02-04, Derivative (1) - Univariable Scalar Funtion\n1111-11-11, Derivative (2) - Chain Rule & Partial Derivative\n1111-11-11, Derivative (3) - Higher Order Derivative\n1111-11-11, Derivative (4) - Mean Value Theorem\n1111-11-11, Derivative (5) - Gradient\n\n2023-03-15, Talyer’s Series\n1111-11-11, Gradient Direction\n1111-11-11, Random Variable\n1111-11-11, Probability Distribution\n1111-11-11, Information Theory - Entropy\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n\n\n\n\n\n1111-11-11, Vector Space\n1111-11-11, Subspace\n1111-11-11, Inner Product\n1111-11-11, Linear Combination\n1111-11-11, Quadratic Form\n1111-11-11, Linear Independence\n1111-11-11, Basis, Dimension, & Rank\n1111-11-11, Outer Product\n1111-11-11, Eigen Value & Eigen Vector\n1111-11-11, Eigen Decomposition\n1111-11-11, Singular Value Decomposition (SVD)\n1111-11-11, Gram-Schmidt\n1111-11-11, Group\n1111-11-11, Orthogonal Matrix\n1111-11-11, Rotation & Group\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n\n\n\n\n\n2023-03-23, Minimizer & Minimum\n1111-11-11, Convex Set\n1111-11-11, Convex Function\n1111-11-11, Unconstrained Optimization\n1111-11-11, Non-linear Least Square\n1111-11-11, Largrange Multiplier Method\n\n1111-11-11, Largrange Primal Function\n1111-11-11, Largrange Dual Function\n1111-11-11, KKT conditions\n\n1111-11-11, Gradient Descent Optimizers\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n\n\n\n\n\nStatistics\n\nGeorge Casella & Rogeer L. Berger - Statistcal Inference, 2nd Edition\n슬기로운 통계생활 - https://www.youtube.com/@statisticsplaybook\n슬기로운 통계생활 - https://github.com/statisticsplaybook\n다수의 Youtube, and Documents from Googling\n\nMathematics\n\nJames Stewart - Calculus Early Transcedentals, 7th Eidition\nany James Stewart series\n임장환 - 머신러닝, 인공지능, 컴퓨터 비전 전공자를 위한 최적화 이론\n다수의 Youtube, and Documents from Googling\n\nDeep Learning\n\n조준우 - 머신러닝·딥러닝에 필요한 기초 수학 with 파이썬\n조준우 - https://github.com/metamath1/noviceml\n동빈나 - https://www.youtube.com/c/dongbinna\n혁펜하임 - https://www.youtube.com/channel/UCcbPAIfCa4q0x7x8yFXmBag\n다수의 Youtube, and Documents from Googling"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html#basic",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html#basic",
    "title": "Blog Guide Map, Mathematics",
    "section": "Basic",
    "text": "Basic\n\n1111-11-11, Function\n\n2023-01-31, Univariable Scalar Function (One to One)\n2023-01-31, Multi-variable Scalar Function (Many to One)\n2023-01-31, Univariable Vector Function (One to Many)\n2023-01-31, Multi-variable Vector Function (Many to Many)\n\n1111-11-11, Vector & Matrix\n1111-11-11, Limit, \\(\\epsilon-\\delta\\) Method\n1111-11-11, Differintiation\n\n1111-11-11, Univariable Differintiation\n1111-11-11, Multivariable Differintiation\n\n1111-11-11, Chain Rule\n1111-11-11, Partial Derivative\n1111-11-11, Gradient\n1111-11-11, Talyer’s Series\n1111-11-11, Gradient Direction\n1111-11-11, Random Variable\n1111-11-11, Probability Distribution\n1111-11-11, Information Theory - Entropy\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html#inferencce",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html#inferencce",
    "title": "Blog Guide Map, Statistics",
    "section": "Inferencce",
    "text": "Inferencce\n\n1111-11-11, Hypothesis Testing\n2022-12-28, p-values\n1111-11-11, Permutation Test\n1111-11-11, Power\n1111-11-11, Multiple Testing\n1111-11-11, Sample Size Calculation\n1111-11-11, A/B Testing\n2023-01-07, ANOVA\n\n2023-01-27, ANCOVA\n2023-01-27, repeated measures ANOVA\n2023-01-28, MANOVA\n2023-01-28, MANCOVA"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html#regression",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html#regression",
    "title": "Blog Guide Map, Statistics",
    "section": "Regression",
    "text": "Regression\n\n1111-11-11, Least Square and Simple Linear Regression\n1111-11-11, Multiple Linear Regression\n\n\nGeneralized Linear Models\n\n1111-11-11, Logistic Regression\n1111-11-11, Multinomial Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression\n\n\n\nMixed Models\n\n1111-11-11, Linear Mixed Models"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html#generalized-additive-models",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html#generalized-additive-models",
    "title": "Blog Guide Map, Statistics",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html#survival-analysis",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html#survival-analysis",
    "title": "Blog Guide Map, Statistics",
    "section": "Survival Analysis",
    "text": "Survival Analysis\n\n1111-11-11, Cox-Hazard Model"
  },
  {
    "objectID": "docs/blog/posts/ML/guide_map/index.html",
    "href": "docs/blog/posts/ML/guide_map/index.html",
    "title": "Content List, Machine Learning",
    "section": "",
    "text": "(Draft)"
  },
  {
    "objectID": "docs/blog/posts/ML/guide_map/index.html#basic",
    "href": "docs/blog/posts/ML/guide_map/index.html#basic",
    "title": "Content List, Machine Learning",
    "section": "Basic",
    "text": "Basic\n\nR\n\n\nPython\n\nTensor Flow Framework\n\n2023-02-03, Tensor Flow Introduction\n\n\n\nPytorch Framework\n\n2023-02-03, Pytorch Introduction"
  },
  {
    "objectID": "docs/blog/posts/ML/guide_map/index.html#inferencce",
    "href": "docs/blog/posts/ML/guide_map/index.html#inferencce",
    "title": "Blog Guide Map, Machine Learning",
    "section": "Inferencce",
    "text": "Inferencce\n\n1111-11-11, Hypothesis Testing\n2022-12-28, p-values\n1111-11-11, Permutation Test\n1111-11-11, Power\n1111-11-11, Multiple Testing\n1111-11-11, Sample Size Calculation\n1111-11-11, A/B Testing\n2023-01-07, ANOVA\n\n2023-01-27, ANCOVA\n2023-01-27, repeated measures ANOVA\n2023-01-28, MANOVA\n2023-01-28, MANCOVA"
  },
  {
    "objectID": "docs/blog/posts/ML/guide_map/index.html#regression",
    "href": "docs/blog/posts/ML/guide_map/index.html#regression",
    "title": "Blog Guide Map, Machine Learning",
    "section": "Regression",
    "text": "Regression\n\n1111-11-11, Least Square and Simple Linear Regression\n1111-11-11, Multiple Linear Regression\n\n\nGeneralized Linear Models\n\n1111-11-11, Logistic Regression\n1111-11-11, Multinomial Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression\n\n\n\nMixed Models\n\n1111-11-11, Linear Mixed Models"
  },
  {
    "objectID": "docs/blog/posts/ML/guide_map/index.html#generalized-additive-models",
    "href": "docs/blog/posts/ML/guide_map/index.html#generalized-additive-models",
    "title": "Blog Guide Map, Machine Learning",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models"
  },
  {
    "objectID": "docs/blog/posts/ML/guide_map/index.html#survival-analysis",
    "href": "docs/blog/posts/ML/guide_map/index.html#survival-analysis",
    "title": "Blog Guide Map, Machine Learning",
    "section": "Survival Analysis",
    "text": "Survival Analysis\n\n1111-11-11, Cox-Hazard Model"
  },
  {
    "objectID": "docs/blog/posts/Patent/guide_map/index.html",
    "href": "docs/blog/posts/Patent/guide_map/index.html",
    "title": "Content List, Patent",
    "section": "",
    "text": "(Draft)"
  },
  {
    "objectID": "docs/blog/posts/Patent/guide_map/index.html#basic",
    "href": "docs/blog/posts/Patent/guide_map/index.html#basic",
    "title": "Content List, Patent",
    "section": "Basic",
    "text": "Basic"
  },
  {
    "objectID": "docs/blog/posts/Patent/guide_map/index.html#inferencce",
    "href": "docs/blog/posts/Patent/guide_map/index.html#inferencce",
    "title": "Content List, Statistics",
    "section": "Inferencce",
    "text": "Inferencce\n\n1111-11-11, Hypothesis Testing\n2022-12-28, p-values\n1111-11-11, Permutation Test\n1111-11-11, Power\n1111-11-11, Multiple Testing\n1111-11-11, Sample Size Calculation\n1111-11-11, A/B Testing\n2023-01-07, ANOVA\n\n2023-01-27, ANCOVA\n2023-01-27, repeated measures ANOVA\n2023-01-28, MANOVA\n2023-01-28, MANCOVA"
  },
  {
    "objectID": "docs/blog/posts/Patent/guide_map/index.html#regression",
    "href": "docs/blog/posts/Patent/guide_map/index.html#regression",
    "title": "Content List, Statistics",
    "section": "Regression",
    "text": "Regression\n\n1111-11-11, Least Square and Simple Linear Regression\n1111-11-11, Multiple Linear Regression\n\n\nGeneralized Linear Models\n\n1111-11-11, Logistic Regression\n1111-11-11, Multinomial Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression\n\n\n\nMixed Models\n\n1111-11-11, Linear Mixed Models"
  },
  {
    "objectID": "docs/blog/posts/Patent/guide_map/index.html#generalized-additive-models",
    "href": "docs/blog/posts/Patent/guide_map/index.html#generalized-additive-models",
    "title": "Content List, Statistics",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models"
  },
  {
    "objectID": "docs/blog/posts/Patent/guide_map/index.html#survival-analysis",
    "href": "docs/blog/posts/Patent/guide_map/index.html#survival-analysis",
    "title": "Content List, Statistics",
    "section": "Survival Analysis",
    "text": "Survival Analysis\n\n1111-11-11, Cox-Hazard Model"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html",
    "title": "MANOVA",
    "section": "",
    "text": "다변량 분산분석(Multivariate Analysis of Variance, MANOVA)\n\n2개 이상의 종속변수가 있을 경우 집단별 차이를 동시에 검정\n연구의 타당성 증가"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#blog-guide-map-link",
    "title": "ANCOVA",
    "section": "4 Blog Guide Map Link",
    "text": "4 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#example",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#example",
    "title": "MANOVA",
    "section": "2 Example",
    "text": "2 Example\n\n2.1 Load Libraries and Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(faraway)\nlibrary(markdown)\nlibrary(heplots)\nlibrary(HH)\nlibrary(psych)\n\n\n\n\n2.2 Data Description\n\n\nCode\nstr(Skulls)\n\n\n'data.frame':   150 obs. of  5 variables:\n $ epoch: Ord.factor w/ 5 levels \"c4000BC\"<\"c3300BC\"<..: 1 1 1 1 1 1 1 1 1 1 ...\n $ mb   : num  131 125 131 119 136 138 139 125 131 134 ...\n $ bh   : num  138 131 132 132 143 137 130 136 134 134 ...\n $ bl   : num  89 92 99 96 100 89 108 93 102 99 ...\n $ nh   : num  49 48 50 44 54 56 48 48 51 51 ...\n\n\nR console에 ?Skulls를 입력하면 다음과 같은 설명이 나온다.\nMeasurements made on Egyptian skulls from five epochs.\n\nThe epochs correspond to the following periods of Egyptian history:\n\nthe early predynstic period (circa 4000 BC);\nthe late predynatic period (circa 3300 BC);\nthe 12th and 13t dynasties (circa 1850 BC);\nthe Ptolemiac peiod (circa 200 BC);\nthe Roman period(circa 150 AD).\n\n\nThe question is hether the measurements change over time. Non-constant measurements of the skulls over time would indicate interbreeding with immigrant populations. Note that using polynomial contrasts for epoch essentially treats the time points as equally spaced\n즉, skulls 고대 이집트 왕조 부터 로마시대까지 이집트 지역에서 발군된 두개골의 크기를 측정한 데이터 이집트 역사를 5개의 시대로 구분하고 각 시대별로 30개씩의 두개골을 4개의 지표로 측정\n이 data는 5개의 변수와 150개의 samples을 포함한다.\n\nepoch :\nmb :\nbh :\nbl :\nnh :"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#eda",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#eda",
    "title": "MANOVA",
    "section": "3 EDA",
    "text": "3 EDA\n\n3.1 Descriptive Statistics\n\n\nCode\nlibrary(heplots)\n#skulls 고대 이집트 왕조 부터 로마시대까지 이집트 지역에서 발군된 두개골의 크기를 측정한 데이터\n# 이집트 역사를 5개의 시대로 구분하고 각 시대별로 30개씩의 두개골을 4개의 지표로 측정\n# epoch: 이집트의 시대를 5개로 구분, 독립변수\n# mb : 두개골의 폭, 종속 변수\n# bh : 두개골의 높이, 종속 변수\n# bl : 두개골의 길이, 종속 변수\n# nh : 코의 높이, 종속 변수\n\nlibrary(dplyr)\nsample_n(Skulls,10)\n\n\n      epoch  mb  bh  bl nh\n140  cAD150 145 129  89 47\n133  cAD150 135 135  95 56\n131  cAD150 143 120  95 51\n2   c4000BC 125 131  92 48\n15  c4000BC 141 140 100 51\n110  c200BC 131 125  88 48\n125  cAD150 138 127  86 47\n81  c1850BC 132 130  91 52\n16  c4000BC 131 134  97 54\n75  c1850BC 129 142 104 47\n\n\nCode\nattach(Skulls)# Skulls를 작업 경로에 포함시키기\nsearch() # 작업 경로 확인인\n\n\n [1] \".GlobalEnv\"           \"Skulls\"               \"package:psych\"       \n [4] \"package:HH\"           \"package:gridExtra\"    \"package:multcomp\"    \n [7] \"package:TH.data\"      \"package:MASS\"         \"package:survival\"    \n[10] \"package:mvtnorm\"      \"package:latticeExtra\" \"package:grid\"        \n[13] \"package:lattice\"      \"package:heplots\"      \"package:broom\"       \n[16] \"package:car\"          \"package:carData\"      \"package:markdown\"    \n[19] \"package:faraway\"      \"package:forcats\"      \"package:stringr\"     \n[22] \"package:dplyr\"        \"package:purrr\"        \"package:readr\"       \n[25] \"package:tidyr\"        \"package:tibble\"       \"package:ggplot2\"     \n[28] \"package:tidyverse\"    \"tools:quarto\"         \"tools:quarto\"        \n[31] \"package:stats\"        \"package:graphics\"     \"package:grDevices\"   \n[34] \"package:utils\"        \"package:datasets\"     \"package:methods\"     \n[37] \"Autoloads\"            \"package:base\"        \n\n\nCode\n# 종속 변수를 결합시켜 하나의 행렬로 만들기\ny<-cbind(mb,bh,bl,nh)\n# 시대별 두개골  길이의 평균 보기\naggregate(y,by=list(epoch),mean) # 언뜻 보기에 차이가 있는 것 처럼 보임\n\n\n  Group.1       mb       bh       bl       nh\n1 c4000BC 131.3667 133.6000 99.16667 50.53333\n2 c3300BC 132.3667 132.7000 99.06667 50.23333\n3 c1850BC 134.4667 133.8000 96.03333 50.56667\n4  c200BC 135.5000 132.3000 94.53333 51.96667\n5  cAD150 136.1667 130.3333 93.50000 51.36667\n\n\nCode\n# 모집단으로 일반화하기 위해 통계적 검정 시행\nskulls_manova<-manova(y~epoch)\nsummary(skulls_manova)\n\n\n           Df  Pillai approx F num Df den Df    Pr(>F)    \nepoch       4 0.35331    3.512     16    580 4.675e-06 ***\nResiduals 145                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# 시대별 두개골 측정값이 차이가 있는 것으로 보임\n\n# 구체적으로 어느 두개 골 측정값에서 차이가 나는지 확인\nsummary.aov(skulls_manova)\n\n\n Response mb :\n             Df  Sum Sq Mean Sq F value    Pr(>F)    \nepoch         4  502.83 125.707  5.9546 0.0001826 ***\nResiduals   145 3061.07  21.111                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response bh :\n             Df Sum Sq Mean Sq F value  Pr(>F)  \nepoch         4  229.9  57.477  2.4474 0.04897 *\nResiduals   145 3405.3  23.485                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response bl :\n             Df Sum Sq Mean Sq F value    Pr(>F)    \nepoch         4  803.3 200.823  8.3057 4.636e-06 ***\nResiduals   145 3506.0  24.179                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response nh :\n             Df Sum Sq Mean Sq F value Pr(>F)\nepoch         4   61.2  15.300   1.507 0.2032\nResiduals   145 1472.1  10.153               \n\n\nCode\n# nh는 차이가 없는 것으로 보임\n\n## 시간에 따라 두개골 측정이 다르다는 것은 이민족 유입의 혼혈 가능성이 있음\n\ndetach(Skulls)# 작업경로에서 삭제제\n\n\n\n\n3.2 One-Way ANOVA"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#blog-guide-map-link",
    "title": "MANOVA",
    "section": "4 Blog Guide Map Link",
    "text": "4 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-07-anova/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-01-07-anova/index.html#blog-guide-map-link",
    "title": "ANOVA",
    "section": "5 Blog Guide Map Link",
    "text": "5 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-16_normality/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-01-16_normality/index.html#blog-guide-map-link",
    "title": "Normality Check",
    "section": "2 Blog Guide Map Link",
    "text": "2 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANCOVA/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-01-28_MANCOVA/index.html#blog-guide-map-link",
    "title": "MANOVA",
    "section": "1 Blog Guide Map Link",
    "text": "1 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#test",
    "href": "docs/blog/posts/statistics/guide_map/index.html#test",
    "title": "Content List, Statistics",
    "section": "Test",
    "text": "Test\n\n1111-11-11, Hypothesis Testing\n2022-12-28, p-values\n1111-11-11, Permutation Test\n1111-11-11, Power\n1111-11-11, Multiple Testing\n1111-11-11, Sample Size Calculation\n1111-11-11, A/B Testing\n2023-01-07, ANOVA\n\n2023-01-27, ANCOVA\n2023-01-27, repeated measures ANOVA\n2023-01-28, MANOVA"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#준비중",
    "href": "docs/projects/LLFS/eda.html#준비중",
    "title": "EDA",
    "section": "준비중",
    "text": "준비중\nplease, read the English section first."
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#univariable-analysis",
    "href": "docs/projects/LLFS/eda.html#univariable-analysis",
    "title": "EDA",
    "section": "Univariable Analysis",
    "text": "Univariable Analysis\n\nNormality Test\n\n\nCode\n# raw data\nnormality_test_result<-multiple_shapiro_test(all_data)%>%\n    group_by(type)%>%\n    summarise(count=n())%>%\n    mutate(proportion=round(count/sum(count),3),\n    total=sum(count))%>%\n    dplyr::select(type,total,everything())\n    \nnormality_test_result%>%\n    knitr::kable(caption=\"Summary of the Result of Shapiro Wilk Tests on Numeric Variables\")\n\n\n\nSummary of the Result of Shapiro Wilk Tests on Numeric Variables\n\n\ntype\ntotal\ncount\nproportion\n\n\n\n\nnormal\n5002\n5001\n1\n\n\nnot_normal\n5002\n1\n0\n\n\n\n\n\nOut of 5002 numeric variables, the variables following a normal distribution are 5001 (100%) and the ones that do not are 1 (0%).\n\n\nVisualization\n\n16 Variables That Follow Normal Distributions\n\n\nCode\n# 16 numeric variables randomly selected\nnormal_variables<-\n    multiple_shapiro_test(all_data)%>%\n        filter(p_value>0.05)%>%\n            dplyr::select(column_name)%>%\n            pull%>%sample(16)\n\nnormal_data<-\n    all_data%>%\n        dplyr::select(outcome,normal_variables)%>%\n        gather(key=metabolite,value=value,normal_variables)\n\nnormal_data%>%\n    ggplot(aes(x=value))+\n    geom_histogram(aes(y=..density..))+\n    geom_density(color='red',size=1.5)+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%>%\n    ggplot(aes(x=metabolite,y=value))+\n    geom_boxplot()+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%>%\n    ggplot(aes(x=value,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(size=1,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Numeric Variables Following Normal Distribution Grouped by Disease Status\")    \n\n\n\n\n\nCode\nnormal_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\n\n\n16 Variables That Do Not Follow Normal Distributions\n\n\nCode\n# 16 numeric variables randomly selected\nnot_normal_variables<-\n    multiple_shapiro_test(all_data)%>%\n        filter(p_value<0.05)%>%\n            dplyr::select(column_name)%>%\n            pull%>%sample(16)\n\nnot_normal_data<-\n    all_data%>%\n        dplyr::select(outcome,not_normal_variables)%>%\n        gather(key=metabolite,value=value,not_normal_variables)\n\nnot_normal_data%>%\n    ggplot(aes(x=value))+\n    geom_histogram(aes(y=..density..))+\n    geom_density(color='red',size=1.5)+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables not Following Normal Distribution\")\n\n\n\n\n\nCode\nnot_normal_data%>%\n    ggplot(aes(x=metabolite,y=value))+\n    geom_boxplot()+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables not Following Normal Distribution\")\n\n\n\n\n\nCode\nnot_normal_data%>%\n    ggplot(aes(x=value,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(size=1,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(not_normal_data$outcome))))+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Numeric Variables not Following Normal Distribution Grouped by Disease Status\")    \n\n\n\n\n\nCode\nnot_normal_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(not_normal_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables not Following Normal Distribution\")\n\n\n\n\n\nRandomly Sampled 16 variables that do not follow a normal distribution to visualize their distributions."
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#bivariable-analysis",
    "href": "docs/projects/LLFS/eda.html#bivariable-analysis",
    "title": "EDA",
    "section": "Bivariable Analysis",
    "text": "Bivariable Analysis\n\nAD vs Metabolites\n\nHomoscedasticity Test\n\n\nCode\nleven_test_result<-main_statistical_test(in_data=all_data,method=\"levene\",categorical_variable=\"outcome\")\n\nhomo_variable<-leven_test_result%>%\nfilter(p_adjusted>0.05)%>%\ndplyr::select(column_name)%>%\npull()%>%\nunique()\n\nhetero_variable<-leven_test_result%>%\nfilter(p_adjusted<0.05)%>%\ndplyr::select(column_name)%>%\npull()%>%\nunique()\n\nhetero_variable_data<-all_data%>%dplyr::select(outcome,hetero_variable)%>%\n        gather(key=metabolite,value=value,hetero_variable)\n\nhetero_variable_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(hetero_variable_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Metabolites with Heteroscedasticity\")\n\n\n\n\n\n\n\nUnpaired Two Sample Mean t Test\n\n\nCode\nt_test_result<-main_statistical_test(in_data=all_data,method=\"student\",categorical_variable=\"outcome\",homo_variables=homo_variable,hetero_variables=hetero_variable)\n\nmetabolites_associated_AD<-t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    arrange(p_adjusted)%>%\n    dplyr::select(column_name)%>%pull\n\nmetabolites_associated_AD_data<-all_data%>%\n    dplyr::select(outcome,sex,genotype,metabolites_associated_AD)%>%\n    gather(key=metabolite,value=value,metabolites_associated_AD)        \n\nmetabolites_associated_AD_data%>%\n    group_by(outcome)%>%\n    summarise(n=n(),mean=mean(value),sd=sd(value))%>%\n    knitr::kable()\n\n\n\n\n\noutcome\nn\nmean\nsd\n\n\n\n\nnegative\n769230\n0.3457544\n0.9691240\n\n\npositive\n525770\n-0.5343944\n0.8796064\n\n\n\n\n\nCode\ntop_metabolites_associated_AD<-t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    arrange(p_adjusted)%>%head(16)%>%\n    dplyr::select(column_name)%>%pull\n\ntop_metabolites_associated_AD_data<-all_data%>%\ndplyr::select(outcome,top_metabolites_associated_AD)%>%\n        gather(key=metabolite,value=value,top_metabolites_associated_AD)        \n\ntop_metabolites_associated_AD_data%>%\nggplot(aes(x=metabolite,y=value,fill=outcome))+\ngeom_boxplot()+\nscale_fill_manual(values=color_function(length(unique(hetero_variable_data$outcome))))+\ntheme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\nlabs(title=\"Boxplot, Metabolites Associated with AD\")\n\n\n\n\n\n\n\n\nAge vs Catogrical Variables\n\n\nCode\ngetNumericSummaryTable(data=all_data,group_variable='outcome',summary_variable='age')\n\n\n  variable    group summary   n proportion  mean   sd min IQR_min Q1 median\n1  outcome negative     age 594      59.4% 87.89 6.23  67  72.000 84     88\n2  outcome positive     age 406      40.6% 80.38 5.93  65  65.375 77     80\n     Q3 IQR_max max\n1 92.00 104.000 105\n2 84.75  96.375  95\n\n\n\n\nCode\nad_age_summary=getNumericSummaryTable(data=all_data,group_variable=\"outcome\",summary_variable=\"age\")\nsex_age_summary=getNumericSummaryTable(data=all_data,group_variable=\"sex\",summary_variable=\"age\")\ngenotype_age_summary=getNumericSummaryTable(data=all_data,group_variable=\"genotype\",summary_variable=\"age\")\nage_summary=rbind(\n    ad_age_summary,\n    sex_age_summary,\n    genotype_age_summary)\n\n\nAge is known as a strong risk factor of AD or dementia. Human nerve system gets damaged as people are aged and the nerve fibrosis symptoms progress gradually (some reference, to be added after figuring out how to add bibliography in quarto). For this reason, we need to look into how the sample data are distributed in terms of age.\n\n\nCode\nage_summary%>%knitr::kable()\n\n\nThe table above shows the summary statistics of age grouped by the affected status, AD and non AD. The difference of age between the two groups are about -7.51, but their standard deviations are 5.93 and 6.23. Thus, it is hard to say their age in average differ in the affected status because the age variations of the two groups are overlapped. This research has two conflicting characteristics at the population level. First, the participants are the elderly whose age in average is 84.84, which indicates they are likely to develop dementia, an aging disease. Second, the data were collected from the longevity village where people live long and healthy lifes and it is expected that where will be some protective factors against dementia. These two conflicting traits may have contributed to this unclear difference.\nThe glaring difference of age is also shown below. As you can see, the people with a negative status a bit younger than those with a positive one.\n\n\nCode\nplot<- ggarrange(\n    getNumericSummary(data=all_data,group_variable=\"outcome\",summary_variable=\"age\")[[2]],\n    getNumericSummary(data=all_data,group_variable=\"sex\",summary_variable=\"age\")[[2]],\n    getNumericSummary(data=all_data,group_variable=\"genotype\",summary_variable=\"age\")[[2]],\n    ncol=2, nrow=2,legend=\"bottom\")\n\nd=lapply(all_data[,sapply(all_data,function(x)is.numeric(x))],function(x)t.test(x~outcome,data=all_data))\n\nstr(d[[1]])\nlapply(d,function(x)\nc('variable'=names(d),\nx$estimate[1],\nx$estimate[2],\n'p'=x$p.value,\n'lower'=x$conf.int[1],\n'upper'=x$conf.int[2]))\nsd=lapply(d,function(x)c('p'=x$p.value))%>%do.call(\"rbind\",.)%>%as.data.frame%>%mutate(variable=rownames(.))\nnames(sd)[1]='p'\n\nsdf=sd%>%mutate(test=ifelse(p<0.05/5000,1,0))\ntable(sdf$test)\n\n\n\n\nMetabolites vs Categorical Variable\n\nMetabolites vs AD\nt-test\n\n\nCode\n#F-test to test for homogeneity in variances\n#| echo: false\n#| eval: false\nvar.test\n\n\nfunction (x, ...) \nUseMethod(\"var.test\")\n<bytecode: 0x000001c8198a8d30>\n<environment: namespace:stats>\n\n\n\n\nMetabolites vs Genotypes\nOne way Anova\n\n\nMetabolites vs treatment\nOne way Anova\n\n\n\nMetabolites vs Genotypes"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#ad-vs-categorical-variable",
    "href": "docs/projects/LLFS/eda.html#ad-vs-categorical-variable",
    "title": "EDA",
    "section": "AD vs Categorical Variable",
    "text": "AD vs Categorical Variable\n\nAD vs Sex\n\n\nAD vs Genotypes\n\n\nCode\nmetabolites_associated_AD_data%>%\n    group_by(outcome,sex)%>%\n    summarise(n=n(),mean=mean(value),sd=sd(value))%>%\n    knitr::kable()\n\n\n\n\n\noutcome\nsex\nn\nmean\nsd\n\n\n\n\nnegative\nman\n384615\n0.3344609\n0.9468158\n\n\nnegative\nwoman\n384615\n0.3570479\n0.9908026\n\n\npositive\nman\n292670\n-0.4933901\n0.8663950\n\n\npositive\nwoman\n233100\n-0.5858776\n0.8932587\n\n\n\n\n\nCode\nmetabolites_associated_AD_data%>%\n    group_by(outcome,genotype)%>%\n    summarise(n=n(),mean=mean(value),sd=sd(value))%>%\n    knitr::kable()\n\n\n\n\n\noutcome\ngenotype\nn\nmean\nsd\n\n\n\n\nnegative\ne3\n594405\n0.3456064\n0.9642017\n\n\nnegative\ne2\n71225\n0.5066689\n0.9728335\n\n\nnegative\ne4\n103600\n0.2359751\n0.9792923\n\n\npositive\ne3\n428645\n-0.5444384\n0.8763686\n\n\npositive\ne2\n31080\n-0.5691303\n0.8914347\n\n\npositive\ne4\n66045\n-0.4528612\n0.8904509\n\n\n\n\n\nChisquare test\n\n\nAD vs Treatment\nchisquare test"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#multivariate-analysis",
    "href": "docs/projects/LLFS/eda.html#multivariate-analysis",
    "title": "EDA",
    "section": "Multivariate Analysis",
    "text": "Multivariate Analysis"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#section",
    "href": "docs/projects/LLFS/eda.html#section",
    "title": "EDA",
    "section": "",
    "text": "PCA (Principal Component Analysis)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html#linear-algebra",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html#linear-algebra",
    "title": "Blog Guide Map, Mathematics",
    "section": "Linear Algebra",
    "text": "Linear Algebra\n\n1111-11-11, Vector Space\n1111-11-11, Subspace\n1111-11-11, Inner Product\n1111-11-11, Linear Combination\n1111-11-11, Quadratic Form\n1111-11-11, Linear Independence\n1111-11-11, Basis, Dimension, & Rank\n1111-11-11, Outer Product\n1111-11-11, Eigen Value & Eigen Vector\n1111-11-11, Eigen Decomposition\n1111-11-11, Singular Value Decomposition (SVD)\n1111-11-11, Gram-Schmidt\n1111-11-11, Group\n1111-11-11, Orthogonal Matrix\n1111-11-11, Rotation & Group\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html#optimization",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html#optimization",
    "title": "Blog Guide Map, Mathematics",
    "section": "Optimization",
    "text": "Optimization\n\n1111-11-11, Convex Set\n1111-11-11, Convex Function\n1111-11-11, Unconstrained Optimization\n1111-11-11, Non-linear Least Square\n1111-11-11, Largrange Multiplier Method\n\n1111-11-11, Largrange Primal Function\n1111-11-11, Largrange Dual Function\n1111-11-11, KKT conditions\n\n1111-11-11, Gradient Descent Optimizers\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/function/index.html",
    "href": "docs/blog/posts/Mathmatics/function/index.html",
    "title": "Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\\[\ny=f(x;\\alpha)\n\\]\n\n독립변수 (independent variable): \\(x\\), 함수의 출력값을 결정하는 변수로서, feature, decision variable 등으로도 표현\n매개변수 (parameter): \\(\\alpha\\), 함수의 모양을 결정하기 위한 변수\n종속 변수 (dependent variable): \\(y\\) or \\(f(x)\\) 독립변수와 매개 변수에 의해 값이 결정되는 변수\n\n\n\n\n독립 변수와 종속 변수는 각 분야에서 쓰이는 이름이 다르다. 아래 표 참고 (정확하진 않음. 아직도 업데이트 중) See Table 1.\n(The below table may be incorrect still under research)\n\n\nTable 1: Confusing Terms in Data Science\n\n\n\n\n\n\n\n\n\nTerms\nMathmatics\nStatistics\nComputer Science\nData Engineering\n\n\n\n\ngraph\nvisulaized plot\nvisulaized plot\nconnections among entities\ndata structure\n\n\nvariable\nindependent variable\npredictor, experimental variable, explanatory variable\nfeature, input\nattribute, column\n\n\noutcome\ndependent variable\nresponse variable, outcome\noutput, target\nattribute, column\n\n\nRecords\npattern, example\ncase, sample, observation\ninstance, record, row\ninstance, record, row\n\n\n\n\n\n\n\n\n함수, 분수함수, 지수함수, 로그함수, 삼각함수 등\n\n\n\n\n\n2차 함수\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef f(x,a=1,b=2,c=3): #x: independent variable, f(x): dependent variable\n    return a*x**2+b*x+c # a,b,c : parameters\nx=np.linspace(-10,10,1000)\nfig=plt.figure()\nax=plt.axes()\nax.plot(x,f(x))\nax.plot(x,f(x,2,-3,2))\nplt.show()\n\n\n\n\n\n\n\\(f(x)=2x^2-3x+2\\)\n\n함수: \\(f(x;a,b,c)\\)\n종속 변수: \\(f(x)\\)\n독립 변수: \\(x\\)\n매개 변수 (parameter): \\(a=2, \\space b=-3, \\space c=2\\)\n\n예시2\n\nlinear regression\n\n매개변를 수학적으로 최적화하여 데이터에 맞는 직선의 방정식을 찾는 알고리즘\n함수 : \\(f(x;\\mathbf \\beta)=\\beta_1x+\\beta_0\\)\n종속 변수: \\(f(x)=y\\)\n독립 변수: \\(x\\)\n매개 변수 (parameter): \\(\\mathbf \\beta = (\\beta_0, \\space \\beta_1)\\)\n\n\n\n\\[\n\\begin{aligned}\nL(oss)&=\\frac{1}{2}\\sum_{n=1}^{N}(y_n-f(x_n,\\mathbf \\beta))^2 \\\\\n&=\\frac{1}{2}\\sum_{n=1}^{N}(y_n-\\beta_1x_n-\\beta_0)^2\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/function/multivariable_scalar_function.html",
    "href": "docs/blog/posts/Mathmatics/function/multivariable_scalar_function.html",
    "title": "Function - Multivariable Scalar Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\\[\ny=f(\\mathbf x), \\space f:\\mathbb R^n \\rightarrow \\mathbb R\n\\]\n\nMany to One\n예시)\n\n\\(y=f(x_1,x_2)=x_1+x_2, \\space f:\\mathbb R^2\\rightarrow \\mathbb R\\)\n\ny값이 곡면의 형태를 띈다\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom mpl_toolkits import mplot3d\nimport matplotlib.font_manager as mfm\n\nfig = plt.figure(figsize=(7, 7))\nax = plt.axes(projection='3d')\nx1=np.linspace(0,1,100)\nx2=np.linspace(0,1,100)\nx1,x2 = np.meshgrid(x1,x2) # multivariate scalar function 만들 때 사용\n\n#print(x2)  #x 축으로 값이 불변하고 y축으로는 값이 변함\n\nz = 50*(x2 - x1**2)**2 + (2-x1)**2\n#print(z)\n\nax.scatter3D(x1, x2, z, marker='.', color='gray')\nplt.show()\n\n\n\n\n\n\n\n\n\nContour(등고선) Graph: 한쪽 변이 상수로 고정\n\\(f(x,y)=c\\) (\\(x\\) 와 \\(y\\)의 관계가 가려져 있어서 음함수)\nbivariable scalar function \\(f(x_1,x_2)=c\\)\n\n\n\nCode\nfig = plt.figure(figsize=(7, 7))\nax = fig.add_subplot(1, 1, 1)\nax.xaxis.set_tick_params(labelsize=18)\nax.yaxis.set_tick_params(labelsize=18)\n\nx1 = np.linspace(-2, 2, 51)\nx2 = np.linspace(-1, 3, 51)\nX1, X2 = np.meshgrid(x1, x2)\nZ = 50*(X2 - X1**2)**2 + (2-X1)**2\n\ncontours = ax.contour(X1, X2, Z, levels=[30, 200, 600],  colors='k', \n            linestyles=['solid','dashed', 'dotted'])\n\nax.clabel(contours, contours.levels, fmt=\"%d\", inline=True, fontsize=20)\n\nax.set_xlabel(r'$x_1$', fontsize=25)\nax.set_ylabel(r'$x_2$', fontsize=25)\n\n#if file_print == True :\n#    fig.savefig(\"imgs/chap3/fig3-6.png\", dpi=300, bbox_inches='tight')\n#    fig.savefig(\"imgs/chap3/fig3-6.pdf\", format='pdf', bbox_inches='tight')\n#    \nplt.show()\n\n\n\n\n\n\n\nCode\npaths200=contours.collections[0].get_paths()\npaths600=contours.collections[1].get_paths()\n\n\nfig = plt.figure(figsize=(7, 7))\n\nx1 = np.linspace(-2, 2, 51)\nx2 = np.linspace(-1, 3, 51)\nX1, X2 = np.meshgrid(x1, x2)\nZ = 50*(X2 - X1**2)**2 + (2-X1)**2\n\nax = plt.axes(projection='3d')\nax.xaxis.set_tick_params(labelsize=15)\nax.yaxis.set_tick_params(labelsize=15)\nax.zaxis.set_tick_params(labelsize=15)\n\nax.plot_surface(X1, X2, Z, cmap=plt.cm.binary, edgecolor=\"k\")\n#ax.plot_wireframe(X1, X2, Z, cmap=plt.cm.OrRd, edgecolor=\"k\")\n\nax.plot3D(paths200[0].vertices[:,0], paths200[0].vertices[:,1], [200]*paths200[0].vertices.shape[0], \n          lw=2, color='k', linestyle='--')\nax.plot3D(paths200[1].vertices[:,0], paths200[1].vertices[:,1], [200]*paths200[1].vertices.shape[0], \n          lw=2, color='k', linestyle='--')\nax.plot3D(paths200[2].vertices[:,0], paths200[2].vertices[:,1], [200]*paths200[2].vertices.shape[0], \n          lw=2, color='k', linestyle='--')\n\nax.plot3D(paths600[0].vertices[:,0], paths600[0].vertices[:,1], [600]*paths600[0].vertices.shape[0], \n          lw=2, color='w', linestyle='-')\nax.plot3D(paths600[1].vertices[:,0], paths600[1].vertices[:,1], [600]*paths600[1].vertices.shape[0], \n          lw=2, color='w', linestyle='-')\n\nax.text(1.5, -1,  400, r\"$f(x_1,x_2)=200$\", color='k', fontsize=18)\nax.text(1., -1, 800, r\"$f(x_1,x_2)=600$\", color='k', fontsize=18)\n\nax.set_xlabel(r'$x_1$', fontsize=20)\nax.set_ylabel(r'$x_2$', fontsize=20)\nax.set_zlabel(r'$z$', fontsize=20)\n\nax.view_init(50, 80)\n\n#if file_print == True :\n#    fig.savefig(\"imgs/chap3/fig3-5.png\", dpi=300, bbox_inches='tight')\n#    fig.savefig(\"imgs/chap3/fig3-5.pdf\", format='pdf', bbox_inches='tight')\n#    \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/function/univariable_scalar_function.html",
    "href": "docs/blog/posts/Mathmatics/function/univariable_scalar_function.html",
    "title": "Function - Univariable Scalar Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\\[\ny=f(x), \\space f: \\mathbb R \\rightarrow \\mathbb R\n\\]\n\nOne to One\n\\(f:\\mathbb{R} \\rightarrow \\mathbb{R}\\) or \\(y=f(x)\\) 으로 표현\n\n\\(f:\\mathbb{R} \\rightarrow \\mathbb{R}\\) : 입력과 출력의 개수를 직관적으로 알 수 있음\n예시: \\(f(x)=x^2, f(x)=2^x, f(x)=log_3x\\)\n\n\n\n\n\nPolynomials\n\nmonomial: one term, 상수 또는 변수 항 하나로 이루어진 식\n\nex) \\(2,x,x^2,\\frac{x}{3}\\)\n\nbinomial: two terms, 단항식이 덧셈과 뻴셈으로 연결된 식\n\nex) \\(3x^2+2\\), coefficient=3. \\(x\\)= 변수, degree = 2, constant = 2\n\ntrinomial: three terms\nquadrinomial: four terms\nquintinomial: five terms\nmultinomial: more than one terms, a combination of nomials with +/-.\n\nex) \\(x^2+2x+3\\)\n\n\nMultinomial Function\n\n다항식으로 구성된 함수\nex) \\(f(x)=x^2+2x+4\\)\n연속이고 미분 가능\n\nExponential Function\n\n정의 : \\(y=a^x\\) where \\(z>0, \\space a\\not=1\\)\n특징\n\n\\(a>1\\): 양의 방향으로 증가\n\\(0<a<1\\): 양의 방향으로 갈 수록 감소\n지수법칙\n입력되는 x 값에 비해 출력되는 y값이 급격히 변화\n미분 가능 & 연속\nExponent Rule\n\n\nLog Function\n\n정의 : \\(log_ax=c \\leftrightarrow a^c=x\\)\n특징\n\n미분 가능 & 연속\n지수함수와 역함수 관계\n입력되는 x의 변화량에 비해 출력되는 y의 변화량이 작음\n\n\\(y=log_{10}x\\) : x가 10만큼 증가해야 y가 1 증가함\n\\(y=log_{e}x\\) : x가 e(약 2.718)만큼 증가해야 y가 1 증가함\n\n\n\nLogistic Sigmoid Function\n\n정의: \\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\)\n특징\n\n미분 가능 & 연속\n신경망에서 뉴런의 활성을 결정하는 활성함수로 사용\n모든 실수\\(z\\)를 \\(0~1\\) 사이로 변환시킴\n입력값을 확률 값으로 출력하기 위해 사용됨\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/function/test.html",
    "href": "docs/blog/posts/Mathmatics/function/test.html",
    "title": "Kwangmin Kim",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(7, 7))\nax = plt.axes(projection='3d')\nx1=np.linspace(0,1,100)\nx2=np.linspace(0,1,100)\nx1,x2 = np.meshgrid(x1,x2) # multivariate scalar function 만들 때 사용\n\n#print(x2)  #x 축으로 값이 불변하고 y축으로는 값이 변함\n\nz = 50*(x2 - x1**2)**2 + (2-x1)**2\n#print(z)\n\nax.scatter3D(x1, x2, z, marker='.', color='gray')\nplt.show()"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/function/mutivariable_vector_function.html",
    "href": "docs/blog/posts/Mathmatics/function/mutivariable_vector_function.html",
    "title": "Function - Multivariable Vector Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbf{y} & =\\mathbf F(\\mathbf{x}),\\space f:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{n}\\\\\n\\mathbf{F}(\\mathbf{x}) & =(f_{1}(\\mathbf{x}),f_{2}(\\mathbf{x}),...,f_{n}(\\mathbf{x}))^{T},\\mathbf{x}\\in\\mathbb{R}^{m}\n\\end{align*}\n\\]\n\n입력: vector\n출력: vector\n예시\n\n\\(\\mathbf s(u,v)=(u,v,1+u^2+\\frac{v}{1+v^2})^T, 0\\leq u,v\\leq 1\\) \\[\\begin{aligned}\n\\mathbb R^2 &\\rightarrow \\mathbb R^3 \\\\\n(u,v) &\\rightarrow (x,y,z)\n\\end{aligned}\n\\] where \\(x=u, y=1+u^2, z=1+u^2+\\frac{v}{1+v^2}\\)\n\ngraph 표현\n\n\n\n\n\n\n계산그래프\n\n\n\n\n\n\n인공신경망\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/function/univariable_vector_function.html",
    "href": "docs/blog/posts/Mathmatics/function/univariable_vector_function.html",
    "title": "Function - Univariable Vector Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\\[\nr(t)=(f_1(t),f_2(t),...,f_n(t))^T, f:\\mathbb R\\rightarrow \\mathbb R^n\n\\]\n\none to many\n평면 또는 공간에 존재하는 곡선: 시간에 따른 물체의 이동 경로\n입력: 스칼라\n출력: vector\n예시: \\(t\\rightarrow(x,y,z)\\)\n\n\\[\nr(t)=f(x(t),y(y),z(t))=(\\cos(-10t),\\frac{3}{4}t,\\frac{t^2}{6})^T\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom mpl_toolkits import mplot3d\nimport matplotlib.font_manager as mfm\n\n# numpy 출력 형식 지정\nnp.set_printoptions(precision=4, linewidth=150)\n\n# matplotlib 스타일 지정\nmpl.style.use('bmh')\nmpl.style.use('seaborn-whitegrid')\nstyle = plt.style.library['bmh']\n# 스타일 컬러를 쉽게 쓸 수 있도록 리스트 저장\nstyle_colors = [ c['color'] for c in style['axes.prop_cycle'] ]\n\n# 그림을 로컬 폴더에 저장하고 싶으면 True로 수정 \nfile_print = False\n\n\n\n\nCode\nfig = plt.figure(figsize=(7, 7))\nax = plt.axes(projection='3d')\nax.xaxis.set_tick_params(labelsize=15)\nax.yaxis.set_tick_params(labelsize=15)\nax.zaxis.set_tick_params(labelsize=15)\nax.set_xlabel('$x$', fontsize=20)\nax.set_ylabel('$y$', fontsize=20)\nax.set_zlabel('$z$', fontsize=20)\n\nt = np.linspace(0, 2, 101)\nx = np.sin(6*t)\ny = 1/4 * t\nz = t**2 / 2\n\nax.plot3D(x, y, z, c='k')\nax.plot([x[0]],  [y[0]],  [z[0]],  'o', markersize=10, color='k',\n        label=\"t = {:.2f}\".format(t[0]))\nax.plot([x[50]], [y[50]], [z[50]], '^', markersize=10, color='k',\n        label=\"t = {:.2f}\".format(t[50]))\nax.plot([x[-1]], [y[-1]], [z[-1]], '*', markersize=10, color='k',\n        label=\"t = {:.2f}\".format(t[-1])) \n\nax.legend(fontsize=15, loc=\"upper left\")\n    \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "docs/blog/posts/ML/2023-02-03_tesnsor_manipulation/index.html",
    "href": "docs/blog/posts/ML/2023-02-03_tesnsor_manipulation/index.html",
    "title": "Tensor Introduction",
    "section": "",
    "text": "pytorch 이전 까지 deep learning framework로 가장 많이 사용되었다.\n2020년 이후로 pytorch를 더 많이 사용하지만 여전히 많은 사람들이 사용중이다.\n데이터 자료형으로 텐서(tensor) 객체를 사용\nTensorflow에서는 텐서(tensor)를 NumPy 배열처럼 사용할 수 있다.\nGPU 사용 지원\n\n\n\n\nGPU를 사용하면 텐서플로우에서 딥러닝 모델을 더욱 효과적으로 학습할 수 있다.\n각 텐서(tensor)와 연산이 어떠한 장치에 할당되었는지 출력할 수 있다.\n\n\n\nCode\nimport tensorflow as tf\n# placement 함수: 각 텐서와 연산이 어떠한 장치에 할당되었는지 출력하기\ntf.debugging.set_log_device_placement(True)\n\n# 텐서 생성\na = tf.constant([\n    [1, 1],\n    [2, 2]\n])\nb = tf.constant([\n    [5, 6],\n    [7, 8]\n])\n\nc = tf.matmul(a, b)\nprint(c)\n\ntf.debugging.set_log_device_placement(False)\n\n\n\n\nCode\nfrom tensorflow.python.client import device_lib\n# 구체적으로 사용 중인 장치(device) 정보 출력\ndevice_lib.list_local_devices()\n\n\n\n\n\n\nTensorFlow에서의 텐서(tensor)는 기능적으로 넘파이(NumPy)와 매우 유사하다.\n기본적으로 다차원 배열을 처리하기에 적합한 자료구조로 이해할 수 있다.\nTensorFlow의 텐서는 “자동 미분” 기능을 제공한다.\nTensorFlow는 기능적으로 Pytorch와 거의 같음, 하지만 문법이 불편함\nTensorFlow 2.0부터는 pytorch와 문법적으로 유사\n\n\n\n\n\n특징\n\n기본적으로 다차원 배열을 처리하기에 적합한 자료구조로 이해할 수 있다\nTensorFlow에서의 텐서(tensor)는 기능적으로 넘파이(NumPy)의 ndarray 객체와 유사\n기본 python 데이터 유형을 자동 변환 (e.g., list)\nTensorFlow의 텐서는 “자동 미분” 기능을 제공한다.\n\n속성\n\n크기 (shape)\n자료형 (data type)\n저장된 장치, 가속기 메모리에 상주 가능 (e.g., GPU )\n\nNumpy 배열과 tf.Tensor의 차이점\n\n텐서는 가속기 메모리(GPU, TPU)에서 사용 가능\n텐서는 불변성(immutable)\n\n\n\n\n\n\n\nCode\n# 기본적인 모양(shape), 자료형(data type) 출력\n\ndata = [\n    [1, 2],\n    [3, 4]\n]\nx = tf.constant(data) # list -> tensor object로 변환\nprint(x)\nprint(tf.rank(x)) # 축(axis)의 개수 출력 = 차원의 개수 출력\n\ndata = tf.constant(\"String\") # 문자열 (string)도 int형 tensor로 변환 가능\nprint(data)\n\n# NumPy 배열에서 텐서를 초기화할 수 있다.\n\n## 파이썬의 리스트 넘파이는 compatible하다. 상호보완적으로 교체가 가능\n\na = tf.constant([5])\nb = tf.constant([7])\n\nc = (a + b).numpy()\nprint(c)\nprint(type(c))\n\nresult = c * 10\ntensor = tf.convert_to_tensor(result) # numpy -> tensor\nprint(tensor)\nprint(type(tensor))\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\nprint(tf.math.add(1, 2))\nprint(tf.math.add([1, 2], [3, 4]))\nprint(tf.math.square(5))\nprint(tf.math.reduce_sum([1, 2, 3]))\n\n# Operator overloading is also supported\nprint(tf.math.square(2) + tf.math.square(3))\n\ndata = [\n    [1,2],\n    [3,4]\n]\nx = tf.constant(data)\nprint(x)\nprint(x.shape)\nprint(x.dtype)\nprint(tf.rank(x)) # tf.rank() : 축(axis)의 개수 출력 (차원의 개수)\n\n\n\n\n\n\nTensorFlow 연산은 자동으로 NumPy 배열을 텐서(tensor)로 변환\nNumPy 연산은 자동으로 텐서(tensor)를 NumPy 배열로 변환\n\n\n\nCode\nimport numpy as np\nndarray = np.ones([3, 3])\nndarray\n\ntensor = tf.math.multiply(ndarray, 42)\ntensor\nnp.add(tensor, 1)\ntensor.numpy() # numpy.ndarray\ntype(tensor.numpy())\nctensor = tf.constant(ndarray)\nctensor\n\n\n\n\n\n\n텐서(tensor) 객체 생성 (tf.Tensor)\ntf.ones_like(x) : 값이 1이고 x와 shape & data type이 동일한 텐서 생성\n\n\n\nCode\nx = tf.constant([\n    [5, 7],\n    [3, 2]\n])\n\nx_ones = tf.ones_like(x)\nx_ones\n     \n\nx = tf.constant([\n    [5.1, 7.0],\n    [3.4, 2.1]\n])\n\nx_ones = tf.ones_like(x)\nx_ones\n\n# tf.random.uniform(shape, dtype) : 랜덤 값으로 원하는 shape과 dtype을 갖는 텐서 생성\nx_rand = tf.random.uniform(shape=x.shape, dtype=tf.float32)\nx_rand\n\n\n\n\n\n특정 차원 접근\n\n\nCode\ntensor = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8],\n    [9,10,11,12]\n])\n\nprint(tensor[0])       # first row\nprint(tensor[:, 0])    # first column\nprint(tensor[..., -1]) # last column\n\n\n텐서 Concatenate\naxis : 어느 축을 기준으로 객체를 이어붙일지 결정\naxis=0 : 0번째 축 (=row) axis=1 : 1번째 축 (=column)\n\n\nCode\ntensor = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8],\n    [9,10,11,12]\n])\n\ntensor_concat = tf.concat([tensor, tensor, tensor], axis=0) # row\ntensor_concat\n\ntensor_concat = tf.concat([tensor, tensor, tensor], axis=1) # column\ntensor_concat\n\n\n\n\n\n\nCode\na = tf.constant([2])   # dtype: int32\nb = tf.constant([5.0]) # dtype: float32\n\nprint('a dtype: ', a.dtype, '\\nb dtype: ', b.dtype)\n\n\n\n\nCode\na + b # dtype 불일치 -> InvalidArgumentError 발생\n\n\n\n\nCode\ntf.cast(a, tf.float32) + b # a의 dtype을 b의 dtype으로 변환 후 계산\n\n\n\n\n\n\n\nCode\nx = tf.Variable([1,2,3,4,5,6,7,8])\ny = tf.reshape(x, (4,2))           # row=4, col=2\ny\n\n\n\n\n\n\n\nCode\nx.assign_add([1,1,1,1,1,1,1,1])\nprint(x) # 1씩 더해짐\nprint(y) # 값 변화 X\n\n\n\n\n\ntf.transpose(a, perm=[], ...) a의 차원 순서를 바꾼다. perm=[2, 1, 0]일 경우, a의 2번째 축을 첫번째로, 1번째 축을 두번째로, 0번째 축을 세번째로 교환하겠다는 의미\n\n\nCode\na = tf.random.uniform((64, 32, 3))\nprint(a.shape)\n\nb = tf.transpose(a, perm=[2, 1, 0]) # 차원 자체를 교환\nprint(b.shape)\n\n\n\n\n\nelement끼리 연산한다\n\n\nCode\na = tf.constant([\n    [1,2],\n    [3,4]\n])\nb = tf.constant([\n    [1,2],\n    [3,4]\n])\n\nprint(a + b)\nprint(a - b)\nprint(a * b)\nprint(a / b)\n\n\n\n\n\n\n\nCode\na = tf.constant([\n    [1,2],\n    [3,4]\n])\nb = tf.constant([\n    [1,2],\n    [3,4]\n])\ntf.matmul(a, b)\n\n\n\n\n\n차원을 축소하며 평균을 계산\n\ntf.reduce_mean(a, axis=0) : 0차원(행)을 축소하여 평균 계산 -> 각 열에 대한 평균\ntf.reduce_mean(a, axis=1) : 1차원(열)을 축소하여 평균 계산 -> 각 행에 대한 평균\n\n\n\nCode\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\n\nprint(tf.reduce_mean(a))         # a 전체 평균\nprint(tf.reduce_mean(a, axis=0)) # 각 column에 대한 평균\nprint(tf.reduce_mean(a, axis=1)) # 각 row에 대한 평균\n\n\n\n\n\n차원을 축소하며 합계를 계산 (평균과 동일하게 동작)\n\n\nCode\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\n\nprint(tf.reduce_sum(a))         # a 전체 합계\nprint(tf.reduce_sum(a, axis=0)) # 각 column에 대한 합계\nprint(tf.reduce_sum(a, axis=1)) # 각 row에 대한 합계\n\n\n\n\n\n\nmax() : 원소의 최댓값 반환\nargmax() : 최댓값의 index를 반환\n\n\n\nCode\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\n\nprint(tf.reduce_max(a))         # a 전체 원소의 최댓값\nprint(tf.reduce_max(a, axis=0)) # 각 column에 대한 최댓값\nprint(tf.reduce_max(a, axis=1)) # 각 row에 대한 최댓값\nprint(tf.argmax(a, axis=0)) # 각 column에 대한 최댓값의 index\nprint(tf.argmax(a, axis=1)) # 각 row에 대한 최댓값의 index\n\n\n\n차원 축소\n\nsqueeze() : 크기가 1인 차원을 제거\n\n차원 확장\n\nexpand_dims() : 크기가 1인 차원을 추가\n흔히 배치(batch) 차원을 추가하기 위한 목적으로 사용됨\npytorch에서는 차원 축소 시, unsqueeze() 사용\n\n\n\n\nCode\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\nprint('original a shape: ', a.shape)\n\na = tf.expand_dims(a, 0) # 첫번째 축에 차원 추가\nprint('add 0th dims: ', a.shape)\n\na = tf.expand_dims(a, 3) # 세번째 축에 차원 추가\nprint('add 3rd dims: ', a.shape)\n\n\nprint(tf.squeeze(a).shape)         # 크기가 1인 차원을 모두 제거 \nprint(tf.squeeze(a, axis=3).shape) # 세번째 차원을 제거\n\n\ntf.squeeze(a, axis=1) # 제거하려는 차원의 크기가 1이 아닐 경우 오류 발생\n\n\n\n\n\n\n\n기울기 테이프 (Gradient Tape)\n중간 연산들을 테이프에 기록하고 역전파(back propagation)를 수행했을 때 기울기가 계산됨\nTensorFlow에서는 변수가 아닌 상수에 대해 기본적으로 기울기를 측정하지 않음 (not watched). 또한 변수여도 학습 가능하지 않으면(not trainable) 자동 미분을 사용하지 않음\n\n\n\nCode\nx = tf.Variable([3.0, 4.0])\ny = tf.Variable([3.0, 4.0])\n\n# with 구문 안에서 진행되는 모든 연산들을 기록\nwith tf.GradientTape() as tape:\n  z = x + y\n  loss = tf.math.reduce_mean(z)\n\ndx = tape.gradient(loss, x) # loss가 scalar이므로 계산 가능\nprint(dx)\n\n\nTensorFlow에서는 변수가 아닌 상수에 대해 기본적으로 기울기를 측정하지 않음 (not watched). 또한 변수여도 학습 가능하지 않으면(not trainable) 자동 미분을 사용하지 않음\n\n\nCode\nx = tf.linspace(-10, 10, 100) # -10 ~ 10까지 100r개의 데이터 생성\n\nwith tf.GradientTape() as tape:\n  tape.watch(x) # x에 대해 기울기를 측정할거니까 기록해줘\n  y = tf.nn.sigmoid(x)\n\ndx = tape.gradient(y, x)\nprint(dx)\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.plot(x, y, 'r', label=\"y\")\nplt.plot(x, dx, 'b--', label=\"dy/dx\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "docs/blog/posts/ML/2023-02-03_pytorch_overview/index.html",
    "href": "docs/blog/posts/ML/2023-02-03_pytorch_overview/index.html",
    "title": "Pytorch Introduction",
    "section": "",
    "text": "PyTorch는 기계 학습 프레임워크(framework) 중 하나다.\n\nPyTorch의 텐서(tensor)는 NumPy 배열과 매우 유사하다.\nTensor flow 보다 사용 비중이 늘어나고 있다.\nTensor: 고차원 데이터 (배열)를 의미, 3차원 배열 이상\n\nPyTorch를 사용하면, GPU 연동을 통해 효율적으로 딥러닝 모델을 학습할 수 있다.\nGoogle Colab을 이용하면, 손쉽게 PyTorch를 시작할 수 있다.\nGoogle Colab에서는 [런타임] - [런타임 유형 변경]에서 GPU를 선택할 수 있다.\nGoogle Colab에선 pytoch가 내장되어 있기 때문에 따로 설치할 필요 없음\n\n\n\n\n텐서간의 연산을 수행할 때, 기본적으로 두 텐서가 같은 장치에 있어야 한다.\n연산을 수행하는 텐서들을 모두 GPU에 올린 뒤에 연산을 수행한다.\nGPU는 고차원 행렬곱같은 병렬 처리 연산에 최적화 되어 있다.\n\ntensor 자체가 고차원 배열이기 때문에 데이터를 불러오면 tensor 형태로 바꿀 수 있다.\n\n\n\n\n\nCode\n# data initialization: 초기화된 데이터는 gpu에 있음\ndata = [\n  [1, 2],\n  [3, 4]\n]\n\nx = torch.tensor(data) # list를 tensor 형태로 바꾸기. \nprint(x.is_cuda)\n\nx = x.cuda() # CPU -> GPU로 옮기기\nprint(x.is_cuda)\n\nx = x.cpu() # GPU -> CPU로 옮기기\nprint(x.is_cuda)\n\n\n\n서로 다른 장치(device)에 있는 텐서끼리 연산을 수행하면 오류가 발생한다.\n\n\n\nCode\n# GPU 장치의 텐서\na = torch.tensor([\n    [1, 1],\n    [2, 2]\n]).cuda()\n\n# CPU 장치의 텐서\nb = torch.tensor([\n    [5, 6],\n    [7, 8]\n])\n\n# print(torch.matmul(a, b)) # 오류 발생\nprint(torch.matmul(a.cpu(), b))\n\n\n\n\n\n\n\n\n텐서의 기본 속성으로는 다음과 같은 것들이 있다.\n\n모양(shape): 텐서 객체의 차원을 확인할 수 있다. (tensor_var.shape)\n자료형(data type) : 텐서의 기본 자료형은 float type (tensor_var.dtype)\n저장된 장치: CPU인지 GPU인지 확인 (tensor_var.device)\n\n\n\n\nCode\ntensor = torch.rand(3, 4)\n\nprint(tensor)\nprint(f\"Shape: {tensor.shape}\")\nprint(f\"Data type: {tensor.dtype}\")\nprint(f\"Device: {tensor.device}\")\n\n\n\n\n\n\n리스트 데이터에서 직접 텐서를 초기화할 수 있다.\n\n\n\nCode\ndata = [\n  [1, 2],\n  [3, 4]\n]\nx = torch.tensor(data)\n\nprint(x)\n\n\n\nNumPy 배열에서 텐서를 초기화할 수 있다.\n\n\n\nCode\na = torch.tensor([5])\nb = torch.tensor([7])\n\nc = (a + b).numpy() # tensor -> numpy array\nprint(c)\nprint(type(c)) # ndarray: numpy array \n\nresult = c * 10\ntensor = torch.from_numpy(result) # numpy array -> tensor \nprint(tensor)\nprint(type(tensor))\n\n\n\n\n\n\n다른 텐서의 정보를 토대로 텐서를 초기화할 수 있다.\n텐서의 속성은 모양(shape)과 자료형(data type)이 있다\n\n\n\nCode\nx = torch.tensor([\n    [5, 7],\n    [1, 2]\n])\n\n# x와 같은 shape와 data type을 가지지만, 값이 1인 텐서 생성\nx_ones = torch.ones_like(x)\nprint(x_ones)\n# x와 같은 shape를 가지되, 자료형은 float으로 덮어쓰고, 값은 랜덤으로 채우기\nx_rand = torch.rand_like(x, dtype=torch.float32) # uniform distribution [0, 1)\nprint(x_rand)\n\n\n\n\n\n\n\n텐서는 넘파이(NumPy) 배열처럼 조작할 수 있다.\n\n\n\n\n텐서의 원하는 차원에 접근할 수 있다.\n\n\n\nCode\ntensor = torch.tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12]\n])\n\nprint(tensor[0]) # the first row\nprint(tensor[:, 0]) # indexing the first column with all the rows\n# whatever the previous columns are, indexing the last column with all the rows\nprint(tensor[..., -1]) \n\n\n\n\n\n\n두 텐서를 이어 붙여 연결하여 새로운 텐서를 만들 수 있다.\n\n\n\nCode\ntensor = torch.tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12]\n])\n\n# dim: 텐서를 이어 붙이기 위한 축\n# 0번 축(행)을 기준으로 이어 붙이기: 즉, row bind로 연결\nresult = torch.cat([tensor, tensor, tensor], dim=0) \nprint(result)\nprint(result.shape) # 9x4\n\n# 1번 축(열)을 기준으로 이어 붙이기: 즉, column bind로 연결\nresult = torch.cat([tensor, tensor, tensor], dim=1)  \nprint(result)\nprint(result.shape) # 3x12\n\n\n\n\n\n\n텐서의 자료형(정수, 실수 등)을 변환할 수 있다.\n\n\n\nCode\na = torch.tensor([2], dtype=torch.int) # integers\nb = torch.tensor([5.0]) # real numbers\n\nprint(a.dtype)\nprint(b.dtype)\n\n# 텐서 a는 자동으로 float32형으로 형변환 처리\nprint(a + b)\n# 텐서 b를 int32형으로 형변환하여 덧셈 수행\nprint(a + b.type(torch.int32))\n\n\n\n\n\n\nview()는 텐서의 shape를 변경할 때 사용한다.\n이때, 텐서(tensor)의 순서는 변경되지 않는다.\n\n\n\nCode\n# view()는 텐서의 모양을 변경할 때 사용한다.\n# 이때, 텐서(tensor)의 순서는 변경되지 않는다.\na = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8])\nb = a.view(4, 2) # 4*2=8 개, # call by reference\nprint(b)\n\n# a의 값을 변경하면 b도 변경: 메모리 주소값을 공유\na[0] = 7\nprint(b)\n\n# a의 값을 복사(copy)한 뒤에 변경, 새로운 메모리값 할당\nc = a.clone().view(4, 2) # call by value, 아예 다른 객체\na[0] = 9\nprint(c)\n\n\n\n\n\n\n하나의 텐서에서 특정한 차원끼리 순서를 교체할 수 있다.\n\n\n\nCode\na = torch.rand((64, 32, 3))\nprint(a.shape)\n\nb = a.permute(2, 1, 0) # 차원을 바꿔줌\n# (2번째 축, 1번째 축, 0번째 축)의 형태가 되도록 한다.\nprint(b.shape)\n\n\n\n\n\n\n\n\n\n텐서에 대하여 사칙연산 등 기본적인 연산을 수행할 수 있다.\n\n\n\nCode\n# 같은 크기를 가진 두 개의 텐서에 대하여 사칙연산 가능\n# 기본적으로 요소별(element-wise) 연산, 행렬의 연산과 다름\na = torch.tensor([\n    [1, 2],\n    [3, 4]\n])\nb = torch.tensor([\n    [5, 6],\n    [7, 8]\n])\nprint(a + b)\nprint(a - b)\nprint(a * b)\nprint(a / b)\n\n\n\n행렬 곱을 수행할 수 있다.\n\n\n\nCode\na = torch.tensor([\n    [1, 2],\n    [3, 4]\n])\nb = torch.tensor([\n    [5, 6],\n    [7, 8]\n])\n# 행렬 곱(matrix multiplication) 수행\nprint(a.matmul(b))\nprint(torch.matmul(a, b))\n\n\n\n\n\n\n텐서의 평균(mean)을 계산할 수 있다.\n\n\n\nCode\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.mean()) # 전체 원소에 대한 평균\nprint(a.mean(dim=0)) # 각 열에 대하여 평균 계산\nprint(a.mean(dim=1)) # 각 행에 대하여 평균 계산\n\n\n\n\n\n\n텐서의 합계(sum)를 계산할 수 있다.\n\n\n\nCode\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.sum()) # 전체 원소에 대한 합계\nprint(a.sum(dim=0)) # 각 열에 대하여 합계 계산\nprint(a.sum(dim=1)) # 각 행에 대하여 합계 계산\n\n\n\n\n\n\nmax() 함수는 원소의 최댓값을 반환한다.\nargmax() 함수는 가장 큰 원소(최댓값)의 인덱스를 반환한다.\n\n\n\nCode\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.max()) # 전체 원소에 대한 최댓값\nprint(a.max(dim=0)) # 각 열에 대하여 최댓값 계산\nprint(a.max(dim=1)) # 각 행에 대하여 최댓값 계산\n\n\n\n\nCode\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.argmax()) # 전체 원소에 대한 최댓값의 인덱스\nprint(a.argmax(dim=0)) # 각 열에 대하여 최댓값의 인덱스 계산\nprint(a.argmax(dim=1)) # 각 행에 대하여 최댓값의 인덱스 계산\n\n\n\n\n\n\nunsqueeze() 함수는 크기가 1인 차원을 추가한다.\n\n배치(batch) 차원을 추가하기 위한 목적으로 흔히 사용된다.\n\nsqueeze() 함수는 크기가 1인 차원을 제거한다.\n\n\n\nCode\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a.shape)\n\n# 첫 번째 축에 차원 추가\na = a.unsqueeze(0)\nprint(a)\nprint(a.shape)\n\n# 네 번째 축에 차원 추가\na = a.unsqueeze(3)\nprint(a)\nprint(a.shape)\n\n\n\n\nCode\n# 크기가 1인 차원 제거\na = a.squeeze()\nprint(a)\nprint(a.shape)\n\n\n\n\n\n\n\nPyTorch에서는 연산에 대하여 자동 미분을 수행할 수 있다.\n각 텐서 변수에 대해 gradient추적이 가능하여 텐서 연산에 각 텐서 변수의 기울기(민감도)를 추적할 수 있다.\n\n\n\nCode\nimport torch\n\n# requires_grad를 설정할 때만 기울기 추적\nx = torch.tensor([3.0, 4.0], requires_grad=True)\ny = torch.tensor([1.0, 2.0], requires_grad=True)\nz = x + y #z를 연산하는데 x와 y의 민감도를 추적할 수 있다.\n# x or y의 민감도 즉 gradient가 크다는 것은 변수의 값이 조금만 바뀌어도 z값에 큰 영향을 미친다는것을 의미 \n\nprint(z) # [4.0, 6.0]\nprint(z.grad_fn) # 더하기(add), \n# AddBackward: 기울기를 구하는 과정에서 Add를 사용한다. 뭔뜻인지? ㅋ\n# Add를 연산하는 과정에서 기울기를 구하는거 아님?\n\nout = z.mean()\nprint(out) # 5.0\nprint(out.grad_fn) # 평균(mean)\n\nout.backward() # scalar에 대하여 모든 연산의 기울기를 추적 가능\nprint(x.grad) # tensor([0.5000, 0.5000]), 0.5: x의 값이 1만큼 바뀔 때 output값이 0.5만큼 바뀐다는것을 의미\nprint(y.grad) # tensor([0.5000, 0.5000]),\nprint(z.grad) # leaf variable에 대해서만 gradient 추적이 가능하다. 따라서 None.\n\n\n\n일반적으로 모델을 학습할 때는 기울기(gradient)를 추적한다.\n\n왜냐면, 가중치를 기울기에 따라 업데이트 해야하기 때문.\n\n하지만, 학습된 모델을 사용할 때는 파라미터를 업데이트하지 않으므로, 기울기를 추적하지 않는 것이 일반적이다.\n\n\n\nCode\ntemp = torch.tensor([3.0, 4.0], requires_grad=True)# tape,라 부름. 왜?\nprint(temp.requires_grad)\nprint((temp ** 2).requires_grad)\n\n# 기울기 추적을 하지 않기 때문에 계산 속도가 더 빠르다.\nwith torch.no_grad():\n    temp = torch.tensor([3.0, 4.0], requires_grad=True)\n    print(temp.requires_grad)\n    print((temp ** 2).requires_grad)"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html",
    "href": "docs/blog/posts/Language/guide_map/index.html",
    "title": "Content List, Language",
    "section": "",
    "text": "(Draft)"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html#r",
    "href": "docs/blog/posts/Language/guide_map/index.html#r",
    "title": "Content List, Language",
    "section": "R",
    "text": "R\n\n1111-11-11, tidyverse\n\n1111-11-11, dplyr\n1111-11-11, ggplot2\n1111-11-11, tidyr\n1111-11-11, readr\n1111-11-11, purrr\n1111-11-11, tibble\n1111-11-11, stringr\n1111-11-11, forcats\n\n1111-11-11, tidymodels\n1111-11-11, R shiny"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html#python",
    "href": "docs/blog/posts/Language/guide_map/index.html#python",
    "title": "Content List, Language",
    "section": "Python",
    "text": "Python\n\n1111-11-11, numpy\n1111-11-11, pandas\n1111-11-11, matplotlib\n1111-11-11, seaborn\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html#sql",
    "href": "docs/blog/posts/Language/guide_map/index.html#sql",
    "title": "Content List, Language",
    "section": "SQL",
    "text": "SQL\n\nSQLite\n\n\nOracle SQL\n\n\nMS-SQL\n\n\nPostgre SQL"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html#linux",
    "href": "docs/blog/posts/Language/guide_map/index.html#linux",
    "title": "Content List, Language",
    "section": "Linux",
    "text": "Linux"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html#powershell",
    "href": "docs/blog/posts/Language/guide_map/index.html#powershell",
    "title": "Content List, Language",
    "section": "Powershell",
    "text": "Powershell"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html#c",
    "href": "docs/blog/posts/Language/guide_map/index.html#c",
    "title": "Content List, Language",
    "section": "C++",
    "text": "C++"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html#javascript",
    "href": "docs/blog/posts/Language/guide_map/index.html#javascript",
    "title": "Content List, Language",
    "section": "Javascript",
    "text": "Javascript"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_priority_queue/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-20_priority_queue/index.html",
    "title": "Data Structure (9) Priority Queue",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n우선순위 큐는 우선순위에 따라서 데이터를 추출하는 자료구조다.\n컴퓨터 운영체제, 온라인 게임 매칭 등에서 활용된다.\n우선순위 큐는 일반적으로 힙(heap)을 이용해 구현한다.\n\n\n\nTable 1: a list of data structure\n\n\nNumber\nData Structure\n추출되는 데이터\n\n\n\n\n1\nstack\n가장 나중에 삽입된 데이터\n\n\n2\nqueue\n가장 먼저 삽입된 데이터\n\n\n3\npriority queue\n가장 우선 순위가 높은 데이터\n\n\n\n\nSee Table 2.\n\n\n\n\n우선순위 큐는 다양한 방법으로 구현할 수 있다.\n데이터의 개수가 N개일 때, 구현 방식에 따른 시간 복잡도는 다음과 같다.\n삭제 할때는 우선 순위가 가장 높은 것을 찾아야 하기 때문에 \\(O(N)\\) 만큼 소요될 수 있다.\n\n\n\nTable 2: a list of the priority queue building methods in Python\n\n\nNumber\n우선 순위 큐 구현 방식\n삽입 시간\n삭제 시간\n\n\n\n\n1\n리스트 자료형\n\\(O(1)\\)\n\\(O(N)\\)\n\n\n2\n힙 (Heap)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\n\n\n\nSee Table 2.\n\n일반적인 형태의 큐는 선형적인 구조를 가진다.\n반면에 우선순위 큐는 이진 트리(binary tree) 구조를 사용하는 것이 일반적이다.\n\n\n\n\n\n이진 트리(binary tree)는 최대 2개까지의 자식을 가질 수 있다.\n\n\n\n\n포화 이진 트리는 리프 노드를 제외한 모든 노드가 두 자식을 가지고 있는 트리다.\n\n\n\n\n\n완전 이진 트리는 모든 노드가 왼쪽 자식부터 차근차근 채워진 트리다.\n\n\n\n\n\n왼쪽 자식 트리와 오른쪽 자식 트리의 높이가 1 이상 차이 나지 않는 트리다.\n\n\n\n\n\n\n힙(Heap)은 원소들 중에서 최댓값 혹은 최솟값을 빠르게 찾아내는 자료구조다.\n최대 힙(Max Heap): 값이 큰 원소부터 추출한다.\n최소 힙(Min Heap): 값이 작은 원소부터 추출한다.\n힙은 원소의 삽입과 삭제를 위해 \\(O(logN)\\) 의 수행 시간을 요구한다.\n단순한 N개의 데이터를 힙에 넣었다가 모두 꺼내는 작업은 정렬과 동일하다.\n이 경우 시간 복잡도는 \\(O(NlogN)\\) 이다.\n\n\n\n\n최대 힙(max heap)은 부모 노드가 자식 노드보다 값이 큰 완전 이진 트리를 의미한다.\n최대 힙(max heap)의 루트 노드는 전체 트리에서 가장 큰 값을 가진다는 특징이 있다.\n\n\n\n\n\n힙은 완전 이진 트리 자료구조를 따른다.\n힙에서는 우선순위가 높은 노드가 루트(root)에 위치한다.\n\n\n최대 힙(max heap)\n\n\n부모 노드의 키 값이 자식 노드의 키 값보다 항상 크다.\n루트 노드가 가장 크며, 값이 큰 데이터가 우선순위를 가진다.\n\n\n최소 힙(min heap)\n\n\n부모 노드의 키 값이 자식 노드의 키 값보다 항상 작다.\n루트 노드가 가장 작으며, 값이 작은 데이터가 우선순위를 가진다.\n힙의 삽입과 삭제 연산을 수행할 때를 고려해 보자.\n직관적으로, 거슬러 갈 때마다 처리해야 하는 범위에 포함된 원소의 개수가 절반씩 줄어든다.\n따라서 삽입과 삭제에 대한 시간 복잡도는 O logN 이다.\n\n\n\n\n\n(상향식) 부모로 거슬러 올라가며, 부모보다 자신이 더 작은 경우에 위치를 교체한다.\n\n\n\n\n\n(상향식) 부모로 거슬러 올라가며, 부모보다 자신이 더 작은 경우에 위치를 교체한다.\n새로운 원소가 삽입되었을 때 O logN 의 시간 복잡도로 힙 성질을 유지하도록 할 수 있다.\n\n\n\n\n\n원소가 제거되었을 때 O logN 의 시간 복잡도로 힙 성질을 유지하도록 할 수 있다.\n원소를 제거할 때는 가장 마지막 노드가 루트 노드의 위치에 오도록 한다.\n이후에 루트 노드에서부터 하향식으로(더 작은 자식 노드로) Heapify()를 진행한다.\n\n\n\n\n\n파이썬에서는 힙(Heap) 라이브러리를 제공한다.\nheapq 라이브러리의 삽입 및 삭제에 대한 시간 복잡도는 모두 O logN 이다.\n\n\n\n\n단순히 하나의 빈 리스트를 만들면, 그것을 힙(heap) 자료구조로 사용할 수 있다.\n\n\n\nCode\nimport heapq\n\nheap = []\nprint(heap)\n\n\n[]\n\n\n\n\n\n\n원소의 삽입: heappush() 메서드\n원소의 추출: heappop() 메서드\n\n\n\nCode\nimport heapq\n\nheap = []\n\nheapq.heappush(heap, 7)\nheapq.heappush(heap, 4)\nheapq.heappush(heap, 5)\nheapq.heappush(heap, 8)\n\nwhile heap:\n  element = heapq.heappop(heap)\n  print(element, end=\" \")\n\n\n4 5 7 8 \n\n\n\n\n\n\n원소의 삽입: heappush() 메서드\n원소의 추출: heappop() 메서드\n\n\n\nCode\nimport heapq\n\nheap = []\n\nheapq.heappush(heap, 7)\nheapq.heappush(heap, 4)\nheapq.heappush(heap, 5)\nheapq.heappush(heap, 8)\n\nprint(heap[0])\n\n\n4\n\n\n\n\n\n\nheappush() 메서드를 이용해 하나씩 원소를 삽입하지 않을 수 있다.\nheapify() 메서드는 새로운 리스트를 반환하지 않고, 리스트 내부를 직접 수정한다.\n\n\n\nCode\nimport heapq\n\nheap = [9, 1, 5, 4, 3, 8, 7]\nheapq.heapify(heap)\n\nwhile heap:\n  element = heapq.heappop(heap)\n  print(element, end=\" \")\n\n\n1 3 4 5 7 8 9 \n\n\n\n\n\n\n파이썬의 heapq 라이브러리는 기본적으로 최소 힙 기능을 제공한다.\n최대 힙을 위해서는 1 삽입과 2 추출할 때 키(key)에 음수(-) 부호를 취한다.\n\n\n\nCode\nimport heapq\n\narr = [9, 1, 5, 4, 3, 8, 7]\nheap = []\n\nfor x in arr:\n  heapq.heappush(heap, -x)\n\nwhile heap:\n  x = -heapq.heappop(heap)\n  print(x, end=\" \")\n\n\n9 8 7 5 4 3 1 \n\n\n\n\n\n\n단순히 힙에 원소를 넣었다가 꺼내는 것 만으로도 정렬을 수행할 수 있다.\n\n\n\nCode\nimport heapq\n\ndef heap_sort(arr):\n  heap = []\n  for x in arr:\n    heapq.heappush(heap, x)\n\n  result = []\n  while heap:\n    x = heapq.heappop(heap)\n    result.append(x)\n\n  return result\n\nprint(heap_sort([9, 1, 5, 4, 3, 8, 7]))\n\n\n[1, 3, 4, 5, 7, 8, 9]\n\n\n\n\n\n\n최소 힙이나 최대 힙을 사용하여 n번째로 작은 값이나 n번째로 큰 값을 얻을 수 있다.\n힙(heap)을 만든 뒤에 추출(pop) 함수를 n번 호출한다.\n\n\n\nCode\nimport heapq\n\ndef n_smallest(n, arr):\n  heap = []\n  for x in arr:\n    heapq.heappush(heap, x)\n    result = None\n  for _ in range(n):\n    result = heapq.heappop(heap)\n  return result\n\narr = [9, 1, 5, 4, 3, 8, 7]\nprint(n_smallest(3, arr))\n\n\n4\n\n\n\n파이썬에서는 N번째로 작은 값을 구하는 메서드를 제공한다.\nnsmallest() 메서드는 가장 작은 n개의 값을 반환한다.\n\n\n\nCode\nimport heapq\n\narr = [9, 1, 5, 4, 3, 8, 7]\nresult = heapq.nsmallest(3, arr)\nprint(result[-1])\n\n\n4\n\n\n\n\n\n\n파이썬에서는 N번째로 큰 값을 구하는 메서드를 제공한다.\nnlargest() 메서드는 가장 큰 n개의 값을 반환한다.\n\n\n\nCode\nimport heapq\n\narr = [9, 1, 5, 4, 3, 8, 7]\nresult = heapq.nlargest(3, arr)\nprint(result[-1])\n\n\n7\n\n\n\n\nCode\nclass Heap(object):\n    def __init__(self):\n        # 첫번째 원소는 사용하지 않음\n        self.arr = [None]\n\n    # 원소 삽입(push)\n    def push(self, x):\n        # 마지막 위치에 원소를 삽입\n        self.arr.append(x)\n        # 첫 원소인 경우 종료\n        if len(self.arr) == 2:\n            return\n        # 값의 크기를 비교하며 부모를 타고 올라감\n        i = len(self.arr) - 1\n        while True:\n            parent = i // 2\n            # 작은 값을 부모 쪽으로 계속 이동\n            if 1 <= parent and self.arr[parent] > self.arr[i]:\n                self.arr[parent], self.arr[i] = self.arr[i], self.arr[parent]\n                i = parent\n            else:\n                break\n\n    # 원소 추출(pop)\n    def pop(self):\n        # 마지막 원소\n        i = len(self.arr) - 1\n        # 남은 원소가 없다면 종료\n        if i < 1:\n            return None\n        # 루트 원소와 마지막 원소를 교체하여, 마지막 원소 추출\n        self.arr[1], self.arr[i] = self.arr[i], self.arr[1]\n        result = self.arr.pop()\n        # 루트(root)에서부터 원소 정렬\n        self.heapify()\n        return result\n\n    # 루트(root)에서부터 자식 방향으로 내려가며 재정렬\n    def heapify(self):\n        # 남은 원소가 1개 이하라면 종료\n        if len(self.arr) <= 2:\n            return\n        # 루트 원소\n        i = 1\n        while True:\n            # 왼쪽 자식\n            child = 2 * i\n            # 왼쪽 자식과 오른쪽 자식 중 더 작은 것을 선택\n            if child + 1 < len(self.arr):\n                if self.arr[child] > self.arr[child + 1]:\n                    child += 1\n            # 더 이상 자식이 없거나, 적절한 위치를 찾은 경우\n            if child >= len(self.arr) or self.arr[child] > self.arr[i]:\n                break\n            # 원소를 교체하며, 자식 방향으로 내려가기\n            self.arr[i], self.arr[child] = self.arr[child], self.arr[i]\n            i = child\n\n\n\n\nCode\narr = [9, 1, 5, 4, 3, 8, 7]\nheap = Heap()\n\nfor x in arr:\n    heap.push(x)\n\nwhile True:\n    x = heap.pop()\n    if x == None:\n        break\n    print(x, end=\" \")\n\n\n1 3 4 5 7 8 9 \n\n\n\n\nCode\nimport random\nimport time\n\n\n# N개의 무작위 데이터 생성\narr = []\nn = 100000\nfor _ in range(n):\n    arr.append(random.randint(0, 1000000))\n\n# 시간 측정 시작\nstart_time = time.time()\n\n# 힙에 모든 원소 삽입\nheap = Heap()\nfor x in arr:\n    heap.push(x)\n\n# 힙에서 모든 원소 추출\nresult = []\nwhile True:\n    x = heap.pop()\n    if x == None:\n        break\n    result.append(x)\n\n# 시간 측정 종료\nprint(f\"Elapsed time: {time.time() - start_time} seconds.\")\n\n# 오름차순 정렬 여부 확인\nascending = True\nfor i in range(n - 1):\n    if result[i] > result[i + 1]:\n        ascending = False\nprint(\"Sorted:\", ascending)\n\n# 가장 작은 5개 원소와 가장 큰 5개 원소 출력\nprint(result[:5])\nprint(result[-5:])\n\n\nElapsed time: 0.5804822444915771 seconds.\nSorted: True\n[0, 2, 7, 13, 20]\n[999936, 999971, 999984, 999989, 999994]"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/derivatives/2023-02-04_uni_derivative.html",
    "href": "docs/blog/posts/Mathmatics/derivatives/2023-02-04_uni_derivative.html",
    "title": "Differentiation",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nDefinition 1 The slope of line connected with the two points \\(P_1(x_1,y_1) and P_2(x_2,y_2)\\) is \\[\nm=\\frac{y_2-y_1}{x_2-x_1}\n\\]\n\n\nTheorem 1 The point slope equation of line through \\(P_1(x_1,y_1)\\) with slope \\(m\\) is \\[\ny-y_1=m(x-x_1)\n\\]\n\n\nTheorem 2 The slope intercept euation of line with slope m and y-intercept b is \\[\ny=mx+b\n\\]\n\n\\[\n\\begin{aligned}\n\\text{기울기}(slope)&=\\frac{\\text{출력의 변화량}}{\\text{입력의 변화량}}\\\\\n&= \\text{입력 변화량에 대한 출력 변화량} \\\\\n&=\\frac{\\Delta output}{\\Delta input}\\\\\n&= \\text{단위 입력당 출력의 변화량}\\\\\n&= \\text{민감도, 평균 변화율 (Rates Of Change), or etc}.\\\\\n\\end{aligned}\n\\]\n\nDefinition 2 \\[\n\\begin{aligned}\n\\text{평균 변화율}&=\\Delta x\\text{에 대한} \\Delta y\\text{의 비율}\\\\\n&=\\frac{\\Delta y}{\\Delta x}=\\frac{f(b)-f(a)}{b-a}\\\\\n&=\\frac{f(a+\\Delta x)-f(a)}{\\Delta x}\n\\end{aligned}\n\\]\n\n\n입력에 대한 변화율을 조절하고 싶을 때 필요한 개념\n\nex) 시약 농도 대비 증폭의 변화율을 관찰\n\n하지만 관심의 대상의 관계 그래프가 직선이 아닌 곡선의 형태의 경우 평균 변화율이 자세한 정보를 제공해주지 못함\n\nsigmoid 형태의 경우 첫 포인트와 마지막 포인트의 평균 변화율을 보는 것 보다 구간을 짧게하여 여러 군데서 관찰하는 것이 graph를 더 잘 설명하는 것\n\nb와 a의 간격을 충분히 가깝게 하여 관찰 (\\(\\epsilon-\\delta\\) method)\n\n\nDefinition 3 The tangent line to the curve \\(y=f(x)\\) at the point \\(P(a,f(a))\\) is the line through P with slope\n\\[\nm = \\lim_{x\\to a} f(x)\n\\]\nprovided that this limit exists.\n\n\nDefinition 4 When \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\) is continuous and differentiable, the derivative of a function \\(f\\) at a number \\(a \\in \\mathbb R\\), denoted by \\(f'(a)\\), is\n\\[\n\\begin{aligned}\nf'(a) &= \\lim_{h\\to 0} \\frac{f(a+h)-f(a)}{h}\n      &= \\lim_{x\\to a} \\frac{f(x)-f(a)}{x-a}\n\\end{aligned}\n\\]\nprovided that this limit exists. 이때 위의 함수의 극한값, \\(f'(a)\\) 라고도 표시하며 점 \\(a\\) 에서의 \\(f(x)\\) 의 도함수 (derivative) 라고 한다.\n\n\n\n\nJames Stewart - Calculus Early Transcedentals, 7th Eidition, P.143\n\n\n\nDefinition 5 A function \\(f\\) is differentiable at \\(a\\) if \\(f'(a)\\) exists. It is differentiable on an open interval (a,b) [or (a,\\(\\infty\\)), (-\\(\\infty\\),a) or (-\\(\\infty\\),\\(\\infty\\))] if it is differentiable at every number in the interval.\n\n\nTheorem 3 If \\(f\\) is differentiable at \\(a\\), then \\(f\\) is continuous at \\(a\\).\n\n\n순간 변화율 = 미분 계수 = 접선의 기울\n도함수 (derivative) : 특정 포인트에서의 순간 변화율 (값)을 출력하는 함수\n문제를 풀때 도함수를 구하는 것인지, 미분계수를 계산하는 것인지를 구별해야함\n전체 도함수를 구하는것은 보통 굉장히 어려움. 하지만 한점에서의 순간변화율 즉, 미분계수를 구하는 것은 가능\n에러를 줄이는 데에는 값으로 나오는 순간 변화율을 구하는 것이 일반적으로 실현성이 있는 문제\n\n\nDefinition 6 The natural number, \\(e\\) is the number such that \\(\\lim_{h\\to 0} \\frac{e^h-1}{h}=1\\).\n\n모든 지수 함수 \\(f(x)=a^x\\) 중에서 \\(f(x)=e^x\\) 가 점 (0.1) 에서의 접선의 기울기가 \\(f'(0)=1\\) 이 되는 수를 \\(e=2.71828...\\) 라고 정의한다.\n\n\n\\(f'(x)\\) 는 다음과 같은 기호들로도 흔히 표현된다.\n\nLagrange’s notation: \\(y', f'(x)\\)\nLeibniz’s notation: \\(\\frac{dy}{dx}=\\frac{df}{dx}=\\frac{d}{dx}f(x)\\)\nNewton’s notation: \\(\\dot{y}, \\ddot{y}\\)\nEuler’s notation: \\(D_xy, D_xf(x)\\)\n\n\n\n\n다음의 함수를 정의를 이용하여 도함수를 계산하시오\n\n\\(f(x)=c\\) where c is a constant\n\\(f(x)=\\log x\\)\n\\(f(x)=e^x\\)\n\\(f(x)=\\sin x\\)\n\n\nTheorem 4  \n\nThe Power Rule, if \\(n\\) is any real number, then the power function, \\(x^n\\) is differentiated like the following: \\[\n\\frac{d}{dx}(x^n)=nx^{n-1}\n\\]\nThe Constant Multiple Rule, if \\(c\\) is a constand and \\(f\\) is a differentiable function, then \\[\n\\frac{d}{dx}(cf'(x))=c\\frac{d}{dx}f(x)=cf'(x)\n\\]\nThe Sum Rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\n\\frac{d}{dx}[f(x)+g(x)]=\\frac{d}{dx}[f(x)]+\\frac{d}{dx}[g(x)]=f'(x) +g'(x)\n\\]\nThe Difference Rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\n\\frac{d}{dx}[f(x)-g(x)]=\\frac{d}{dx}[f(x)]-\\frac{d}{dx}[g(x)]=f'(x) -g'(x)\n\\]\nThe Product Rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\ny=f(x)g(x), y'=f'(x)g(x)+f(x)g'(x)\n\\]\nThe quotient rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\ny=\\frac{f(x)}{g(x)}, y'=\\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}\n\\]\n\n\n증명은 James Stewart의 Calculus Series 중 1개를 골라 참고하시기 바랍니다.\n\n\n\n\n\\(S(x)=\\frac{1}{1+e^{-ax}}\\) 를 \\(x\\) 에 대해 미분해보시오.\n\\(f(x)=\\alpha_1 + \\frac{\\alpha_2-\\alpha_1}{1+10^{-\\alpha_4(x-\\alpha_3)}}\\) 를 어떻게 미분할 것인지 생각해 보시오.\n\\(y=f(x)=(4x+3)^2\\) 를 \\(x\\) 에 대해 미분해보시오\n\\(y=f(x)=(4x+3)^{20}\\) 를 \\(x\\) 에 대해 어떻게 미분할 것인지 생각해보시오. (hint: composite function - Leibniz)\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#simulation-1",
    "href": "docs/projects/LLFS/data_preparation.html#simulation-1",
    "title": "Data Preparation",
    "section": "4 Simulation",
    "text": "4 Simulation\n\n4.1 Package Loading and Option Settings\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(mixOmics)\nset.seed(20230121) # the date writing this\nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n4.2 Global Variables\n\n\nShow the code\n# the number of samples\nsample_size <- 1000\n# the number of predictors\npredictor_size <- 5000\n# the number of groups\ngroup_size <- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors <- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors<-floor(significant_predictors*0.4) \nnegatively_associated_predictors<-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list<-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%>%round(3) \nnames(group_proportion_list)<-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix <- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data<-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%>%\n        mutate(\n            group_n=(predictor_size*group_proportion_list)%>%round(0), # the number of predictors within each group \n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), # the 1st index of predictors in each group\n            last_index=cumsum(group_n), # the last index of predictors in each group\n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE), # within-group correlations among the within-group predictors\n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) # effect of each group on an outcome variable\n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]<-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]<-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata<-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix<-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients <- rnorm(predictor_size,0,0.05)\n\nanswer_list<-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n4.3 Functions\n\n\nShow the code\n## Function List\n\nscale_function=function(vector=x,min=a,max=b,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data that are statistically associated with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)<-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite<-\n        temp_df%>%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%>%\n        filter(abs_mean_diff==max(abs_mean_diff))%>%\n        dplyr::select(metabolite)%>%pull\n    \n    ## generate age data with min max normalization\n    age_data<-\n        data%>%\n        dplyr::select(strong_metabolite)%>%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%>%\n        rename(age=1)%>%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data that are jointly and statistically associated with a continuous data and a binary data (I am not so sure if I can generate data that are statistically associated with some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n    binary_proportion<-as.numeric(table(in_response)/sample_size) #the simulated proportion for the disease vs non-disease cases\n    genotype_proportion<-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion<-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n4.4 Simulation Algorithm\n\n\nShow the code\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range <- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] <- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) <- 1\n    data[, group_range] <- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] <-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names<-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] <- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]<-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities <- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%>%\n    ifelse(.>1,1,.)%>%\n    ifelse(.<0,0,.)\n\nresponse <-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%>%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data <- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data <- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data <- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data<-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%>%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n4.5 Load Data\n\n\nShow the code\n# load simulation data\nsimulated_data<-read_rds(datapath)\n\n# simple data pre-processing\nall_data<-\n    simulated_data%>%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]<-paste0(\"meta\",1:predictor_size)"
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html#data-description-1",
    "href": "docs/projects/LLFS/data_preparation.html#data-description-1",
    "title": "Data Preparation",
    "section": "5 Data Description",
    "text": "5 Data Description\nThis data include 1000 samples and 5005 variables:\n\nid: sample ID.\noutcome: a disease status (negative, positive), negative is an affected status, positive is an unaffected status, and the reference group is negative.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e2, e4\n\n\nmeta1 ~ meta5000: a list of metabolites that were blood-sampled from the APOE carriers."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html",
    "href": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html",
    "title": "Bayes’ Rule",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nBayes’ rule (베이즈 정리)는 prior probability(사전 확률)과 posterior probability(사후 확률)의 관계를 조건부 확률을 이용하여 확립한 것이다.\n\nprior probability(사전 확률)는 데이터를 얻기 전 연구자의 가설이 들어간 일종의 사건 발생의 신뢰도로 해석하기도 하고 prior probability density function (사전 확률 밀도 함수) 라고도 표현된다.\nposterior probability(사후 확률)는 데이터가 주어진 후 연구자의 가설이 들어간 사건 발생의 신뢰도로 해석하기도 하고 posterior probability desnsity function(사후 확률 밀도 함수) 라고도 표현된다.\n\n좀 더 구체적으로, 2개의 사건 A와 B로 한정시켜 생각해봤을 때, 조건부 확률 \\(P(A|B)\\) 는 각 사건의 확률 \\(P(A), P(B), P(B|A)\\) 를 사용하여 게산될 수 있다. 그래서 베이즈 정리는 \\(P(B|A)\\), \\(P(B|\\overline{A})\\), \\(P(A)\\) 의 정보를 알고있거나 계산 가능할 때 아래와 같은 \\(P(A|B)\\) 의 확률을 구할 수 있는 공식을 제공한다(Equation 1).\n\n\nBayes’ rule을 좀 더 직관적으로 이해하기 위해선 Bayes’ rule와 연관된 친숙한 개념들을 상기시킬 필요가 있다. 우리에게 친숙한 개념인 연역법과 귀납법에 대해서 간단이 살펴본다.\n\n\n\n\n연역법 (deduction method or deductive reasoning)는 하나 (=대전제) 또는 둘 이상의 명제(=대전제+소전제들)를 전제로 하여 명확한 논리에 근거해 새로운 명제(결론)를 도출하는 방법이다. 보통 일반적인 명제에서 구체적인 명제로 도출해내는 방식으로 연역법을 설명하기도 한다. 연역법은 전제와 결론의 타당성보다는 결론을 이끌어내는 논리 전개에 엄격함을 요구한다. 그래서 명쾌한 논리가 보장된다면 연역적 추론의 결론은 그 전제들이 결정적 근거가 되어 전제와 결론이 필연성을 갖게 된다. 따라서, 전제가 진리(=참)이면 결론도 항상 진리(=참)이고 전제가 거짓이면 결론도 거짓으로 도출된다. 하지만, 모든 연역적 추론에서 출발되는 최초의 명제가 결코 연역에 의해 도출될 수 없다는 약점을 갖고있다. 즉, 반드시 검증된 명제를 대전제로 하여 연역적 추리를 시작해야한다. Source: naver encyclopedia -deductive method (cf. 귀류법)\n예를 들어, 아리스토텔레스의 삼단논법의 논리 형식이 가장 많이 인용이 된다. 대전제와 소전제가 하나씩있는 둘 이상의 명제로부터 결론이 도출되는 예를 살펴보자.\n\n대전제: 모든 사람은 죽는다.\n소전제: 소크라테스는 사람이다.\n결론: 그러므로 소크라테스도 죽는다.\n\n\n\n\n귀납법 (Induction method or Inductive reasoning)은 전제와 결론을 뒷받침하는 논리에 의해 그 타당성이 평가된다. 귀납적 추론은 관찰과 실험에서 얻은 특수한 사례 (= data)를 근거로 전체에 적용시키는 귀납적 비약을 통해 이루어진다. 이와 같이 귀납에서 얻어진 결론은 일정한 개연성을 지닐 뿐이며, 특정 data에 따라 귀납적 추론의 타당성에 영향을 미친다. 그러므로, 검증된 data가 많을 수록 신뢰도와 타당성이 증가한다는 특징이 있다.하지만, 귀납적 추론의 결론이 진리인 것은 아니다. Source: naver encyclopedia - inductive method\n\n특수한 사례 (or data): 소크라테스는 죽었다, 플라톤도 죽었다, 아리스토텔레스도 죽었다.\n소전제: 소크라테스는 사람이다.\n결론: 그러므로 소크라테스도 죽는다.\n\n위와 같이 연역적 추론과 귀납적 추론은 서로 반대되는 개념으로 각 각 강점과 약점이 있으며 현실에서는 서로 상호 보완적으로 쓰이고 있다. 따라서, 전제로 삼는 대전제 역시 검증 과정이 필요하고 그 가설에서 몇 개의 명제를 연역해 실험과 관찰 등을 수행하는 가설연역법(hypothetical deductive method)이 널리 쓰이고 있다.\n\n\n\n\n통계학에선 모수(parameter)를 추정하는 여러 방법론들이 있는데 이번 블로그에서는 Frequentism와 Bayeseanism, 이 2가지 방법론에 초점을 둔다.\n\n\n통계학에서 가장 널리쓰이고 있는 방법론으로, 연역법에 근거한 결론 도출 방식을 이용한다. 간단히 말하면, 이미 알려진 분포에서 연구자의 관측치가 발생할 확률을 관찰하여 결론을 유도 하는 방법이다. p-value에 의한 결론 도출방식이 그 대표적인 예이다. 연구자의 데이터가 여러 수학자와 통계학자들이 증명해 놓은 분포하에서 발생한 사실이 입증이 됐을 때 연구자의 관측치가 그 named distribution(like normal distribution)에서 발생할 확률이 낮을 수록 p-value가 작아지고 일정 유의수준에 따라 연구자는 귀무가설을 기각하는 논리방식을 따른다.\n\n\n\n통계학에서 역시 많이 쓰이는 방법론으로, 귀납법에 근거한 결론 도출 방식을 이용한다. 간단히 말하면, 확률을 확률변수가 갖는 sample space에 대한 특정 사건이 발생한 사건의 비로 보는 것 (equally likely라고 가정)이 아니라 내가 설정한 가설에 대한 신뢰도로 바라보는 것이다. 따라서, 사전에 이미 알고있는 데이터가 있어 사전 확률 (prior probability)을 알고있고 이 사전 확률이 추가적인 data에 의해 조정되는 사후 확률 (posterior probability)이 계산된다. 이때 사전 확률자체보다는 추가적인 data와 사후 확률을 계산하는데 사용되는 likelihood의 타당성이 더 중요하다. 더 구체적인 내용은 Bayesean statistics에 기본이 되는 Bayes’ rule에서 살펴보기로 한다.\nsource: Frequentism vs Bayeseanism\n\n\n\n\n\n\nTheorem 1 \\[\n\\begin{aligned}\nP(A|B)&=\\frac{P(B|A)P(A)}{P(B)}\\\\\n      &=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\n\\end{aligned}\n\\tag{1}\\]\n\n\n\\(P(A|B)\\): posterior probability, B(data)가 주어졌을때 가설 A에 대한 신뢰도\n\\(P(A)\\): prior probability, 가설 A에대한 신뢰도\n\\(P(B|A)\\): likelihood, 가설 A가 주어졌을때 B(Data)가 발생할 신뢰도\n\\(P(B)\\): marginal probability, Data의 신뢰도\n\nEquation 1 의 두 분째 등식을 이해하기 위해선, Law of Total Probability (전 확률 법칙) 또는 Total Probability Rule (전 확률 정리)을 이해해야한다.\n\n\n\n\nTheorem 2 Let \\(A_1, A_2, ..., A_k\\) be a set of mutually exclusive and exhaustive events. Let \\(A\\) be a event and a partition of sample space \\(\\Omega\\), then\n\\[\nP(B)=\\sum_{i=1}^{n}P(B|A_i)P(A_i)\n\\]\n\n\n\n\n\n\nFigure 1: Law of Total Probability Example - Two Events\n\n\nSource: Law of ToTal Probability with Proof\n\n\n\n\nFigure 2: Law of Total Probability Example - Multiple Events\n\n\nSource: MIT RES.6-012 Introduction to Probability, Spring 2018 - Youtube\n\n\n\\[\n\\begin{aligned}\nP(B)&=P(B\\cap A) + P(B\\cap \\overline A)\\\\\n    &=P(B\\cap A) + P(B\\cap \\overline A)\\\\\n    &=P(B|A)P(A)+P(B|\\overline A)P(\\overline A)\\\\\nP(A\\cap B)&=P(B|A)P(A)=P(A|B)P(B)\\\\\n\\therefore\nP(A|B)&=\\frac{P(A \\cap B)}{P(B)}\\\\\n      &=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\n\\end{aligned}\n\\tag{2}\\]\nLaw of total probability 를 이용하여 Bayes’ rule이 Equation 2 와 같이 변형되었다. 최종식을 보면 좀 더 직관적인 해석이 가능해지는데 P(B) 가 A와의 교집합 확률의 총합이 되면서 분자가 그 일부가 되는 비율의 개념으로 해석될 수 있다. Figure 1 를 보면 \\(P(A|B)=\\frac{P(B \\cap A)}{P(B)}=\\frac{P(B \\cap A)}{P(B \\cap A)+P(B \\cap \\overline A)}\\) 로 표현되는 것을 볼 수 있다. 그 것을 조금 더 일반화 한 경우는 Figure 2 를 참고하여 유추할 수 있다.\n\n\n\n\\[\n\\begin{aligned}\nP(A|B)&=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\\\\\nP(\\theta|x)&=\\frac{P(x|\\theta)P(\\theta)}{P(x|\\theta)P(\\theta)+P(x|\\overline \\theta)P(\\overline \\theta)}\n\\end{aligned}\n\\]\n많은 참고 문헌에서 사건 A, B를 모수, \\(\\theta\\) 와 data, \\(x\\) 로 표현하기도 한다. 즉, data \\(x\\) 가 주어졌을 때 모수 \\(\\theta\\) 가 발생할 확률이 data에 의해서 update된다.\n\n\\(P(\\theta)\\)\n\nprior probability density function\n데이터없이 초기에 임시로 부여된 모델 또는 모수의 확률\n\n\\(P(x|\\theta)\\)\n\nlikelihood\n초기에 임시로 부여된 모델 또는 모수가 주어졌을 때 data x가 발생할 우도\n좀 더 파격적으로 해석하면, 초기에 임시로 부여된 모델 또는 모수가 data x에 들어맞을(or fittng) 확률\n\n\\(P(x)\\)\n\nmarginal proability\n데이터가 발생할 확률로 \\(\\theta\\) 와 상관없기 때문에 상수로 취급한다.\n\n\\(P(\\theta|x)\\)\n\nposterior probability density function\ndata가 주어졌을 때 모델 또는 모수의 확률\nBayes’ Rule에 의한 최적화에서 다음 최적화 iteration에서 Prior로 쓰인다.\n\n\n\\(P(x)\\) 는 상수이기 때문에 생략가능 하여 아래의 식과 같이 정리 할 수 있다. \\[\nP(\\theta|x)\\propto P(x|\\theta)P(\\theta)\n\\]\n\\(P(\\theta|x)\\) 는 \\(P(x|\\theta)P(\\theta)\\) 에만 영향을 받는 것을 볼 수 있다.\n\n\n\n펭수는 평소 관심이 있던 코니에게서 초콜릿을 선물받았다. 펭수는 초콜릿을 준 코니가 나를 좋아하는지가 궁금하기 때문에 이것을 통계적으로 계산해본다.\n펭수는 먼저 다음 두 상황을 가정한다.\n\n\\(P(like)=0.5\\)\n\n코니가 펭수를 좋아한다는 가설의 신뢰도는 반 반이다. 즉, 정보없는 상태에서의 펭수의 prior probability.\n0.5로 설정한 이유는 다음의 원리를 따랐다. The Principle of Insufficient Reason(이유불충분의 원리- 하나의 사건을 기대할만한 어떤 이유가 없는 경우에는 가능한 모든 사건에 동일한 확률을 할당해야 한다는 원칙).\n\n\\(P(choco)\\)\n\n초콜릿을 받았다라는 data가 발생할 신뢰도\n\n\n펭수는 코니에게 자신을 좋아하는지 알 길이 없으니 사람이 호감이 있을 때에 대한 초콜릿 선물 데이터를 조사하기 시작한다. 즉, 호감의 근거는 초콜릿으로 한정했고 초콜릿 선물 방식의 불확실성을 호감으로 설명하는 문헌을 찾기 시작했다. 그리고 펭수는 도서관에 있는 일반인 100명을 대상으로 초콜릿과 호감과의 관계를 연구한 초콜릿과 호감 논문을 통해 두 가지 정보를 알게된다.\n\n일반적으로, 어떤 사람이 상대방에게 호감이 있어서 초콜릿을 줄 확률은 \\(40%\\) 이다. 즉, \\(P(choco|like)=0.4\\)\n일반적으로, 어떤 사람이 상대방에게 호감이 없지만 예의상 초콜릿을 줄 확률은 \\(30%\\) 이다. 즉, \\(P(choco|\\overline{like})=0.3\\)\n위의 2가지 정보로 유추 가능한 정보\n\n\\(P(\\overline{choco}|like)=0.6\\)\n\\(P(\\overline{choco}|\\overline{like})=0.7\\)\n\n초콜릿에 관한 조사를 토대로 얻은 4가지 정보로 유추할 수 있는 정보\n\n\\(P(choco|like)=0.4\\): like를 받고 있는 50명 중 \\(40%\\) 인 20명은 초콜릿을 받는다.\n\\(P(\\overline{choco}|like)=0.6\\): like를 받고 있는 50명 중 \\(60%\\) 인 30명은 초콜릿을 받지 못한다.\n\\(P(choco|\\overline{like})=0.3\\): like를 받지 않는 50명 중 \\(30%\\) 인 15명은 예의상 준 초콜릿을 받는다.\n\\(P(\\overline{choco}|\\overline{like})=0.7\\): like를 받지 않는 50명 중 \\(70%\\) 인 35명은 초콜릿을 받지 못한다.\n\n\n펭수의 관점으로 정보를 재분류\n\n펭수가 궁금한 정보\n\n\\(P(like|choco)=?\\), posterior probability\n\n펭수가 가정한 정보\n\n\\(P(like)=0.5\\), prior probability by The Principle of Insufficient Reason\n\n펭수가 조사한 정보\n\n\\(P(choco|like)=0.4\\), likelihood\n\\(P(choco)\\): marginal probability\n\n\\(P(choco)=P(choco|like)+P(choco|\\overline{like})=\\frac{20+15}{100}=0.35\\)\n\n\n\n위의 정리한 정보를 Bayes’ rule에 대입하면,\n\\[\nP(like|choco)=\\frac{P(choco|like)\\times P(like)}{P(choco)}=\\frac{0.4\\times 0.5}{0.35}=0.57\n\\]\n펭수의 prior probability(\\(P(A)=0.5\\))가 posterior probability(\\(P(A|B)=0.57\\))로 업데이트 될 수 있다. 초콜릿과 호감 논문을 읽고 코니가 자신을 좋아할 확률이 높아진 것에 대해 기대감을 얻은 용기가 없는 펭수는 100명 보다 더 많은 독립적인 사람들로 실험한 논문을 찾아 다시 자신의 업데이트 된 사전 확률을 계속해서 업데이트할 생각이다. 그리고 자신의 사전 확률을 추가적인 데이터를 갖고 사후 확률로 계속해서 업데이트시켜 정확한 확률을 구한다.\n위의 예시는 영상 자료: 초콜릿을 준 코니의 마음을 시청하고 영감을 얻은 슬기로운 통계생활 tistory에 있는 Source: 베이즈 정리(Bayes’ rule) 완벽히 정리하기 슬기로운 통계생활 블로그를 요약 및 약간의 각색을 한 것이다.\n\n\n\n\n\nTheorem 3 Let \\(A_1, A_2, ..., A_k\\) be a set of mutually exclusive and exhaustive events. Let \\(A\\) be a event, then\n\\[\nP(A_i|B)=\\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^{k}P(B|A_i)P(A_i)}\n\\]\n\n\n\n\nMaximum a posterior estimation는 statistical estimation methods의 큰 기둥 중 하나인 maximum likelihood estimation과 더불어 parameter \\(\\theta\\) 를 추정하는데 많이 사용되는 방법론이다. 사후 확률 밀도 함수 \\(f(x|\\theta)\\) 또는 \\(P(x|\\theta)\\) 를 최대화하는 \\(\\theta\\) 의 추정치를 구하는 방법이며 아래와 같은 argument로 표현할 수 있다.\n\\[\n\\begin{aligned}\n\\hat{\\theta}&=\\arg \\max_{\\theta}\\frac{f(x|\\theta)f(\\theta)}{\\int f(x|\\theta)f(\\theta)}\\\\\n            &\\propto\\arg \\max_{\\theta}f(x|\\theta)f(\\theta)\n\\end{aligned}\n\\]\n최대 우도 추정량과 달리 최대 사후 추정량에는 최대화하는 식에 사전 확률이 추가되어 있는 것을 볼 수 있다. 그러므로 분자 부분인 \\(f(x|\\theta)f(\\theta)\\) 만을 최대화 한다. 분모 부분인 \\(\\int f(x|\\theta)f(\\theta)\\) nomarlizing penalty 또는 constant로 간주한다. 여기서 \\(P(\\theta)\\) 초기 가정치 인데 아무렇게나 설정하기 보다는 good estimate로 설정해야 통계학자들로부터의 공격을 최소화시킬 수 있다. MAP는 나이브 베이즈의 알고리즘의 핵심이다.\n[참고] 최대 우도 추정량 \\[\n\\begin{aligned}\n\\hat{\\theta}=\\arg \\max_{\\theta}L(x|\\theta)=\\arg \\max_{\\theta}\\Pi_{i=1}^{n}f(x|\\theta)\n\\end{aligned}\n\\]\n\n\n\nNaive Bayes에 대한 구체적인 글은 다른 블로그에 소개한다. Naive Bayes는 Bayes’ Rule을 이용해 \\(\\theta\\) 를 최적화 시킨다. Naive Bayes의 Naive는 features 또는 explanotry variables이 서로 conditionally indepdent라고 가정한 것에서 이름 붙여졌다.\n\n\n\n\n\nBayes’ rule provides a formula how to calculate \\(P(A|B)\\) if \\(P(B|A)\\), \\(P(B|\\overline{A})\\), \\(P(A)\\) are available\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html",
    "href": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html",
    "title": "Conditional Probability",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n조건부 확률은 조건이 주어졌을 때 한 사건이 발생할 확률이며 하나의 사건이 다른 사건에 영향을 미쳐 그 확률값이 달라지는 것을 의미한다. 즉, 한 사건, B가 조건으로 주어지고 다른 사건A가 발생할 확률 \\(P(A|B)\\) 가 사건 A가 발생할 확률 \\(P(A)\\) 와 다르다는 것을 의미한다 (\\(P(A)\\not=P(A|B)\\)).\n예를 들어, 주사위를 던질 때 특정 주사위의 눈 (1~6)이 나올 확률은 \\(\\frac{1}{6}\\) 으로 같다 (eqaully likely)라고 가정할 때 주사위의 눈이 나올 수 있는 모든 집합 표본 공간 \\(S\\) 에 대한 특정 주사위의 눈이 나오는 사건 \\(A\\) 가 발생할 확률은 \\(\\frac{n(A)}{n(S)}\\) 와 같다. 다시 말해서, 조건부 확률은 2개 이상의 사건에 대해서 하나의 사건이 다른 사건이 발생할 확률에 영향을 미치는 개념을 말한다. 가장 간단한 2개의 사건 \\(A, B\\) 에 대해서 살펴볼 때 조건부 확률은 다음과 (Equation 1)과 같다.\n\nDefinition 1 If \\(A\\) and \\(B\\) are events in sample space \\(S\\), and \\(P(B)>0\\), then the conditional probability of \\(A\\) given \\(B\\), written \\(P(A|B)\\), is \\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n\\tag{1}\\]\nwhere \\(0 < P(B) \\le 1\\)\n\n위의 정의에서 볼 수 있듯이, sample space \\(S\\) 가 B로 update 되어 P(B|B)=1이 되고 사건 A의 outcome이 B에 관해서 조정된다.\n예를 들어, 사건 \\(A\\) 는 주사위의 눈이 1이 나오는 사건, 사건 \\(B\\) 는 주사위의 눈이 3 이하가 나오는 사건이라고 했을 때 사건 \\(A\\) 가 사건 \\(B\\) 의 부분 집합이므로 두 사건이 서로 독립이 아니다. 즉, $ AB $ 사건에서 주사위의 눈이 1 나오는 경우 밖에 없다. 이렇게 사건 \\(B\\) 가 주어졌을 때 혹은 \\(B\\) 가 먼저 일어났을 때 1이 나올 확률은 달라지게 된다. 즉, \\(A\\) 의 sample space = \\(\\{1,2,3,4,5,6\\}\\) 이고 \\(A|B\\) 의 sample space = \\(\\{1,2,3\\}\\) 이 되기 때문에 \\(P(A) \\not= P(A|B)\\) 가 된다.\n좀 더 구체적으로 계산을 하게 되면, \\(P(A)=\\frac{1}{6}, P(B)=\\frac{3}{6}, P(A\\cap B)=\\frac{1}{6}\\) 일 때,\n\\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}=\\frac{\\frac{1}{6}}{\\frac{3}{6}}=\\frac{1}{3}\n\\]\n인 것을 알 수 있다.\n\nDefinition 2 Independence \\[\n\\begin{aligned}\nP(A)&=P(A|B)\\\\\nP(B)&=P(B|A)\\\\\n\\end{aligned}\n\\tag{2}\\]\nA and B are independent.\n\n사건 A, B가 독립일 때 두 두사건이 동시에 발생할 확률은 \\(P(A \\cap B)=P(A)P(B)=P(B)P(A)\\) 이다. 반면에, 두 사건이 독립이 아니라면 Equation 1 을 이용하여 \\(P(A \\cap B)\\) 는 \\(P(B|A)P(A)\\) 또는 \\(P(A|B)P(B)\\) 로 표현될 수 있다.독립일 때 동시에 발생할 확률에서 \\(P(A)\\) 가 B를 조건으로 봤을 때 동시에 발생할 확률 \\(P(A|B)\\) 로 바뀐 것을 볼 수 있다.\n\nTheorem 1 Multiplicative Rule A,B dependent \\[\n\\begin{aligned}\nP(A\\cap B)&=P(A)P(B|A)\n\\end{aligned}\n\\]\nA,B independent \\[\n\\begin{aligned}\nP(A\\cap B)&=P(A)P(B)\n\\end{aligned}\n\\]\n\n\nTheorem 2 Generalized multiplicative rule $$\n$$\n\n\nTheorem 3 Total Probability Rule $$\n$$\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html",
    "title": "Probability",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nexperiment(실험): 연구 수행 방식.\ntrial (시행): 연구 실험 시행.\nsample space(표본 공간, (\\(\\text{S or } \\Omega\\)): 실험의 가능한 모든 결과의 모음 또는 근원 사상의 집합.\nelement (근원 사상, \\(\\omega\\)): 표본 공간의 원소.\nevent (사건, \\(E\\)): 근원 사상의 집합 또는 표본 공간의 부분 집합.\n\n\n\n\n\n\\(\\omega \\in A\\) : \\(\\omega\\) is an element of a set A\n\\(\\omega \\not\\in A\\) : \\(\\omega\\) is not an element of a set A\n\\(B \\subset A\\) : B is a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\in A\\)\n\\(A = B\\) : \\(B \\subset A\\), \\(A \\subset B\\)\n\\(B \\not\\subset A\\) : B is not a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\not\\in A\\)\n\n\n\n\n확률은 우연 (또는 가능성)과 불확실성에 대한 연구이다. 그 이론은 집합 이론을 기반으로 한다. 고전 확률은 가능성 조합 게임, 이론 오류와 같은 도박과 같다. 확률은 통계학, 경제학, 연산연구, 심리학, 생물학, 역학, 의학 등에 사용된다. 확률을 이해하기 위해 미적분학과 집합론의 지식이 요구되며 관련 학문은 ​​해석학, 측도론, 확률과정 등이 있다. 확률을 정의하려면 사건 수집에 대한 규칙성이 필요하다.\n\n\n\n\n확률은 사건이 발생할 가능성을 나타낸다.\n고전적 정의\n\n\nDefinition 1 The probability of event A is the sum of the probabilities assigned to all sample points in event A. Therefore, \\(0 \\le P(A) \\le 1, P(\\emptyset)=0, P(\\Omega)=1\\). In addition, if \\(A_1, A_2, A_3, ...\\) are mutually exclusive,\n\\[\nP(A_1 \\cup A_2 \\cup A_3 \\dots)=P(A_1) + P(A_2) + P(A_3) + \\dots =\\sum_{i=1}^{\\infty}P(A_i)\n\\]\n\n위의 정의에서 the probabilities assigned to all sample points in event A 의 표현은 사건 A안에 있는 element의 가중치로 해석할 수 있다. 즉, 쉽게 말하면, 확률은 표본 공간, sample space \\(\\Omega\\) 의 원소 (element) \\(\\omega\\) 에 할당된 가중치를 더한 것이다.\n예를 들어, 주사위 모양을 어떤식으로든 조작해 홀수가 짝수보다 2배 더 많이 발생하게끔 만들어 1 번 던질 때 3 보다 작은 수가 나올 사건을 A라고 하면 \\(\\Omega=\\{1,2,3,4,5,6\\}\\) 이고 각 홀수 원소에 가중치가 2배씩 붙기 때문에 홀수 눈이 발생할 확률은 \\(\\frac{2x}{2x+x+2x+x+2x+x}=\\frac{2}{9}\\), 반면에, 짝수의 눈이 나올 확률은 \\(\\frac{x}{2x+x+2x+x+2x+x}=\\frac{1}{9}\\) 이다. 이 때 확률은 위의 정의를 따라야 하므로\n\n\\(0\\le P(evenNumber)=\\frac{2}{9}, P(oddNumber)=\\frac{1}{9}\\le 1\\) 이고\n\\(P(\\Omega=\\{1,2,3,4,5,6\\})=P(evenNumbers)+P(oddNumbers)=1\\)\n\n이므로 확률이라고 할 수있다.\n\n그러므로 \\(P(A={1,2})=P(1)+P(2)=\\frac{2}{9}+\\frac{1}{9}=\\frac{3}{9}\\) 이다.\n\n\nTheorem 1 Countable sample space \\(\\Omega\\) consists of \\(N\\) distinctive equally likely elements (i.e. \\(n(S)=N\\)), and an event \\(A\\) is a subset of the sample space. The event A consists of \\(n\\) distinctive equally likely elements (i.e. \\(n(A)=n\\)). Then \\[\nP(A)=\\frac{n}{N}\n\\]\n\nelement가 오직 동일한 확률로 발생할 때에만 (equally likely), 확률은 \\(\\frac{n(A)}{n(\\Omega \\space or \\space S)}\\) 의 비율(proportion)로 표현될 수 있다.\n예를 들어, 주사위의 눈이 3 보다 작은 수가 나올 사건을 A라고 하면 \\(P(A)= \\frac{n(A)}{n(S)}=\\frac{n(\\{1,2\\})}{n(\\{1,2,3,4,5,6\\})}=\\frac{2}{6}\\) 가 된다.\n\nTheorem 2 If in \\(N\\) identical and independent repeated experiments, an event \\(A\\) happens \\(n\\) times, the the probability of \\(A\\) is defined by \\[\nP(A)=\\lim_{N\\to\\infty}\\frac{n}{N}\n\\]\n\n\nTheorem 3 Basic probability theorem: the complement and additive rule. \\[\n\\begin{aligned}\nP(E^c)&=1-P(E) \\\\\nP(E_1 \\cup E_2)&= P(E_1) + P(E_2) - P(E_1 \\cap E_2) \\\\\n\\end{aligned}\n\\]\n\\(E_1\\) and \\(E_2\\) are mutually exclusive.\n\n\nTheorem 4  \nGeneralized additive rule $$\n\\[\\begin{aligned}\n\n\\end{aligned}\\]\n$$\n\n나머지는 확률 이론 서적을 살펴 보길 바란다.\n\n\n\n\n\n\n\n\nexperiment: the way carry out a study, study design.\ntrial: study experiment trial.\nsample space (\\(\\text{S or } \\Omega\\)): the set of all possible elements (i.e. the collection of all possible outcomes of an experiment).\nelement (\\(\\omega\\)): each outcome of sample space, it is also called ‘sample point’.\nevent (\\(E\\)): a set of sample points or outcomes or a subset of sample space.\n\n\n\n\n\n\\(\\omega \\in A\\) : \\(\\omega\\) is an element of a set A\n\\(\\omega \\not\\in A\\) : \\(\\omega\\) is not an element of a set A\n\\(B \\subset A\\) : B is a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\in A\\)\n\\(A = B\\) : \\(B \\subset A\\), \\(A \\subset B\\)\n\\(B \\not\\subset A\\) : B is not a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\not\\in A\\)\n\n\n\n\nProbability is on study of chance and uncertainty. Its theory builds on set theory. Classic probability is like a gambling of combinatorial games of chance, theory errors. Probability is used in statistics, economics, operation research, psychology, biology, epidemiology, medicine, etc. The prerequisite for probability is calculus and set theory, and the related study is real analysis, measure theory, and stochastic process. To define probability, some regularity on the collection of events is required.\n\n\n\n\nprobability shows the possibility of the occurrence of an event.\nclassic definition ::: {#def-classic}\n\nCountable sample space \\(\\Omgega\\) consists of \\(N\\) distinctive equally likely elements (i.e. \\(n(S)=N\\)), and an event \\(A\\) is a subset of the sample space. The event A consists of \\(n\\) distinctive equally likely elements (i.e. \\(n(A)=n\\)). Then \\[\nP(A)=\\frac{n}{N}\n\\]\n\n\n\nTheorem 5 If in \\(N\\) identical and independent repeated experiments, an event \\(A\\) happens \\(n\\) times, the the probability of \\(A\\) is defined by \\[\nP(A)=\\lim_{N\\to\\infty}\\frac{n}{N}\n\\]\n\n\nThe case of the sample space consisting of \\(N\\) distinctive not equally likely elements,\nThe case of the uncountable sample space\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/ML/2023-02-06_naive_bayes/index.html",
    "href": "docs/blog/posts/ML/2023-02-06_naive_bayes/index.html",
    "title": "Naive Bayes Classification",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n얼굴 인식: 분류기로 얼굴, 코, 입, 눈 등과 같은 여러 특징을 식별\n날씨 예측: 날씨가 좋을지 나쁠지 예측\n의료 진단: 의료 전문가는 나이브 베이즈를 사용하여 환자가 심장병, 암 및 기타 질병과 같은 특정 질병 및 상태에 대한 고위험군인지 여부를 확인\n뉴스 분류: google 뉴스는 뉴스 유형을 분류\n쇼핑: 한 사람이 제품을 구매할지 여부를 예측하기 위해 요일, 할인 및 무료 배송의 특정 조합으로 나이브 베이즈 분류기 사용. 쇼핑한 날이 주중인지 주말인지 공휴일인지 기록하고 지정된 날짜에 대해 할인 및 무료 배송이 있는지 여부를 확인.\n\n\n\n\n\n간단하고 구현하기 쉬움\n훈련 데이터가 많이 필요하지 않음\n연속 데이터와 이산 데이터를 모두 처리\n예측 변수와 데이터 포인트의 수로 확장성이 뛰어남\n빠르고 실시간 예측에 사용\n관련 없는 특성에 민감하지 않음\n텍스트 분류는 나이브 베이즈 분류기의 가장 인기있는 응용 프로그램\n\n\n\n\n나이브 베이즈를 이용하면 조건이 주어질 때의 사건 발생 여부를 에측한다. 즉, 주어진 데이터를 이용해 사건 발생 확률 모형을 생성하고 새로운 데이터가 들어왔을 때 예측을 한다. 예측 결과는 사건이 발생할 확률과 사건이 발생하지 않을 확률이 출력되며 확률 높은 쪽을 선택하여 결과를 출력한다. 주어진 데이터는 조건이라고 가정하고 사건 조건부 발생 활률을 추정하는 것이다.\n조건이 주어졌을 때 조건부 확률을 계산하는 방식은 베이즈 정리를 이용한다.\n\n\n\n\n\n\n\n\n\n\nThe radius of the circle is 10."
  },
  {
    "objectID": "docs/blog/posts/ML/2023-02-06_naive_bayes/text_classification.html",
    "href": "docs/blog/posts/ML/2023-02-06_naive_bayes/text_classification.html",
    "title": "Kwangmin Kim",
    "section": "",
    "text": "# 필요 라이브러리 로딩\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nfrom sklearn.datasets import fetch_20newsgroups\n\ndata = fetch_20newsgroups()\nprint(data.target_names)\n\nModuleNotFoundError: No module named 'seaborn'\n\n\n\n# 모든 카테고리 정의\ncategories =['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', \n             'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', \n             'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', \n             'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', \n             'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', \n             'talk.politics.misc', 'talk.religion.misc']\n# 모든 카테고리 데이터 훈련하기\ntrain = fetch_20newsgroups(subset='train', categories=categories)\n# 모든 카테고리 데이터 테스트하기\ntest = fetch_20newsgroups(subset='test', categories=categories)\n\n\n# 훈련 데이터 보기\nprint(test.data[5])\n# print(train.data[5])\n#print(len(train.data))\n\nFrom: banschbach@vms.ocom.okstate.edu\nSubject: Re: Candida(yeast) Bloom, Fact or Fiction\nOrganization: OSU College of Osteopathic Medicine\nLines: 91\nNntp-Posting-Host: vms.ocom.okstate.edu\n\nIn article <1rp8p1$2d3@usenet.INS.CWRU.Edu>, esd3@po.CWRU.Edu (Elisabeth S. Davidson) writes:\n> \n> In a previous article, banschbach@vms.ocom.okstate.edu () says:\n>>least a few \"enlightened\" physicians practicing in the U.S.  It's really \n>>too bad that most U.S. medical schools don't cover nutrition because if \n>>they did, candida would not be viewed as a non-disease by so many in the \n>>medical profession.\n> \n> Case Western Reserve Med School teaches nutrition in its own section as\n> well as covering it in other sections as they apply (i.e. B12\n> deficiency in neuro as a cause of neuropathy, B12 deficiency in\n> hematology as a cause of megaloblastic anemia), yet I sill\n> hold the viewpoint of mainstream medicine:  candida can cause\n> mucocutaneous candidiasis, and, in already very sick patients\n> with damaged immune systems like AIDS and cancer patients,\n> systemic candida infection.  I think \"The Yeast Connection\" is\n> a bunch of hooey.  What does this have to do with how well\n> nutrition is taught, anyway?\n\nElisabeth, let's set the record straight for the nth time, I have not read \n\"The Yeast Connection\".  So anything that I say is not due to brainwashing \nby this \"hated\" book.  It's okay I guess to hate the book, by why hate me?\nElisabeth, I'm going to quote from Zinsser's Microbiology, 20th Edition.\nA book that you should be familiar with and not \"hate\". \"Candida species \ncolonize the mucosal surfaces of all humans during birth or shortly \nthereafter.  The risk of endogenous infection is clearly ever present.  \nIndeed, candidiasis occurs worldwide and is the most common systemic \nmycosis.\"  Neutrophils play the main role in preventing a systemic \ninfection(candidiasis) so you would have to have a low neutrophil count or \n\"sick\" neutrophils to see a systemic infection.  Poor diet and persistent \nparasitic infestation set many third world residents up for candidiasis.\nYour assessment of candidiasis in the U.S. is correct and I do not dispute \nit.\n\nWhat I posted was a discussion of candida blooms, without systemic \ninfection.  These blooms would be responsible for local sites of irritation\n(GI tract, mouth, vagina and sinus cavity).  Knocking down the bacterial \ncompetition for candida was proposed as a possible trigger for candida \nblooms.  Let me quote from Zinsser's again: \"However, some factors, such as \nthe use of a broad-spectrum antibacterial antibiotic, may predispose to \nboth mucosal and systemic infections\".  I was addressing mucosal infections\n(I like the term blooms better).  The nutrition course that I teach covers \nthis effect of antibiotic treatment as well as the \"cure\".  I guess that \nyour nutrition course does not, too bad.  \n\n\n>>Here is a brief primer on yeast.  Yeast infections, as they are commonly \n>>called, are not truely caused by yeasts.  The most common organism responsible\n>>for this type of infection is Candida albicans or Monilia which is actually a \n>>yeast-like fungus.  \n> \n> Well, maybe I'm getting picky, but I always thought that a yeast\n> was one form that a fungus could exist in, the other being the\n> mold form.  Many fungi can occur as either yeasts or molds, \n> depending on environment.  Candida exibits what is known as\n> reverse dimorphism - it exists as a mold in the tissues\n> but exists as a yeast in the environment.  Should we maybe\n> call it a mold infection?  a fungus infection?  Maybe we\n> should say it is caused by a mold-like fungus.\n>  \n>> \n>>Martin Banschbach, Ph.D.\n>>Professor of Biochemistry and Chairman\n>>Department of Biochemistry and Microbiology\n>>OSU College of Osteopathic Medicine\n>>1111 West 17th St.\n>>Tulsa, Ok. 74107\n>>\n> \n> You're the chairman of Biochem and Micro and you didn't know \n> that a yeast is a form of a fungus?  (shudder)\n> Or maybe you did know, and were oversimplifying?\n\nMy, my Elisabeth, do I detect a little of Steve Dyer in you?  If you \nnoticed my faculty rank, I'm a biochemist, not a microbiologist.\nCandida is classifed as a fungus(according to Zinsser's).  But, as you point \nout, it displays dimorphism.  It is capable of producing yeast cells, \npseudohyphae and true hyphae.  Elisabeth, you are probably a microbiologist \nand that makes a lot of sense to you.  To a biochemist, it's a lot of \nGreek.  So I called it a yeast-like fungus, go ahead and crucify me.\n\nYou know Elisabeth, I still haven't been able to figure out why such a small \nlittle organism like Candida can bring out so much hostility in people in \nSci. Med.  And I must admitt that I got sucked into the mud slinging too.\nI keep hoping that if people will just take the time to think about what \nI've said, that it will make sense.  I'm not asking anyone here to buy into \n\"The Yeast Connection\" book because I don't know what's in that book, plain \nand simple. And to be honest with you, I'm beginning to wish that it was never \nwritten.\n\nMarty B.\n\n\n\n\n# 필수 라이브러리 임포트\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline\n\n# 다항식 나이브베이즈(Multinomial Navie Bayes) 기반 모델 생성\nmodel = make_pipeline(TfidfVectorizer(), MultinomialNB())\n# 훈련데이터로 모델 훈련하기\nmodel.fit(train.data, train.target)\n\n# 테스트 데이터를 위한 레이블 생성하기\nlabels = model.predict(test.data)\n\n\n# 혼동 행렬과 히트 맵 생성하기\nfrom sklearn.metrics import confusion_matrix\nmat = confusion_matrix(test.target, labels)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, \n            xticklabels=train.target_names, yticklabels=train.target_names)\n\n# 혼동 행렬의 히트 맵 플로팅하기\nplt.xlabel('true label')\nplt.xlabel('predicted label')\n\nText(0.5, 12.453125, 'predicted label')\n\n\n\n\n\n\n# 훈련 모델 기반의 새로운 데이터 상 카테고리 예측하기\ndef predict_category(s, train=train, model=model):\n    pred = model.predict([s])\n    return train.target_names[pred[0]]\n\n\npredict_category('Jesus Christ')\n\n'soc.religion.christian'\n\n\n\npredict_category('Sending load to International Space Station')\n\n'sci.space'\n\n\n\npredict_category('Audio is better than BMW')\n\n'sci.electronics'\n\n\n\npredict_category('Prsident of India')\n\n'soc.religion.christian'"
  },
  {
    "objectID": "docs/blog/posts/ML/guide_map/index.html#machine-learning-methods",
    "href": "docs/blog/posts/ML/guide_map/index.html#machine-learning-methods",
    "title": "Content List, Machine Learning",
    "section": "Machine Learning Methods",
    "text": "Machine Learning Methods\n\nSupervised Learning\n\n0000-00=00, [Linear Regression]\n0000-00=00, [Logistic Regression]\n0000-00=00, [Generative Models]\n\n0000-00=00, [Linear Discriminant Analysis]\n0000-00=00, [Quadratic Discriminant Analysis]\n0000-00=00, [Naive Bayes]\n\n0000-00=00, [Resampling Methods]\n0000-00=00, [Regularization]\n0000-00=00, [Smoothing]\n0000-00=00, [Tree Based Methods]\n0000-00=00, [Support Vector Machine]\n0000-00=00, [PCR]\n0000-00=00, [PLS]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n\n\n\nUnupervised Learning\n\n0000-00=00, [PCA]\n0000-00=00, [K means clustering]\n0000-00=00, [Hierarchical Clustering]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/differentiation/2023-02-04_uni_derivative.html",
    "href": "docs/blog/posts/Mathmatics/differentiation/2023-02-04_uni_derivative.html",
    "title": "Differentiation - Univariabe Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n미분은 최적화 문제를 푼다는 의미는 목적함수를 최적화한다는 말이고 그 과정에서 대부분의 경우 미분을 사용하여 상황에 맞게 목적함수의 극소값이나 극대값을 구하게 된다. 예를 들어, 목적 함수 \\(f(x;w)\\) 가 매개변수 또는 가중치 \\(w\\) 에 의해 그 모양이 결정되므로 \\(f(x;w)\\) 를 최소화하는 최적화 문제를 풀고 싶을 때 \\(f(x;w)\\) 를 cost function (=\\(f(x;w)\\) 의 함수) 또는 loss function (=\\(f(x;w)\\) 의 함수) 으로 잘 정의를 한 후 cost function 또는 loss function을 최소화하는 매개변수 \\(w\\) 를 구해야한다. 대부분의 경우, Machine Learning (Deep Learning 포함) 에서 미분은 error를 줄이기 위해 사용한다.\n목적 함수 \\(f(x;w)\\) 가 정의 됐을 때 목적 함수의 극 값을 구하기 위해서 변화량을 관찰해야 한다. 비단 목적함수를 포함한 어떤 현상을 함수로 표현할 때에도 문제를 해결하기 위해 변화량을 관찰하는 경우가 빈번하다.\n예를 들어, 선풍기 바람의 세기를 조절할 때 입력값은 바람 세기 버튼 출력값은 바람의 세기로 가정한다면 선풍기 바람 세기 입력에 따라 적절한 출력값을 갖도록 조정하고 싶을 것이다. 이때 변화량 관찰이 요구된다. 변화량을 잘 대변하는 것이 함수의 기울기이다. 함수의 기울기는 민감도 (sensitivity)로 표현되기도 한다(wiki). 기울기는 아래와 같이 정의된다.\n\n\n\n\n직접 미분\n\n이번 블로그에서 정리한 내용\n수동 미분: 식을 알고 손으로 미분하여 도함수를 얻음\nsymbolic 연산: 기호를 이용하여 미분 연산 \\(\\rightarrow\\) sympy package 이용\n함수 복잡하면 풀기 매우 어려움\n\n간접 미분\n\n수치 미분 (numerical differentiation): 도함수를 몰라도 미분계수를 구하는데 사용\n함수 복잡하면 시간 오래 걸림\n\n자동 미분 (automatic differentiation)\n\n복잡한 함수를 간단한 함수로 쪼개어 직접 미분 - Pytorch\n실무에서, 정합성 검사를 위해 자동 미분의 결과를 간접 미분의 결과와 맞추기도 한다.\n간접 미분이 정답이 된다.\n\n\n\n\n\n\nDefinition 1 The slope of line connected with the two points \\(P_1(x_1,y_1)\\) and \\(P_2(x_2,y_2)\\) is \\[\nm=\\frac{y_2-y_1}{x_2-x_1}\n\\]\n\n\nTheorem 1 The point slope equation of line through \\(P_1(x_1,y_1)\\) with slope \\(m\\) is \\[\ny-y_1=m(x-x_1)\n\\]\n\n\nTheorem 2 The slope intercept euation of line with slope m and y-intercept b is \\[\ny=mx+b\n\\]\n\n\\[\n\\begin{aligned}\n\\text{기울기}(slope)&=\\frac{\\text{출력의 변화량}}{\\text{입력의 변화량}}\\\\\n&= \\text{입력 변화량에 대한 출력 변화량} \\\\\n&=\\frac{\\Delta output}{\\Delta input}\\\\\n&= \\text{단위 입력당 출력의 변화량}\\\\\n&= \\text{민감도, 평균 변화율 (Rates Of Change), or etc}.\\\\\n\\end{aligned}\n\\]\n\nDefinition 2 \\[\n\\begin{aligned}\n\\text{평균 변화율}&=\\Delta x\\text{에 대한} \\Delta y\\text{의 비율}\\\\\n&=\\frac{\\Delta y}{\\Delta x}=\\frac{f(b)-f(a)}{b-a}\\\\\n&=\\frac{f(a+\\Delta x)-f(a)}{\\Delta x}\n\\end{aligned}\n\\]\n\n\n입력에 대한 변화율을 조절하고 싶을 때 필요한 개념\n\nex) 시약 농도 대비 신호 증폭의 변화율을 관찰\n\n하지만 관심의 대상의 관계 그래프가 직선이 아닌 곡선의 형태의 경우 평균 변화율이 대략적인 추세 정보만 제공해줄뿐 변화량의 자세한 정보를 제공해주지 못함\n\nsigmoid 형태의 경우 첫 포인트와 마지막 포인트의 평균 변화율을 보는 것 보다 구간을 짧게하여 여러 군데서 관찰하는 것이 graph의 shape를 더 잘 설명하는 것\n\n이 때, 입력값의 구간을 충분히 짧게 만들어 출력값의 변화량을 관찰하는 것이 미분이다. (limit의 개념, \\(\\epsilon-\\delta\\) method)\n\n\nDefinition 3 The tangent line to the curve \\(y=f(x)\\) at the point \\(P(a,f(a))\\) is the line through P with slope\n\\[\nm = \\lim_{x\\to a} f(x)\n\\]\nprovided that this limit exists.\n\n\nDefinition 4 When \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\) is continuous and differentiable, the derivative of a function \\(f\\) at a number \\(a \\in \\mathbb R\\), denoted by \\(f'(a)\\), is\n\\[\n\\begin{aligned}\nf'(a) &= \\lim_{h\\to 0} \\frac{f(a+h)-f(a)}{h}\\\\\n      &= \\lim_{x\\to a} \\frac{f(x)-f(a)}{x-a}\n\\end{aligned}\n\\]\nprovided that this limit exists. 이때 위의 함수의 극한값, \\(f'(a)\\) 라고도 표시하며 점 \\(a\\) 에서의 \\(f(x)\\) 의 도함수 (derivative) 라고 한다.\n\n\n\n\nJames Stewart - Calculus Early Transcedentals, 7th Eidition, P.143\n\n\n\nDefinition 5 A function \\(f\\) is differentiable at \\(a\\) if \\(f'(a)\\) exists. It is differentiable on an open interval (a,b) [or (a,\\(\\infty\\)), (-\\(\\infty\\),a) or (-\\(\\infty\\),\\(\\infty\\))] if it is differentiable at every number in the interval.\n\n\nTheorem 3 If \\(f\\) is differentiable at \\(a\\), then \\(f\\) is continuous at \\(a\\).\n\n\n순간 변화율 = 미분 계수 = 접선의 기울기\n미분 (differentiation) : 순간 변화율 구하는 행위\n도함수 (derivative) : 도함수 자체는 equation 으로, 특정 포인트에서의 순간 변화율 (값)을 출력하는 함수\n문제를 풀때 도함수를 구하는 것인지, 미분계수를 계산하는 것인지를 구별해야함\n전체 도함수를 구하는것은 보통 굉장히 어려움. 하지만 한점에서의 순간변화율 즉, 미분계수를 구하는 것은 가능\n에러를 줄이는 데에는 값으로 나오는 순간 변화율을 구하는 것이 일반적으로 실현성이 있는 문제\n\n\nDefinition 6 The natural number, \\(e\\) is the number such that \\(\\lim_{h\\to 0} \\frac{e^h-1}{h}=1\\).\n\n모든 지수 함수 \\(f(x)=a^x\\) 중에서 \\(f(x)=e^x\\) 가 점 (0.1) 에서의 접선의 기울기가 \\(f'(0)=1\\) 이 되는 수를 \\(e=2.71828...\\) 라고 정의한다.\n\n\n\\(f'(x)\\) 는 다음과 같은 기호들로도 흔히 표현된다.\n\nLagrange’s notation\n\n\\(y', f'(x)\\)\n어떤 변수로 미분하는지에 대해서 명시적으로 표현되지 않았음. 고등학교때까진 univariable function을 미분 했기떄문에 이 표기법이 많이 사용되었음.\n\nLeibniz’s notation\n\n\\(\\frac{dy}{dx}=\\frac{df}{dx}=\\frac{d}{dx}f(x)\\)\n입력 변수와 출력 변수까지 모두 명시되어있음\n\nNewton’s notation:\n\n\\(\\dot{y}, \\ddot{y}\\)\n최적화 논문과 financial engineering 에서 본적 있음\n\nEuler’s notation\n\n\\(D_xy, D_xf(x)\\)\n시계열 논문과 미분방정식 논문에서 본적 있음\n\n\n\n\n\n다음의 함수를 미분의 정의를 이용하여 도함수를 계산하시오\n\n\\(f(x)=c\\) where c is a constant\n\\(f(x)=\\log x\\)\n\\(f(x)=e^x\\)\n\\(f(x)=\\sin x\\)\n\nDerivative Formula는 모두 미분의 정의를 이용해서 구할 수 있음\n\nTheorem 4  \n\nThe Power Rule, if \\(n\\) is any real number, then the power function, \\(x^n\\) is differentiated like the following: \\[\n\\frac{d}{dx}(x^n)=nx^{n-1}\n\\]\nThe Constant Multiple Rule, if \\(c\\) is a constand and \\(f\\) is a differentiable function, then \\[\n\\frac{d}{dx}(cf'(x))=c\\frac{d}{dx}f(x)=cf'(x)\n\\]\nThe Sum Rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\n\\frac{d}{dx}[f(x)+g(x)]=\\frac{d}{dx}[f(x)]+\\frac{d}{dx}[g(x)]=f'(x) +g'(x)\n\\]\nThe Difference Rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\n\\frac{d}{dx}[f(x)-g(x)]=\\frac{d}{dx}[f(x)]-\\frac{d}{dx}[g(x)]=f'(x) -g'(x)\n\\]\nThe Product Rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\ny=f(x)g(x), y'=f'(x)g(x)+f(x)g'(x)\n\\]\nThe quotient rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\ny=\\frac{f(x)}{g(x)}, y'=\\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}\n\\]\n\n\n증명은 James Stewart의 Calculus Series 중 1개를 골라 참고하시기 바랍니다.\n\n\n\n\n\\(S(x)=\\frac{1}{1+e^{-ax}}\\) 를 \\(x\\) 에 대해 미분해보시오.\n\\(f(x)=\\alpha_1 + \\frac{\\alpha_2-\\alpha_1}{1+e^{-\\alpha_4(x-\\alpha_3)}}\\) 를 어떻게 미분할 것인지 생각해 보시오.\n\\(y=f(x)=(4x+3)^2\\) 를 \\(x\\) 에 대해 미분해보시오\n\\(y=f(x)=(4x+3)^{20}\\) 를 \\(x\\) 에 대해 어떻게 미분할 것인지 생각해보시오. (hint: composite function - Leibniz)\n\n\n\n\n\n\n앞서와 언급한대로 파이썬 sympy package를 사용할 것인데 간단한 예를 본다. 먼저 기호의 정의를 해주고 수학 연산을 진행하면 된다.\n\n\nCode\n# import packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sympy as S\nimport matplotlib as mpl\nfrom mpl_toolkits import mplot3d\nimport matplotlib.font_manager as mfm\n\n\n\n\nCode\n# 심볼 정의\nx=S.Symbol('x')\nh=S.Symbol('h')\nn=S.Symbol('n')\na=S.Symbol('a')\na1=S.Symbol('a1')\na2=S.Symbol('a2')\na3=S.Symbol('a3')\na4=S.Symbol('a4')\n\n## 인수분해\nS.factor(x**2+4*x+4)\n\n\n\\(\\displaystyle \\left(x + 2\\right)^{2}\\)\n\n\n\n인수분해와 값 넣기\n\n\n\nCode\nf=S.factor(x**2-5*x+6)\nprint(f.subs({x:3}))\nprint(f.subs({x:5}))\nf\n\n\n0\n6\n\n\n\\(\\displaystyle \\left(x - 3\\right) \\left(x - 2\\right)\\)\n\n\n\n\\(2^x\\) 미분과 값 넣기\n\n\n\nCode\ndf=S.limit((2**(x+h)-2**x)/h,h,0)\nprint(df.subs({x:3}))\nprint(df.subs({x:5}))\n\nprint(df.subs({x:3}).evalf()) # 정확한 값을 원할 경우\ndf\n\n\n8*log(2)\n32*log(2)\n5.54517744447956\n\n\n\\(\\displaystyle 2^{x} \\log{\\left(2 \\right)}\\)\n\n\n\n\\(x^n\\) 미분\n\n\n\nCode\ndf=S.limit(((x+h)**n-x**n)/h,h,0)\nprint(df.subs({x:3}))\nprint(df.subs({x:5}))\ndf\n\n\n3**(n - 1)*n\n5**(n - 1)*n\n\n\n\\(\\displaystyle n x^{n - 1}\\)\n\n\n\n\n\n\n\\(f(x)=c\\) 의 도함수 where c is a constant. c=2로 고정\n\n\n\nCode\nf=2\ndf=S.limit(((2)-2)/h,h,0)\ndf\n\n\n\\(\\displaystyle 0\\)\n\n\n\n\\(f(x)=\\log x\\) 의 도함수\n\n\n\nCode\nf=S.log(x)\ndf=S.limit((S.log(x+h)-S.log(x))/h,h,0)\ndf\n\n\n\\(\\displaystyle \\frac{1}{x}\\)\n\n\n\n\\(f(x)=e^x\\) 의 도함수\n\n\n\nCode\nf=S.exp(x)\ndf=S.limit((S.exp(x+h)-S.exp(x))/h,h,0)\ndf\n\n\n\\(\\displaystyle e^{x}\\)\n\n\n\n\\(f(x)=\\sin x\\) 의 도함수\n\n\n\nCode\nf=S.sin(x)\ndf=S.limit((S.sin(x+h)-S.sin(x))/h,h,0)\ndf\n\n\n\\(\\displaystyle \\cos{\\left(x \\right)}\\)\n\n\n\n\\(S(x)=\\frac{1}{1+e^{-ax}}\\) 를 \\(x\\) 에 대해 미분해보시오.\n\n\n\nCode\nf=1/(1+S.exp(-a*x))\ndf=S.limit((1/(1+S.exp(-a*(x+h)))-1/(1+S.exp(-a*x)))/h,h,0)\ndf\n\n\n\\(\\displaystyle \\frac{a e^{a x}}{\\left(e^{a x} + 1\\right)^{2}}\\)\n\n\n이번 문제에서 \\(\\frac{ae^x}{(e^{-ax}+1)^2}\\) 라는 sigmoid function의 도함수를 얻었다. 이 도함수를 간단한 수학적 조작으로 다른 표현으로 유도해보면 다음과 같다. \\[\n\\begin{aligned}\n\\frac{d}{dx}S(x)&=\\frac{ae^{-ax}}{(e^{ax}+1)^2}\\\\\n                &=a\\frac{1}{(e^{ax}+1)}\\frac{e^{-ax}}{(e^{ax}+1)}\\\\\n                &=a\\frac{1}{(e^{ax}+1)}\\frac{1+e^{-ax}-1}{(e^{ax}+1)}\\\\\n                &=a\\frac{1}{(e^{ax}+1)}(1-\\frac{1}{(e^{ax}+1)})\\\\\n                &=aS(x)(1-S(x))\\\\\n\\end{aligned}\n\\]\n위와 같이 \\(S(x)\\) 의 도함수는 \\(aS(x)(1-S(x))\\) 로 표현될 수 있다. sigmoid function은 neural network에서 activation function으로 사용되는데 forward propagation에서 이미 한 번 계산이 된다. backward propagation에서 activation function인 \\(S(x)\\) 의 도함수를 다시 연산을 해야하는데 도함수가 \\(aS(x)(1-S(x))\\) 것을 알면 복잡한 고차원 행렬곱 연산을 다시 수행하지 않아도 된다. 그래서 \\(S(x)\\) 의 도함수를 \\(\\frac{ae^x}{(e^{-ax}+1)^2}\\) 라고 코딩하는 것 보다는 \\(S(x)\\) 의 행렬을 재활용하여 \\(aS(x)(1-S(x))\\) 로 코딩해놓으면 연산 과정에서의 시간 복잡도를 줄일 수 있다. 이처럼 machine learning에서 수학적 통계적 지식을 잘 활용하면 좀 더 효율적인 모델링을 구현 할 수 있다.\n\n\nCode\nx1 = np.linspace(-6, 6, 100)\nsx = 1/(1+np.exp(-x1))\nd_sx = sx*(1-sx)\n\nplt.plot(x1,sx,color='black',label='S(x)')\nplt.plot(x1,d_sx,color='red',label='dS(x)')\n\nplt.xlabel('X')\nplt.xlabel('S(x)')\nplt.title('Simgoid Curve with its Derivative')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\\(f(x;\\mathbf\\alpha)=\\alpha_1 + \\frac{\\alpha_2-\\alpha_1}{1+e^{-\\alpha_4(x-\\alpha_3)}}\\) 를 어떻게 미분할 것인지 생각해 보시오.\n\n위의 식은 logistic fucntion의 genral formular 형태인데 sigmoid function이 특수한 예이다. 함수의 shpae는 parameter에 의해 결정되는데 위의 경우 \\(\\alpha_1\\) 은 함수의 최솟값 , \\(\\alpha_2\\) 은 함수의 최댓값, \\(\\alpha_3\\) 은 함수의 변곡점 및 \\(\\alpha_4\\) logistic curve가 변곡점을 지나면서 증가하는 변화율을 묘사한다. sigmoid 형태의 data를 fitting하기 위해 위의 함수를 이용한다면 error를 최소화하는 parameter를 구해야하는데 이 또한 최적화 문제로 4개의 변수에 대한 미분이 필요하다. 2개 이상의 변수에 대해서 미분은 partial derivative (편미분)라고 하는데 다음 블로그에서 다룰 것이다.\n\n\\(y=f(x)=(4x+3)^2\\) 를 \\(x\\) 에 대해 미분해보시오.\n\n\n\nCode\nS.limit(((4*(x+h)+3)**2-(4*x+3)**2)/h,h,0)\n\n\n\\(\\displaystyle 32 x + 24\\)\n\n\n위의 도함수는 미분 공식 중 곱의 법칙을 사용하면 구할 수 있다.\n\n\\(y=f(x)=(4x+3)^{20}\\) 를 \\(x\\) 에 대해 어떻게 미분할 것인지 생각해보시오. (hint: composite function - Leibniz)\n\n위의 문제처럼 곱의 법칙을 사용하면 20개의 인수에 대해서 차례대로 미분을 해야하므로 계산량이 엄청나게 많아진다. 이때 composite function (합성 함수)의 derivative를 구하는 chain rule을 이용하면 간단한 연산으로 도함수를 구할 수 있게 된다. deep learning 모델의 기초인 neural network는 layer nodes이 복잡하게 합성이 되는 합성 함수를 만들면서 forward propagation을 진행하고 backward propabation에서 이 복잡한 합성 함수의 미분을 수행하게 된다. 그러므로 합성 함수의 미분이 어떻게 수행되는지 아는 것은 deep learning을 수리적으로 이해하고 싶은 사람에게 있어서 중요할 수 있다. 합성 함수의 미분은 다른 블로그에서 다루도록 하겠다.\n\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/differentiation/2023-02-10_composite_derivative.html",
    "href": "docs/blog/posts/Mathmatics/differentiation/2023-02-10_composite_derivative.html",
    "title": "Differentiation - Compostite Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\nNeural Network (NN)는 여러 노드(node)가 여러 층(layer)을 갖고 층 사이에 함수 관계가 있는 구조이다. NN의 결과값을 계산할 때 순방향(forward)으로 전 층의 노드가 그 다음 층의 입력값으로 들어가 복잡한 합성 함수가 만들어져 결과값을 출력하고, 학습을 할 때 역방향(backword)으로 그 합성 함수의 미분을 수행하게 된다 (Back Propagation). NN은 deep learning의 기본 구조가 되므로 합성 함수가 무엇이고 어떤 원리로 미분이 되는지 알 필요가 있다.\n\n\n\n\nDefinition 1 The composite function is \\[\nm=\\frac{y_2-y_1}{x_2-x_1}\n\\]\n\n\nTheorem 1 If the fucntions, \\(f(x)\\), \\(g(x)\\), are differentiable and the composite function of \\(f(x)\\) and \\(g(x)\\) is \\(u(x)=g(f(x))\\), \\[\n\\begin{aligned}\nu'(x)&=g'(f(x))f'(x) \\space\\space \\text{or} \\\\\n\\frac{du}{dx} &= \\frac{du}{df}\\frac{df}{dx}\n\\end{aligned}\n\\]\nIt is called ‘Chain Rule’\n\n\\(\\Delta x\\rightarrow \\Delta y \\rightarrow \\Deltaz\\)\n\n\n다음 식들의 도함수를 chain rule에 따라 구해본다.\n\n\\(u(x)=(4x^2+7x)^{50}\\)\n\n\\[\n\\begin{aligned}\n&\\text{when the two functions are }f(x)=4x^2+7x, \\space g(x)=x^{50} \\\\\n&u(x)=(4x^2+7x)^{50}=g(f(x))\\\\\n&f'(x)=8x+7\\\\\n&g'(x)=50x^{49}\\\\\n&u'(x)=g'(f(x))=g'(f(x))f'(x)=50f(x)^{49}(8x+7)=50(4x^2+7x)^{49}(8x+7)\n\\end{aligned}\n\\]\n\\(u=g(x)=\\sqrt{x+1}\\), \\(y=f(x)=x+1\\) ,\n\\[\n\\begin{aligned}\nz&=g(y)=\\sqrt{y+1}\\\\\nz&=g(f(x))=g(x+1)=\\sqrt{(x+1)+1}\\\\\n\\therefore z&=\\sqrt{y+1}=\\sqrt{f(x)}=g(f(x))\n\\end{aligned}\n\\]\n$$\n\\[\\begin{aligned}\n\\lim_{\\Delta x \\to0}\\frac{\\Delta z}{\\Delta x}&=\n\\lim_{\\Delta y \\to0}\\frac{\\Delta z}{\\Delta y}\\lim_{\\Delta x \\to0}\\frac{\\Delta y}{\\Delta x}\\\\&=\\lim_{\\Delta y \\to0}\\frac{\\Delta z}{\\Delta y}\\frac{dy}{dx}\\\\\n\\frac{dz}{dx}&=\\frac{dz}{dy}\\frac{dy}{dx} \\\\\n&(\\because \\Delta x \\to 0 \\Longrightarrow \\Delta y \\to 0 )\n\n\\end{aligned}\\]\n$$\n\nTheorem 2 The slope intercept euation of line with slope m and y-intercept b is \\[\ny=mx+b\n\\]\n\n\\[\n\\begin{aligned}\n\\text{기울기}(slope)&=\\frac{\\text{출력의 변화량}}{\\text{입력의 변화량}}\\\\\n&= \\text{입력 변화량에 대한 출력 변화량} \\\\\n&=\\frac{\\Delta output}{\\Delta input}\\\\\n&= \\text{단위 입력당 출력의 변화량}\\\\\n&= \\text{민감도, 평균 변화율 (Rates Of Change), or etc}.\\\\\n\\end{aligned}\n\\]\n\nDefinition 2 \\[\n\\begin{aligned}\n\\text{평균 변화율}&=\\Delta x\\text{에 대한} \\Delta y\\text{의 비율}\\\\\n&=\\frac{\\Delta y}{\\Delta x}=\\frac{f(b)-f(a)}{b-a}\\\\\n&=\\frac{f(a+\\Delta x)-f(a)}{\\Delta x}\n\\end{aligned}\n\\]\n\n\n입력에 대한 변화율을 조절하고 싶을 때 필요한 개념\n\nex) 시약 농도 대비 신호 증폭의 변화율을 관찰\n\n하지만 관심의 대상의 관계 그래프가 직선이 아닌 곡선의 형태의 경우 평균 변화율이 대략적인 추세 정보만 제공해줄뿐 변화량의 자세한 정보를 제공해주지 못함\n\nsigmoid 형태의 경우 첫 포인트와 마지막 포인트의 평균 변화율을 보는 것 보다 구간을 짧게하여 여러 군데서 관찰하는 것이 graph의 shape를 더 잘 설명하는 것\n\n이 때, 입력값의 구간을 충분히 짧게 만들어 출력값의 변화량을 관찰하는 것이 미분이다. (limit의 개념, \\(\\epsilon-\\delta\\) method)\n\n\nDefinition 3 The tangent line to the curve \\(y=f(x)\\) at the point \\(P(a,f(a))\\) is the line through P with slope\n\\[\nm = \\lim_{x\\to a} f(x)\n\\]\nprovided that this limit exists.\n\n\nDefinition 4 When \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\) is continuous and differentiable, the derivative of a function \\(f\\) at a number \\(a \\in \\mathbb R\\), denoted by \\(f'(a)\\), is\n\\[\n\\begin{aligned}\nf'(a) &= \\lim_{h\\to 0} \\frac{f(a+h)-f(a)}{h}\n      &= \\lim_{x\\to a} \\frac{f(x)-f(a)}{x-a}\n\\end{aligned}\n\\]\nprovided that this limit exists. 이때 위의 함수의 극한값, \\(f'(a)\\) 라고도 표시하며 점 \\(a\\) 에서의 \\(f(x)\\) 의 도함수 (derivative) 라고 한다.\n\n\n\n\nJames Stewart - Calculus Early Transcedentals, 7th Eidition, P.143\n\n\n\nDefinition 5 A function \\(f\\) is differentiable at \\(a\\) if \\(f'(a)\\) exists. It is differentiable on an open interval (a,b) [or (a,\\(\\infty\\)), (-\\(\\infty\\),a) or (-\\(\\infty\\),\\(\\infty\\))] if it is differentiable at every number in the interval.\n\n\nTheorem 3 If \\(f\\) is differentiable at \\(a\\), then \\(f\\) is continuous at \\(a\\).\n\n\n순간 변화율 = 미분 계수 = 접선의 기울기\n미분 (differentiation) : 순간 변화율 구하는 행위\n도함수 (derivative) : 도함수 자체는 equation 으로, 특정 포인트에서의 순간 변화율 (값)을 출력하는 함수\n문제를 풀때 도함수를 구하는 것인지, 미분계수를 계산하는 것인지를 구별해야함\n전체 도함수를 구하는것은 보통 굉장히 어려움. 하지만 한점에서의 순간변화율 즉, 미분계수를 구하는 것은 가능\n에러를 줄이는 데에는 값으로 나오는 순간 변화율을 구하는 것이 일반적으로 실현성이 있는 문제\n\n\nDefinition 6 The natural number, \\(e\\) is the number such that \\(\\lim_{h\\to 0} \\frac{e^h-1}{h}=1\\).\n\n모든 지수 함수 \\(f(x)=a^x\\) 중에서 \\(f(x)=e^x\\) 가 점 (0.1) 에서의 접선의 기울기가 \\(f'(0)=1\\) 이 되는 수를 \\(e=2.71828...\\) 라고 정의한다.\n\n\n\n\\(f'(x)\\) 는 다음과 같은 기호들로도 흔히 표현된다.\n\nLagrange’s notation\n\n\\(y', f'(x)\\)\n어떤 변수로 미분하는지에 대해서 명시적으로 표현되지 않았음. 고등학교때까진 univariable function을 미분 했기떄문에 이 표기법이 많이 사용되었음.\n\nLeibniz’s notation\n\n\\(\\frac{dy}{dx}=\\frac{df}{dx}=\\frac{d}{dx}f(x)\\)\n입력 변수와 출력 변수까지 모두 명시되어있음\n\nNewton’s notation:\n\n\\(\\dot{y}, \\ddot{y}\\)\n최적화 논문과 financial engineering 에서 본적 있음\n\nEuler’s notation\n\n\\(D_xy, D_xf(x)\\)\n시계열 논문과 미분방정식 논문에서 본적 있음\n\n\n\n\n\n다음의 함수를 미분의 정의를 이용하여 도함수를 계산하시오\n\n\\(f(x)=c\\) where c is a constant\n\\(f(x)=\\log x\\)\n\\(f(x)=e^x\\)\n\\(f(x)=\\sin x\\)\n\nDerivative Formula는 모두 미분의 정의를 이용해서 구할 수 있음\n\nTheorem 4  \n\nThe Power Rule, if \\(n\\) is any real number, then the power function, \\(x^n\\) is differentiated like the following: \\[\n\\frac{d}{dx}(x^n)=nx^{n-1}\n\\]\nThe Constant Multiple Rule, if \\(c\\) is a constand and \\(f\\) is a differentiable function, then \\[\n\\frac{d}{dx}(cf'(x))=c\\frac{d}{dx}f(x)=cf'(x)\n\\]\nThe Sum Rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\n\\frac{d}{dx}[f(x)+g(x)]=\\frac{d}{dx}[f(x)]+\\frac{d}{dx}[g(x)]=f'(x) +g'(x)\n\\]\nThe Difference Rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\n\\frac{d}{dx}[f(x)-g(x)]=\\frac{d}{dx}[f(x)]-\\frac{d}{dx}[g(x)]=f'(x) -g'(x)\n\\]\nThe Product Rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\ny=f(x)g(x), y'=f'(x)g(x)+f(x)g'(x)\n\\]\nThe quotient rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\ny=\\frac{f(x)}{g(x)}, y'=\\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}\n\\]\n\n\n증명은 James Stewart의 Calculus Series 중 1개를 골라 참고하시기 바랍니다.\n\n\n\n\n\\(S(x)=\\frac{1}{1+e^{-ax}}\\) 를 \\(x\\) 에 대해 미분해보시오.\n\\(f(x)=\\alpha_1 + \\frac{\\alpha_2-\\alpha_1}{1+e^{-\\alpha_4(x-\\alpha_3)}}\\) 를 어떻게 미분할 것인지 생각해 보시오.\n\\(y=f(x)=(4x+3)^2\\) 를 \\(x\\) 에 대해 미분해보시오\n\\(y=f(x)=(4x+3)^{20}\\) 를 \\(x\\) 에 대해 어떻게 미분할 것인지 생각해보시오. (hint: composite function - Leibniz)\n\n\n\n\n\n\n앞서와 언급한대로 파이썬 sympy package를 사용할 것인데 간단한 예를 본다. 먼저 기호의 정의를 해주고 수학 연산을 진행하면 된다.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sympy as S\n\n# 심볼 정의\nx=S.Symbol('x')\nh=S.Symbol('h')\nn=S.Symbol('n')\na=S.Symbol('a')\na1=S.Symbol('a1')\na2=S.Symbol('a2')\na3=S.Symbol('a3')\na4=S.Symbol('a4')\n\n\n## 인수분해\nS.factor(x**2+4*x+4)\n\n\n\\(\\displaystyle \\left(x + 2\\right)^{2}\\)\n\n\n\n인수분해와 값 넣기\n\n\n\nCode\nf=S.factor(x**2-5*x+6)\nprint(f.subs({x:3}))\nprint(f.subs({x:5}))\nf\n\n\n0\n6\n\n\n\\(\\displaystyle \\left(x - 3\\right) \\left(x - 2\\right)\\)\n\n\n\n\\(2^x\\) 미분과 값 넣기\n\n\n\nCode\ndf=S.limit((2**(x+h)-2**x)/h,h,0)\nprint(df.subs({x:3}))\nprint(df.subs({x:5}))\n\nprint(df.subs({x:3}).evalf()) # 정확한 값을 원할 경우\ndf\n\n\n8*log(2)\n32*log(2)\n5.54517744447956\n\n\n\\(\\displaystyle 2^{x} \\log{\\left(2 \\right)}\\)\n\n\n\n\\(x^n\\) 미분\n\n\n\nCode\ndf=S.limit(((x+h)**n-x**n)/h,h,0)\nprint(df.subs({x:3}))\nprint(df.subs({x:5}))\ndf\n\n\n3**(n - 1)*n\n5**(n - 1)*n\n\n\n\\(\\displaystyle n x^{n - 1}\\)\n\n\n\n\n\n\n\\(f(x)=c\\) 의 도함수 where c is a constant. c=2로 고정\n\n\n\nCode\nf=2\ndf=S.limit(((2)-2)/h,h,0)\ndf\n\n\n\\(\\displaystyle 0\\)\n\n\n\n\\(f(x)=\\log x\\) 의 도함수\n\n\n\nCode\nf=S.log(x)\ndf=S.limit((S.log(x+h)-S.log(x))/h,h,0)\ndf\n\n\n\\(\\displaystyle \\frac{1}{x}\\)\n\n\n\n\\(f(x)=e^x\\) 의 도함수\n\n\n\nCode\nf=S.exp(x)\ndf=S.limit((S.exp(x+h)-S.exp(x))/h,h,0)\ndf\n\n\n\\(\\displaystyle e^{x}\\)\n\n\n\n\\(f(x)=\\sin x\\) 의 도함수\n\n\n\nCode\nf=S.sin(x)\ndf=S.limit((S.sin(x+h)-S.sin(x))/h,h,0)\ndf\n\n\n\\(\\displaystyle \\cos{\\left(x \\right)}\\)\n\n\n\n\\(S(x)=\\frac{1}{1+e^{-ax}}\\) 를 \\(x\\) 에 대해 미분해보시오.\n\n\n\nCode\nf=1/(1+S.exp(-a*x))\ndf=S.limit((1/(1+S.exp(-a*(x+h)))-1/(1+S.exp(-a*x)))/h,h,0)\ndf\n\n\n\\(\\displaystyle \\frac{a e^{a x}}{\\left(e^{a x} + 1\\right)^{2}}\\)\n\n\n이번 문제에서 \\(\\frac{ae^x}{(e^{-ax}+1)^2}\\) 라는 sigmoid function의 도함수를 얻었다. 이 도함수를 간단한 수학적 조작으로 다른 표현으로 유도해보면 다음과 같다. \\[\n\\begin{aligned}\n\\frac{d}{dx}S(x)&=\\frac{ae^{-ax}}{(e^{ax}+1)^2}\\\\\n                &=a\\frac{1}{(e^{ax}+1)}\\frac{e^{-ax}}{(e^{ax}+1)}\\\\\n                &=a\\frac{1}{(e^{ax}+1)}\\frac{1+e^{-ax}-1}{(e^{ax}+1)}\\\\\n                &=a\\frac{1}{(e^{ax}+1)}(1-\\frac{1}{(e^{ax}+1)})\\\\\n                &=aS(x)(1-S(x))\\\\\n\\end{aligned}\n\\]\n위와 같이 \\(S(x)\\) 의 도함수는 \\(aS(x)(1-S(x))\\) 로 표현될 수 있다. sigmoid function은 neural network에서 activation function으로 사용되는데 forward propagation에서 이미 한 번 계산이 된다. backward propagation에서 activation function인 \\(S(x)\\) 의 도함수를 다시 연산을 해야하는데 도함수가 \\(aS(x)(1-S(x))\\) 것을 알면 복잡한 고차원 행렬곱 연산을 다시 수행하지 않아도 된다. 그래서 \\(S(x)\\) 의 도함수를 \\(\\frac{ae^x}{(e^{-ax}+1)^2}\\) 라고 코딩하는 것 보다는 \\(S(x)\\) 의 행렬을 재활용하여 \\(aS(x)(1-S(x))\\) 로 코딩해놓으면 연산 과정에서의 시간 복잡도를 줄일 수 있다. 이처럼 machine learning에서 수학적 통계적 지식을 잘 활용하면 좀 더 효율적인 모델링을 구현 할 수 있다.\n\n\\(f(x;\\mathbf\\alpha)=\\alpha_1 + \\frac{\\alpha_2-\\alpha_1}{1+e^{-\\alpha_4(x-\\alpha_3)}}\\) 를 어떻게 미분할 것인지 생각해 보시오.\n\n위의 식은 logistic fucntion의 genral formular 형태인데 sigmoid function이 특수한 예이다. 함수의 shpae는 parameter에 의해 결정되는데 위의 경우 \\(\\alpha_1\\) 은 함수의 최솟값 , \\(\\alpha_2\\) 은 함수의 최댓값, \\(\\alpha_3\\) 은 함수의 변곡점 및 \\(\\alpha_4\\) logistic curve가 변곡점을 지나면서 증가하는 변화율을 묘사한다. sigmoid 형태의 data를 fitting하기 위해 위의 함수를 이용한다면 error를 최소화하는 parameter를 구해야하는데 이 또한 최적화 문제로 4개의 변수에 대한 미분이 필요하다. 2개 이상의 변수에 대해서 미분은 partial derivative (편미분)라고 하는데 다음 블로그에서 다룰 것이다.\n\n\\(y=f(x)=(4x+3)^2\\) 를 \\(x\\) 에 대해 미분해보시오.\n\n\n\nCode\ndf=S.limit(((4*(x+h)+3)**2-(4*x+3)**2)/h,h,0)\ndf\n\n\n\\(\\displaystyle 32 x + 24\\)\n\n\n위의 도함수는 미분 공식 중 곱의 법칙을 사용하면 구할 수 있다.\n\n\\(y=f(x)=(4x+3)^{20}\\) 를 \\(x\\) 에 대해 어떻게 미분할 것인지 생각해보시오. (hint: composite function - Leibniz)\n\n위의 문제처럼 곱의 법칙을 사용하면 20개의 인수에 대해서 차례대로 미분을 해야하므로 계산량이 엄청나게 많아진다. 이때 composite function (합성 함수)의 derivative를 구하는 chain rule을 이용하면 간단한 연산으로 도함수를 구할 수 있게 된다. deep learning 모델의 기초인 neural network는 layer nodes이 복잡하게 합성이 되는 합성 함수를 만들면서 forward propagation을 진행하고 backward propabation에서 이 복잡한 합성 함수의 미분을 수행하게 된다. 그러므로 합성 함수의 미분이 어떻게 수행되는지 아는 것은 deep learning을 수리적으로 이해하고 싶은 사람에게 있어서 중요할 수 있다. 합성 함수의 미분은 다른 블로그에서 다루도록 하겠다.\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/differentiation/2023-02-10_composite_partial_derivative.html",
    "href": "docs/blog/posts/Mathmatics/differentiation/2023-02-10_composite_partial_derivative.html",
    "title": "Differentiation - Chain Rule & Partial Derivative",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nNeural Network (NN)는 여러 노드(node)가 여러 층(layer)을 갖고 층 사이에 함수 관계가 있는 구조이다. NN의 결과값을 계산할 때 순방향(forward)으로 전 층의 노드가 그 다음 층의 입력값으로 들어가 복잡한 합성 함수가 만들어져 결과값을 출력하고, 학습을 할 때 역방향(backword)으로 그 합성 함수의 미분을 수행하게 된다 (Back Propagation). NN은 deep learning의 기본 구조가 되므로 합성 함수가 무엇이고 어떤 원리로 미분이 되는지 알 필요가 있다.\n\n\n\n\nTheorem 1 If the fucntions, \\(f(x)\\), \\(g(x)\\), are differentiable, then the composite function of \\(f(x)\\) and \\(g(x)\\), \\(u=f\\circ g\\) defined by \\(u(x)=g(f(x))\\) is differentiable at \\(x\\) and \\(u\\) is given by the product, the chain rule is \\[\n\\begin{aligned}\nu'(x)&=g'(f(x))f'(x)\n\\end{aligned}\n\\]\nIn Leibniz notation, if \\(y=f(x)\\) and \\(u=g(y)\\) are both differentiable functions, then \\[\n\\frac{du}{dx} = \\frac{du}{dy}\\frac{dy}{dx}\n\\]\n\nChain rule은 합성 함수의 미분으로 겉에 있는 함수를 미분하고 안에 있는 함수를 미분을 연달아 하는 방식이다.\n\n\n다음 식들의 도함수를 chain rule에 따라 구해본다.\n\n\\(u(x)=(4x^2+7x)^{50}\\)\n\n\\[\n\\begin{aligned}\n&\\text{when the two functions are }f(x)=4x^2+7x, \\space g(x)=x^{50}, \\\\\n&u(x)=(4x^2+7x)^{50}=g(f(x))\\\\\n&f'(x)=8x+7\\\\\n&g'(x)=50x^{49}\\\\\n&u'(x)=g'(f(x))f'(x)\\\\\n&u'(x)=50f(x)^{49}(8x+7)=50(4x^2+7x)^{49}(8x+7)\n\\end{aligned}\n\\]\n\n\\(f(x)=x+1\\), \\(g(x)=\\sqrt{x+1}\\), \\(u(x)=g(f(x))\\)\n\n\\[\n\\begin{aligned}\n&f(x)=x+1, \\space g(x)=\\sqrt{x+1} \\\\\n&u(x)=g(f(x))=\\sqrt{(x+1)+1}\\\\\n&f'(x)=1\\\\\n&g'(x)=\\frac{1}{2\\sqrt{x+1}}\\\\\n&u'(x)=g'(f(x))f'(x)\\\\\n&u'(x)=\\frac{1}{2\\sqrt{f(x)+1}}f'(x)=\\frac{1}{2\\sqrt{(x+1)+1}}\n\\end{aligned}\n\\]\n$$\n\\[\\begin{aligned}\n  \\lim_{\\Delta x \\to0}\\frac{\\Delta u}{\\Delta x}&=\n  \\lim_{\\Delta y \\to0}\\frac{\\Delta u}{\\Delta y}\\lim_{\\Delta x \\to0}\\frac{\\Delta y}{\\Delta x}\\\\&=\\lim_{\\Delta y \\to0}\\frac{\\Delta u}{\\Delta y}\\frac{dy}{dx}\\\\\n  \\frac{dz}{dx}&=\\frac{dz}{dy}\\frac{dy}{dx} &(\\because \\Delta x \\to 0 \\Longrightarrow \\Delta y \\to 0 )\n\n\\end{aligned}\\]\n$$\n\\(\\Delta x\\rightarrow \\Delta y \\rightarrow \\Delta z\\)\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html",
    "href": "docs/projects/LLFS/self_description.html",
    "title": "Project Description",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n보안상의 이유로 프로젝트에서 사용됐던 실제 data를 사용하지 못하기 때문에 분석을 위해 사용됐던 방법론을 구체적으로 보여주기 어렵다. 이에 따라 대략적인 분석 방식을 고차원의 서로 상관 관계가 있는 data를 simulation을 통해 만들어 보여주려고 한다.\n\n\n\n이 시뮬레이션 연구의 목표는 AD와 None AD와 관련된 biomarkers를 구별할 수 있는 일련의 예측인자(또는 대사물질 또는 생화학물질)를 식별하는데 사용됐던 방법론을 소개하는 것이다.\n\n\n\n\n보안 문제로 인해 이 프로젝트에 사용된 실제 데이터와 전체 분석 파이프라인을 보여주기 어렵다.\n이 시뮬레이션 연구에서는 다변량 정규분포 하에서 대사 물질 데이터를 생성하여 대사 단계에서 가상의 데이터를 생성하고 분석 방법론을 기술하는 데에만 집중할 것이다.\n시뮬레이션 경험이 많지 않아 시뮬레이션이 수학적으로 통계적으로 틀린 부분이 있을 수 있다.\n시뮬레이션은 내가 수행했던 분석 방법론을 간단히 재현하는 용도로 사용하는 것이기 때문에 시뮬레이션 자체에 많은 시간을 할애하진 않았다.\n시뮬레이션 데이터는 실제 연구를 위해 표본으로 쓰인 sample 데이터의 분포를 전혀 반영하지 않았다.\n이 시뮬레이션 연구에서, 실제 데이터의 분포를 반영하지 않았고 범주형 변수 및 연속형 변수와 종속 변수를 통계적으로 잘 연관시키지 못했기 때문에 분석 결과가 생물학적인 사실과 많이 다를 수 있다.\n\n\n\n\n\nOperating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 used for coding in the Korean section\nPython 3.11 used for coding in the English section\n\n\n\n\n\n\n\nIt is difficult to show the methodology used for analysis in detail because the actual data used in the project cannot be used for security reasons. Accordingly, I am going to show a rough analysis method through simulation of high-dimensional, mutually correlated data.\n\n\n\nThe aim of this simulation study is to identify a set of predictors (or metabolites or bio-chemicals) that will enable to differentiate bio-markers that are associated with AD vs. non-AD.\n\n\n\n\nIn this article, due to security concerns, it is difficult to display the real data and the entire analysis pipeline used in this project.\nIn this simulation study, I will focus only on generating fake data at the metabolomic stage by generating data under multivariate normal distributions.\nSince I don’t have much experience in simulation, there may be mathematically and statistically incorrect parts in the simulation.\nI did not put a lot of effort into the simulation itself because the simulation was used to simply reproduce the analysis methodology I had performed.\nThe simulated data does not reflect the distribution of the truely sampled data used in the LLFS at all.\nIn this simulation, since the categorical and continuous variables and the dependent variable could not be statistically associated properly, the analysis result for the discrete variables could be very different from the biological or medical fact.\n\n\n\n\n\nOperating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 used for coding in the Korean section\nPython 3.11 used for coding in the English section"
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#introduction",
    "href": "docs/projects/LLFS/self_description.html#introduction",
    "title": "Description",
    "section": "Introduction",
    "text": "Introduction\n보안상의 이유로 프로젝트에서 사용됐던 실제 data를 사용하지 못하기 때문에 분석을 위해 사용됐던 방법론을 구체적으로 보여주기 어려웠다. 이에 따라 대략적인 분석 방식을 고차원의 서로 상관 관계가 있는 data를 simulation을 통해 만들어 보여주려고 한다."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#goal",
    "href": "docs/projects/LLFS/self_description.html#goal",
    "title": "Description",
    "section": "Goal",
    "text": "Goal\n이 시뮬레이션 연구에서는 정규분포 하에서 대사 물질 데이터를 생성하여 대사 단계에서 가상의 데이터를 생성하는 데에만 집중할 것이다. 시뮬레이션 경험이 많지 않아 시뮬레이션이 수학적으로 통계적으로 틀린 부분이 있을 수 있다. 시뮬레이션은 내가 수행했던 분석 방법론을 간단히 재현하는 용도로 사용하는 것이라 시뮬레이션 자체에 많은 시간을 할애하진 않았다. 이 시뮬레이션 연구의 목표는 AD와 None AD와 관련된 biomarkers를 구별할 수 있는 일련의 예측인자(또는 대사물질 또는 생화학물질)를 식별하는데 사용됐던 방법론을 소개하는 것이다."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#feature",
    "href": "docs/projects/LLFS/self_description.html#feature",
    "title": "Description",
    "section": "Feature",
    "text": "Feature\n\n보안 문제로 인해 이 프로젝트에 사용된 실제 데이터와 전체 분석 파이프라인을 보여주기 어렵다. 따라서 시뮬레이션을 통해 대략적인 분석 파이프라인을 재현 및 시연하기 위해 시뮬레이션 데이터를 생성한다.시뮬레이션 데이터는 실제 연구를 위해 표본으로 쓰인 sample 데이터의 분포를 전혀 반영하지 않았다.\n이 시뮬레이션 연구에서, 실제 데이터의 분포를 반영하지 않았고 범주형 변수 및 연속형 변수와 종속 변수를 통계적으로 잘 연관시키지 못했기 때문에 분석 결과가 생물학적인 사실과 많이 다를 수 있다."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#development-environment",
    "href": "docs/projects/LLFS/self_description.html#development-environment",
    "title": "Description",
    "section": "Development Environment",
    "text": "Development Environment\n\nOperating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 used for coding in the Korean section\nPython 3.11 used for coding in the English section"
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#introduction-1",
    "href": "docs/projects/LLFS/self_description.html#introduction-1",
    "title": "Description",
    "section": "Introduction",
    "text": "Introduction\nBecause the actual data used in the project cannot be used for security reasons, it was difficult to show the methodology used for analysis in detail. Accordingly, I am going to show a rough analysis method through simulation of high-dimensional, mutually correlated data."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#goal-1",
    "href": "docs/projects/LLFS/self_description.html#goal-1",
    "title": "Description",
    "section": "Goal",
    "text": "Goal\nIn this simulation study, I will focus only on generating fake data at the metabolomic stage by generating data under multivariate normal distributions. The aim of this simulation study is to identify a set of predictors (or metabolites or bio-chemicals) that will enable to differentiate bio-markers that are associated with AD vs. non-AD."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#feature-1",
    "href": "docs/projects/LLFS/self_description.html#feature-1",
    "title": "Description",
    "section": "Feature",
    "text": "Feature\n\nIn this article, due to security concerns, it is difficult to display the real data and the entire analysis pipeline used in this project. Therefore, I prepared simulated data to reproduce and demonstrate a rough analysis pipeline. The simulated data does not reflect the distribution of the truely sampled data used in the LLFS at all.\nIn this simulation, since the categorical and continuous variables and the dependent variable could not be statistically associated properly, the analysis result for the discrete variables could be very different from the biological or medical fact."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html#development-environment-1",
    "href": "docs/projects/LLFS/self_description.html#development-environment-1",
    "title": "Description",
    "section": "Development Environment",
    "text": "Development Environment\n\nOperating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 used for coding in the Korean section\nPython 3.11 used for coding in the English section"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#function-list",
    "href": "docs/projects/LLFS/eda.html#function-list",
    "title": "EDA",
    "section": "Function List",
    "text": "Function List\n\n\nCode\ncolor_function<-function(category_number){\nreturn(\n    if(category_number==2){\n        c(\"darkblue\",\"darkred\")\n    }else if(category_number==3){\n        c(\"darkblue\",\"darkred\",\"yellow4\")\n    }else if(category_number==4){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\")\n    }else if(category_number==5){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\")\n    }else{\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\",\"darkgreen\")\n    }\n    )\n}\n\nscale_function=function(vector=x,min=NULL,max=NULL,method){\n    scaling_methods<-c('min_max normalization','customized normalization','standardization')\n\n    if(method==\"min-max\"){\n        output=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        output=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else if(method==\"standarized\"){\n        output=(vector-mean(vector))/sd(vector)\n    }else{\n        paste0(\"Error!, no such a scaling method in this module. Please, put the first word of each method you want to use in the 'method' argument among the following tests: \", paste(scaling_methods,collapse=\", \"))\n    }\n  return(output)\n}\n\nmultiple_shapiro_test<-function(in_data){\n        normality_test<-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                            function(x)shapiro.test(x))\n        temp<-data.frame(matrix(nrow=length(normality_test),ncol=4))\n        for (i in 1:length(normality_test)){\n            temp[i,]<-c(\n                coloumn_name=names(normality_test)[i],\n                statistic=normality_test[[i]]$statistic,\n                p_value=normality_test[[i]]$p.value,\n                method=normality_test[[i]]$method)\n        }\n        names(temp)<-c('column_name','statistic','p_value','method')\n        output<-temp%>%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted<0.05,'not_normal','normal'))%>%\n            dplyr::select('column_name','statistic','p_value','p_adjusted','type','method')\n        return(output)\n}    \n\nmultiple_levene_test<-function(in_data,categorical_variable){\n        homoscedasticity_test<-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                                    function(x)leveneTest(x~in_data[,categorical_variable]))\n        temp<-data.frame(matrix(nrow=length(homoscedasticity_test),ncol=6))\n            for (i in 1:length(homoscedasticity_test)){\n                temp[i,]<-c(\n                    coloumn_name=names(homoscedasticity_test)[i],\n                    group_df=homoscedasticity_test[[i]]$Df[1],\n                    residual_df=homoscedasticity_test[[i]]$Df[2],\n                    statistic=homoscedasticity_test[[i]]$`F value`[1],\n                    p_value=homoscedasticity_test[[i]]$`Pr(>F)`[1],\n                    method=\"levene's test\")\n            }\n            names(temp)<-c('column_name','group_df','residual_df','statistic','p_value','method')\n            output<-temp%>%\n                mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n                type=ifelse(p_adjusted<0.05,'heteroscedasticity','homoscedasticity'))%>%\n                dplyr::select('column_name','group_df','residual_df','statistic','p_value','p_adjusted','type','method')\n        return(output)} \n\ncategorical_variable='outcome'\n\nmultiple_unpaired_t_test<-function(in_data,categorical_variable,homo_variables,hetero_variables){\n    homo_unpaired_t_test<-apply(in_data[,unlist(lapply(in_data, is.numeric))][,homo_variables],2,\n                                    function(x)t.test(x~in_data[,categorical_variable],var.equal=TRUE))\n    hetero_unpaired_t_test<-apply(in_data[,unlist(lapply(in_data, is.numeric))][,hetero_variables],2,\n                                    function(x)t.test(x~in_data[,categorical_variable],var.equal=FALSE)) \n    unpaired_t_test<-c(homo_unpaired_t_test,hetero_unpaired_t_test)\n\n    temp<-data.frame(matrix(nrow=length(unpaired_t_test),ncol=7))\n        for (i in 1:length(unpaired_t_test)){\n            temp[i,]<-c(names(unpaired_t_test)[i], \n                        unpaired_t_test[[i]]$estimate,\n                        unpaired_t_test[[i]]$parameter,\n                        unpaired_t_test[[i]]$statistic,\n                        unpaired_t_test[[i]]$p.value,\n                        unpaired_t_test[[i]]$method)\n        }\n        names(temp)<-c('column_name',names(unpaired_t_test[[1]]$estimate),'df','statistic','p_value','method')\n        output<-temp%>%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted<0.05,'significant','insignificant'))%>%\n            dplyr::select('column_name',names(unpaired_t_test[[1]]$estimate),'df','statistic','p_value','p_adjusted','type','method')\n    return(output)} \n\n\nmultiple_correlation_test<-function(in_data,in_numeric_variable){\n    correlation_test<-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                                    function(x)cor.test(x,in_data[,in_numeric_variable],method='pearson'))\n    temp<-data.frame(matrix(nrow=length(correlation_test),ncol=6))\n        for (i in 1:length(correlation_test)){\n            temp[i,]<-c(names(correlation_test)[i], \n                        correlation_test[[i]]$estimate,\n                        correlation_test[[i]]$parameter,\n                        correlation_test[[i]]$statistic,\n                        correlation_test[[i]]$p.value,\n                        correlation_test[[i]]$method)\n        }\n        names(temp)<-c('column_name',names(correlation_test[[1]]$estimate),'df','statistic','p_value','method')\n        output<-temp%>%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted<0.05,'significant','insignificant'))%>%\n            dplyr::select('column_name',names(correlation_test[[1]]$estimate),'df','statistic','p_value','p_adjusted','type','method')\n    return(output)} \n\nmultiple_anova_test<-function(in_data, in_categorical_variable){\n    aov_test<-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                function(x)aov(x~get(in_categorical_variable),data=in_data)%>%summary)\n\n    temp<-data.frame(matrix(nrow=length(aov_test),ncol=10))\n    for (i in 1:length(aov_test)){\n        temp[i,]<-c(names(aov_test)[i], \n                    aov_test[[i]][[1]]$`Df`[1],\n                    aov_test[[i]][[1]]$`Df`[2],\n                    aov_test[[i]][[1]]$`Sum Sq`[1],\n                    aov_test[[i]][[1]]$`Sum Sq`[2],\n                    aov_test[[i]][[1]]$`Mean Sq`[1],\n                    aov_test[[i]][[1]]$`Mean Sq`[2],\n                    aov_test[[i]][[1]]$`F value`[1],\n                    aov_test[[i]][[1]]$`Pr(>F)`[1],\n                    'one_way_anova')\n    }\n    names(temp)<-c('column_name','group_df','residual_df','group_ssq','residual_ssq',\n                    'group_msq','residual_msq','F_value','p_value','method')\n    output<-temp%>%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted<0.05,'significant','insignificant'))%>%\n            dplyr::select('column_name','group_df','residual_df','group_ssq','residual_ssq',\n                    'group_msq','residual_msq','F_value','p_value','method',\n                    'p_adjusted','type','method')\n    return(output)} \n\nmain_statistical_test<-function(\n    in_data,method,categorical_variable,in_numeric_variable,\n    homo_variables=NULL,hetero_variables=NULL,\n    fun1=multiple_shapiro_test,\n    fun2=multiple_levene_test,\n    fun3=multiple_unpaired_t_test){\n    test_list<-c(\"shapiro wilks test\",\"levene's test\",\"student t test\",\"pearson correlation test\",\"anova\")#,\"ANCOVA\",\"MANOVA\",\"wilcoxon manwhitney\",'kruskal wallis test')\n    error_massage<-paste0(\"Error!, no such a test in this module. Please, put the first word of each method you want to use in the 'method' argument among the following tests: \", paste(test_list,collapse=\", \"))\n    if(grepl('shapiro',method)){\n        output=multiple_shapiro_test(in_data)\n    }else if(grepl('levene',method)){\n        output=multiple_levene_test(in_data,categorical_variable)\n        # var.test()\n    }else if(grepl('student',method)){\n        # code unpaired vs paired t test in the future\n        output=multiple_unpaired_t_test(in_data,categorical_variable,homo_variables,hetero_variables)\n    }else if(grepl('kruskal',method)){\n        return(error_massage)\n    }else if(grepl('wilcoxon|manwhitney',method)){\n        return(error_massage)\n    }else if(grepl('anova|aov',method)){\n        output=multiple_anova_test(in_data,categorical_variable)\n    }else if(grepl('cor',method)){\n        output=multiple_correlation_test(in_data,in_numeric_variable)\n    }else{\n        return(error_massage)\n    }\n    return(output)\n}\n\n\ngetNumericSummaryTable=function(in_data,group_variable,summary_variable,set_color=color_function,...){\n    # table\n    temp<-in_data %>% \n    #group_by_at(vars(...)) %>% \n    group_by_at(vars(group_variable)) %>% \n    mutate(count=n())%>%\n    summarise_at(vars(summary_variable,count),\n                 list(mean=mean,\n                 sd=sd,\n                 min=min,\n                 Q1=~quantile(., probs = 0.25),\n                 median=median, \n                 Q3=~quantile(., probs = 0.75),\n                 max=max))%>%\n                 as.data.frame()%>%\n                 rename(\n                 n=count_mean)%>%\n                 dplyr::select(-contains('count'))%>%\n                 as.data.frame()\n    names(temp)<-c(\"group\",\n    sapply(names(temp)[-1],function(x)str_replace(x,paste0(summary_variable,\"_\"),\"\")))\n    output<-temp%>%\n    mutate(\n        variable=group_variable,\n        summary=summary_variable,\n        mean=mean%>%round(2),\n        sd=sd%>%round(2),\n        min=min%>%round(2),\n        Q1=Q1%>%round(2),\n        Q4=Q3%>%round(2),\n        max=max%>%round(2),\n        IQR_min=Q1-(Q3-Q1)*1.5%>%round(2),\n    IQR_max=Q3+(Q3-Q1)*1.5%>%round(2),\n    proportion=paste0(round(n/nrow(all_data)*100,2),\"%\"))%>%\n    dplyr::select(variable,group,summary,n,proportion,mean,sd,min,IQR_min,Q1,median,Q3,IQR_max,max)\n    return(output)\n}\n\ngetNumericSummaryPlot=function(\n    in_data=all_data,group_variable,summary_variable,\n    set_color=color_function,\n    summary_function=getNumericSummaryTable,...){\n    # plot\n    temp=getNumericSummaryTable(in_data,group_variable,summary_variable)\n    temp2=temp\n    names(temp2)[2]=group_variable\n    plot<-\n    in_data%>%\n    dplyr::select(group_variable,summary_variable)%>%\n    inner_join(.,temp2,by=group_variable)%>%\n    ggplot(aes(x=age,fill=get(group_variable),color=get(group_variable)))+\n    geom_histogram(aes(y=..density..),binwidth=1,alpha=0.5, position=\"identity\")+\n    geom_vline(aes(xintercept=mean,color=get(group_variable)), linetype=\"dashed\", size=1.5) + \n    geom_density(aes(y=..density..),alpha=0.3) +\n    scale_color_manual(values=set_color(nrow(temp2)))+\n    scale_fill_manual(values=set_color(nrow(temp2)))+\n    theme_bw()+\n    theme(legend.position = c(.95, .95),\n    legend.justification = c(\"right\", \"top\"),\n    legend.margin = margin(6, 6, 6, 6),\n    legend.text = element_text(size = 10))+\n    guides(fill=guide_legend(title=group_variable),\n    color=FALSE)+\n    geom_text(aes(label=round(mean,1),y=0,x=mean),\n                vjust=-1,col='yellow',size=5)+\n    ggtitle(paste0(\"Histogram & Density, \", summary_variable, \" Grouped by \", group_variable))+\n        labs(x=summary_variable, y = \"Density\")\n\n    result<-plot\n    return(result)\n}\n\n\n\n\nCode\n# load simulation data\nsimulated_data<-read_rds(datapath)\n\n# simple data pre-processing\nall_data<-\n    simulated_data%>%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]<-paste0(\"meta\",1:predictor_size)"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#eda",
    "href": "docs/projects/LLFS/eda.html#eda",
    "title": "EDA",
    "section": "2.1 EDA",
    "text": "2.1 EDA\n\n2.1.1 Univariable Analysis\n\n2.1.1.1 Normality Test\n\n\nCode\n# raw data\nnormality_test_result<-multiple_shapiro_test(all_data)%>%\n    filter(column_name!='id')%>%\n    group_by(type)%>%\n    summarise(count=n())%>%\n    mutate(proportion=round(count/sum(count),3),\n    total=sum(count))%>%\n    dplyr::select(type,total,everything())\n    \nnormality_test_result%>%\n    knitr::kable(caption=\"Summary of the Result of Shapiro Wilk Tests on Numeric Variables\")\n\n\n\nSummary of the Result of Shapiro Wilk Tests on Numeric Variables\n\n\ntype\ntotal\ncount\nproportion\n\n\n\n\nnormal\n1001\n1001\n1\n\n\n\n\n\nOut of 1001 numeric variables, the variables following a normal distribution are 1001 (100%) and the ones that do not are NA (NA%).\n\n\n2.1.1.2 16 Variables That Follow Normal Distributions\n\n\nCode\n# 16 numeric variables randomly selected\nnormal_variables<-\n    multiple_shapiro_test(all_data)%>%\n        filter(p_value>0.05,column_name!='id')%>%\n            dplyr::select(column_name)%>%\n            pull%>%sample(16)\n\nnormal_data<-\n    all_data%>%\n        dplyr::select(outcome,normal_variables)%>%\n        gather(key=metabolite,value=value,normal_variables)\n\nnormal_data%>%\n    ggplot(aes(x=value))+\n    geom_histogram(aes(y=..density..))+\n    geom_density(color='red',linewidth=1.5)+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%>%\n    ggplot(aes(x=metabolite,y=value))+\n    geom_boxplot()+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%>%\n    ggplot(aes(x=value,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution Grouped by Disease Status\")    \n\n\n\n\n\nCode\nnormal_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution Grouped by Disease Status\")\n\n\n\n\n\n\n\n2.1.1.3 Variables That Do Not Follow Normal Distributions\nThere is no variable that do not follow a normal distribution.\n\n\n\n2.1.2 Bivariable Analysis\n\n2.1.2.1 AD vs Metabolites\nThrough the exploratory data analysis above, it was confirmed that all variables follow a normal distribution, and t tests were conducted to select metabolites that have significant relationships with the disease status, AD. To minimize type 1 error due to multiple testings, bonferroni correction was used in the EDA\n\nHomoscedasticity Test\n\nLeven’s test is performed to confirm that each variable has equality of variance, one of the assumptions of the t test and ANOVA.\n\n\nCode\nleven_test_result<-\n    main_statistical_test(in_data=all_data,method=\"levene\",categorical_variable=\"outcome\")%>%\n    filter(column_name!='id')\n\nhomo_variable<-leven_test_result%>%\nfilter(p_adjusted>0.05)%>%\ndplyr::select(column_name)%>%\npull()%>%\nunique()\n\nhetero_variable<-leven_test_result%>%\nfilter(p_adjusted<0.05)%>%\ndplyr::select(column_name)%>%\npull()%>%\nunique()\n\n\n\n10 variables randomly selected out of 1001 variables with equal variance: meta408, meta959, meta796, meta957, meta740, meta212, meta171, meta769, meta986, meta178\n0 variables randomly selected with equal variance:\n\n\n\nCode\nhomo_variable_sample<-homo_variable%>%sample(10)\nhetero_variable_sample<-hetero_variable\n\nstratified_levene_data<-all_data%>%\n    dplyr::select(outcome,homo_variable_sample,hetero_variable_sample)%>%\n    gather(key=metabolite,value=value,c(homo_variable_sample,hetero_variable_sample))%>%\n    mutate(levene_test=ifelse(metabolite%in%(homo_variable_sample),\"homoscedasticity\",\"heteroscedasticity\"))\n\nstratified_levene_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(stratified_levene_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    facet_wrap(.~levene_test)+\n    labs(title=\"Boxplot, Metabolites with Heteroscedasticity vs Homoscedasticity\")\n\n\n\n\n\n\nUnpaired Two Sample Mean t Test\n\n\n\nCode\nt_test_result<-\n    main_statistical_test(in_data=all_data,method=\"student\",\n                        categorical_variable=\"outcome\",\n                        homo_variables=homo_variable,\n                        hetero_variables=hetero_variable)\n\nmetabolites_associated_AD<-\n    t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    arrange(p_adjusted)%>%\n    dplyr::select(column_name)%>%pull\n\nmetabolites_associated_AD_data<-\n    all_data%>%\n    dplyr::select(outcome,sex,genotype,metabolites_associated_AD)%>%\n    gather(key=metabolite,value=value,metabolites_associated_AD)        \n\nmetabolites_associated_AD_data%>%\n    group_by(outcome)%>%\n    summarise(mean=mean(value),sd=sd(value))%>%\n    knitr::kable()\n\n\n\n\n\noutcome\nmean\nsd\n\n\n\n\nnegative\n0.2184112\n0.9869302\n\n\npositive\n-0.4186542\n0.9830175\n\n\n\n\n\nCode\ntop_metabolites_associated_AD<-t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    arrange(p_adjusted)%>%head(10)%>%\n    dplyr::select(column_name)%>%pull\n\ntop_metabolites_associated_AD_data<-\n    all_data%>%\n    dplyr::select(outcome,top_metabolites_associated_AD)%>%\n    gather(key=metabolite,value=value,top_metabolites_associated_AD)        \n\nbottom_metabolites_associated_AD<-t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    arrange(p_adjusted)%>%tail(10)%>%\n    dplyr::select(column_name)%>%pull\n\nbottom_metabolites_associated_AD_data<-\n    all_data%>%\n    dplyr::select(outcome,bottom_metabolites_associated_AD)%>%\n    gather(key=metabolite,value=value,bottom_metabolites_associated_AD)        \n\na1<-top_metabolites_associated_AD_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(top_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Strongly Associated with AD\")\n\na2<-bottom_metabolites_associated_AD_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(bottom_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Weakly Associated with AD\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\nCode\nsignificant_metabolites<-\n    t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    dplyr::select(column_name)%>%pull\n\n\nAs a result of the t tests, there are 201 metabolites that are significantly associated with AD status at the 5% significance level. Metabolites with the highest significance were designated as strong metabolites and metabolites with the lowest significance among metabolites significantly related to AD status were designated as weak metabolites, and the expression level of metabolites between the disease status was confirmed through visualization. As a result, a greater difference was observed in strong metabolites than in weak metabolites, but both groups were not clearly separated in terms of AD status.\n\n\n2.1.2.2 AD vs Sex\n\n\nCode\nad_sex_summary<-all_data%>%\n    group_by(outcome,sex)%>%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%>%\n            ungroup%>%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%>%\n    dplyr::select(outcome, sex,count,proportion,mean_age,sd_age)%>%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$sex)$p.value,\n    method=\"chisquare_test\")\n\nad_sex_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\nsex\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\nman\n150\n30%\n85.79333\n5.146572\n0.1049983\nchisquare_test\n\n\nnegative\nwoman\n160\n32%\n84.96875\n5.284868\n0.1049983\nchisquare_test\n\n\npositive\nman\n77\n15.4%\n79.85714\n5.167495\n0.1049983\nchisquare_test\n\n\npositive\nwoman\n113\n22.6%\n81.29204\n4.896576\n0.1049983\nchisquare_test\n\n\n\n\n\n\n\n2.1.2.3 AD vs Genotype\n\n\nCode\nad_genotype_summary<-all_data%>%\n    group_by(outcome,genotype)%>%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%>%\n            ungroup%>%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%>%\n    dplyr::select(outcome, genotype,count,proportion,mean_age,sd_age)%>%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\")\n\ngenotype_ad_summary<-all_data%>%\n    group_by(genotype,outcome)%>%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%>%\n            ungroup%>%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%>%\n    dplyr::select(genotype,outcome,count,proportion,mean_age,sd_age)%>%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\") \n\nad_genotype_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\ngenotype\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\ne3\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\nnegative\ne2\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\nnegative\ne4\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\npositive\ne3\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\npositive\ne2\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\npositive\ne4\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nCode\ngenotype_ad_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenotype\noutcome\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\ne3\nnegative\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\ne3\npositive\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\ne2\nnegative\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\ne2\npositive\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\ne4\nnegative\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\ne4\npositive\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nAs you see the tables above, the count of AD status and that of genotype status are proportionately the same. Thus, there is no relation between AD and genotype in this data at the significant level 5%.\n\n\n2.1.2.4 Age vs AD, Sex, Genotype\nAge is known as a strong risk factor of AD or dementia. Human nerve system gets damaged as people are aged and the nerve fibrosis symptoms progress gradually. For this reason, we need to look into how the sample data are distributed in terms of age.\n\n\nCode\nad_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\")\nsex_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\")\ngenotype_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\")\nage_summary=rbind(\n    ad_age_summary,\n    sex_age_summary,\n    genotype_age_summary)\n\n\n\n\nCode\nage_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\ngroup\nsummary\nn\nproportion\nmean\nsd\nmin\nIQR_min\nQ1\nmedian\nQ3\nIQR_max\nmax\n\n\n\n\noutcome\nnegative\nage\n310\n62%\n85.37\n5.23\n72\n69.625\n81.25\n85\n89\n100.625\n105\n\n\noutcome\npositive\nage\n190\n38%\n80.71\n5.04\n65\n66.500\n77.00\n81\n84\n94.500\n93\n\n\nsex\nman\nage\n227\n45.4%\n83.78\n5.86\n65\n68.000\n80.00\n84\n88\n100.000\n105\n\n\nsex\nwoman\nage\n273\n54.6%\n83.45\n5.43\n67\n69.500\n80.00\n83\n87\n97.500\n100\n\n\ngenotype\ne3\nage\n387\n77.4%\n83.61\n5.64\n65\n69.500\n80.00\n84\n87\n97.500\n102\n\n\ngenotype\ne2\nage\n40\n8%\n85.83\n5.62\n78\n70.875\n81.75\n86\n89\n99.875\n105\n\n\ngenotype\ne4\nage\n73\n14.6%\n82.30\n5.25\n67\n68.500\n79.00\n82\n86\n96.500\n94\n\n\n\n\n\n\n\nCode\nplot<- ggarrange(\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\"),\n    ncol=2, nrow=2,legend=\"bottom\")\nplot\n\n\n\n\n\nThe table above shows the summary statistics of age grouped by the affected status, AD and non AD. The difference of age between the two groups are about -4.66, but their standard deviations are 5.04 and 5.23. Thus, it is hard to say their age in average differ in the affected status because the age variations of the two groups are overlapped. This research has two conflicting characteristics at the population level.\nThe glaring difference of age is also shown below. As you can see, the people with a negative status 4.66 years younger than those with a positive one.\n\n\nCode\na1<-all_data%>%\n    dplyr::select(outcome,age)%>%\n    ggplot(aes(x=age,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(legend.position=\"top\")+\n    labs(title=\"Histogram, Age Distribution Grouped by Disease Status\")   \na2<-all_data%>%\n    dplyr::select(outcome,age)%>%\n    ggplot(aes(x=outcome,y=age,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Age Distribution Grouped by Disease Status\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n2.1.2.5 Age vs Metabolites\n\n\nCode\nage_correlation_data<-\n    main_statistical_test(in_data=all_data,in_numeric_variable = 'age',method = \"cor\")%>%\n    filter(p_adjusted<0.05,column_name!='id')\n\n\n151 metabolites are significantly associated with age.\n\n\n2.1.2.6 Genotype vs Metabolites\n\n\nCode\ngenotype_aov<-main_statistical_test(in_data=all_data,categorical_variable=\"genotype\",method='aov')\ngenotype_aov%>%\nfilter(column_name!='id',p_adjusted<0.05)%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn_name\ngroup_df\nresidual_df\ngroup_ssq\nresidual_ssq\ngroup_msq\nresidual_msq\nF_value\np_value\nmethod\np_adjusted\ntype\n\n\n\n\n\n\n\n0 metabolites are significantly associated with the genotype variable.\n\n\n\n2.1.3 Multivariable Analysis\n\n2.1.3.1 AD vs Sex vs Genotype vs Age\n\n\nCode\nall_data%>%\n    group_by(outcome,sex,genotype)%>%\n    summarise(count=n(),\n    mean_age=mean(age),\n    sd_age=sd(age))%>%ungroup%>%\n    mutate(sum_count=sum(count),\n    proportion=paste0(round(count/sum_count*100,3),'%'))%>%\n    dplyr::select(outcome,sex,genotype,count,proportion, mean_age,sd_age)%>%\n    knitr::kable()\n\n\n\n\n\noutcome\nsex\ngenotype\ncount\nproportion\nmean_age\nsd_age\n\n\n\n\nnegative\nman\ne3\n113\n22.6%\n85.56637\n5.091793\n\n\nnegative\nman\ne2\n14\n2.8%\n88.57143\n6.548215\n\n\nnegative\nman\ne4\n23\n4.6%\n85.21739\n4.067125\n\n\nnegative\nwoman\ne3\n129\n25.8%\n85.07752\n5.370074\n\n\nnegative\nwoman\ne2\n12\n2.4%\n85.75000\n5.446016\n\n\nnegative\nwoman\ne4\n19\n3.8%\n83.73684\n4.628920\n\n\npositive\nman\ne3\n66\n13.2%\n79.74242\n5.384624\n\n\npositive\nman\ne2\n5\n1%\n83.00000\n3.000000\n\n\npositive\nman\ne4\n6\n1.2%\n78.50000\n3.082207\n\n\npositive\nwoman\ne3\n79\n15.8%\n81.65823\n4.784821\n\n\npositive\nwoman\ne2\n9\n1.8%\n83.22222\n3.632416\n\n\npositive\nwoman\ne4\n25\n5%\n79.44000\n5.260545\n\n\n\n\n\nExcept for the disease status variable, it looks like there is no difference of age in average and count in the other categorical variables. Taking into account age varialble is significantly associated with the metabolites that are significantly associated with the disease status, outcome (or AD). There is no need to conduct futher exploratory data analysis in the multivariable analysis senction."
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#data-mining",
    "href": "docs/projects/LLFS/eda.html#data-mining",
    "title": "EDA",
    "section": "2.2 Data Mining",
    "text": "2.2 Data Mining\n\n2.2.1 PCA (Principal Component Analysis)\n\n\n\n\n\n2.2.2 K-means Clustering\n\n\nCode\n# K means\nkm_fit = kmeans(all_data[,-c(1:5)],centers = 2,iter.max = 300 )\n\n# \"K-Means Clustering- Confusion matrix\")\n# table(all_data[,1],km_fit$cluster)\n\nmat_avgss = matrix(nrow = 20, ncol = 2)\n\n# Average within the cluster sum of square\nprint(paste(\"Avg. Within sum of squares\"))\n\n\n[1] \"Avg. Within sum of squares\"\n\n\nCode\nfor (i in (1:20)){\n  km_fit = kmeans(all_data[,-c(1:6)],centers = i,iter.max = 300 )\n  mean_km = mean(km_fit$withinss)\n  print(paste(\"K-Value\",i,\",Avg.within sum of squares\",round(mean_km,2)))\n  mat_avgss[i,1] = i\n  mat_avgss[i,2] = mean_km\n}\n\n\n[1] \"K-Value 1 ,Avg.within sum of squares 5084212.57\"\n[1] \"K-Value 2 ,Avg.within sum of squares 2307327.83\"\n[1] \"K-Value 3 ,Avg.within sum of squares 1485870.86\"\n[1] \"K-Value 4 ,Avg.within sum of squares 1084452.78\"\n[1] \"K-Value 5 ,Avg.within sum of squares 853053.2\"\n[1] \"K-Value 6 ,Avg.within sum of squares 701172.72\"\n[1] \"K-Value 7 ,Avg.within sum of squares 594677.21\"\n[1] \"K-Value 8 ,Avg.within sum of squares 515120.19\"\n[1] \"K-Value 9 ,Avg.within sum of squares 454068.18\"\n[1] \"K-Value 10 ,Avg.within sum of squares 405213.23\"\n[1] \"K-Value 11 ,Avg.within sum of squares 365245.16\"\n[1] \"K-Value 12 ,Avg.within sum of squares 332931.58\"\n[1] \"K-Value 13 ,Avg.within sum of squares 305412.98\"\n[1] \"K-Value 14 ,Avg.within sum of squares 282339.37\"\n[1] \"K-Value 15 ,Avg.within sum of squares 262314.99\"\n[1] \"K-Value 16 ,Avg.within sum of squares 244329.68\"\n[1] \"K-Value 17 ,Avg.within sum of squares 228893.25\"\n[1] \"K-Value 18 ,Avg.within sum of squares 215403.56\"\n[1] \"K-Value 19 ,Avg.within sum of squares 202990.99\"\n[1] \"K-Value 20 ,Avg.within sum of squares 192329.12\"\n\n\nCode\nplot(mat_avgss[,1],mat_avgss[,2],type = 'o',xlab = \"K_Value\",ylab = \"Avg. within sum of square\")\ntitle(\"Avg. within sum of squares vs. K-value\")\n\n\n\n\n\nCode\nmat_varexp = matrix(nrow = 20, ncol = 2)\n# Percentage of Variance explained\nprint(paste(\"Percent. variance explained\"))\n\n\n[1] \"Percent. variance explained\"\n\n\nCode\nfor (i in (1:20)){\n  km_fit = kmeans(all_data[,-c(1:6)],centers = i,iter.max = 300 )\n  var_exp = km_fit$betweenss/km_fit$totss\n  print(paste(\"K-Value\",i,\",Percent var explained\",round(var_exp,4)))\n  mat_varexp[i,1]=i\n  mat_varexp[i,2]=var_exp\n}\n\n\n[1] \"K-Value 1 ,Percent var explained 0\"\n[1] \"K-Value 2 ,Percent var explained 0.0924\"\n[1] \"K-Value 3 ,Percent var explained 0.1232\"\n[1] \"K-Value 4 ,Percent var explained 0.1468\"\n[1] \"K-Value 5 ,Percent var explained 0.1602\"\n[1] \"K-Value 6 ,Percent var explained 0.1734\"\n[1] \"K-Value 7 ,Percent var explained 0.1818\"\n[1] \"K-Value 8 ,Percent var explained 0.1895\"\n[1] \"K-Value 9 ,Percent var explained 0.1965\"\n[1] \"K-Value 10 ,Percent var explained 0.202\"\n[1] \"K-Value 11 ,Percent var explained 0.2077\"\n[1] \"K-Value 12 ,Percent var explained 0.2138\"\n[1] \"K-Value 13 ,Percent var explained 0.2194\"\n[1] \"K-Value 14 ,Percent var explained 0.2209\"\n[1] \"K-Value 15 ,Percent var explained 0.2261\"\n[1] \"K-Value 16 ,Percent var explained 0.2317\"\n[1] \"K-Value 17 ,Percent var explained 0.2333\"\n[1] \"K-Value 18 ,Percent var explained 0.236\"\n[1] \"K-Value 19 ,Percent var explained 0.2392\"\n[1] \"K-Value 20 ,Percent var explained 0.2418\"\n\n\nCode\nplot(mat_varexp[,1],mat_varexp[,2],type = 'o',xlab = \"K_Value\",ylab = \"Percent Var explained\")\ntitle(\"Avg. within sum of squares vs. K-value\")"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#summary-of-exploratory-data-analysis-eda",
    "href": "docs/projects/LLFS/eda.html#summary-of-exploratory-data-analysis-eda",
    "title": "EDA",
    "section": "1.1 Summary of Exploratory Data Analysis (EDA)",
    "text": "1.1 Summary of Exploratory Data Analysis (EDA)"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#summary-of-data-mining",
    "href": "docs/projects/LLFS/eda.html#summary-of-data-mining",
    "title": "EDA",
    "section": "3.2 Summary of Data Mining",
    "text": "3.2 Summary of Data Mining"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#function-list-1",
    "href": "docs/projects/LLFS/eda.html#function-list-1",
    "title": "EDA",
    "section": "Function List",
    "text": "Function List\n\n\nCode\ncolor_function<-function(category_number){\nreturn(\n    if(category_number==2){\n        c(\"darkblue\",\"darkred\")\n    }else if(category_number==3){\n        c(\"darkblue\",\"darkred\",\"yellow4\")\n    }else if(category_number==4){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\")\n    }else if(category_number==5){\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\")\n    }else{\n        c(\"darkblue\",\"darkred\",\"yellow4\",\"blueviolet\",\"darkorange\",\"darkgreen\")\n    }\n    )\n}\n\nscale_function=function(vector=x,min=NULL,max=NULL,method){\n    scaling_methods<-c('min_max normalization','customized normalization','standardization')\n\n    if(method==\"min-max\"){\n        output=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        output=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else if(method==\"standarized\"){\n        output=(vector-mean(vector))/sd(vector)\n    }else{\n        paste0(\"Error!, no such a scaling method in this module. Please, put the first word of each method you want to use in the 'method' argument among the following tests: \", paste(scaling_methods,collapse=\", \"))\n    }\n  return(output)\n}\n\nmultiple_shapiro_test<-function(in_data){\n        normality_test<-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                            function(x)shapiro.test(x))\n        temp<-data.frame(matrix(nrow=length(normality_test),ncol=4))\n        for (i in 1:length(normality_test)){\n            temp[i,]<-c(\n                coloumn_name=names(normality_test)[i],\n                statistic=normality_test[[i]]$statistic,\n                p_value=normality_test[[i]]$p.value,\n                method=normality_test[[i]]$method)\n        }\n        names(temp)<-c('column_name','statistic','p_value','method')\n        output<-temp%>%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted<0.05,'not_normal','normal'))%>%\n            dplyr::select('column_name','statistic','p_value','p_adjusted','type','method')\n        return(output)\n}    \n\nmultiple_levene_test<-function(in_data,categorical_variable){\n        homoscedasticity_test<-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                                    function(x)leveneTest(x~in_data[,categorical_variable]))\n        temp<-data.frame(matrix(nrow=length(homoscedasticity_test),ncol=6))\n            for (i in 1:length(homoscedasticity_test)){\n                temp[i,]<-c(\n                    coloumn_name=names(homoscedasticity_test)[i],\n                    group_df=homoscedasticity_test[[i]]$Df[1],\n                    residual_df=homoscedasticity_test[[i]]$Df[2],\n                    statistic=homoscedasticity_test[[i]]$`F value`[1],\n                    p_value=homoscedasticity_test[[i]]$`Pr(>F)`[1],\n                    method=\"levene's test\")\n            }\n            names(temp)<-c('column_name','group_df','residual_df','statistic','p_value','method')\n            output<-temp%>%\n                mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n                type=ifelse(p_adjusted<0.05,'heteroscedasticity','homoscedasticity'))%>%\n                dplyr::select('column_name','group_df','residual_df','statistic','p_value','p_adjusted','type','method')\n        return(output)} \n\ncategorical_variable='outcome'\n\nmultiple_unpaired_t_test<-function(in_data,categorical_variable,homo_variables,hetero_variables){\n    homo_unpaired_t_test<-apply(in_data[,unlist(lapply(in_data, is.numeric))][,homo_variables],2,\n                                    function(x)t.test(x~in_data[,categorical_variable],var.equal=TRUE))\n    hetero_unpaired_t_test<-apply(in_data[,unlist(lapply(in_data, is.numeric))][,hetero_variables],2,\n                                    function(x)t.test(x~in_data[,categorical_variable],var.equal=FALSE)) \n    unpaired_t_test<-c(homo_unpaired_t_test,hetero_unpaired_t_test)\n\n    temp<-data.frame(matrix(nrow=length(unpaired_t_test),ncol=7))\n        for (i in 1:length(unpaired_t_test)){\n            temp[i,]<-c(names(unpaired_t_test)[i], \n                        unpaired_t_test[[i]]$estimate,\n                        unpaired_t_test[[i]]$parameter,\n                        unpaired_t_test[[i]]$statistic,\n                        unpaired_t_test[[i]]$p.value,\n                        unpaired_t_test[[i]]$method)\n        }\n        names(temp)<-c('column_name',names(unpaired_t_test[[1]]$estimate),'df','statistic','p_value','method')\n        output<-temp%>%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted<0.05,'significant','insignificant'))%>%\n            dplyr::select('column_name',names(unpaired_t_test[[1]]$estimate),'df','statistic','p_value','p_adjusted','type','method')\n    return(output)} \n\n\nmultiple_correlation_test<-function(in_data,in_numeric_variable){\n    correlation_test<-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                                    function(x)cor.test(x,in_data[,in_numeric_variable],method='pearson'))\n    temp<-data.frame(matrix(nrow=length(correlation_test),ncol=6))\n        for (i in 1:length(correlation_test)){\n            temp[i,]<-c(names(correlation_test)[i], \n                        correlation_test[[i]]$estimate,\n                        correlation_test[[i]]$parameter,\n                        correlation_test[[i]]$statistic,\n                        correlation_test[[i]]$p.value,\n                        correlation_test[[i]]$method)\n        }\n        names(temp)<-c('column_name',names(correlation_test[[1]]$estimate),'df','statistic','p_value','method')\n        output<-temp%>%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted<0.05,'significant','insignificant'))%>%\n            dplyr::select('column_name',names(correlation_test[[1]]$estimate),'df','statistic','p_value','p_adjusted','type','method')\n    return(output)} \n\nmultiple_anova_test<-function(in_data, in_categorical_variable){\n    aov_test<-apply(in_data[,unlist(lapply(in_data, is.numeric))],2,\n                function(x)aov(x~get(in_categorical_variable),data=in_data)%>%summary)\n\n    temp<-data.frame(matrix(nrow=length(aov_test),ncol=10))\n    for (i in 1:length(aov_test)){\n        temp[i,]<-c(names(aov_test)[i], \n                    aov_test[[i]][[1]]$`Df`[1],\n                    aov_test[[i]][[1]]$`Df`[2],\n                    aov_test[[i]][[1]]$`Sum Sq`[1],\n                    aov_test[[i]][[1]]$`Sum Sq`[2],\n                    aov_test[[i]][[1]]$`Mean Sq`[1],\n                    aov_test[[i]][[1]]$`Mean Sq`[2],\n                    aov_test[[i]][[1]]$`F value`[1],\n                    aov_test[[i]][[1]]$`Pr(>F)`[1],\n                    'one_way_anova')\n    }\n    names(temp)<-c('column_name','group_df','residual_df','group_ssq','residual_ssq',\n                    'group_msq','residual_msq','F_value','p_value','method')\n    output<-temp%>%\n            mutate(p_adjusted=p.adjust(p_value,method=\"bonferroni\"),\n            type=ifelse(p_adjusted<0.05,'significant','insignificant'))%>%\n            dplyr::select('column_name','group_df','residual_df','group_ssq','residual_ssq',\n                    'group_msq','residual_msq','F_value','p_value','method',\n                    'p_adjusted','type','method')\n    return(output)} \n\nmain_statistical_test<-function(\n    in_data,method,categorical_variable,in_numeric_variable,\n    homo_variables=NULL,hetero_variables=NULL,\n    fun1=multiple_shapiro_test,\n    fun2=multiple_levene_test,\n    fun3=multiple_unpaired_t_test){\n    test_list<-c(\"shapiro wilks test\",\"levene's test\",\"student t test\",\"pearson correlation test\",\"anova\")#,\"ANCOVA\",\"MANOVA\",\"wilcoxon manwhitney\",'kruskal wallis test')\n    error_massage<-paste0(\"Error!, no such a test in this module. Please, put the first word of each method you want to use in the 'method' argument among the following tests: \", paste(test_list,collapse=\", \"))\n    if(grepl('shapiro',method)){\n        output=multiple_shapiro_test(in_data)\n    }else if(grepl('levene',method)){\n        output=multiple_levene_test(in_data,categorical_variable)\n        # var.test()\n    }else if(grepl('student',method)){\n        # code unpaired vs paired t test in the future\n        output=multiple_unpaired_t_test(in_data,categorical_variable,homo_variables,hetero_variables)\n    }else if(grepl('kruskal',method)){\n        return(error_massage)\n    }else if(grepl('wilcoxon|manwhitney',method)){\n        return(error_massage)\n    }else if(grepl('anova|aov',method)){\n        output=multiple_anova_test(in_data,categorical_variable)\n    }else if(grepl('cor',method)){\n        output=multiple_correlation_test(in_data,in_numeric_variable)\n    }else{\n        return(error_massage)\n    }\n    return(output)\n}\n\n\ngetNumericSummaryTable=function(in_data,group_variable,summary_variable,set_color=color_function,...){\n    # table\n    temp<-in_data %>% \n    #group_by_at(vars(...)) %>% \n    group_by_at(vars(group_variable)) %>% \n    mutate(count=n())%>%\n    summarise_at(vars(summary_variable,count),\n                 list(mean=mean,\n                 sd=sd,\n                 min=min,\n                 Q1=~quantile(., probs = 0.25),\n                 median=median, \n                 Q3=~quantile(., probs = 0.75),\n                 max=max))%>%\n                 as.data.frame()%>%\n                 rename(\n                 n=count_mean)%>%\n                 dplyr::select(-contains('count'))%>%\n                 as.data.frame()\n    names(temp)<-c(\"group\",\n    sapply(names(temp)[-1],function(x)str_replace(x,paste0(summary_variable,\"_\"),\"\")))\n    output<-temp%>%\n    mutate(\n        variable=group_variable,\n        summary=summary_variable,\n        mean=mean%>%round(2),\n        sd=sd%>%round(2),\n        min=min%>%round(2),\n        Q1=Q1%>%round(2),\n        Q4=Q3%>%round(2),\n        max=max%>%round(2),\n        IQR_min=Q1-(Q3-Q1)*1.5%>%round(2),\n    IQR_max=Q3+(Q3-Q1)*1.5%>%round(2),\n    proportion=paste0(round(n/nrow(all_data)*100,2),\"%\"))%>%\n    dplyr::select(variable,group,summary,n,proportion,mean,sd,min,IQR_min,Q1,median,Q3,IQR_max,max)\n    return(output)\n}\n\ngetNumericSummaryPlot=function(\n    in_data=all_data,group_variable,summary_variable,\n    set_color=color_function,\n    summary_function=getNumericSummaryTable,...){\n    # plot\n    temp=getNumericSummaryTable(in_data,group_variable,summary_variable)\n    temp2=temp\n    names(temp2)[2]=group_variable\n    plot<-\n    in_data%>%\n    dplyr::select(group_variable,summary_variable)%>%\n    inner_join(.,temp2,by=group_variable)%>%\n    ggplot(aes(x=age,fill=get(group_variable),color=get(group_variable)))+\n    geom_histogram(aes(y=..density..),binwidth=1,alpha=0.5, position=\"identity\")+\n    geom_vline(aes(xintercept=mean,color=get(group_variable)), linetype=\"dashed\", size=1.5) + \n    geom_density(aes(y=..density..),alpha=0.3) +\n    scale_color_manual(values=set_color(nrow(temp2)))+\n    scale_fill_manual(values=set_color(nrow(temp2)))+\n    theme_bw()+\n    theme(legend.position = c(.95, .95),\n    legend.justification = c(\"right\", \"top\"),\n    legend.margin = margin(6, 6, 6, 6),\n    legend.text = element_text(size = 10))+\n    guides(fill=guide_legend(title=group_variable),\n    color=FALSE)+\n    geom_text(aes(label=round(mean,1),y=0,x=mean),\n                vjust=-1,col='yellow',size=5)+\n    ggtitle(paste0(\"Histogram & Density, \", summary_variable, \" Grouped by \", group_variable))+\n        labs(x=summary_variable, y = \"Density\")\n\n    result<-plot\n    return(result)\n}\n\n\n\n\nCode\n# load simulation data\nsimulated_data<-read_rds(datapath)\n\n# simple data pre-processing\nall_data<-\n    simulated_data%>%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]<-paste0(\"meta\",1:predictor_size)"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#summary-of-exploratory-data-analysis-eda-1",
    "href": "docs/projects/LLFS/eda.html#summary-of-exploratory-data-analysis-eda-1",
    "title": "EDA",
    "section": "3.1 Summary of Exploratory Data Analysis (EDA)",
    "text": "3.1 Summary of Exploratory Data Analysis (EDA)"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#summary-of-data-mining-1",
    "href": "docs/projects/LLFS/eda.html#summary-of-data-mining-1",
    "title": "EDA",
    "section": "3.2 Summary of Data Mining",
    "text": "3.2 Summary of Data Mining"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#eda-1",
    "href": "docs/projects/LLFS/eda.html#eda-1",
    "title": "EDA",
    "section": "4.1 EDA",
    "text": "4.1 EDA\n\n4.1.1 Univariable Analysis\n\n\n4.1.2 Normality Test\n\n\nCode\n# raw data\nnormality_test_result<-multiple_shapiro_test(all_data)%>%\n    filter(column_name!='id')%>%\n    group_by(type)%>%\n    summarise(count=n())%>%\n    mutate(proportion=round(count/sum(count),3),\n    total=sum(count))%>%\n    dplyr::select(type,total,everything())\n    \nnormality_test_result%>%\n    knitr::kable(caption=\"Summary of the Result of Shapiro Wilk Tests on Numeric Variables\")\n\n\n\nSummary of the Result of Shapiro Wilk Tests on Numeric Variables\n\n\ntype\ntotal\ncount\nproportion\n\n\n\n\nnormal\n1001\n1001\n1\n\n\n\n\n\nOut of 1001 numeric variables, the variables following a normal distribution are 1001 (100%) and the ones that do not are NA (NA%).\n\n\n4.1.3 Visualization\n\n4.1.3.1 16 Variables That Follow Normal Distributions\n\n\nCode\n# 16 numeric variables randomly selected\nnormal_variables<-\n    multiple_shapiro_test(all_data)%>%\n        filter(p_value>0.05,column_name!='id')%>%\n            dplyr::select(column_name)%>%\n            pull%>%sample(16)\n\nnormal_data<-\n    all_data%>%\n        dplyr::select(outcome,normal_variables)%>%\n        gather(key=metabolite,value=value,normal_variables)\n\nnormal_data%>%\n    ggplot(aes(x=value))+\n    geom_histogram(aes(y=..density..))+\n    geom_density(color='red',linewidth=1.5)+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%>%\n    ggplot(aes(x=metabolite,y=value))+\n    geom_boxplot()+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%>%\n    ggplot(aes(x=value,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution Grouped by Disease Status\")    \n\n\n\n\n\nCode\nnormal_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution Grouped by Disease Status\")\n\n\n\n\n\n\n\n4.1.3.2 Variables That Do Not Follow Normal Distributions\nThere is no variable that do not follow a normal distribution.\n\n\n\n4.1.4 Bivariable Analysis\n\n4.1.4.1 AD vs Metabolites\nThrough the exploratory data analysis above, it was confirmed that all variables follow a normal distribution, and t tests were conducted to select metabolites that have significant relationships with the disease status, AD. To minimize type 1 error due to multiple testings, bonferroni correction was used in the EDA\n\nHomoscedasticity Test\n\nLeven’s test is performed to confirm that each variable has equality of variance, one of the assumptions of the t test and ANOVA.\n\n\nCode\nleven_test_result<-\n    main_statistical_test(in_data=all_data,method=\"levene\",categorical_variable=\"outcome\")%>%\n    filter(column_name!='id')\n\nhomo_variable<-leven_test_result%>%\nfilter(p_adjusted>0.05)%>%\ndplyr::select(column_name)%>%\npull()%>%\nunique()\n\nhetero_variable<-leven_test_result%>%\nfilter(p_adjusted<0.05)%>%\ndplyr::select(column_name)%>%\npull()%>%\nunique()\n\n\n\n10 variables randomly selected out of 1001 variables with equal variance: meta408, meta959, meta796, meta957, meta740, meta212, meta171, meta769, meta986, meta178\n0 variables randomly selected with equal variance:\n\n\n\nCode\nhomo_variable_sample<-homo_variable%>%sample(10)\nhetero_variable_sample<-hetero_variable\n\nstratified_levene_data<-all_data%>%\n    dplyr::select(outcome,homo_variable_sample,hetero_variable_sample)%>%\n    gather(key=metabolite,value=value,c(homo_variable_sample,hetero_variable_sample))%>%\n    mutate(levene_test=ifelse(metabolite%in%(homo_variable_sample),\"homoscedasticity\",\"heteroscedasticity\"))\n\nstratified_levene_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(stratified_levene_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    facet_wrap(.~levene_test)+\n    labs(title=\"Boxplot, Metabolites with Heteroscedasticity vs Homoscedasticity\")\n\n\n\n\n\n\nUnpaired Two Sample Mean t Test\n\n\n\nCode\nt_test_result<-\n    main_statistical_test(in_data=all_data,method=\"student\",\n                        categorical_variable=\"outcome\",\n                        homo_variables=homo_variable,\n                        hetero_variables=hetero_variable)\n\nmetabolites_associated_AD<-\n    t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    arrange(p_adjusted)%>%\n    dplyr::select(column_name)%>%pull\n\nmetabolites_associated_AD_data<-\n    all_data%>%\n    dplyr::select(outcome,sex,genotype,metabolites_associated_AD)%>%\n    gather(key=metabolite,value=value,metabolites_associated_AD)        \n\nmetabolites_associated_AD_data%>%\n    group_by(outcome)%>%\n    summarise(mean=mean(value),sd=sd(value))%>%\n    knitr::kable()\n\n\n\n\n\noutcome\nmean\nsd\n\n\n\n\nnegative\n0.2184112\n0.9869302\n\n\npositive\n-0.4186542\n0.9830175\n\n\n\n\n\nCode\ntop_metabolites_associated_AD<-t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    arrange(p_adjusted)%>%head(10)%>%\n    dplyr::select(column_name)%>%pull\n\ntop_metabolites_associated_AD_data<-\n    all_data%>%\n    dplyr::select(outcome,top_metabolites_associated_AD)%>%\n    gather(key=metabolite,value=value,top_metabolites_associated_AD)        \n\nbottom_metabolites_associated_AD<-t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    arrange(p_adjusted)%>%tail(10)%>%\n    dplyr::select(column_name)%>%pull\n\nbottom_metabolites_associated_AD_data<-\n    all_data%>%\n    dplyr::select(outcome,bottom_metabolites_associated_AD)%>%\n    gather(key=metabolite,value=value,bottom_metabolites_associated_AD)        \n\na1<-top_metabolites_associated_AD_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(top_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Strongly Associated with AD\")\n\na2<-bottom_metabolites_associated_AD_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(bottom_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Weakly Associated with AD\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\nCode\nsignificant_metabolites<-\n    t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    dplyr::select(column_name)%>%pull\n# significant_metabolites%>%write_rds(.,file='./docs/data/llfs_fake_significant_metabolites.rds')\n\n\nAs a result of the t tests, there are 201 metabolites that are significantly associated with AD status. Metabolites with the highest significance were designated as strong metabolites and metabolites with the lowest significance among metabolites significantly related to AD status were designated as weak metabolites, and the expression level of metabolites between the disease status was confirmed through visualization. As a result, a greater difference was observed in strong metabolites than in weak metabolites, but both groups were not clearly separated in terms of AD status.\n\n\n4.1.4.2 AD vs Sex\n\n\nCode\nad_sex_summary<-all_data%>%\n    group_by(outcome,sex)%>%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%>%\n            ungroup%>%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%>%\n    dplyr::select(outcome, sex,count,proportion,mean_age,sd_age)%>%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$sex)$p.value,\n    method=\"chisquare_test\")\n\nad_sex_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\nsex\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\nman\n150\n30%\n85.79333\n5.146572\n0.1049983\nchisquare_test\n\n\nnegative\nwoman\n160\n32%\n84.96875\n5.284868\n0.1049983\nchisquare_test\n\n\npositive\nman\n77\n15.4%\n79.85714\n5.167495\n0.1049983\nchisquare_test\n\n\npositive\nwoman\n113\n22.6%\n81.29204\n4.896576\n0.1049983\nchisquare_test\n\n\n\n\n\n\n\n4.1.4.3 AD vs Genotype\n\n\nCode\nad_genotype_summary<-all_data%>%\n    group_by(outcome,genotype)%>%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%>%\n            ungroup%>%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%>%\n    dplyr::select(outcome, genotype,count,proportion,mean_age,sd_age)%>%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\")\n\ngenotype_ad_summary<-all_data%>%\n    group_by(genotype,outcome)%>%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%>%\n            ungroup%>%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%>%\n    dplyr::select(genotype,outcome,count,proportion,mean_age,sd_age)%>%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\") \n\nad_genotype_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\ngenotype\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\ne3\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\nnegative\ne2\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\nnegative\ne4\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\npositive\ne3\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\npositive\ne2\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\npositive\ne4\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nCode\ngenotype_ad_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenotype\noutcome\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\ne3\nnegative\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\ne3\npositive\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\ne2\nnegative\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\ne2\npositive\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\ne4\nnegative\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\ne4\npositive\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nAs you see the tables above, the count of AD status and that of genotype status are proportionately the same. Thus, there is no relation between AD and genotype in this data at the significant level 5%.\n\n\n4.1.4.4 Age vs AD, Sex, Genotype\nAge is known as a strong risk factor of AD or dementia. Human nerve system gets damaged as people are aged and the nerve fibrosis symptoms progress gradually. For this reason, we need to look into how the sample data are distributed in terms of age.\n\n\nCode\nad_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\")\nsex_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\")\ngenotype_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\")\nage_summary=rbind(\n    ad_age_summary,\n    sex_age_summary,\n    genotype_age_summary)\n\n\n\n\nCode\nage_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\ngroup\nsummary\nn\nproportion\nmean\nsd\nmin\nIQR_min\nQ1\nmedian\nQ3\nIQR_max\nmax\n\n\n\n\noutcome\nnegative\nage\n310\n62%\n85.37\n5.23\n72\n69.625\n81.25\n85\n89\n100.625\n105\n\n\noutcome\npositive\nage\n190\n38%\n80.71\n5.04\n65\n66.500\n77.00\n81\n84\n94.500\n93\n\n\nsex\nman\nage\n227\n45.4%\n83.78\n5.86\n65\n68.000\n80.00\n84\n88\n100.000\n105\n\n\nsex\nwoman\nage\n273\n54.6%\n83.45\n5.43\n67\n69.500\n80.00\n83\n87\n97.500\n100\n\n\ngenotype\ne3\nage\n387\n77.4%\n83.61\n5.64\n65\n69.500\n80.00\n84\n87\n97.500\n102\n\n\ngenotype\ne2\nage\n40\n8%\n85.83\n5.62\n78\n70.875\n81.75\n86\n89\n99.875\n105\n\n\ngenotype\ne4\nage\n73\n14.6%\n82.30\n5.25\n67\n68.500\n79.00\n82\n86\n96.500\n94\n\n\n\n\n\n\n\nCode\nplot<- ggarrange(\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\"),\n    ncol=2, nrow=2,legend=\"bottom\")\nplot\n\n\n\n\n\nThe table above shows the summary statistics of age grouped by the affected status, AD and non AD. The difference of age between the two groups are about -4.66, but their standard deviations are 5.04 and 5.23. Thus, it is hard to say their age in average differ in the affected status because the age variations of the two groups are overlapped. This research has two conflicting characteristics at the population level.\nThe glaring difference of age is also shown below. As you can see, the people with a negative status 4.66 years younger than those with a positive one.\n\n\nCode\na1<-all_data%>%\n    dplyr::select(outcome,age)%>%\n    ggplot(aes(x=age,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(legend.position=\"top\")+\n    labs(title=\"Histogram, Age Distribution Grouped by Disease Status\")   \na2<-all_data%>%\n    dplyr::select(outcome,age)%>%\n    ggplot(aes(x=outcome,y=age,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Age Distribution Grouped by Disease Status\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n4.1.4.5 Age vs Metabolites\n\n\nCode\nage_correlation_data<-\n    main_statistical_test(in_data=all_data,in_numeric_variable = 'age',method = \"cor\")%>%\n    filter(p_adjusted<0.05,column_name!='id')\n\n\n151 metabolites are significantly associated with age.\n\n\n4.1.4.6 Genotype vs Metabolites\n\n\nCode\ngenotype_aov<-main_statistical_test(in_data=all_data,categorical_variable=\"genotype\",method='aov')\ngenotype_aov%>%\nfilter(column_name!='id',p_adjusted<0.05)%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn_name\ngroup_df\nresidual_df\ngroup_ssq\nresidual_ssq\ngroup_msq\nresidual_msq\nF_value\np_value\nmethod\np_adjusted\ntype\n\n\n\n\n\n\n\n0 metabolites are significantly associated with the genotype variable.\n\n\n\n4.1.5 Multivariable Analysis\n\n4.1.5.1 AD vs Sex vs Genotype vs Age\n\n\nCode\nall_data%>%\n    group_by(outcome,sex,genotype)%>%\n    summarise(count=n(),\n    mean_age=mean(age),\n    sd_age=sd(age))%>%ungroup%>%\n    mutate(sum_count=sum(count),\n    proportion=paste0(round(count/sum_count*100,3),'%'))%>%\n    dplyr::select(outcome,sex,genotype,count,proportion, mean_age,sd_age)%>%\n    knitr::kable()\n\n\n\n\n\noutcome\nsex\ngenotype\ncount\nproportion\nmean_age\nsd_age\n\n\n\n\nnegative\nman\ne3\n113\n22.6%\n85.56637\n5.091793\n\n\nnegative\nman\ne2\n14\n2.8%\n88.57143\n6.548215\n\n\nnegative\nman\ne4\n23\n4.6%\n85.21739\n4.067125\n\n\nnegative\nwoman\ne3\n129\n25.8%\n85.07752\n5.370074\n\n\nnegative\nwoman\ne2\n12\n2.4%\n85.75000\n5.446016\n\n\nnegative\nwoman\ne4\n19\n3.8%\n83.73684\n4.628920\n\n\npositive\nman\ne3\n66\n13.2%\n79.74242\n5.384624\n\n\npositive\nman\ne2\n5\n1%\n83.00000\n3.000000\n\n\npositive\nman\ne4\n6\n1.2%\n78.50000\n3.082207\n\n\npositive\nwoman\ne3\n79\n15.8%\n81.65823\n4.784821\n\n\npositive\nwoman\ne2\n9\n1.8%\n83.22222\n3.632416\n\n\npositive\nwoman\ne4\n25\n5%\n79.44000\n5.260545\n\n\n\n\n\nExcept for the disease status variable, it looks like there is no difference of age in average and count in the other categorical variables. Taking into account age varialble is significantly associated with the metabolites that are significantly associated with the disease status, outcome (or AD). There is no need to conduct futher exploratory data analysis in the multivariable analysis senction."
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#data-mining-1",
    "href": "docs/projects/LLFS/eda.html#data-mining-1",
    "title": "EDA",
    "section": "4.2 Data Mining",
    "text": "4.2 Data Mining\n\n4.2.1 PCA (Principal Component Analysis)\n\n\n\n\n\n4.2.2 K-means Clustering\n\n\nCode\n# K means\nkm_fit = kmeans(all_data[,-c(1:5)],centers = 2,iter.max = 300 )\n\n# \"K-Means Clustering- Confusion matrix\")\n# table(all_data[,1],km_fit$cluster)\n\nmat_avgss = matrix(nrow = 20, ncol = 2)\n\n# Average within the cluster sum of square\nprint(paste(\"Avg. Within sum of squares\"))\n\n\n[1] \"Avg. Within sum of squares\"\n\n\nCode\nfor (i in (1:20)){\n  km_fit = kmeans(all_data[,-c(1:6)],centers = i,iter.max = 300 )\n  mean_km = mean(km_fit$withinss)\n  print(paste(\"K-Value\",i,\",Avg.within sum of squares\",round(mean_km,2)))\n  mat_avgss[i,1] = i\n  mat_avgss[i,2] = mean_km\n}\n\n\n[1] \"K-Value 1 ,Avg.within sum of squares 5084212.57\"\n[1] \"K-Value 2 ,Avg.within sum of squares 2307327.83\"\n[1] \"K-Value 3 ,Avg.within sum of squares 1485870.86\"\n[1] \"K-Value 4 ,Avg.within sum of squares 1084452.78\"\n[1] \"K-Value 5 ,Avg.within sum of squares 853053.2\"\n[1] \"K-Value 6 ,Avg.within sum of squares 701172.72\"\n[1] \"K-Value 7 ,Avg.within sum of squares 594677.21\"\n[1] \"K-Value 8 ,Avg.within sum of squares 515120.19\"\n[1] \"K-Value 9 ,Avg.within sum of squares 454068.18\"\n[1] \"K-Value 10 ,Avg.within sum of squares 405213.23\"\n[1] \"K-Value 11 ,Avg.within sum of squares 365245.16\"\n[1] \"K-Value 12 ,Avg.within sum of squares 332931.58\"\n[1] \"K-Value 13 ,Avg.within sum of squares 305412.98\"\n[1] \"K-Value 14 ,Avg.within sum of squares 282339.37\"\n[1] \"K-Value 15 ,Avg.within sum of squares 262314.99\"\n[1] \"K-Value 16 ,Avg.within sum of squares 244329.68\"\n[1] \"K-Value 17 ,Avg.within sum of squares 228893.25\"\n[1] \"K-Value 18 ,Avg.within sum of squares 215403.56\"\n[1] \"K-Value 19 ,Avg.within sum of squares 202990.99\"\n[1] \"K-Value 20 ,Avg.within sum of squares 192329.12\"\n\n\nCode\nplot(mat_avgss[,1],mat_avgss[,2],type = 'o',xlab = \"K_Value\",ylab = \"Avg. within sum of square\")\ntitle(\"Avg. within sum of squares vs. K-value\")\n\n\n\n\n\nCode\nmat_varexp = matrix(nrow = 20, ncol = 2)\n# Percentage of Variance explained\nprint(paste(\"Percent. variance explained\"))\n\n\n[1] \"Percent. variance explained\"\n\n\nCode\nfor (i in (1:20)){\n  km_fit = kmeans(all_data[,-c(1:6)],centers = i,iter.max = 300 )\n  var_exp = km_fit$betweenss/km_fit$totss\n  print(paste(\"K-Value\",i,\",Percent var explained\",round(var_exp,4)))\n  mat_varexp[i,1]=i\n  mat_varexp[i,2]=var_exp\n}\n\n\n[1] \"K-Value 1 ,Percent var explained 0\"\n[1] \"K-Value 2 ,Percent var explained 0.0924\"\n[1] \"K-Value 3 ,Percent var explained 0.1232\"\n[1] \"K-Value 4 ,Percent var explained 0.1468\"\n[1] \"K-Value 5 ,Percent var explained 0.1602\"\n[1] \"K-Value 6 ,Percent var explained 0.1734\"\n[1] \"K-Value 7 ,Percent var explained 0.1818\"\n[1] \"K-Value 8 ,Percent var explained 0.1895\"\n[1] \"K-Value 9 ,Percent var explained 0.1965\"\n[1] \"K-Value 10 ,Percent var explained 0.202\"\n[1] \"K-Value 11 ,Percent var explained 0.2077\"\n[1] \"K-Value 12 ,Percent var explained 0.2138\"\n[1] \"K-Value 13 ,Percent var explained 0.2194\"\n[1] \"K-Value 14 ,Percent var explained 0.2209\"\n[1] \"K-Value 15 ,Percent var explained 0.2261\"\n[1] \"K-Value 16 ,Percent var explained 0.2317\"\n[1] \"K-Value 17 ,Percent var explained 0.2333\"\n[1] \"K-Value 18 ,Percent var explained 0.236\"\n[1] \"K-Value 19 ,Percent var explained 0.2392\"\n[1] \"K-Value 20 ,Percent var explained 0.2418\"\n\n\nCode\nplot(mat_varexp[,1],mat_varexp[,2],type = 'o',xlab = \"K_Value\",ylab = \"Percent Var explained\")\ntitle(\"Avg. within sum of squares vs. K-value\")"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/transformation/index.html",
    "href": "docs/blog/posts/Mathmatics/transformation/index.html",
    "title": "Transofrmations of Functions",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nThanslations are about vertical and horizontal sifts. To be more sepecific, if \\(c\\) is a positive number, then\n\n\\(y=f(x)+c\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units upward\n\\(y=f(x)-c\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units downward\n\\(y=f(x-c)\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units to the right\n\\(y=f(x+c)\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units to the left\n\n\n\ndraw \\(y=x\\), \\(y=(x-3)\\), \\(y=x-3\\), \\(y=(x+3)\\), \\(y=x+3\\)\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = np.linspace(-10, 10, 1000)\ny = x\ny2 = x-3\ny3 = x+3\n\n\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.grid(True, which='both')\n\nplt.plot(x,y,color='black',label='y=x')\nplt.plot(x,y2,color='red',label='y=(x-3) or (y+3)=x')\nplt.plot(x,y3,color='blue',label='y=(x+3) or (y-3)=x')\n\nplt.title('Traslation of Functions')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThere are largely two types of transofrmations of functions: stretching and reflecting. To be more sepecific, if \\(c\\) is larger than 1, then\n\n\\(y=cf(x)\\), stretch the graph of \\(y=f(x)\\) vertically by a factor of \\(c\\)\n\\(y=\\frac{1}{c}f(x)\\), shrink the graph of \\(y=f(x)\\) vertically by a factor of \\(c\\)\n\\(y=f(cx)\\), shrink the graph of \\(y=f(x)\\) horizontally by a factor of \\(c\\)\n\\(y=f(\\frac{x}{c})\\), stretch the graph of \\(y=f(x)\\) horizontally by a factor of \\(c\\)\n\n\n\ndraw \\(y=\\sin x\\), \\(y=\\sin 2x\\), \\(y=\\frac{1}{2} \\sin x\\)\n\n\nCode\nx = np.linspace(-10, 10, 1000)\ny = np.sin(x)\ny2 = np.sin(2*x)\ny3 = np.sin(x/2)\ny4 = 2*np.sin(x)\ny5 = np.sin(x)/2\n\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.grid(True, which='both')\n\nplt.plot(x,y,color='black',label='y=sin(x)')\nplt.plot(x,y2,color='red',label='y=sin(2x)')\nplt.plot(x,y3,color='blue',label=r'y=sin($\\frac{x}{2}$)')\nplt.plot(x,y4,color='green',label=r'y=2sin(x)')\nplt.plot(x,y5,color='orange',label=r'y=$\\frac{1}{2}$sin(x) or (2y)=sin(x)')\n\nplt.title('Trasformation of Functions')\nplt.legend(shadow=True, loc=(-0.2, 1.05), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\n\n\n\ndraw \\(x^2+y^2=1\\), \\(\\frac{x^2}{4}+\\frac{y^2}{9}=1\\), \\((x-3)^2+(y-3)^2=1\\), \\(\\frac{(x-3)^2}{4}+\\frac{(y-3)^2}{9}=1\\)\n\n\n\nCode\n# initialize x and y using radian\n# theta = np.linspace(0, 2*np.pi, 1000)\n# x = np.cos(theta)\n# y = np.sin(theta)\n\n# initialize x and y without using radian\nx = np.linspace(-1, 1, 1000)\ny1 = np.sqrt(1 - x*x)\ny2 = -np.sqrt(1 - x*x)\n\nfig, ax = plt.subplots()\n\n# unit circle\nax.plot(x, y1, color='black', label=r'$x^2+y^2=1$')\nax.plot(x, y2, color='black')\nax.plot(0,0,'o',color='black', label='(0,0)')\n\n# unit circle translated by 2\nax.plot(x+3, y1+3, color='red', label=r'$(x-3)^2+(y-3)^2=1$')\nax.plot(x+3, y2+3, color='red')\nax.plot(3,3,'o',color='red', label='(3,3)')\n\n# eplipse: a unit circle transformed by 2,3 in x, y\nax.plot(2*x, 3*y1, color='blue', label=r'$\\frac{x^2}{4}+\\frac{y^2}{9}=1$')\nax.plot(2*x, 3*y2, color='blue')\nax.plot(0,-np.sqrt(9-4), 'o', color='blue', label=r'$F_1=(0,\\sqrt{5}),F_2=(0,-\\sqrt{5})$')\nax.plot(0,np.sqrt(9-4), 'o', color='blue')\n\n# translated eplipse: a unit circle translated by 2 and transformed by 2,3 in x, y\nax.plot(2*x+3, 3*y1+3, color='green', label=r'$\\frac{(x-3)^2}{4}+\\frac{(y-3)^2}{9}=1$')\nax.plot(2*x+3, 3*y2+3, color='green')\nax.plot(0+3,-np.sqrt(9-4)+3, 'o', color='green', label=r'$F_1=(3,\\sqrt{5}+3),F_2=(3,-\\sqrt{5}+3)$')\nax.plot(0+3,np.sqrt(9-4)+3, 'o', color='green')\n\n# height/width=1\nax.set_aspect(1)\n\nax.set_xlim([-4, 7])\nax.set_ylim([-4, 7])\n\nax.grid(True)\nax.set_title(\"Transformation of Functions\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\n\nplt.legend(shadow=True, loc=(-0.5, 1.05), handlelength=1.5, fontsize=8)\n\n# show the plot\nplt.show()\n\n\n\n\n\n\nTry using \\(y=\\sin (\\theta)\\), \\(x=\\cos (\\theta)\\), by yourself, to draw \\(x^2+y^2=1\\), \\(\\frac{x^2}{4}+\\frac{y^2}{9}=1\\), \\((x-3)^2+(y-3)^2=1\\), \\(\\frac{(x-3)^2}{4}+\\frac{(y-3)^2}{9}=1\\)\n\n\n\n\n\n\n\\(y=-f(x)\\), reflect the graph of \\(y=f(x)\\) about the x-axis\n\\(y=f(-x)\\), reflect the graph of \\(y=f(x)\\) about the y-axis\n\\(-y=f(-x)\\), reflect the graph of \\(y=f(x)\\) about the origin on the 2D plain\n\\(x=f(y)\\), reflect the graph of \\(y=f(x)\\) about the \\(y=x\\)\n\n\n\n\ndraw \\(y=\\sin x\\), \\(y=\\sin (-x)\\), \\(y=-\\sin x\\), \\(-y=-\\sin x\\), \\(x=-\\sin y\\)\n\n\n\nCode\nx = np.linspace(0, 2*np.pi, 1000)\nx1 = np.linspace(-2*np.pi, 2*np.pi, 1000)\ny = np.sin(x)\n\nfig, ax = plt.subplots()\n\n# unit circle\nax.plot(x, y, color='black', label='$y=\\sin (x)$')\nax.plot(-x, y, color='blue', label='$y=\\sin (-x)$')\nax.plot(x, -y, color='green', label='$y=-\\sin (x)$')\nax.plot(-x, -y, color='orange', label='$-y=-\\sin (x)$')\nax.plot(x1,x1, color='red', label='$y=x$')\nax.plot(np.sin(y),x, color='black', label='$y=sin^{-1}(x)$',linestyle='dashed')\n\n# height/width=1\nax.set_aspect(1)\n\nax.set_xlim([-7, 7])\nax.set_ylim([-7, 7])\n\nax.grid(True)\nax.set_title(\"Reflection of Functions\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\n# show the plot\nplt.show()\n\n\n\n\n\n\nDraw \\(S(x)=\\frac{1}{1+e^{-x}}\\)\n\n\n\nCode\nx = np.linspace(-10, 10, 1000)\ny = 1/(1+np.exp(-x))\n\nfig, ax = plt.subplots()\n\n# unit circle\nax.plot(x, y, color='black', label='$y=S(x)$')\nax.plot(-x, y, color='blue', label='$y=S(-x)$')\nax.plot(x, -y, color='green', label='$y=-S(x)$')\nax.plot(-x, -y, color='orange', label='$-y=-S(x)$')\nax.plot(x,x, color='red', label='$y=x$')\nax.plot(np.sin(y),x, color='black', label='$y=S^{-1}(x)$',linestyle='dashed')\n\n# height/width=1\nax.set_aspect(1)\n\nax.set_xlim([-3, 3])\nax.set_ylim([-3, 3])\n\nax.grid(True)\nax.set_title(\"Reflection of Functions\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nDistribution of Raw Data\n\n\n\nCode\ndata1 = np.random.normal(loc=5,scale=6.0,size=1000) #loc=mean, scale=sd\ndata2 = np.random.normal(loc=-4,scale=2.0,size=1000)\ndata3 = np.random.normal(loc=-7,scale=2.5,size=1000)\ndata4 = np.random.normal(loc=0,scale=1.0,size=1000)\n\n# the range of values to evaluate the PDF\nx = np.linspace(data2.min(), data1.max(), 10000)\n\nbins_number=100\n\n# Plot the data and PDF\nplt.hist(data1, density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(5,6^2)$')\nplt.hist(data2, density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-4,2^2)$')\nplt.hist(data3, density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-7,2.5^2)$')\nplt.hist(data4, density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Raw Data\")\nplt.show()\n\n\n\n\n\n\nDistribution of Translated Raw Data\n\n\n\nCode\n# Plot the data and PDF\nplt.hist(data1-data1.mean(), density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(5-5,6^2)$')\nplt.hist(data2-data2.mean(), density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-4-(-4),2^2)$')\nplt.hist(data3-data3.mean(), density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-7-(-7),2.5^2)$')\nplt.hist(data4-data4.mean(), density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0-0,1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Translated Raw Data\")\nplt.show()\n\n\n\n\n\n\nDistribution of Transformed Raw Data\n\n\n\nCode\n# Plot the data and PDF\nplt.hist(data1/data1.std(), density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(5,6^2/6^2)$')\nplt.hist(data2/data2.std(), density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-4,2^2/2^2)$')\nplt.hist(data3/data3.std(), density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-7,2.5^2/5^2)$')\nplt.hist(data4/data4.std(), density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1/1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Transformed Raw Data\")\nplt.show()\n\n\n\n\n\n\nDistribution of Standardized Raw Data\n\n\n\nCode\n# Plot the data and PDF\nplt.hist((data1-data1.mean())/data1.std(), density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\nplt.hist((data2-data2.mean())/data2.std(), density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\nplt.hist((data3-data3.mean())/data3.std(), density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\nplt.hist((data4-data4.mean())/data4.std(), density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Standardized Raw Data\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTransformation in Statistics? ex) transformation of random variable\nTransformation in Linear Algebra? ex) linear transformation or linear mapping (transformation matrix)\nTransformation in Machine Learning? ex) scaling (min-max normalization or standardization)\n\n\n\n\n\n\n\n\nThanslations are about vertical and horizontal sifts. To be more sepecific, if \\(c\\) is a positive number, then\n\n\\(y=f(x)+c\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units upward\n\\(y=f(x)-c\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units downward\n\\(y=f(x-c)\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units to the right\n\\(y=f(x+c)\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units to the left\n\n\n\ndraw \\(y=x\\), \\(y=(x-3)\\), \\(y=x-3\\), \\(y=(x+3)\\), \\(y=x+3\\)\n\n\nCode\nx = np.linspace(-10, 10, 1000)\ny = x\ny2 = x-3\ny3 = x+3\n\n\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.grid(True, which='both')\n\nplt.plot(x,y,color='black',label='y=x')\nplt.plot(x,y2,color='red',label='y=(x-3) or (y+3)=x')\nplt.plot(x,y3,color='blue',label='y=(x+3) or (y-3)=x')\n\nplt.title('Traslation of Functions')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThere are largely two types of transofrmations of functions: stretching and reflecting. To be more sepecific, if \\(c\\) is larger than 1, then\n\n\\(y=cf(x)\\), stretch the graph of \\(y=f(x)\\) vertically by a factor of \\(c\\)\n\\(y=\\frac{1}{c}f(x)\\), shrink the graph of \\(y=f(x)\\) vertically by a factor of \\(c\\)\n\\(y=f(cx)\\), shrink the graph of \\(y=f(x)\\) horizontally by a factor of \\(c\\)\n\\(y=f(\\frac{x}{c})\\), stretch the graph of \\(y=f(x)\\) horizontally by a factor of \\(c\\)\n\n\n\ndraw \\(y=\\sin x\\), \\(y=\\sin 2x\\), \\(y=\\frac{1}{2} \\sin x\\)\n\n\nCode\nx = np.linspace(-10, 10, 1000)\ny = np.sin(x)\ny2 = np.sin(2*x)\ny3 = np.sin(x/2)\ny4 = 2*np.sin(x)\ny5 = np.sin(x)/2\n\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.grid(True, which='both')\n\nplt.plot(x,y,color='black',label='y=sin(x)')\nplt.plot(x,y2,color='red',label='y=sin(2x)')\nplt.plot(x,y3,color='blue',label=r'y=sin($\\frac{x}{2}$)')\nplt.plot(x,y4,color='green',label=r'y=2sin(x)')\nplt.plot(x,y5,color='orange',label=r'y=$\\frac{1}{2}$sin(x) or (2y)=sin(x)')\n\nplt.title('Trasformation of Functions')\nplt.legend(shadow=True, loc=(-0.2, 1.05), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\n\n\n\ndraw \\(x^2+y^2=1\\), \\(\\frac{x^2}{4}+\\frac{y^2}{9}=1\\), \\((x-3)^2+(y-3)^2=1\\), \\(\\frac{(x-3)^2}{4}+\\frac{(y-3)^2}{9}=1\\)\n\n\n\nCode\n# initialize x and y using radian\n# theta = np.linspace(0, 2*np.pi, 1000)\n# x = np.cos(theta)\n# y = np.sin(theta)\n\n# initialize x and y without using radian\nx = np.linspace(-1, 1, 1000)\ny1 = np.sqrt(1 - x*x)\ny2 = -np.sqrt(1 - x*x)\n\nfig, ax = plt.subplots()\n\n# unit circle\nax.plot(x, y1, color='black', label=r'$x^2+y^2=1$')\nax.plot(x, y2, color='black')\nax.plot(0,0,'o',color='black', label='(0,0)')\n\n# unit circle translated by 2\nax.plot(x+3, y1+3, color='red', label=r'$(x-3)^2+(y-3)^2=1$')\nax.plot(x+3, y2+3, color='red')\nax.plot(3,3,'o',color='red', label='(3,3)')\n\n# eplipse: a unit circle transformed by 2,3 in x, y\nax.plot(2*x, 3*y1, color='blue', label=r'$\\frac{x^2}{4}+\\frac{y^2}{9}=1$')\nax.plot(2*x, 3*y2, color='blue')\nax.plot(0,-np.sqrt(9-4), 'o', color='blue', label=r'$F_1=(0,\\sqrt{5}),F_2=(0,-\\sqrt{5})$')\nax.plot(0,np.sqrt(9-4), 'o', color='blue')\n\n# translated eplipse: a unit circle translated by 2 and transformed by 2,3 in x, y\nax.plot(2*x+3, 3*y1+3, color='green', label=r'$\\frac{(x-3)^2}{4}+\\frac{(y-3)^2}{9}=1$')\nax.plot(2*x+3, 3*y2+3, color='green')\nax.plot(0+3,-np.sqrt(9-4)+3, 'o', color='green', label=r'$F_1=(3,\\sqrt{5}+3),F_2=(3,-\\sqrt{5}+3)$')\nax.plot(0+3,np.sqrt(9-4)+3, 'o', color='green')\n\n# height/width=1\nax.set_aspect(1)\n\nax.set_xlim([-4, 7])\nax.set_ylim([-4, 7])\n\nax.grid(True)\nax.set_title(\"Transformation of Functions\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\n\nplt.legend(shadow=True, loc=(-0.5, 1.05), handlelength=1.5, fontsize=8)\n\n# show the plot\nplt.show()\n\n\n\n\n\n\nTry using \\(y=\\sin (\\theta)\\), \\(x=\\cos (\\theta)\\), by yourself, to draw \\(x^2+y^2=1\\), \\(\\frac{x^2}{4}+\\frac{y^2}{9}=1\\), \\((x-3)^2+(y-3)^2=1\\), \\(\\frac{(x-3)^2}{4}+\\frac{(y-3)^2}{9}=1\\)\n\n\n\n\n\n\n\\(y=-f(x)\\), reflect the graph of \\(y=f(x)\\) about the x-axis\n\\(y=f(-x)\\), reflect the graph of \\(y=f(x)\\) about the y-axis\n\\(-y=f(-x)\\), reflect the graph of \\(y=f(x)\\) about the origin on the 2D plain\n\\(x=f(y)\\), reflect the graph of \\(y=f(x)\\) about the \\(y=x\\)\n\n\n\n\ndraw \\(y=\\sin x\\), \\(y=\\sin (-x)\\), \\(y=-\\sin x\\), \\(-y=-\\sin x\\), \\(x=-\\sin y\\)\n\n\n\nCode\nx = np.linspace(0, 2*np.pi, 1000)\nx1 = np.linspace(-2*np.pi, 2*np.pi, 1000)\ny = np.sin(x)\n\nfig, ax = plt.subplots()\n\n# unit circle\nax.plot(x, y, color='black', label='$y=\\sin (x)$')\nax.plot(-x, y, color='blue', label='$y=\\sin (-x)$')\nax.plot(x, -y, color='green', label='$y=-\\sin (x)$')\nax.plot(-x, -y, color='orange', label='$-y=-\\sin (x)$')\nax.plot(x1,x1, color='red', label='$y=x$')\nax.plot(np.sin(y),x, color='black', label='$y=sin^{-1}(x)$',linestyle='dashed')\n\n# height/width=1\nax.set_aspect(1)\n\nax.set_xlim([-7, 7])\nax.set_ylim([-7, 7])\n\nax.grid(True)\nax.set_title(\"Reflection of Functions\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\n# show the plot\nplt.show()\n\n\n\n\n\n\nDraw \\(S(x)=\\frac{1}{1+e^{-x}}\\)\n\n\n\nCode\nx = np.linspace(-10, 10, 1000)\ny = 1/(1+np.exp(-x))\n\nfig, ax = plt.subplots()\n\n# unit circle\nax.plot(x, y, color='black', label='$y=S(x)$')\nax.plot(-x, y, color='blue', label='$y=S(-x)$')\nax.plot(x, -y, color='green', label='$y=-S(x)$')\nax.plot(-x, -y, color='orange', label='$-y=-S(x)$')\nax.plot(x,x, color='red', label='$y=x$')\nax.plot(np.sin(y),x, color='black', label='$y=S^{-1}(x)$',linestyle='dashed')\n\n# height/width=1\nax.set_aspect(1)\n\nax.set_xlim([-3, 3])\nax.set_ylim([-3, 3])\n\nax.grid(True)\nax.set_title(\"Reflection of Functions\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nDistribution of Raw Data\n\n\n\nCode\ndata1 = np.random.normal(loc=5,scale=6.0,size=1000) #loc=mean, scale=sd\ndata2 = np.random.normal(loc=-4,scale=2.0,size=1000)\ndata3 = np.random.normal(loc=-7,scale=2.5,size=1000)\ndata4 = np.random.normal(loc=0,scale=1.0,size=1000)\n\n# the range of values to evaluate the PDF\nx = np.linspace(data2.min(), data1.max(), 10000)\n\nbins_number=100\n\n# Plot the data and PDF\nplt.hist(data1, density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(5,6^2)$')\nplt.hist(data2, density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-4,2^2)$')\nplt.hist(data3, density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-7,2.5^2)$')\nplt.hist(data4, density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Raw Data\")\nplt.show()\n\n\n\n\n\n\nDistribution of Translated Raw Data\n\n\n\nCode\n# Plot the data and PDF\nplt.hist(data1-data1.mean(), density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(5-5,6^2)$')\nplt.hist(data2-data2.mean(), density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-4-(-4),2^2)$')\nplt.hist(data3-data3.mean(), density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-7-(-7),2.5^2)$')\nplt.hist(data4-data4.mean(), density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0-0,1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Translated Raw Data\")\nplt.show()\n\n\n\n\n\n\nDistribution of Transformed Raw Data\n\n\n\nCode\n# Plot the data and PDF\nplt.hist(data1/data1.std(), density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(5,6^2/6^2)$')\nplt.hist(data2/data2.std(), density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-4,2^2/2^2)$')\nplt.hist(data3/data3.std(), density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-7,2.5^2/5^2)$')\nplt.hist(data4/data4.std(), density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1/1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Transformed Raw Data\")\nplt.show()\n\n\n\n\n\n\nDistribution of Standardized Raw Data\n\n\n\nCode\n# Plot the data and PDF\nplt.hist((data1-data1.mean())/data1.std(), density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\nplt.hist((data2-data2.mean())/data2.std(), density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\nplt.hist((data3-data3.mean())/data3.std(), density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\nplt.hist((data4-data4.mean())/data4.std(), density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Standardized Raw Data\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTransformation in Statistics? ex) transformation of random variable\nTransformation in Linear Algebra? ex) linear transformation or linear mapping (transformation matrix)\nTransformation in Machine Learning? ex) scaling (min-max normalization or standardization)\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/function/composite_function.html",
    "href": "docs/blog/posts/Mathmatics/function/composite_function.html",
    "title": "Composite Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nDefinition 1 Given two functions f and g, the composite function \\(f\\circ g\\) (also called the composition of \\(f\\) and \\(g\\)) is defined by\n\\[\n(f\\circ g)(x)=f(g(x))\n\\tag{1}\\]\n\nThe domain of \\(f\\circ g\\) is the set of all \\(x\\) in the domain of \\(g\\) such that \\(g(x)\\) is in the domain of \\(f\\). In other words, \\((f\\circ g)(x)\\) is defined whenever both \\(g(x)\\) and \\(f(g(x))\\) are defined.\n\n\nIf \\(f(x)=x^2\\) and \\(g(x)=e^x\\), find \\(f\\circ g\\) and \\(g\\circ f\\).\n\\[\n\\begin{aligned}\n(f\\circ g)(x)&=f(g(x))=f(e^x)=(e^x)^2=e^{2x}\\\\\n(g\\circ f)(x)&=g(f(x))=g(x^2)=e^{x^2}=e^{x^2}\\\\\n\\end{aligned}\n\\]\n위의 예제처럼, \\(f\\circ g \\ne f\\circ g\\).\n\n\n\n\n\n\n\nDefinition 2 Given two functions f and g, the composite function \\(f\\circ g\\) (also called the composition of \\(f\\) and \\(g\\)) is defined by\n\\[\n(f\\circ g)(x)=f(g(x))\n\\tag{2}\\]\n\nThe domain of \\(f\\circ g\\) is the set of all \\(x\\) in the domain of \\(g\\) such that \\(g(x)\\) is in the domain of \\(f\\). In other words, \\((f\\circ g)(x)\\) is defined whenever both \\(g(x)\\) and \\(f(g(x))\\) are defined.\n\n\nIf \\(f(x)=x^2\\) and \\(g(x)=e^x\\), find \\(f\\circ g\\) and \\(g\\circ f\\).\n\\[\n\\begin{aligned}\n(f\\circ g)(x)&=f(g(x))=f(e^x)=(e^x)^2=e^{2x}\\\\\n(g\\circ f)(x)&=g(f(x))=g(x^2)=e^{x^2}=e^{x^2}\\\\\n\\end{aligned}\n\\]\nLike the example above, \\(f\\circ g \\ne f\\circ g\\).\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-21_transformation/index.html",
    "href": "docs/blog/posts/statistics/2023-02-21_transformation/index.html",
    "title": "Transformation of Random Variables",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n확률 변수 2개 이상에 대한 확률 분포를 joint probability distribution (결합확률분포)라고 하는데 두 확률 변수 \\(X\\) 와 \\(Y\\) 의 관계에 대해서 규명해야 할 때가 있다. 예를 들어, X 와 Y의 높은 상관계수라든지 또는 비선형적인 관계가 관찰될 때 그 관계가 수리적으로 모델링이 가능하고 한 확률 변수의 분포에 대한 정보를 알고있다면 미지의 다른 확률 변수의 분포가 추정가능해진다. 이 때 두 변수에 대한 관계 정도가 높으면 높을수록 추정이 쉬워진다.\n이번 블로그에서는 주어진 확률 변수 \\(X\\) 에 대해서 \\(X\\) 의 pmf (probability mass function) 또는 pdf (probability density function) \\(f_x(x)\\) 를 알고있을 때 확률 변수에 \\(X\\) 에 적절한 함수의 변환을 적용해 확률 변수 \\(Y\\) 를 \\(Y=u(X)\\) 라는 관계식이 정의 가능할 때 \\(Y\\) 의 pmf 또는 pdf를 구하는 방법에 집중한다. 후에 MGF (Momment Generating Function) 학습에 응용될 수 있는 개념으로 잘 정리할 필요가 있다.\n\n\n\n\nTheorem 1 Discrete random variable \\(X\\) 의 probability distribution이 \\(f_X(x)\\) 이고 \\(X\\) 와 \\(Y\\) 사이에는 \\(Y=u(X)\\) 라는 one-to-one relation이 성립될 때 \\(y=u(x)\\) 를 유일한 \\(x\\) 를 \\(y\\) 에 대한 함수인 \\(x=w(y)\\) 로 표현 가능하다면 \\(Y\\) 의 probability distribution는 \\[\nf_Y(y)=f_X(w(y))\n\\] 이다.\n\n\n\n\nIn the case of a One-to-One relation\n\n동전을 독립적으로 2번 던질 때, 확률 변수 \\(X\\) 가 앞면이 나오는 수라고 정의했을 때, 확률 분포 아래 표 (a)와 같다. \\(Y=2X+1\\) 라는 관계가 성립할 때 \\(Y\\) 의 분포는 아래 표 (b)와 같다.\n\n\nTable 1: Exmaple: Transformation of Discrete Random Variable (One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\\(X\\)\n0\n1\n2\n\n\n\n\n\\(P_X(X=x)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y=2X+1\\)\n\n\n\\(Y=2X+1\\)\n1\n3\n5\n\n\n\n\n\\(P_Y(Y=y)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n\nIn the case of not a One-to-One relation\n\n동전을 독립적으로 2번 던질 때, 확률 변수 \\(X\\) 가 앞면이 나오는 수의 합이라고 정의 했을때, \\(X\\) 의 확률 분포는 아래 표 (a) 와 같다. 이 때 \\(Y=mod(X,2)\\) 라는 관계가 성립할 때 \\(Y\\) 의 분포는 아래 표 (b) 같다.\n\n\nTable 2: Exmaple: Transformation of Discrete Random Variable (Not One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\\(X\\)\n0\n1\n2\n\n\n\n\n\\(P_X(X=x)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y=mod(x,2)\\)\n\n\n\\(Y=mod(x,2)\\)\n0\n1\n\n\n\n\n\\(P_Y(Y=y)\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{2}{4}\\)\n\n\n\n\n\n\n위의 예시와 같이 두 확률 변수가 one to one 관계일 때는 확률분포가 그대로 유지되어서 쉽게 변환된 확률 변수의 분포가 추정가능하지만 one to one 관계가 아닐 경우 확률 분포가 바뀌게 된다.\n\nanother example (In the case of not a One-to-One relation)\n\n기하분포 (Geometric Distribution)란 동일한 베르누이 (Bernoulli) 분포의 시행을 독립적으로 반복할 때 첫 성공까지의 시행 횟수를 확률변수 \\(X\\) 로 하는 분포이다. 즉, \\(x-1\\) 번째까지 베르누이 시행이 실패하고 \\(x\\) 번째 시행에서 성공할 확률 분포를 말한다.\nNotation은 \\(X \\sim Geometric(p)\\) 또는 \\(X \\sim Geo(p)\\) 라고 표현하고, \\(p\\) 는 독립 시행에서 성공할 확률이다. (참고: \\(text{E}(X)=\\frac{1}{p}\\), \\(\\text{Var}(X)=\\frac{1-p}{p^2}\\))\n\\(X \\sim Geometric(\\frac{4}{5})\\) 일 때 \\(X\\) 의 확률분포 \\(f(x)=\\frac{4}{5}(\\frac{1}{5})^{(x-1)}\\), \\(x=1,2,3 ...\\) 가 기하분포일 때 \\(Y=X^2\\) 의 확률분포는\n\\[\n\\begin{aligned}\n  y&=u(x)=x^2 \\\\\n  x&=w(y)=\\sqrt{y} \\text{ } (x>0)\\\\\n  f_Y(y)&=f_X(w(y))=f_X(\\sqrt{y})=\\frac{4}{5}(\\frac{1}{5}^{(\\sqrt{y}-1)})\\\\\n  \\therefore f_Y(y)&=\n  \\begin{cases}\n    \\frac{4}{5}(\\frac{1}{5}^{(\\sqrt{y}-1)}) &\\text{if} \\text{  } y =1, 4, 9, ...\\\\\n     0 \\text{  } & \\text{otherwise}\n    \\end{cases}\n\\end{aligned}\n\\]\n이다.\n\n\n\n\n\nTheorem 2 Two discrete random variables \\(X_1\\) 과 \\(X_2\\) 의 joint probability distribution이 \\(f(x_1,x_2)\\) 이고 \\((x_1,x_2)\\) 와 \\((y_1,y_2)\\) 가 one-to-one relation이 성립되어 \\(y_1=u_1(x_1,x_2)\\) 와 \\(y_2=u_2(x_1,x_2)\\) 를 유일한 \\(x_1=w_1(y_1,y_2)\\) 와 \\(x_2=w_2(y_1,y_2)\\) 의 함수로 표현 가능하다면 새로운 확률변수 \\(\\mathbf{Y}\\), 즉, \\(Y_1\\) 과 \\(Y_2\\) 의 joint probability distribution는 \\[\nf_Y(y_1,y_2)=f_X(w_1(y_1,y_2),w_2(y_1,y_2))\n\\] 이다.\n\n\n\n\nexmaple 1\n\n다항 분포(multinomial distribution)란 2개 이상의 독립적인 확률 변수 \\(\\mathbf{X}=X_1, X_2, ...\\) 들에 대한 확률분포로, 여러 독립 시행에서 각 각의 값이 특정 횟수가 나타날 확률을 정의하고 독립 변수가 2개인 경우 다항 분포의 특별한 case로 이항 분포 (binomial distribution)가 된다.\n참고( Source: wiki) :\n\\[\n\\begin{aligned}\n  f_(x) & =\\frac{n!}{x_1!x_2!\\dots x_k!}p_1^{x_1}p_2^{x_2}\\dots p_k^{x_k}\\\\\n  \\text{E}(x)&=np_i\\\\\n  \\text{Var}(x)&=np_i(1-p_i)\n\\end{aligned}\n\\]\n\\(X_1\\) and \\(X_2\\) 가 다음과 같은 multinomial distribution을 따를 때 \\[\nf(x_1,x_2) = \\binom{2}{x_1,x_2,2-x_1-x_2}\\frac{1}{4}^{x_1}\\frac{1}{3}^{x_2}\\frac{5}{12}^{2-x_1-x_2} x_1=0,1,2, \\space x_2=0,1,2, \\space x_1+x_2\\le 2\n\\]\n\\(Y_1=X_1+X_2\\) 와 \\(Y_2=X_1-X_2\\) 의 결합 확률 분포는\n\\[\n\\begin{aligned}\n  y_1&=u_1(x_1,x_2)=x_1+x_2\\\\\n  y_2&=u_2(x_1,x_2)=x_1-x_2\\\\\n  x_1&=w_1(y_1,y_2)=\\frac{y_1+y_2}{2}\\\\\n  x_2&=w_2(y_1,y_2)=\\frac{y_1-y_2}{2}\\\\\n  f_Y(y_1,y_2)&=\\binom{2}{\\frac{y_1+y_2}{2},\\frac{y_1-y_2}{2},2-y_1 }\\frac{1}{4}^{\\frac{y_1+y_2}{2}}\\frac{5}{12}^{2-y_1} y_1=0,1,2, \\space y_2=-2,-1,0,1,2\n\\end{aligned}\n\\] 이다.\n\nexmaple 2\n\n\\(X_1\\) 과 \\(X_2\\) 의 결합확률분포가 다음과 같을 때 변환된 \\(Y_1=X_1+X_2\\) 와 \\(Y_2=X_1X_2\\) 의 결합확률 분포는?\n\n\nTable 3: Exmaple: Transformation of Joint Discrete Random Variable (Not One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\n\n\n\n\n\n\n\\((x_1,x_2)\\)\n(0, 0)\n(0, 1)\n(1, 0)\n(1, 1)\n\n\n\n\n\\(f_X(x_1,x_2)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y\\)\n\n\n\\((y_1,y_2)\\)\n(0, 0)\n(1, 0)\n(2, 1)\n\n\n\n\n\n\\(f_Y(y_1,y_2)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 3 continuous random variable \\(X\\) 의 probability distribution이 \\(f_X(x)\\) 이고 \\(X\\) 와 \\(Y\\) 사이에는 \\(Y=u(X)\\) 라는 one-to-one relation이 성립될 때 \\(y=u(x)\\) 를 유일한 \\(x\\) 를 \\(y\\) 에 대한 함수인 \\(x=w(y)\\) 로 표현 가능할 때 Y의 확률 분포는 \\[\nf_Y(y)=f_X(w(y))|J|\n\\] 이다. (\\(J=w'(y)\\), called ‘Jacobian’)\n\n\n\n\nIn the case of a One-to-One relation\n\n지수분포(exponential distribution)는 연속 확률 분포 중 하나로, 사건이 서로 독립적일 때, 일정 시간동안 발생하는 사건의 횟수가 푸아송 분포를 따른다면, 다음 사건이 발생할 때까지의 대기 시간을 확률 변수 \\(X \\in [0, \\infty)\\) 로 하는 분포이다. (Source: Wiki).\n참고: \\[\n\\begin{aligned}\n  X&\\sim Exp(\\lambda) \\\\\n  \\text E[X] &= \\frac{1}{\\lambda}\\\\\n  \\text{Var}[X] &= \\frac{1}{\\lambda^2}\\\\\n  f(x;\\lambda)&=\n  \\begin{cases}\n    \\lambda e^{-\\lambda x} & x \\ge 0\\\\\n    0 & x<0\n  \\end{cases}\\\\\n\\end{aligned}\n\\] \\(\\lambda>0\\) is the parameter of the distribution, called ‘rate parameter’.\nTransformation of Linear Function. When two random variables \\(X \\sim \\text{exp}(\\lambda)\\) and \\(Y\\) has the relation, \\(Y=aX+b\\) \\((a\\ne 0)\\). When \\(f_X(x)=\\lambda e^{-\\lambda x}\\), \\(f_Y(y)\\) is\nBy Theorem\n\\[\n\\begin{aligned}\n  Y&=u(X)=aX+b\\\\\n  y&=u(x)=ax+b\\\\\n  x&=w(y)=\\frac{y-b}{a}\\\\\n  f_Y(y)&=f_X(w(y))|J|=f_X(\\frac{y-b}{a})|J|\\\\\n        &=f_X(\\frac{y-b}{a})|w'(y)|\\\\\n        &=f_X(\\frac{y-b}{a})|\\frac{1}{a}|\\\\\n        &=\\frac{f_X(\\frac{y-b}{a})}{|a|}\\\\\n\\end{aligned}\n\\]\nBy Definition\n\n\n\n\\(a>0\\) \\[\n\\begin{aligned}\nF_Y(Y)&=P_Y(Y\\le y)\\\\\n      &=P_Y(aX+b\\le y)\\\\\n      &=P_Y(X\\le \\frac{y-b}{a})\\\\\n      &=F_X(\\frac{y-b}{a})\\\\\nF'_Y(Y)&=\\frac{d}{dy}F_X(\\frac{y-b}{a})\\\\\n       &=f_X(\\frac{y-b}{a})\\frac{1}{a}\\\\\n\\end{aligned}\n\\]\n\n\n\n\\(a\\le 0\\) \\[\n\\begin{aligned}\nF_Y(Y)&=P_Y(Y\\le y)\\\\\n      &=P_Y(aX+b\\le y)\\\\\n      &=P_Y(X> \\frac{y-b}{a})\\\\\n      &=1-F_X(\\frac{y-b}{a})\\\\\nF'_Y(Y)&=\\frac{d}{dy}(1-F_X(\\frac{y-b}{a}))\\\\\n       &=-f_X(\\frac{y-b}{a})\\frac{1}{a}\\\\  \n\\end{aligned}\n\\]\n\n\\[\n\\therefore f_Y(y)=\\frac{f_X(\\frac{y-b}{a})}{|a|}=\\frac{\\lambda exp(-\\lambda (\\frac{y-b}{a}))}{|a|}\n\\]\n\n위의 정리를 유도를 하진 않았지만 증명을 해보면 사실, \\(|J|\\) 는 역함수의 미분의 계수인 것을 할 수 있다. 위의 예시를 By definition 으로 푼 것을 보면 \\(y=u(x)\\) 와 \\(x=w(y)\\) 의 관계인 것을 알 수 있고 \\(|J|\\) 에 해당되는 \\(\\frac{1}{a}\\) 는 CDF \\(F(Y)\\) 의 derivative를 구하는 과정에서 발생하는 chain rule에 의해 생긴 것을 알 수 있다.\n\\[\n\\begin{aligned}\n  y&=u(x)\\\\\n  \\frac{dy}{dx}&=u'(x)\\\\\n  x&=w(y)=u^{-1}(y)\\\\\n  \\frac{dx}{dy}&=w'(y) \\\\\n  w'(y)&=\\frac{dx}{dy}=(\\frac{dy}{dx})^{-1}=\\frac{1}{u'(x)}\n\\end{aligned}\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 20, 1000)\ny = 2*x+1\ny2 = x/2+1\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,x,color='black',label=r'$y=x$', linestyle='dashed')\nax.plot(x,2*x-2,color='black',label=r'$x=w(y) \\rightarrow y=2x-2 $')\nax.plot(x,y2,color='red',label=r'$y=u(x)=\\frac{1}{2}x+1$')\nax.set_aspect(1)\nax.set_title(r\"Example of Relation of X and Y, $y=u(x)$ vs $x=w(y)$\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\nax.set_xlim(-20, 40)\n\nplt.legend(shadow=True, loc=(-0.5, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 20, 1000)\ny = 2*x+1\ny2 = x/2+1\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,y,color='black',label=r'$y=2x+1$')\nax.plot(x,y2,color='red',label=r'$y=\\frac{1}{2}x+1$')\nax.set_aspect(1)\nax.set_title(r\"Transformation of Linear Function, $f_Y(y)=\\frac{f_X(\\frac{y-b}{a})}{|a|}$\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y=u(X)\")\nax.set_xlim(-20, 40)\nax.text(10, 0,'|', color='black', horizontalalignment='center', verticalalignment='center')\nax.text(10, -5, r'$\\frac{y-b}{a}$', horizontalalignment='center',color='black')\nax.text(0, 5,'--', color='black', horizontalalignment='center', verticalalignment='center')\nax.text(-9, 5, r'$ax+b$', verticalalignment='center',color='black')\n\nplt.legend(shadow=True, loc=(-0.5, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\\(P(Y\\le y)=P(X\\le \\frac{y-b}{a})\\) 이기 때문에 \\(X\\) 의 특정 구간의 확률과 대응되는 \\(Y\\) 의 구간의 확률이 같다고 했을 때 \\(Y \\le y\\) 의 범위는 \\(X\\) 를 \\(b\\) 만큼 이동한 후 \\(a\\) 배한 \\(x\\) 좌표 이하 구간에 대응 된다 즉, \\(X\\le \\frac{y-b}{a}\\). 이 때, 두 구간에서의 확률 밀도가 같아야 하기 때문에 \\(f_x(x)\\) 가 \\(f_Y(y)\\) 의 \\(\\frac{1}{a}\\) 배 인 것을 알 수 있다.\n위의 그림에서 처럼, 두 확률 변수 \\(X\\) 와 \\(Y\\) 의 관계를 \\(y = 2x+1\\) \\((a>1)\\) 와 \\(y = \\frac{1}{2}x+1\\) \\((0 \\le a\\le 1)\\) 로 두 가지의 예로 들면, 위의 그래프의 경우, \\(y = 2x+1\\) \\((a>1)\\) 일 때 \\(0<X<10\\) 는 \\(1<Y<21\\) 의 구간이 2배가 되는 것을 관찰 할 수 있다. 반대로, \\(y = \\frac{1}{2}x+1\\) \\((0 \\le a\\le 1)\\) 일 때 \\(0<X<10\\) 는 \\(1<Y<6\\) 의 구간이 \\(\\frac{1}{2}\\) 배가 된다. 이 때 각 예시의 경우에 \\(X\\) 와 \\(Y\\) 의 확률 값이 같기 때문에 (\\(P(Y\\le y)=P(X\\le \\frac{y-1}{2})\\) 와 \\(P(Y\\le y)=P(X\\le \\frac{y-1}{\\frac{1}{2}})\\)). \\(f_x(x)\\) 가 \\(f_Y(y)\\) 의 각 각 \\(2\\) 배와 \\(\\frac{1}{2}\\) 배 인 것을 추정 할 수 있다.\n\nIn the case of not a One-to-One relation\n\n\\(Y=X^2\\) 일 때,\n\\[\n\\begin{aligned}\n  F_Y(Y)&=P_Y(Y\\le y)\\\\\n        &=P_Y(X^2\\le y)\\\\\n        &=P_Y(-\\sqrt{y}\\le X \\le \\sqrt y)\\\\\n        &=F_X(\\sqrt{y})-F_X(-\\sqrt{y})\\\\\n  F'_Y(Y)&=\\frac{d}{dy}(F_X(\\sqrt{y})-F_X(-\\sqrt{y}))\\\\\n        &=f_X(\\sqrt{y})\\frac{1}{2\\sqrt{y}}+f_X(-\\sqrt{y})\\frac{1}{2\\sqrt{y}}\\\\\n        &=\\frac{f_X(\\sqrt{y})}{2\\sqrt{y}}+\\frac{f_X(-\\sqrt{y})}{2\\sqrt{y}}\\\\\n\\end{aligned}\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 1000)\ny = x*x\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,y,color='black',label=r'$y=x^2$')\n#ax.text(3.0,0.5,r'$\\sqrt{y}$')\n#ax.text(-4,0.5,r'$-\\sqrt{y}$')\nax.set_xlim([-10, 10])\nax.set_ylim([0, 10])\n\nax.set_aspect(1)\nax.set_title(r\"Transformation of Random Variables, $y=x^2$\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\n\nplt.legend(shadow=True, loc=(-0.2, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\\(X\\) 의 분포를 알고 \\(X\\) 와 \\(Y\\) 의 relation을 알고있을 때 \\(X\\) 의 분포로부터 \\(Y\\) 의 분포를 추정 가능한 것의 이점 중 하나는 \\(Y\\) 의 통계량을 \\(Y\\) 의 분포를 사용하지 않고 계산해낼 수 있다는 점이다.\n예를 들어, \\(X \\sim N(0,1)\\) 이고 \\(Y=u(X)\\) 라는 관계를 알고있을 때, \\(Y\\) 의 통계량 \\(E(Y)\\), \\(Var(Y)\\) 를 알고싶다면 굳이 힘들게 \\(Y\\) 의 분포를 계산할 필요가 없다. 이미 \\(X\\) 의 확률 분포 \\(f_X(x)\\) 를 알고있기 때문에 아래와 같이 \\(Y\\) 의 통계량을 계산해낼 수 있다.\n\\[\n\\begin{aligned}\n  \\text{E}(Y)&=\\int Y f_Y(y) dy\\\\\n            &=\\int u(X) f_X(x) dx\\\\\n  \\text{E}(Y^2)&=\\int Y^2 f_Y(y) dy\\\\\n            &=\\int u(X)^2 f_X(x) dx\\\\\n  \\text{Var}(Y)&=\\text{E}(Y^2)-\\text{E}(Y)^2\\\\\n  &=\\text{E}(g(X)^2)-\\text{E}(g(X))^2\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\nTheorem 4 Two continuous random variable \\(X_1\\) 과 \\(X_2\\) 의 joint probability distribution이 \\(f(x_1,x_2)\\) 이고 \\((x_1,x_2)\\) 와 \\((y_1,y_2)\\) 가 one-to-one relation이 성립될 때 \\(y_1=u_1(x_1,x_2)\\) 와 \\(y_2=u_2(x_1,x_2)\\) 를 유일한 \\(x_1=w_1(y_1,y_2)\\) 와 \\(x_2=w_2(y_1,y_2)\\) 의 함수로 표현 가능할 때 \\(Y_1=u_1(X_1,X_2)\\) 와 \\(Y_2=u_2(X_1,X_2)\\) 로 정의되는 새로운 확률변수 \\(Y_1\\) 과 \\(Y_2\\) 의 joint probability distribution는 \\[\ng(y_1.y_2)=f(w_1(y_1,y_2),w_2(y_1,y_2)) |J|\n\\] 이다.\n여기서, \\[\n\\begin{aligned}\n  |J|=|\n  \\begin{bmatrix}\n  \\frac{\\partial x_1}{\\partial y_1}&\\frac{\\partial x_1}{\\partial y_2}\\\\\n  \\frac{\\partial x_2}{\\partial y_1}&\\frac{\\partial x_2}{\\partial y_2}\\\\\n  \\end{bmatrix}| = | \\begin{bmatrix}\n  \\frac{\\partial w_1(y_1,y_2)}{\\partial y_1}&\\frac{\\partial w_1(y_1,y_2)}{\\partial y_2}\\\\\n  \\frac{\\partial w_2(y_1,y_2)}{\\partial y_1}&\\frac{\\partial w_2(y_1,y_2)}{\\partial y_2}\\\\\n  \\end{bmatrix} |\n\\end{aligned}\n\\] called ‘the determinant of jacobian matrix’\n\n\n\n\n예제\n\n연속확률변수 \\(X_1\\) 과 \\(X_2\\) 는 결합확률분포 \\[\nf(x_1,x_2)=\n  \\begin{cases}\n    4x_1x_2, & \\text{if  } 0<x_1<1,& 0< x_2 <1 \\\\\n    0, & \\text{otherwise}\n  \\end{cases}\n\\] 일 때, \\(Y_1=X_1^2\\) 과 \\(Y_2=X_1X_2\\) 의 결합확률 분포는\n\\[\n\\begin{aligned}\ny_1&=u_1(x_1,x_2)= x_1^2\\\\\ny_2&=u_2(x_1,x_2)= x_1x_2\\\\\nx_1&=w_1(y_1,y_2)= \\sqrt{y_1} & (y_1>0)\\\\\nx_2&=w_2(y_1,y_2)= \\frac{y_2}{x_1} = \\frac{y_2}{\\sqrt{y_1}}  & (y_1>0)\\\\\ng(y_1.y_2)&=f(w_1(y_1,y_2),w_2(y_1,y_2))|J|\\\\\ng(y_1.y_2)&=f(w_1(y_1,y_2),w_2(y_1,y_2))|J|\\\\\n          &=4\\sqrt{y_1}\\frac{y_2}{\\sqrt{y_1}}\\frac{1}{2y_1}\\\\\n          &=\\begin{cases}\n          \\frac{2y_2}{y_1} & \\text{if  } y_2^2<y_1<1, \\text{  } 0<y_2<1\\\\\n          0 & \\text{otherwise}\n          \\end{cases}\\\\\n          |J|&=|\n          \\begin{bmatrix}\n          \\frac{\\partial x_1}{\\partial y_1}&\\frac{\\partial x_1}{\\partial y_2}\\\\\n          \\frac{\\partial x_2}{\\partial y_1}&\\frac{\\partial x_2}{\\partial y_2}\\\\\n          \\end{bmatrix}|\\\\\n           &=|\n          \\begin{bmatrix}\n          \\frac{1}{2\\sqrt{y_1}}&0\\\\\n          y_2(-\\frac{1}{2}y_1^{\\frac{-3}{2}})&\\frac{1}{\\sqrt{y_1}}\\\\\n          \\end{bmatrix}|\\\\\n          &=\\frac{1}{2y_1}\n\\end{aligned}\n\\]\n이다.\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/ML/2023-02-03_tf_introduction/index.html",
    "href": "docs/blog/posts/ML/2023-02-03_tf_introduction/index.html",
    "title": "Tensor Introduction",
    "section": "",
    "text": "pytorch 이전 까지 deep learning을 위해 가장 많이 사용되었던 Framework\n2020년 이후로 pytorch를 더 많이 사용하지만 여전히 많은 사람들이 Tensor Flow 사용\n데이터 자료형으로 텐서(tensor) 객체를 사용\nTensorflow에서는 텐서(tensor)를 NumPy 배열처럼 사용할 수 있다.\nGPU 사용 지원\n\n\n\n\nGPU를 사용하면 TensorFlow나 pytorch에서 딥러닝 모델을 효과적 구현 가능\n각 텐서(tensor)와 연산이 어떠한 장치에 할당되었는지 출력할 수 있다.\n\n\n\nCode\nimport tensorflow as tf\n# placement 함수: 각 텐서와 연산이 어떠한 장치에 할당되었는지 출력하기\n#tf.debugging.set_log_device_placement(True)\n\n# 텐서 생성\na = tf.constant([\n    [1, 1],\n    [2, 2]\n])\nb = tf.constant([\n    [5, 6],\n    [7, 8]\n])\n\nc = tf.matmul(a, b)\nprint(\"matrix multiplication: \", c)\n\n#tf.debugging.set_log_device_placement(False)\n\n\nmatrix multiplication:  tf.Tensor(\n[[12 14]\n [24 28]], shape=(2, 2), dtype=int32)\n\n\n\n\nCode\nfrom tensorflow.python.client import device_lib\n# 구체적으로 사용 중인 장치(device) 정보 출력\ndevice_lib.list_local_devices()\n\n\n[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 379260854129484694\n xla_global_id: -1]\n\n\n\n\n\n\nTensorFlow에서의 텐서(tensor)는 기능적으로 넘파이(NumPy)와 매우 유사하다.\n기본적으로 다차원 배열을 처리하기에 적합한 자료구조로 이해할 수 있다.\nTensorFlow의 텐서는 “자동 미분” 기능을 제공한다.\nTensorFlow는 기능적으로 Pytorch와 거의 같음, 하지만 문법이 불편함\nTensorFlow 2.0부터는 pytorch와 문법적으로 유사\n\n\n\n\n\n특징\n\n기본적으로 다차원 배열을 처리하기에 적합한 자료구조로 이해할 수 있다\nTensorFlow에서의 텐서(tensor)는 기능적으로 넘파이(NumPy)의 ndarray 객체와 유사\n기본 python 데이터 유형을 자동 변환 (e.g., list)\nTensorFlow의 텐서는 “자동 미분” 기능을 제공한다.\n\n속성\n\n크기 (shape)\n자료형 (data type)\n저장된 장치, 가속기 메모리에 상주 가능 (e.g., GPU )\n\nNumpy 배열과 tf.Tensor의 차이점\n\n텐서는 가속기 메모리(GPU, TPU)에서 사용 가능\n텐서는 불변성(immutable)\n\n\n\n\n\n\n\nCode\n# 기본적인 모양(shape), 자료형(data type) 출력\n\ndata = [\n    [1, 2],\n    [3, 4]\n]\nx = tf.constant(data) # list -> tensor object로 변환\nprint(x)\nprint(tf.rank(x)) # 축(axis)의 개수 출력 = 차원의 개수 출력\n\ndata = tf.constant(\"String\") # 문자열 (string)도 int형 tensor로 변환 가능\nprint(data)\n\n# NumPy 배열에서 텐서를 초기화할 수 있다.\n\n## 파이썬의 리스트 넘파이는 compatible하다. 상호보완적으로 교체가 가능\n\na = tf.constant([5])\nb = tf.constant([7])\n\nc = (a + b).numpy()\nprint(c)\nprint(type(c))\n\nresult = c * 10\ntensor = tf.convert_to_tensor(result) # numpy -> tensor\nprint(tensor)\nprint(type(tensor))\n\n\ntf.Tensor(\n[[1 2]\n [3 4]], shape=(2, 2), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(b'String', shape=(), dtype=string)\n[12]\n<class 'numpy.ndarray'>\ntf.Tensor([120], shape=(1,), dtype=int32)\n<class 'tensorflow.python.framework.ops.EagerTensor'>\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\nprint(tf.math.add(1, 2))\nprint(tf.math.add([1, 2], [3, 4]))\nprint(tf.math.square(5))\nprint(tf.math.reduce_sum([1, 2, 3]))\n\n# Operator overloading is also supported\nprint(tf.math.square(2) + tf.math.square(3))\n\ndata = [\n    [1,2],\n    [3,4]\n]\nx = tf.constant(data)\nprint(x)\nprint(x.shape)\nprint(x.dtype)\nprint(tf.rank(x)) # tf.rank() : 축(axis)의 개수 출력 (차원의 개수)\n\n\ntf.Tensor(3, shape=(), dtype=int32)\ntf.Tensor([4 6], shape=(2,), dtype=int32)\ntf.Tensor(25, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(13, shape=(), dtype=int32)\ntf.Tensor(\n[[1 2]\n [3 4]], shape=(2, 2), dtype=int32)\n(2, 2)\n<dtype: 'int32'>\ntf.Tensor(2, shape=(), dtype=int32)\n\n\n\n\n\n\nTensorFlow 연산은 자동으로 NumPy 배열을 텐서(tensor)로 변환\nNumPy 연산은 자동으로 텐서(tensor)를 NumPy 배열로 변환\n\n\n\nCode\nimport numpy as np\nndarray = np.ones([3, 3])\nndarray\n\ntensor = tf.math.multiply(ndarray, 42)\ntensor\nnp.add(tensor, 1)\ntensor.numpy() # numpy.ndarray\ntype(tensor.numpy())\nctensor = tf.constant(ndarray)\nctensor\n\n\n<tf.Tensor: shape=(3, 3), dtype=float64, numpy=\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])>\n\n\n\n\n\n\n텐서(tensor) 객체 생성 (tf.Tensor)\ntf.ones_like(x) : 값이 1이고 x와 shape & data type이 동일한 텐서 생성\n\n\n\nCode\nx = tf.constant([\n    [5, 7],\n    [3, 2]\n])\n\nx_ones = tf.ones_like(x)\nx_ones\n     \n\nx = tf.constant([\n    [5.1, 7.0],\n    [3.4, 2.1]\n])\n\nx_ones = tf.ones_like(x)\nx_ones\n\n# tf.random.uniform(shape, dtype) : 랜덤 값으로 원하는 shape과 dtype을 갖는 텐서 생성\nx_rand = tf.random.uniform(shape=x.shape, dtype=tf.float32)\nx_rand\n\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[0.9109676 , 0.42210162],\n       [0.0669024 , 0.5198258 ]], dtype=float32)>\n\n\n\n\n\n특정 차원 접근\n\n\nCode\ntensor = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8],\n    [9,10,11,12]\n])\n\nprint(tensor[0])       # first row\nprint(tensor[:, 0])    # first column\nprint(tensor[..., -1]) # last column\n\n\ntf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\ntf.Tensor([1 5 9], shape=(3,), dtype=int32)\ntf.Tensor([ 4  8 12], shape=(3,), dtype=int32)\n\n\n텐서 Concatenate\naxis : 어느 축을 기준으로 객체를 이어붙일지 결정\naxis=0 : 0번째 축 (=row) axis=1 : 1번째 축 (=column)\n\n\nCode\ntensor = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8],\n    [9,10,11,12]\n])\n\ntensor_concat = tf.concat([tensor, tensor, tensor], axis=0) # row\ntensor_concat\n\ntensor_concat = tf.concat([tensor, tensor, tensor], axis=1) # column\ntensor_concat\n\n\n<tf.Tensor: shape=(3, 12), dtype=int32, numpy=\narray([[ 1,  2,  3,  4,  1,  2,  3,  4,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  5,  6,  7,  8,  5,  6,  7,  8],\n       [ 9, 10, 11, 12,  9, 10, 11, 12,  9, 10, 11, 12]])>\n\n\n\n\n\n\nCode\na = tf.constant([2])   # dtype: int32\nb = tf.constant([5.0]) # dtype: float32\n\nprint('a dtype: ', a.dtype, '\\nb dtype: ', b.dtype)\n\n\na dtype:  <dtype: 'int32'> \nb dtype:  <dtype: 'float32'>\n\n\n\n\nCode\na + b # dtype 불일치 -> InvalidArgumentError 발생\n\n\n\n\nCode\ntf.cast(a, tf.float32) + b # a의 dtype을 b의 dtype으로 변환 후 계산\n\n\n<tf.Tensor: shape=(1,), dtype=float32, numpy=array([7.], dtype=float32)>\n\n\n\n\n\n\n\nCode\nx = tf.Variable([1,2,3,4,5,6,7,8])\ny = tf.reshape(x, (4,2))           # row=4, col=2\ny\n\n\n<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4],\n       [5, 6],\n       [7, 8]])>\n\n\n\n\n\n\n\nCode\nx.assign_add([1,1,1,1,1,1,1,1])\nprint(x) # 1씩 더해짐\nprint(y) # 값 변화 X\n\n\n<tf.Variable 'Variable:0' shape=(8,) dtype=int32, numpy=array([2, 3, 4, 5, 6, 7, 8, 9])>\ntf.Tensor(\n[[1 2]\n [3 4]\n [5 6]\n [7 8]], shape=(4, 2), dtype=int32)\n\n\n\n\n\ntf.transpose(a, perm=[], ...) a의 차원 순서를 바꾼다. perm=[2, 1, 0]일 경우, a의 2번째 축을 첫번째로, 1번째 축을 두번째로, 0번째 축을 세번째로 교환하겠다는 의미\n\n\nCode\na = tf.random.uniform((64, 32, 3))\nprint(a.shape)\n\nb = tf.transpose(a, perm=[2, 1, 0]) # 차원 자체를 교환\nprint(b.shape)\n\n\n(64, 32, 3)\n(3, 32, 64)\n\n\n\n\n\nelement끼리 연산한다\n\n\nCode\na = tf.constant([\n    [1,2],\n    [3,4]\n])\nb = tf.constant([\n    [1,2],\n    [3,4]\n])\n\nprint(a + b)\nprint(a - b)\nprint(a * b)\nprint(a / b)\n\n\ntf.Tensor(\n[[2 4]\n [6 8]], shape=(2, 2), dtype=int32)\ntf.Tensor(\n[[0 0]\n [0 0]], shape=(2, 2), dtype=int32)\ntf.Tensor(\n[[ 1  4]\n [ 9 16]], shape=(2, 2), dtype=int32)\ntf.Tensor(\n[[1. 1.]\n [1. 1.]], shape=(2, 2), dtype=float64)\n\n\n\n\n\n\n\nCode\na = tf.constant([\n    [1,2],\n    [3,4]\n])\nb = tf.constant([\n    [1,2],\n    [3,4]\n])\ntf.matmul(a, b)\n\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 7, 10],\n       [15, 22]])>\n\n\n\n\n\n차원을 축소하며 평균을 계산\n\ntf.reduce_mean(a, axis=0) : 0차원(행)을 축소하여 평균 계산 -> 각 열에 대한 평균\ntf.reduce_mean(a, axis=1) : 1차원(열)을 축소하여 평균 계산 -> 각 행에 대한 평균\n\n\n\nCode\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\n\nprint(tf.reduce_mean(a))         # a 전체 평균\nprint(tf.reduce_mean(a, axis=0)) # 각 column에 대한 평균\nprint(tf.reduce_mean(a, axis=1)) # 각 row에 대한 평균\n\n\ntf.Tensor(4, shape=(), dtype=int32)\ntf.Tensor([3 4 5 6], shape=(4,), dtype=int32)\ntf.Tensor([2 6], shape=(2,), dtype=int32)\n\n\n\n\n\n차원을 축소하며 합계를 계산 (평균과 동일하게 동작)\n\n\nCode\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\n\nprint(tf.reduce_sum(a))         # a 전체 합계\nprint(tf.reduce_sum(a, axis=0)) # 각 column에 대한 합계\nprint(tf.reduce_sum(a, axis=1)) # 각 row에 대한 합계\n\n\ntf.Tensor(36, shape=(), dtype=int32)\ntf.Tensor([ 6  8 10 12], shape=(4,), dtype=int32)\ntf.Tensor([10 26], shape=(2,), dtype=int32)\n\n\n\n\n\n\nmax() : 원소의 최댓값 반환\nargmax() : 최댓값의 index를 반환\n\n\n\nCode\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\n\nprint(tf.reduce_max(a))         # a 전체 원소의 최댓값\nprint(tf.reduce_max(a, axis=0)) # 각 column에 대한 최댓값\nprint(tf.reduce_max(a, axis=1)) # 각 row에 대한 최댓값\nprint(tf.argmax(a, axis=0)) # 각 column에 대한 최댓값의 index\nprint(tf.argmax(a, axis=1)) # 각 row에 대한 최댓값의 index\n\n\ntf.Tensor(8, shape=(), dtype=int32)\ntf.Tensor([5 6 7 8], shape=(4,), dtype=int32)\ntf.Tensor([4 8], shape=(2,), dtype=int32)\ntf.Tensor([1 1 1 1], shape=(4,), dtype=int64)\ntf.Tensor([3 3], shape=(2,), dtype=int64)\n\n\n\n차원 축소\n\nsqueeze() : 크기가 1인 차원을 제거\n\n차원 확장\n\nexpand_dims() : 크기가 1인 차원을 추가\n흔히 배치(batch) 차원을 추가하기 위한 목적으로 사용됨\npytorch에서는 차원 축소 시, unsqueeze() 사용\n\n\n\n\nCode\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\nprint('original a shape: ', a.shape)\n\na = tf.expand_dims(a, 0) # 첫번째 축에 차원 추가\nprint('add 0th dims: ', a.shape)\n\na = tf.expand_dims(a, 3) # 세번째 축에 차원 추가\nprint('add 3rd dims: ', a.shape)\n\n\nprint(tf.squeeze(a).shape)         # 크기가 1인 차원을 모두 제거 \nprint(tf.squeeze(a, axis=3).shape) # 세번째 차원을 제거\n\n\n#tf.squeeze(a, axis=1) # 제거하려는 차원의 크기가 1이 아닐 경우 오류 발생\n\n\noriginal a shape:  (2, 4)\nadd 0th dims:  (1, 2, 4)\nadd 3rd dims:  (1, 2, 4, 1)\n(2, 4)\n(1, 2, 4)\n\n\n\n\n\n\n\n기울기 테이프 (Gradient Tape)\n중간 연산들을 테이프에 기록하고 역전파(back propagation)를 수행했을 때 기울기가 계산됨\nTensorFlow에서는 변수가 아닌 상수에 대해 기본적으로 기울기를 측정하지 않음 (not watched). 또한 변수여도 학습 가능하지 않으면(not trainable) 자동 미분을 사용하지 않음\n\n\n\nCode\nx = tf.Variable([3.0, 4.0])\ny = tf.Variable([3.0, 4.0])\n\n# with 구문 안에서 진행되는 모든 연산들을 기록\nwith tf.GradientTape() as tape:\n  z = x + y\n  loss = tf.math.reduce_mean(z)\n\ndx = tape.gradient(loss, x) # loss가 scalar이므로 계산 가능\nprint(dx)\n\n\ntf.Tensor([0.5 0.5], shape=(2,), dtype=float32)\n\n\nTensorFlow에서는 변수가 아닌 상수에 대해 기본적으로 기울기를 측정하지 않음 (not watched). 또한 변수여도 학습 가능하지 않으면(not trainable) 자동 미분을 사용하지 않음\n\n\nCode\nx = tf.linspace(-10, 10, 100) # -10 ~ 10까지 100r개의 데이터 생성\n\nwith tf.GradientTape() as tape:\n  tape.watch(x) # x에 대해 기울기를 측정할거니까 기록해줘\n  y = tf.nn.sigmoid(x)\n\ndx = tape.gradient(y, x)\nprint(dx)\n\n\ntf.Tensor(\n[4.53958077e-05 5.55575620e-05 6.79936937e-05 8.32130942e-05\n 1.01838442e-04 1.24631609e-04 1.52524715e-04 1.86658091e-04\n 2.28426653e-04 2.79536554e-04 3.42074339e-04 4.18591319e-04\n 5.12206458e-04 6.26731702e-04 7.66824507e-04 9.38173215e-04\n 1.14772200e-03 1.40394326e-03 1.71716676e-03 2.09997591e-03\n 2.56768332e-03 3.13889855e-03 3.83620191e-03 4.68693782e-03\n 5.72413978e-03 6.98759437e-03 8.52504404e-03 1.03935138e-02\n 1.26607241e-02 1.54065171e-02 1.87241696e-02 2.27213903e-02\n 2.75206964e-02 3.32587242e-02 4.00838615e-02 4.81513998e-02\n 5.76152215e-02 6.86149280e-02 8.12573764e-02 9.55919842e-02\n 1.11580066e-01 1.29060077e-01 1.47712989e-01 1.67034879e-01\n 1.86326443e-01 2.04710159e-01 2.21183725e-01 2.34711795e-01\n 2.44347497e-01 2.49363393e-01 2.49363393e-01 2.44347497e-01\n 2.34711795e-01 2.21183725e-01 2.04710159e-01 1.86326443e-01\n 1.67034879e-01 1.47712989e-01 1.29060077e-01 1.11580066e-01\n 9.55919842e-02 8.12573764e-02 6.86149280e-02 5.76152215e-02\n 4.81513998e-02 4.00838615e-02 3.32587242e-02 2.75206964e-02\n 2.27213903e-02 1.87241696e-02 1.54065171e-02 1.26607241e-02\n 1.03935138e-02 8.52504404e-03 6.98759437e-03 5.72413978e-03\n 4.68693782e-03 3.83620191e-03 3.13889855e-03 2.56768332e-03\n 2.09997591e-03 1.71716676e-03 1.40394326e-03 1.14772200e-03\n 9.38173215e-04 7.66824507e-04 6.26731702e-04 5.12206458e-04\n 4.18591319e-04 3.42074339e-04 2.79536554e-04 2.28426653e-04\n 1.86658091e-04 1.52524715e-04 1.24631609e-04 1.01838442e-04\n 8.32130942e-05 6.79936937e-05 5.55575620e-05 4.53958077e-05], shape=(100,), dtype=float64)\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.plot(x, y, 'r', label=\"y\")\nplt.plot(x, dx, 'b--', label=\"dy/dx\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "docs/blog/posts/ML/2023-02-03_pytorch_introduction/index.html",
    "href": "docs/blog/posts/ML/2023-02-03_pytorch_introduction/index.html",
    "title": "Pytorch Introduction",
    "section": "",
    "text": "아직 GPU를 못잡았음 -> Google Colab에서만 돌아감"
  },
  {
    "objectID": "docs/blog/posts/ML/2023-02-03_pytorch_introduction/index.html#pytorch-overview",
    "href": "docs/blog/posts/ML/2023-02-03_pytorch_introduction/index.html#pytorch-overview",
    "title": "Pytorch Introduction",
    "section": "Pytorch Overview",
    "text": "Pytorch Overview\n\nPyTorch는 기계 학습 프레임워크(framework) 중 하나다.\n\nPyTorch의 텐서(tensor)는 NumPy 배열과 매우 유사하다.\nTensor flow 보다 사용 비중이 늘어나고 있다.\nTensor: 고차원 데이터 (배열)를 의미, 3차원 배열 이상\n\nPyTorch를 사용하면, GPU 연동을 통해 효율적으로 딥러닝 모델을 학습할 수 있다.\nGoogle Colab을 이용하면, 손쉽게 PyTorch를 시작할 수 있다.\nGoogle Colab에서는 [런타임] - [런타임 유형 변경]에서 GPU를 선택할 수 있다.\nGoogle Colab에선 pytoch가 내장되어 있기 때문에 따로 설치할 필요 없음\n\n\nGPU 사용 여부 체크하기\n\n텐서간의 연산을 수행할 때, 기본적으로 두 텐서가 같은 장치에 있어야 한다.\n연산을 수행하는 텐서들을 모두 GPU에 올린 뒤에 연산을 수행한다.\nGPU는 고차원 행렬곱같은 병렬 처리 연산에 최적화 되어 있다.\n\ntensor 자체가 고차원 배열이기 때문에 데이터를 불러오면 tensor 형태로 바꿀 수 있다.\n\n\n텐서(tensor) 객체 생성 (기본 python 데이터 유형)\n\n\nCode\nimport torch\n\n# data initialization: 초기화된 데이터는 gpu에 있음\ndata = [\n  [1, 2],\n  [3, 4]\n]\n\nx = torch.tensor(data) # list를 tensor 형태로 바꾸기. \nprint(x.is_cuda)\n\nx = x.cuda() # CPU -> GPU로 옮기기\nprint(x.is_cuda)\n\nx = x.cpu() # GPU -> CPU로 옮기기\nprint(x.is_cuda)\n\n\n\n서로 다른 장치(device)에 있는 텐서끼리 연산을 수행하면 오류가 발생한다.\n\n\n\nCode\n# GPU 장치의 텐서\na = torch.tensor([\n    [1, 1],\n    [2, 2]\n]).cuda()\n\n# CPU 장치의 텐서\nb = torch.tensor([\n    [5, 6],\n    [7, 8]\n])\n\n# print(torch.matmul(a, b)) # 오류 발생\nprint(torch.matmul(a.cpu(), b))\n\n\n\n\n2. 텐서 소개 및 생성 방법\n\n1) 텐서의 속성\n\n텐서의 기본 속성으로는 다음과 같은 것들이 있다.\n\n모양(shape): 텐서 객체의 차원을 확인할 수 있다. (tensor_var.shape)\n자료형(data type) : 텐서의 기본 자료형은 float type (tensor_var.dtype)\n저장된 장치: CPU인지 GPU인지 확인 (tensor_var.device)\n\n\n\n\nCode\ntensor = torch.rand(3, 4)\n\nprint(tensor)\nprint(f\"Shape: {tensor.shape}\")\nprint(f\"Data type: {tensor.dtype}\")\nprint(f\"Device: {tensor.device}\")\n\n\n\n\n2) 텐서 초기화\n\n리스트 데이터에서 직접 텐서를 초기화할 수 있다.\n\n\n\nCode\ndata = [\n  [1, 2],\n  [3, 4]\n]\nx = torch.tensor(data)\n\nprint(x)\n\n\n\nNumPy 배열에서 텐서를 초기화할 수 있다.\n\n\n\nCode\na = torch.tensor([5])\nb = torch.tensor([7])\n\nc = (a + b).numpy() # tensor -> numpy array\nprint(c)\nprint(type(c)) # ndarray: numpy array \n\nresult = c * 10\ntensor = torch.from_numpy(result) # numpy array -> tensor \nprint(tensor)\nprint(type(tensor))\n\n\n\n\n3) 다른 텐서로부터 data를 가져와 텐서 초기화하기\n\n다른 텐서의 정보를 토대로 텐서를 초기화할 수 있다.\n텐서의 속성은 모양(shape)과 자료형(data type)이 있다\n\n\n\nCode\nx = torch.tensor([\n    [5, 7],\n    [1, 2]\n])\n\n# x와 같은 shape와 data type을 가지지만, 값이 1인 텐서 생성\nx_ones = torch.ones_like(x)\nprint(x_ones)\n# x와 같은 shape를 가지되, 자료형은 float으로 덮어쓰고, 값은 랜덤으로 채우기\nx_rand = torch.rand_like(x, dtype=torch.float32) # uniform distribution [0, 1)\nprint(x_rand)\n\n\n\n\n\n3. 텐서의 형변환 및 차원 조작\n\n텐서는 넘파이(NumPy) 배열처럼 조작할 수 있다.\n\n\n1) 텐서의 특정 차원 접근하기\n\n텐서의 원하는 차원에 접근할 수 있다.\n\n\n\nCode\ntensor = torch.tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12]\n])\n\nprint(tensor[0]) # the first row\nprint(tensor[:, 0]) # indexing the first column with all the rows\n# whatever the previous columns are, indexing the last column with all the rows\nprint(tensor[..., -1]) \n\n\n\n\n2) 텐서 이어붙이기(Concatenate)\n\n두 텐서를 이어 붙여 연결하여 새로운 텐서를 만들 수 있다.\n\n\n\nCode\ntensor = torch.tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12]\n])\n\n# dim: 텐서를 이어 붙이기 위한 축\n# 0번 축(행)을 기준으로 이어 붙이기: 즉, row bind로 연결\nresult = torch.cat([tensor, tensor, tensor], dim=0) \nprint(result)\nprint(result.shape) # 9x4\n\n# 1번 축(열)을 기준으로 이어 붙이기: 즉, column bind로 연결\nresult = torch.cat([tensor, tensor, tensor], dim=1)  \nprint(result)\nprint(result.shape) # 3x12\n\n\n\n\n3) 텐서 형변환(Type Casting)\n\n텐서의 자료형(정수, 실수 등)을 변환할 수 있다.\n\n\n\nCode\na = torch.tensor([2], dtype=torch.int) # integers\nb = torch.tensor([5.0]) # real numbers\n\nprint(a.dtype)\nprint(b.dtype)\n\n# 텐서 a는 자동으로 float32형으로 형변환 처리\nprint(a + b)\n# 텐서 b를 int32형으로 형변환하여 덧셈 수행\nprint(a + b.type(torch.int32))\n\n\n\n\n4) 텐서의 모양 변경\n\nview()는 텐서의 shape를 변경할 때 사용한다.\n이때, 텐서(tensor)의 순서는 변경되지 않는다.\n\n\n\nCode\n# view()는 텐서의 모양을 변경할 때 사용한다.\n# 이때, 텐서(tensor)의 순서는 변경되지 않는다.\na = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8])\nb = a.view(4, 2) # 4*2=8 개, # call by reference\nprint(b)\n\n# a의 값을 변경하면 b도 변경: 메모리 주소값을 공유\na[0] = 7\nprint(b)\n\n# a의 값을 복사(copy)한 뒤에 변경, 새로운 메모리값 할당\nc = a.clone().view(4, 2) # call by value, 아예 다른 객체\na[0] = 9\nprint(c)\n\n\n\n\n5) 텐서의 차원 교환\n\n하나의 텐서에서 특정한 차원끼리 순서를 교체할 수 있다.\n\n\n\nCode\na = torch.rand((64, 32, 3))\nprint(a.shape)\n\nb = a.permute(2, 1, 0) # 차원을 바꿔줌\n# (2번째 축, 1번째 축, 0번째 축)의 형태가 되도록 한다.\nprint(b.shape)\n\n\n\n\n\n4. 텐서의 연산과 함수\n\n1) 텐서의 연산\n\n텐서에 대하여 사칙연산 등 기본적인 연산을 수행할 수 있다.\n\n\n\nCode\n# 같은 크기를 가진 두 개의 텐서에 대하여 사칙연산 가능\n# 기본적으로 요소별(element-wise) 연산, 행렬의 연산과 다름\na = torch.tensor([\n    [1, 2],\n    [3, 4]\n])\nb = torch.tensor([\n    [5, 6],\n    [7, 8]\n])\nprint(a + b)\nprint(a - b)\nprint(a * b)\nprint(a / b)\n\n\n\n행렬 곱을 수행할 수 있다.\n\n\n\nCode\na = torch.tensor([\n    [1, 2],\n    [3, 4]\n])\nb = torch.tensor([\n    [5, 6],\n    [7, 8]\n])\n# 행렬 곱(matrix multiplication) 수행\nprint(a.matmul(b))\nprint(torch.matmul(a, b))\n\n\n\n\n2) 텐서의 평균 함수\n\n텐서의 평균(mean)을 계산할 수 있다.\n\n\n\nCode\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.mean()) # 전체 원소에 대한 평균\nprint(a.mean(dim=0)) # 각 열에 대하여 평균 계산\nprint(a.mean(dim=1)) # 각 행에 대하여 평균 계산\n\n\n\n\n3) 텐서의 합계 함수\n\n텐서의 합계(sum)를 계산할 수 있다.\n\n\n\nCode\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.sum()) # 전체 원소에 대한 합계\nprint(a.sum(dim=0)) # 각 열에 대하여 합계 계산\nprint(a.sum(dim=1)) # 각 행에 대하여 합계 계산\n\n\n\n\n4) 텐서의 최대 함수\n\nmax() 함수는 원소의 최댓값을 반환한다.\nargmax() 함수는 가장 큰 원소(최댓값)의 인덱스를 반환한다.\n\n\n\nCode\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.max()) # 전체 원소에 대한 최댓값\nprint(a.max(dim=0)) # 각 열에 대하여 최댓값 계산\nprint(a.max(dim=1)) # 각 행에 대하여 최댓값 계산\n\n\n\n\nCode\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.argmax()) # 전체 원소에 대한 최댓값의 인덱스\nprint(a.argmax(dim=0)) # 각 열에 대하여 최댓값의 인덱스 계산\nprint(a.argmax(dim=1)) # 각 행에 대하여 최댓값의 인덱스 계산\n\n\n\n\n5) 텐서의 차원 줄이기 혹은 늘리기\n\nunsqueeze() 함수는 크기가 1인 차원을 추가한다.\n\n배치(batch) 차원을 추가하기 위한 목적으로 흔히 사용된다.\n\nsqueeze() 함수는 크기가 1인 차원을 제거한다.\n\n\n\nCode\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a.shape)\n\n# 첫 번째 축에 차원 추가\na = a.unsqueeze(0)\nprint(a)\nprint(a.shape)\n\n# 네 번째 축에 차원 추가\na = a.unsqueeze(3)\nprint(a)\nprint(a.shape)\n\n\n\n\nCode\n# 크기가 1인 차원 제거\na = a.squeeze()\nprint(a)\nprint(a.shape)\n\n\n\n\n\n5. 자동 미분과 기울기(Gradient)\n\nPyTorch에서는 연산에 대하여 자동 미분을 수행할 수 있다.\n각 텐서 변수에 대해 gradient추적이 가능하여 텐서 연산에 각 텐서 변수의 기울기(민감도)를 추적할 수 있다.\n\n\n\nCode\nimport torch\n\n# requires_grad를 설정할 때만 기울기 추적\nx = torch.tensor([3.0, 4.0], requires_grad=True)\ny = torch.tensor([1.0, 2.0], requires_grad=True)\nz = x + y #z를 연산하는데 x와 y의 민감도를 추적할 수 있다.\n# x or y의 민감도 즉 gradient가 크다는 것은 변수의 값이 조금만 바뀌어도 z값에 큰 영향을 미친다는것을 의미 \n\nprint(z) # [4.0, 6.0]\nprint(z.grad_fn) # 더하기(add), \n# AddBackward: 기울기를 구하는 과정에서 Add를 사용한다. 뭔뜻인지? ㅋ\n# Add를 연산하는 과정에서 기울기를 구하는거 아님?\n\nout = z.mean()\nprint(out) # 5.0\nprint(out.grad_fn) # 평균(mean)\n\nout.backward() # scalar에 대하여 모든 연산의 기울기를 추적 가능\nprint(x.grad) # tensor([0.5000, 0.5000]), 0.5: x의 값이 1만큼 바뀔 때 output값이 0.5만큼 바뀐다는것을 의미\nprint(y.grad) # tensor([0.5000, 0.5000]),\nprint(z.grad) # leaf variable에 대해서만 gradient 추적이 가능하다. 따라서 None.\n\n\n\n일반적으로 모델을 학습할 때는 기울기(gradient)를 추적한다.\n\n왜냐면, 가중치를 기울기에 따라 업데이트 해야하기 때문.\n\n하지만, 학습된 모델을 사용할 때는 파라미터를 업데이트하지 않으므로, 기울기를 추적하지 않는 것이 일반적이다.\n\n\n\nCode\ntemp = torch.tensor([3.0, 4.0], requires_grad=True)# tape,라 부름. 왜?\nprint(temp.requires_grad)\nprint((temp ** 2).requires_grad)\n\n# 기울기 추적을 하지 않기 때문에 계산 속도가 더 빠르다.\nwith torch.no_grad():\n    temp = torch.tensor([3.0, 4.0], requires_grad=True)\n    print(temp.requires_grad)\n    print((temp ** 2).requires_grad)"
  },
  {
    "objectID": "docs/projects/LLFS/mining.html#data-mining",
    "href": "docs/projects/LLFS/mining.html#data-mining",
    "title": "Data Mining",
    "section": "1 Data Mining",
    "text": "1 Data Mining\n\n1.1 PCA (Principal Component Analysis)\n\n\n\n\n\n1.2 K-means Clustering\n\n\nCode\n# K means\nkm_fit = kmeans(all_data[,-c(1:5)],centers = 2,iter.max = 300 )\n\n# \"K-Means Clustering- Confusion matrix\")\n# table(all_data[,1],km_fit$cluster)\n\nmat_avgss = matrix(nrow = 20, ncol = 2)\n\n# Average within the cluster sum of square\nprint(paste(\"Avg. Within sum of squares\"))\n\n\n[1] \"Avg. Within sum of squares\"\n\n\nCode\nfor (i in (1:20)){\n  km_fit = kmeans(all_data[,-c(1:6)],centers = i,iter.max = 300 )\n  mean_km = mean(km_fit$withinss)\n  print(paste(\"K-Value\",i,\",Avg.within sum of squares\",round(mean_km,2)))\n  mat_avgss[i,1] = i\n  mat_avgss[i,2] = mean_km\n}\n\n\n[1] \"K-Value 1 ,Avg.within sum of squares 498092.66\"\n[1] \"K-Value 2 ,Avg.within sum of squares 235721.28\"\n[1] \"K-Value 3 ,Avg.within sum of squares 153517.94\"\n[1] \"K-Value 4 ,Avg.within sum of squares 113055.72\"\n[1] \"K-Value 5 ,Avg.within sum of squares 89588.37\"\n[1] \"K-Value 6 ,Avg.within sum of squares 73547.47\"\n[1] \"K-Value 7 ,Avg.within sum of squares 62299.86\"\n[1] \"K-Value 8 ,Avg.within sum of squares 53977.29\"\n[1] \"K-Value 9 ,Avg.within sum of squares 47642.59\"\n[1] \"K-Value 10 ,Avg.within sum of squares 42469.12\"\n[1] \"K-Value 11 ,Avg.within sum of squares 38418.71\"\n[1] \"K-Value 12 ,Avg.within sum of squares 35056.93\"\n[1] \"K-Value 13 ,Avg.within sum of squares 32105.26\"\n[1] \"K-Value 14 ,Avg.within sum of squares 29614.67\"\n[1] \"K-Value 15 ,Avg.within sum of squares 27560.73\"\n[1] \"K-Value 16 ,Avg.within sum of squares 25625.99\"\n[1] \"K-Value 17 ,Avg.within sum of squares 24026.77\"\n[1] \"K-Value 18 ,Avg.within sum of squares 22594.17\"\n[1] \"K-Value 19 ,Avg.within sum of squares 21327.01\"\n[1] \"K-Value 20 ,Avg.within sum of squares 20138.19\"\n\n\nCode\nplot(mat_avgss[,1],mat_avgss[,2],type = 'o',xlab = \"K_Value\",ylab = \"Avg. within sum of square\")\ntitle(\"Avg. within sum of squares vs. K-value\")\n\n\n\n\n\nCode\nmat_varexp = matrix(nrow = 20, ncol = 2)\n# Percentage of Variance explained\nprint(paste(\"Percent. variance explained\"))\n\n\n[1] \"Percent. variance explained\"\n\n\nCode\nfor (i in (1:20)){\n  km_fit = kmeans(all_data[,-c(1:6)],centers = i,iter.max = 300 )\n  var_exp = km_fit$betweenss/km_fit$totss\n  print(paste(\"K-Value\",i,\",Percent var explained\",round(var_exp,4)))\n  mat_varexp[i,1]=i\n  mat_varexp[i,2]=var_exp\n}\n\n\n[1] \"K-Value 1 ,Percent var explained 0\"\n[1] \"K-Value 2 ,Percent var explained 0.0535\"\n[1] \"K-Value 3 ,Percent var explained 0.0744\"\n[1] \"K-Value 4 ,Percent var explained 0.0908\"\n[1] \"K-Value 5 ,Percent var explained 0.1055\"\n[1] \"K-Value 6 ,Percent var explained 0.1161\"\n[1] \"K-Value 7 ,Percent var explained 0.122\"\n[1] \"K-Value 8 ,Percent var explained 0.1318\"\n[1] \"K-Value 9 ,Percent var explained 0.1404\"\n[1] \"K-Value 10 ,Percent var explained 0.1477\"\n[1] \"K-Value 11 ,Percent var explained 0.1514\"\n[1] \"K-Value 12 ,Percent var explained 0.1583\"\n[1] \"K-Value 13 ,Percent var explained 0.1614\"\n[1] \"K-Value 14 ,Percent var explained 0.1671\"\n[1] \"K-Value 15 ,Percent var explained 0.171\"\n[1] \"K-Value 16 ,Percent var explained 0.1763\"\n[1] \"K-Value 17 ,Percent var explained 0.1792\"\n[1] \"K-Value 18 ,Percent var explained 0.1839\"\n[1] \"K-Value 19 ,Percent var explained 0.1891\"\n[1] \"K-Value 20 ,Percent var explained 0.1919\"\n\n\nCode\nplot(mat_varexp[,1],mat_varexp[,2],type = 'o',xlab = \"K_Value\",ylab = \"Percent Var explained\")\ntitle(\"Avg. within sum of squares vs. K-value\")"
  },
  {
    "objectID": "docs/projects/LLFS/mining.html#blog-guide-map-link",
    "href": "docs/projects/LLFS/mining.html#blog-guide-map-link",
    "title": "Data Mining",
    "section": "2 Blog Guide Map Link",
    "text": "2 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/projects/LLFS/mining.html",
    "href": "docs/projects/LLFS/mining.html",
    "title": "Data Mining",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmetabolite_data <- all_data[, -c(1:5)]\noutcome_data <- all_data[, 2]\n\n# normalize the metaoblites\nnormalized_metabolite_data <-\n    as.data.frame(lapply(metabolite_data, function(x) scale_function(vector = x, method = \"min-max\")))\nnormalized_significant_metabolite_data <-\n    normalized_metabolite_data %>%\n    dplyr::select(all_of(significant_metabolites))\n\n# extract the latent variables (PCs: Principal Components)\npc_metabolites <-\n    prcomp(normalized_metabolite_data)\npc_significant_metabolites <-\n    prcomp(normalized_significant_metabolite_data)\n\n# calculate scores\nscores <-\n    as.data.frame(pc_metabolites$x) %>%\n    janitor::clean_names() %>%\n    mutate(row_names = 1:n())\nsignificant_scores <-\n    as.data.frame(pc_significant_metabolites$x) %>%\n    janitor::clean_names() %>%\n    mutate(row_names = 1:n())\n\ntemp <-\n    as.data.frame(pc_metabolites$rotation) %>%\n    janitor::clean_names()\nloadings <- temp %>%\n    mutate(\n        metabolites = rownames(.),\n        arrow_size_normalization = min(\n            (max(scores[, \"pc1\"]) - min(scores[, \"pc1\"]) /\n                (max(temp[, \"pc1\"]) - min(temp[, \"pc1\"]))),\n            (max(scores[, \"pc2\"]) - min(scores[, \"pc2\"]) /\n                (max(temp[, \"pc2\"]) - min(temp[, \"pc2\"]))),\n            (max(scores[, \"pc3\"]) - min(scores[, \"pc3\"]) /\n                (max(temp[, \"pc3\"]) - min(temp[, \"pc3\"])))\n        ),\n        arrow_pc1 = arrow_size_normalization * pc1,\n        arrow_pc2 = arrow_size_normalization * pc2,\n        arrow_pc3 = arrow_size_normalization * pc3\n    )\n\ntemp <-\n    as.data.frame(pc_significant_metabolites$rotation) %>%\n    janitor::clean_names()\nsignificant_loadings <- temp %>%\n    mutate(\n        metabolites = rownames(.),\n        arrow_size_normalization = min(\n            (max(scores[, \"pc1\"]) - min(scores[, \"pc1\"]) /\n                (max(temp[, \"pc1\"]) - min(temp[, \"pc1\"]))),\n            (max(scores[, \"pc2\"]) - min(scores[, \"pc2\"]) /\n                (max(temp[, \"pc2\"]) - min(temp[, \"pc2\"]))),\n            (max(scores[, \"pc3\"]) - min(scores[, \"pc3\"]) /\n                (max(temp[, \"pc3\"]) - min(temp[, \"pc3\"])))\n        ),\n        arrow_pc1 = arrow_size_normalization * pc1,\n        arrow_pc2 = arrow_size_normalization * pc2,\n        arrow_pc3 = arrow_size_normalization * pc3\n    )\n# arrow_size_normalization is a normalization factor that\n# ensures the variable loading arrows are scaled appropriately relative to the data points.\n# The min() function to find the smallest ratio between the range of the data points and\n# the range of the variable loadings along each principal component axis (pc1, pc2, and pc3).\n# The reason why I select the first 3 components is that\n# '3' is the maximum dimension that can visualize the PCA results in 3d.\n\noutcome_scores <-\n    scores %>%\n    mutate(\n        outcome = outcome_data,\n        row_names = 1:n()\n    )\noutcome_significant_scores <-\n    significant_scores %>%\n    mutate(\n        outcome = outcome_data,\n        row_names = 1:n()\n    )\n\n\n# total variance\ntotal_variance <-\n    data.frame(\n        pc = 1:length(pc_metabolites$sdev),\n        pc_variance_proportion = summary(pc_metabolites)[[\"importance\"]][\"Proportion of Variance\", ],\n        cumulative_proportion = summary(pc_metabolites)[[\"importance\"]][\"Cumulative Proportion\", ] * 100\n    )\ntotal_variance_significance <-\n    data.frame(\n        pc = 1:length(pc_significant_metabolites$sdev),\n        pc_variance_proportion = summary(pc_significant_metabolites)[[\"importance\"]][\"Proportion of Variance\", ],\n        cumulative_proportion = summary(pc_significant_metabolites)[[\"importance\"]][\"Cumulative Proportion\", ] * 100\n    )\n\nscree_plot <- function(indata) {\n    scree_plot1 <- ggplot(\n        data = indata,\n        aes(x = pc, y = pc_variance_proportion, group = 1)\n    ) +\n        geom_point() +\n        geom_line() +\n        labs(title = \"\", subtitle = paste0(\n            \"Scree Plot, Total Variance(\",\n            round(tail(indata, 1)[\"cumulative_proportion\"], 3),\n            \"%)Explained by \", nrow(indata), \" PCs\"\n        )) +\n        ylab(\"Total Variance Explained\") +\n        xlab(\"Principal Components\")\n    scree_plot2 <- ggplot(\n        data = indata %>% filter(pc < 13),\n        aes(x = pc, y = pc_variance_proportion, group = 1)\n    ) +\n        geom_point() +\n        geom_line() +\n        labs(title = \"\", subtitle = paste0(\n            \"Scree Plot, Part of Variance(\",\n            round(tail(indata %>% filter(pc < 13), 1)[\"cumulative_proportion\"], 3),\n            \"%)Explained by \", indata %>% filter(pc < 13) %>% nrow(), \" PCs\"\n        )) +\n        ylab(\"Total Variance Explained\") +\n        xlab(\"Principal Components\")\n    return(ggarrange(scree_plot1, scree_plot2, nrow = 1))\n}\n\nggarrange(scree_plot(total_variance), scree_plot(total_variance_significance),\n    labels = c(\n        paste0(\"All \", ncol(metabolite_data), \" Metabolites\"),\n        paste0(length(significant_metabolites), \" Significant Metabolites\")\n    ), nrow = 2\n)\n\n\n\n\n\nCode\n# 2D PCA Scatter Plots with PC1 and PC2\n\nscatter_plot <- function(in_data) {\n    p <- ggplot(\n        data = in_data,\n        aes(x = pc1, y = pc2, color = outcome)\n    ) +\n        geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n        stat_ellipse(type = \"norm\", level = .99) +\n        geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n        geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n        scale_color_manual(values = color_function(length(unique(in_data$outcome)))) +\n        labs(\n            title = \"2D Scatter Plot of the First 2 PCs Grouped by AD status\",\n            subtitle = paste0(ncol(in_data) - 2, \" Metabolites\")\n        )\n    return(p)\n}\n\nggarrange(scatter_plot(outcome_scores),\n    scatter_plot(outcome_significant_scores),\n    nrow = 2\n)\n\n\n\n\n\nCode\n# biplot\nbi_plot <- function(in_data) {\n    p <-\n        ggplot(data = in_data, aes(x = pc1, y = pc2, color = outcome)) +\n        geom_text(alpha = .75, size = 3, aes(label = row_names)) +\n        geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n        geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n        coord_equal() +\n        scale_color_manual(values = color_function(length(unique(in_data$outcome)))) +\n        stat_ellipse(type = \"norm\", level = .99) +\n        geom_text(\n            data = loadings, aes(x = arrow_pc1, y = arrow_pc2, label = metabolites),\n            alpha = 0.5, size = 5, vjust = 1, color = \"red\"\n        ) +\n        geom_segment(\n            data = loadings, aes(x = 0, y = 0, xend = arrow_pc1, yend = arrow_pc2),\n            arrow = arrow(length = unit(0.5, \"cm\")), alpha = 0.5, color = \"red\"\n        ) +\n        labs(\n            title = \"Biplot, the Effect of Metabolites on Samples with Disease Status\",\n            subtitle = paste0(ncol(in_data) - 2, \" Metabolites\")\n        ) +\n        ylab(\"PC2\") +\n        xlab(\"PC1\")\n    return(p)\n}\nggarrange(bi_plot(outcome_scores), bi_plot(outcome_significant_scores), nrow = 2)\n\n\n\n\n\nCode\nplot_ly(\n    data = outcome_scores,\n    x = ~pc1, y = ~pc2, z = ~pc3,\n    type = \"scatter3d\", mode = \"markers\", color = ~ outcome_scores$outcome,\n    colors = color_function(2),\n    size = 2\n) %>%\n    layout(\n        title = \"Effect of 500 Metabolites on Samples with Disease Status in 3d\",\n        scene = list(\n            bgcolor = \"#e5ecf6\",\n            xaxis = list(title = \"PC1\"),\n            yaxis = list(title = \"PC2\"),\n            zaxis = list(title = \"PC3\")\n        ),\n        legend = list(title = list(text = \"Disease(AD) Status\"))\n    )\n\n\n\n\n\n\nCode\nplot_ly(\n    data = outcome_significant_scores,\n    x = ~pc1, y = ~pc2, z = ~pc3,\n    type = \"scatter3d\", mode = \"markers\", color = ~ outcome_scores$outcome,\n    colors = color_function(2),\n    size = 2\n) %>%\n    layout(\n        title = \"Effect of 201 Metabolites on Samples with Disease Status in 3d\",\n        scene = list(\n            bgcolor = \"#e5ecf6\",\n            xaxis = list(title = \"PC1\"),\n            yaxis = list(title = \"PC2\"),\n            zaxis = list(title = \"PC3\")\n        ),\n        legend = list(title = list(text = \"Disease(AD) Status\"))\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nkmean_result_list <- list(\n    \"mse_list\" = list(\n        \"all_metabolites\" = matrix(nrow = 20),\n        \"significant_metabolites\" = matrix(nrow = 20)\n    ),\n    \"cluster_sse_list\" = list(\n        \"all_metabolites\" = matrix(nrow = 20, ncol = 2),\n        \"significant_metabolites\" = matrix(nrow = 20, ncol = 2)\n    ),\n    \"Variance_Explained\" = list(\n        \"all_metabolites\" = matrix(nrow = 20),\n        \"significant_metabolites\" = matrix(nrow = 20)\n    )\n)\n\nfor (j in c(\"all_metabolites\", \"significant_metabolites\")) {\n    for (i in (1:20)) {\n        if (j == \"all_metabolites\") {\n            kmean_fit <- kmeans(normalized_metabolite_data, centers = i, iter.max = 300)\n        } else {\n            kmean_fit <- kmeans(normalized_significant_metabolite_data, centers = i, iter.max = 300)\n        }\n        kmean_result_list[[\"mse_list\"]][[j]][i] <- mean(kmean_fit$withinss) %>% round(3)\n        kmean_result_list[[\"cluster_sse_list\"]][[j]][i, 1] <- i\n        kmean_result_list[[\"cluster_sse_list\"]][[j]][i, 2] <- kmean_result_list[[\"mse_list\"]][[j]][i]\n        kmean_result_list[[\"Variance_Explained\"]][[j]][i] <- kmean_fit$betweenss / kmean_fit$totss\n        cat(\n            \"For \", j, \", K: \", i,\n            \"within-cluster MSE: \", kmean_result_list[[\"mse_list\"]][[j]][i],\n            \"Variance_Explained: \", kmean_result_list[[\"Variance_Explained\"]][[j]][i], \"\\n\"\n        )\n    }\n}\n\n\nFor  all_metabolites , K:  1 within-cluster MSE:  13619.32 Variance_Explained:  4.941702e-15 \nFor  all_metabolites , K:  2 within-cluster MSE:  6487.104 Variance_Explained:  0.04736709 \nFor  all_metabolites , K:  3 within-cluster MSE:  4205.437 Variance_Explained:  0.0736459 \nFor  all_metabolites , K:  4 within-cluster MSE:  3107.38 Variance_Explained:  0.08736085 \nFor  all_metabolites , K:  5 within-cluster MSE:  2443.924 Variance_Explained:  0.1027728 \nFor  all_metabolites , K:  6 within-cluster MSE:  2010.74 Variance_Explained:  0.114167 \nFor  all_metabolites , K:  7 within-cluster MSE:  1705.235 Variance_Explained:  0.1235504 \nFor  all_metabolites , K:  8 within-cluster MSE:  1481.469 Variance_Explained:  0.1297835 \nFor  all_metabolites , K:  9 within-cluster MSE:  1306.354 Variance_Explained:  0.1367274 \nFor  all_metabolites , K:  10 within-cluster MSE:  1166.08 Variance_Explained:  0.1438044 \nFor  all_metabolites , K:  11 within-cluster MSE:  1051.345 Variance_Explained:  0.1508531 \nFor  all_metabolites , K:  12 within-cluster MSE:  957.817 Variance_Explained:  0.1560656 \nFor  all_metabolites , K:  13 within-cluster MSE:  880.831 Variance_Explained:  0.1592231 \nFor  all_metabolites , K:  14 within-cluster MSE:  812.71 Variance_Explained:  0.1645731 \nFor  all_metabolites , K:  15 within-cluster MSE:  755.508 Variance_Explained:  0.1679013 \nFor  all_metabolites , K:  16 within-cluster MSE:  702.891 Variance_Explained:  0.1742422 \nFor  all_metabolites , K:  17 within-cluster MSE:  657.791 Variance_Explained:  0.1789275 \nFor  all_metabolites , K:  18 within-cluster MSE:  617.96 Variance_Explained:  0.1832713 \nFor  all_metabolites , K:  19 within-cluster MSE:  582.219 Variance_Explained:  0.1877591 \nFor  all_metabolites , K:  20 within-cluster MSE:  550.074 Variance_Explained:  0.1922144 \nFor  significant_metabolites , K:  1 within-cluster MSE:  2660.227 Variance_Explained:  -8.034324e-15 \nFor  significant_metabolites , K:  2 within-cluster MSE:  1022.505 Variance_Explained:  0.2312645 \nFor  significant_metabolites , K:  3 within-cluster MSE:  632.194 Variance_Explained:  0.28706 \nFor  significant_metabolites , K:  4 within-cluster MSE:  452.85 Variance_Explained:  0.3190806 \nFor  significant_metabolites , K:  5 within-cluster MSE:  354.801 Variance_Explained:  0.3331372 \nFor  significant_metabolites , K:  6 within-cluster MSE:  290.621 Variance_Explained:  0.3445191 \nFor  significant_metabolites , K:  7 within-cluster MSE:  245.829 Variance_Explained:  0.3531377 \nFor  significant_metabolites , K:  8 within-cluster MSE:  212.9 Variance_Explained:  0.3597537 \nFor  significant_metabolites , K:  9 within-cluster MSE:  187.706 Variance_Explained:  0.3649574 \nFor  significant_metabolites , K:  10 within-cluster MSE:  167.108 Variance_Explained:  0.3718263 \nFor  significant_metabolites , K:  11 within-cluster MSE:  150.928 Variance_Explained:  0.3759169 \nFor  significant_metabolites , K:  12 within-cluster MSE:  137.487 Variance_Explained:  0.3798103 \nFor  significant_metabolites , K:  13 within-cluster MSE:  126.018 Variance_Explained:  0.3841728 \nFor  significant_metabolites , K:  14 within-cluster MSE:  116.658 Variance_Explained:  0.3860626 \nFor  significant_metabolites , K:  15 within-cluster MSE:  107.843 Variance_Explained:  0.3919131 \nFor  significant_metabolites , K:  16 within-cluster MSE:  100.885 Variance_Explained:  0.393227 \nFor  significant_metabolites , K:  17 within-cluster MSE:  94.421 Variance_Explained:  0.3966092 \nFor  significant_metabolites , K:  18 within-cluster MSE:  88.62 Variance_Explained:  0.4003684 \nFor  significant_metabolites , K:  19 within-cluster MSE:  83.8 Variance_Explained:  0.4014796 \nFor  significant_metabolites , K:  20 within-cluster MSE:  79.155 Variance_Explained:  0.4049023 \n\n\nCode\nkmean_mse_data <-\n    rbind(\n        data.frame(\n            metabolites = \"all_metabolites\",\n            k = kmean_result_list[[\"cluster_sse_list\"]][[\"all_metabolites\"]][, 1],\n            mse = kmean_result_list[[\"cluster_sse_list\"]][[\"all_metabolites\"]][, 2],\n            variance_exaplained = kmean_result_list[[\"Variance_Explained\"]][[\"all_metabolites\"]]\n        ),\n        data.frame(\n            metabolites = \"significant_metabolites\",\n            k = kmean_result_list[[\"cluster_sse_list\"]][[\"significant_metabolites\"]][, 1],\n            mse = kmean_result_list[[\"cluster_sse_list\"]][[\"significant_metabolites\"]][, 2],\n            variance_exaplained = kmean_result_list[[\"Variance_Explained\"]][[\"significant_metabolites\"]]\n        )\n    )\n\nggarrange(\n    ggplot(\n        data = kmean_mse_data,\n        aes(x = k, y = mse, group = metabolites, color = metabolites)\n    ) +\n        geom_line() +\n        geom_point() +\n        scale_color_manual(values = color_function(2)) +\n        labs(title = \"K Mean Clustering Result: MSE for All Metabolites vs Significant Ones\") +\n        xlab(\"Number of Clusters\") +\n        ylab(\"Mean Squared Error\"),\n    ggplot(\n        data = kmean_mse_data,\n        aes(x = k, y = variance_exaplained, group = metabolites, color = metabolites)\n    ) +\n        geom_line() +\n        geom_point() +\n        scale_color_manual(values = color_function(2)) +\n        labs(title = \"K Mean Clustering Result: Variance Explained for All Metabolites vs Significant Ones\") +\n        xlab(\"Number of Clusters\") +\n        ylab(\"Variance Exaplained\"),\n    ncol = 1\n)\n\n\n\n\n\nCode\n# K means\n\n\nkm_clustering <- kmeans(normalized_metabolite_data, centers = 2, iter.max = 300)\nkm_significant_clustering <- kmeans(normalized_significant_metabolite_data, centers = 2, iter.max = 300)\n\nconfusionMatrix(table(all_data[, 2], ifelse(km_clustering$cluster == 1, \"negative\", \"positive\")))\n\n\nConfusion Matrix and Statistics\n\n          \n           negative positive\n  negative       96      214\n  positive      147       43\n                                          \n               Accuracy : 0.278           \n                 95% CI : (0.2391, 0.3195)\n    No Information Rate : 0.514           \n    P-Value [Acc > NIR] : 1.0000000       \n                                          \n                  Kappa : -0.4344         \n                                          \n Mcnemar's Test P-Value : 0.0005134       \n                                          \n            Sensitivity : 0.3951          \n            Specificity : 0.1673          \n         Pos Pred Value : 0.3097          \n         Neg Pred Value : 0.2263          \n             Prevalence : 0.4860          \n         Detection Rate : 0.1920          \n   Detection Prevalence : 0.6200          \n      Balanced Accuracy : 0.2812          \n                                          \n       'Positive' Class : negative        \n                                          \n\n\nCode\nconfusionMatrix(table(all_data[, 2], ifelse(km_significant_clustering$cluster == 1, \"negative\", \"positive\")))\n\n\nConfusion Matrix and Statistics\n\n          \n           negative positive\n  negative      101      209\n  positive      143       47\n                                          \n               Accuracy : 0.296           \n                 95% CI : (0.2563, 0.3381)\n    No Information Rate : 0.512           \n    P-Value [Acc > NIR] : 1.0000000       \n                                          \n                  Kappa : -0.3999         \n                                          \n Mcnemar's Test P-Value : 0.0005312       \n                                          \n            Sensitivity : 0.4139          \n            Specificity : 0.1836          \n         Pos Pred Value : 0.3258          \n         Neg Pred Value : 0.2474          \n             Prevalence : 0.4880          \n         Detection Rate : 0.2020          \n   Detection Prevalence : 0.6200          \n      Balanced Accuracy : 0.2988          \n                                          \n       'Positive' Class : negative        \n                                          \n\n\nCode\noutcome_pca_km <- outcome_scores %>%\n    mutate(\n        km_clusters = km_clustering$cluster,\n        km_clusters = factor(km_clusters, levels = c(1, 2)),\n        km_significant_clusters = km_significant_clustering$cluster,\n        km_significant_clusters = factor(km_significant_clusters, levels = c(1, 2))\n    )\n\n\nggplot(\n    data = outcome_pca_km,\n    aes(x = pc1, y = pc2, color = km_clusters)\n) +\n    geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n    stat_ellipse(type = \"norm\", level = .99) +\n    geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n    geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n    scale_color_manual(values = color_function(2)) +\n    labs(\n        title = \"2D Scatter Plot of the First 2 PCs Grouped by K Mean Clusters, AD Status\",\n        subtitle = paste0(ncol(outcome_scores) - 2, \" Metabolites\")\n    )\n\n\n\n\n\nCode\nggplot(\n    data = outcome_pca_km,\n    aes(x = pc1, y = pc2, color = km_significant_clusters)\n) +\n    geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n    stat_ellipse(type = \"norm\", level = .99) +\n    geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n    geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n    scale_color_manual(values = color_function(2)) +\n    labs(\n        title = \"2D Scatter Plot of the First 2 PCs Grouped by K Mean Clusters, AD Status\",\n        subtitle = paste0(ncol(outcome_significant_scores) - 2, \" Metabolites\")\n    )\n\n\n\n\n\n비지도 학습 방법인 PCA와 K-means clustering를 이용하여 차원 축소와 군집화를 시도하였으나, 이 방법을 사용하는 모든 대사체에 대해 AD 상태가 명확하게 분류되지 않는 것으로 보인다. PCA와 K-means는 EDA에서 선별된 대사산물로 군집화를 수행했을 때 전체 metabolites 보다 선별된 metabolites 에서 AD 상태에 대한 정보가 조금 더 많이 설명되는 것을 PCA를 통해 관찰할 수 있었다. K means clustering도 선별된 metabolites에 대해서 성능 향상을 보여준다. 그러나 전반적인 정확도가 매우 낮기 때문에 지도 학습을 통해 AD 상태를 잘 설명하는 대사체를 선택할 것이다.\nDimensionality reduction and clustering were attempted using PCA and K-means clustering, which are unsupervised learning methods, but AD status seems to not be clearly classified for all metabolites using the methods. When PCA and K means clustering were performed with the metabolites selected from EDA, it was observed through PCA that a little more information about AD status was explained with the selected metabolites than with the entire set of metaboliotes. K means clustering also showed an improvement in performance with the selected metabolites. However, the overall accuracy is very low, so we will select metabolites that explain AD status well through supervised learning."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html",
    "title": "Binomial Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 성공 확률이 p인 bernoulli distribution을 n 번 시행했을 때 성공횟수를 확률 변수 X로 갖는 probability distribution을 binomial distribution이라 한다. probability mass function은\n즉, \\[\n\\begin{aligned}\n  f_X(x;n,p)&=\\binom{n}{x}p^{x}(1-p)^{n-x} \\text{ } (y=0,1,2, ..., n)\\\\\n\\end{aligned}\n\\] 이고 Notation은 보통 \\(X \\sim Bin(n,p) \\text{ or } X \\sim B(n,p) \\text{ or } X \\sim Binomial(n,p)\\) 와 같이 쓰인다 (binomial distribution은 bernoulli distribution을 전제로 한다).\n\n\n\n\\[\n\\begin{aligned}\n    \\text{Let } &I_i \\text{ be } 1\\{x_i=1\\} \\\\\n    X&=\\sum_{i=1}^{n}I_i=I_1+I_2+ ... +I_n\\\\\n  \\text{E}(X)&=\\text{E}(\\sum_{i=1}^{n}I_i)\\\\\n             &=\\text{E}(I_1+I_2+ ... +I_n)\\\\\n             &=\\text{E}(I_1)+\\text{E}(I_2)+ ... +\\text{E}(I_n)\\\\\n             &=p+p+...+p\\\\\n             &=np\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{Var}(X)&=\\text{Var}(I_1+I_2+ ... +I_n)\\\\\n             &=\\text{Var}(I_1)+\\text{Var}(I_2)+ ... +\\text{Var}(I_n)\\\\\n             &=p(1-p)+p(1-p)+...+p(1-p)\\\\\n             &=np(1-p)\n\\end{aligned}\n\\]\n\n\n\n쌍란이 나올 확률이 0.05라고 가정했을 때 Super Market에서 1 pack of 12 eggs을 구매했을 때\n\n3개의 eggs에서 쌍란이 나올 확률은 \\[\n\\begin{aligned}\nX&\\sim Bin(12,0.05)\\\\\nf(X=3)&=\\binom{12}{3}0.05^3 0.95^9\n\\end{aligned}\n\\] 이다.\n적어도 3개의 eggs에서 쌍란이 나올 확률은 \\[\n\\begin{aligned}\nP(X\\ge3)&=1-F_X(3)\\\\\n         &=1-(f(X=3)+f(X=2)+f(X=1)+f(X=0))\\\\\n         &=1-(\\binom{12}{3}0.05^3 0.95^9+\\binom{12}{2}0.05^2 0.95^{10}+\\\\\n         &\\binom{12}{1}0.05^1 0.95^{11}+\\binom{12}{0}0.05^0 0.95^{12})\n\\end{aligned}\n\\] 이다.\n\n다른 예시로는, 분자 진단 시장에서 golden standard라고 평가받는 PCR (Polynomial Chain Reaction)에 사용되는 medical device가 2000 대 중 5대 꼴로 기계적 결함이 발견된다고 가정할 때, 1년에 평균 100대의 분잔 진단 장비를 공급받는 구매자 입장에서 장비의 결함이 발생할 연간 평균과 분산의 추정은 다음과 같다.\n\\[\n\\begin{aligned}\n    X&\\sim Bin(100,\\frac{5}{2000})\\\\\n    f(X=x)&=\\binom{100}{x}\\frac{5}{2000}^x (1-\\frac{5}{2000})^{100-x}\\\\\n    \\text{E}(X)&=100(\\frac{5}{2000})\\\\\n    \\text{Var}(X)&=100(\\frac{5}{2000})(1-\\frac{5}{2000})\n\\end{aligned}\n\\]\n\n\n\n\n\nDefinition 2 n번의 독립 시행에서 각 각 p_1, p_2, …, p_n 의 성공 확률로 E_1, E_2, …, E_n 중 어느 하나를 발생시킬 때 각 event E_i에 대응되는 발생 횟수를 확률 변수 X_1, X_2, …, X_n 로 갖는 joint probability mass function은\n\\[\n\\begin{aligned}\n  f_X(\\mathbf X = x_1,x_2, ...,x_n)&=\\binom{n}{x_1,x_2, ..., x_n}p_1^{x_1}(p_2)^{x_2}\\dots (p_n)^{x_n} \\\\\n\\end{aligned}\n\\] 이다. (단, \\(\\sum_{i=1}^{n}x_i=n, \\sum_{i=1}^{n}p_i=1\\))\n\n\n\n주사위를 5 번 던질 때 1 또는 6의 눈이 1번, 3, 4 또는 5의 눈이 2번 , 2의 눈이 2번 나올 확률은\n\\[\n\\begin{aligned}\n    &x_1= 1, x_2=2, x_3=2\\\\\n    f(X=(1,2,2))&=\\binom{5}{1,2,3}\\frac{1}{3}^1\\frac{1}{2}^2\\frac{1}{6}^2\n\\end{aligned}\n\\] 이다.\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 6.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 6.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 7.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 7.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 8.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 8.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x;p)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}x^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html",
    "title": "Geometric Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 성공 확률이 \\(p\\) 인 independent Bernoulli trials을 시행할 때 첫 성공할때까지의 시행 횟수를 확률 변수 \\(X\\) 로 갖는 분포를 geometric distribution이라고 하고 \\(X\\) 의 probability mass function은 \\[\n\\begin{aligned}\n  f_X(x;p)&=p(1-p)^{x-1}\n\\end{aligned}\n\\] 이다. (단, \\(x=1,2,...\\))\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x=1}^{\\infty} x f(x)\\\\\n             &=\\sum_{x=1}^{\\infty} x pq^{x-1} &\\text{ }(q=1-p)\\\\\n             &=p\\sum_{x=1}^{\\infty} x q^{x-1} \\\\\n             &=p\\frac{d}{dq}\\sum_{x=1}^{\\infty} q^{x} \\\\\n             &=p\\frac{d}{dq}\\frac{q}{1-q} \\\\\n             &=p\\frac{(1-q)+q}{(1-q)^2} \\\\\n             &=p\\frac{(1-q)+q}{p^2} \\\\\n             &=p\\frac{1}{p^2}\\\\\n             &=\\frac{1}{p}\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X(X-1))&=\\sum_{x=1}^{\\infty}x(x-1)pq^{x-1}\\\\\n                  &=pq\\sum_{x=1}^{\\infty}x(x-1)q^{x-2}\\\\\n                  &=pq\\sum_{x=1}^{\\infty}\\frac{d^2}{dq^2}q^{x}\\\\\n                  &=pq\\frac{d^2}{dq^2}\\sum_{x=1}^{\\infty}q^{x}\\\\\n                  &=pq\\frac{d^2}{dq^2}(\\frac{q}{1-q})\\\\\n                  &=pq\\frac{d}{dq}(\\frac{1}{(1-q)^2})\\\\\n                  &=pq(\\frac{2(1-q)}{(1-q)^4})\\\\\n                  &=\\frac{2q}{p^2}\\\\\n  \\text{E}(X^2)&=\\frac{2q}{p^2}+\\text{E}(X)=\\frac{2q}{p^2}+\\frac{1}{p}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n               &=\\frac{2q}{p^2}+\\frac{1}{p}-\\frac{1}{p^2}\\\\\n               &=\\frac{1-p}{p^2}\\\\\n\\end{aligned}\n\\]\n\n\n\n한 장비 제조 업체에서 사람의 건강과 생명을 진단하는데 사용되는 medical device가 1000 개마다 2개 꼴로 불량이 발생한다고 가정할 때 그 장비를 공급받는 구매자가 medical device의 공급 업체의 불량 평가 기준으로 quality control을 1대 씩 진행할 때 불량 장비가 3번째 검사에서 발생할 확률은\n\\[\n\\begin{aligned}\n   \\text{f}(x=3;p=\\frac{2}{1000})&=(\\frac{998}{1000})^2\\frac{2}{1000}\\\\\n\\end{aligned}\n\\] 이다. 평균과 분산 각 각 \\((\\frac{2}{1000})^{-1}\\), \\(\\frac{1-\\frac{2}{1000}}{(\\frac{2}{1000})^2}\\) 이다.\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_gemetric.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_gemetric.html",
    "title": "Geometric Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 성공 확률이 \\(p\\) 인 independent Bernoulli trials을 시행할 때 첫 성공할때까지의 시행 횟수를 확률 변수 \\(X\\) 로 갖는 분포를 geometric distribution이라고 하고 \\(X\\) 의 probability mass function은 \\[\n\\begin{aligned}\n  f_X(x;p)&=p(1-p)^{x-1}\n\\end{aligned}\n\\] 이다. (단, \\(x=1,2,...\\))\n\n\n\n$$\n\\[\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x=1}^{\\infty} x f(x)\\\\\n             &=\\sum_{x=1}^{\\infty} x pq^{x-1} &\\text{ }(q=1-p)\\\\\n             &=p\\sum_{x=1}^{\\infty} x q^{x-1} \\\\\n             &=p\\frac{d}{dx}\\sum_{x=1}^{\\infty} q^{x} \\\\\n             &=p\\frac{d}{dx}\\frac{q}{1-q} \\\\\n             &=p\\frac{d}{dx}\\frac{(1-q)+q}{(1-q)^2} \\\\\n             &=p\\frac{d}{dx}\\frac{(1-q)+q}{p^2} \\\\\n             &=p\\frac{1}{p^2}\\\\\n             &=\\frac{1}{p}\n\n\\end{aligned}\\]\n$$\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X(X-1))&=\\sum_{x=1}^{\\infty}x(x-1)pq^{x-1}\\\\\n                  &=pq\\sum_{x=1}^{\\infty}x(x-1)q^{x-2}\\\\\n                  &=pq\\sum_{x=1}^{\\infty}\\frac{d^2}{dq^2}q^{x}\\\\\n                  &=pq\\frac{d^2}{dq^2}\\sum_{x=1}^{\\infty}q^{x}\\\\\n                  &=pq\\frac{d^2}{dq^2}(\\frac{1}{1-q})\\\\\n                  &=pq\\frac{d}{dq}(\\frac{1}{(1-q)^2})\\\\\n                  &=pq(\\frac{2(1-q)}{(1-q)^4})\\\\\n                  &=\\frac{2q}{p^2}\\\\\n  \\text{E}(X^2)&=\\frac{2q}{p^2}+\\text{E}(X)=\\frac{2q}{p^2}+\\frac{1}{p}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n               &=\\frac{2q}{p^2}+\\frac{1}{p}-\\frac{1}{p^2}\\\\\n               &=\\frac{1-p}{p^2}\\\\\n\\end{aligned}\n\\]\n\n\n\n한 장비 제조 업체에서 사람의 건강과 생명을 진단하는데 사용되는 medical device가 1000 개마다 평균 2개 꼴로 불량이 발생한다고 가정할 때 그 장비를 공급받는 구매자가 medical device의 공급 업체의 불량 평가 기준으로 quality control을 1대 씩 진행할 때 불량 장비가 3번째 검사에서 발생할 확률은\n\\[\n\\begin{aligned}\n   \\text{f}(x=3;p=\\frac{2}{1000})&=(\\frac{998}{1000})^2\\frac{2}{1000}\\\\\n\\end{aligned}\n\\] 이다. 평균과 분산 각 각 \\((\\frac{2}{1000})^{-1}\\), \\(\\frac{1-\\frac{2}{1000}}{(\\frac{2}{1000})^2}\\) 이다.\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-28_mgf/index.html",
    "href": "docs/blog/posts/statistics/2023-02-28_mgf/index.html",
    "title": "Momment Generating Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 확률 변수 \\(X^r\\) 의 expectation, \\(\\text{E}((X-\\mu)^r)\\) 를 확률 변수 \\(X\\) 의 평균 \\(\\mu\\) 에 대한 \\(r\\) 차 중심 적률(moment) 이라하고 그 notation은 \\(\\mu_r'=\\text{E}((X-\\mu)^r)\\) 로 한다. \\(\\mu_r'=\\text{E}(X^r)\\) 은 원점에 대한 \\(r\\) 차 중심 적률이라 한다.\n즉,\n\\[\n\\begin{aligned}\n  \\mu_{r}'&=\n    \\begin{cases}\n      \\text{E}((X-\\mu)^r)&=\n        \\begin{cases}\n          \\sum_{x}(x-\\mu)^rf(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}(x-\\mu)^rf(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases} & \\text{moment about }\\mu\\\\\n      \\text{E}(X^r)&=\n        \\begin{cases}\n          \\sum_{x}x^rf(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}x^rf(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases} & \\text{moment about origin}\\\\\\\\\n    \\end{cases}  \n\\end{aligned}\n\\]\n이다.\n\n분포의 특징을 묘사하는 parameters 중 많은 종류가 확률 변수의 적률을 이용해 계산될 수 있다. 그 대표적인 예가\n\n평균 (mean): 분포의 위치를 나타내는 척도, 1차 중심 적률로 계산\n분산 (variance): 분포가 평균으로부터 퍼진 정도를 나타내는 척도, 2차 중심 적률로 계산\n왜도 (skewedness): 분포가 기울어진 방향과 정도를 나타내는 척도, 3차 중심 적률로 계산\n첨도 (kurtosis): 분포가 위로 뾰족한 정도를 나타내는 척도, 4차 중심 적률로 계산\n\n\nDefinition 2 확률 변수 \\(X\\) 의 적률 생성 함수 (Moment Generating Function, mgf), \\(\\text{M}_X(t) =\\text{E}(e^{tX})\\) 로 정의한다.\n즉,\n\\[\n\\begin{aligned}\n  M_X(t)=\\text{E}(e^{tX})&=\n        \\begin{cases}\n          \\sum_{x}e^{tx}f(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}e^{tx}f(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases}\n\\end{aligned}\n\\]\n이다.\n\n\nTheorem 1 확률 변수 \\(X\\) 의 적률 생성 함수 (Moment Generating Function, mgf), \\(\\text{M}_X(t) =\\text{E}(e^{tX})\\) 로 r차 적률 계산은 다음과 같이 할 수 있다.\n\\[\n\\begin{aligned}\n  \\frac{d^r}{dt^r}M_X(t) \\bigg|_{t=0}=M_X^r(0)=\\text{E}(X^r)=\\mu_r'\n\\end{aligned}\n\\]\n이다. 즉, 적률 생성 함수 (mgf) \\(M_X(t)\\) 를 구하고 r 번 미번한 후에 \\(t=0\\) 대입하면 r차 중심 적률을 구할 수 있다.\n\nproof (for the only continuous case)\n\\[\n\\begin{aligned}\n  \\text{First Order Moment}\\\\\n  \\frac{d}{dt}M_X(t)&= \\frac{d}{dt}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d}{dt}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}\\frac{d}{dt}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}xe^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}xf(x)dx\\\\\n    &=  \\text{E}(X)\\\\\n    &=  \\mu_1'\\\\\n  \\text{Second Order Moment}\\\\\n  \\frac{d^2}{dt^2}M_X(t)&= \\frac{d^2}{dt^2}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d^2}{dt^2}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}\\frac{d}{dt}xe^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^2e^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^2f(x)dx\\\\\n    &=  \\text{E}(X^2)\\\\\n    &=  \\mu_2'\\\\\n  \\vdots\\\\\n  \\text{r th Order Moment}\\\\\n  \\frac{d^r}{dt^r}M_X(t)&= \\frac{d^r}{dt^r}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d^r}{dt^r}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^re^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^rf(x)dx\\\\\n    &=  \\text{E}(X^r)\\\\\n    &=  \\mu_r'\n\\end{aligned}\n\\]\n\n\n\n\n\nMGF of \\(X\\sim B(n,p)\\)\n\n확률 변수 \\(X \\sim B(n,p)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n\\[\n\\begin{aligned}\n  f(x)&=\\binom{n}{x}p^xq^{n-x}\\\\\n  \\sum_{x=0}^{n}f(x)&=\\sum_{x=0}^{n}\\binom{n}{x}p^xq^{n-x}=(p+q)^n\\\\\n  M_X(t)&=\\sum_{x=0}^{n}e^{tx}f(x)=\\sum_{x=0}^{n}e^{tx}\\binom{n}{x}p^xq^{n-x}\\\\\n        &=\\sum_{x=0}^{n}\\binom{n}{x}(pe^t)^xq^{n-x}=(pe^t+q)^n\\\\\n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}(pe^t+q)^n\\\\\n    &=n(pe^t+q)^{n-1}pe^t\\bigg|_{t=0}\\\\\n    &=n(p+q)^{n-1}p=np\\\\\n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}(n(pe^t+q)^{n-1}pe^t)\\bigg|_{t=0}\\\\\n    &=(n(n-1)(pe^t+q)^{n-2}pe^tpe^t+n(pe^t+q)^{n-1}pe^t)\\bigg|_{t=0}\\\\\n    &=n(n-1)(p+q)^{n-2}p^2+n(p+q)^{n-1}p\\\\\n    &=n(n-1)p^2+np\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=n(n-1)p^2+np-(np)^2\\\\\n    &=n^2p^2-np^2+np-n^2p^2\\\\\n    &=np(1-p)=npq\n\\end{aligned}\n\\]\n\nMGF of \\(X\\sim Poisson(\\lambda)\\)\n\n확률 변수 \\(X \\sim Poisson(\\lambda)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n  M_X(t)&=\\sum_{x=0}^{\\infty}e^{tx}f(x)=\\sum_{x=0}^{\\infty}e^{tx}\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n        &=e^{-\\lambda}\\sum_{x=0}^{\\infty}\\frac{{e^{t}\\lambda^{x}}^x}{x!}\\\\\n        &=e^{-\\lambda}e^{\\lambda e^t} \\because \\text{(Maclaurin's Series)}\\\\\n        &=e^{\\lambda(e^t-1)} \\\\\n        (&\\text{Maclaurin's Series: } \\sum_{n=0}^{\\infty}\\frac{x^n}{n!}=1+x+\\frac{x^2}{2!}+\\frac{x^3}{3!}+ ... =e^x)\\\\     \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda e^{e^t}e^{\\lambda(e^t-1)}\\bigg|_{t=0}=\\lambda\\\\\n  \n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}\\lambda e^{e^t}e^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda e^te^{\\lambda (e^t-1)}+\\lambda e^t\\lambda e^te^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda + \\lambda^2\\\\\n\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=\\lambda + \\lambda^2 -\\lambda^2\\\\\n    &=\\lambda\n\\end{aligned}\\]\n$$\n\n\n\n\nMGF of \\(X\\sim N(0,1)\\)\n\n확률 변수 \\(X \\sim N(0,1)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}(-\\infty < x < \\infty)\\\\\n  M_X(t)&=\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx=\\int_{-\\infty}^{\\infty}e^{tx}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{x^2}{2}+tx}dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(x-t)^2+\\frac{1}{2}t^2}dx\\\\\n        &=e^{\\frac{t^2}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(x-t)^2}dx\\\\\n        &=e^{\\frac{t^2}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}u^2}du \\text{  }(\\because x-t=u \\rightarrow dx=du)\\\\\n        &=e^{\\frac{t^2}{2}} \\\\     \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=te^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=0\\\\\n  \n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}te^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=e^{\\frac{t^2}{2}}+t(te^{\\frac{t^2}{2}})\\bigg|_{t=0}\\\\\n    &=1\\\\\n\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=1\n    \n\\end{aligned}\\]\n$$\n\nMGF of \\(X\\sim N(\\mu,\\sigma^2)\\)\n\n확률 변수 \\(X\\sim N(\\mu,\\sigma^2)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}(-\\infty < x < \\infty)\\\\\n  M_X(t)&=\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^{\\infty}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}+tx}dx\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}(\\frac{(x-(t\\sigma^2+\\mu))}{\\sigma})^2}dx\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{u^2}{2}}du\\\\\n        (&\\because \\frac{x-(t\\sigma^2+\\mu)}{\\sigma}=u \\rightarrow \\frac{dx}{\\sigma}=du) \\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^2}{2}}du\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\\\\n  \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\bigg|_{t=0}\\\\\n    &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}(\\sigma^2t+\\mu)\\bigg|_{t=0}\\\\\n    &=\\mu\\\\\n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}e^{\\frac{t^2\\sigma^2}{2}+t\\mu}(\\sigma^2t+\\mu)\\bigg|_{t=0}\\\\\n    &=\\sigma^2+\\mu^2 \\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=\\sigma^2\n\\end{aligned}\\]\n$$\n\n\n\n\n\nTheorem 2 확률 변수 \\(X\\) 와 \\(Y\\) 가 유한한 같은 적률 생성 함수를 가지면 두 확률 변수는 같은 확률 분포를 갖는다. (단, \\(t\\in[-c,c]\\) where \\(c\\) is a positive constant) \\[\n\\text{M}_X(t) =\\text{M}_Y(t) \\rightarrow F_X(a)=F_Y(a) \\text{ for } a \\in \\mathbb{R}\n\\] 다시 말해서, 확률 변수의 분포의 특징이 적률 생성 함수에 의하여 유일하게 결정된다.\n\nProof Reference-Washington University\nMGF reference\n\nTheorem 3 \\[\n\\text{M}_{X+a}(t) =e^{at}\\text{M}_X(t)\n\\]\n\nProof) \\(\\text{E}(e^{t(x+a)})=\\text{E}(e^{at}e^{tx})=e^{at}\\text{E}(e^{tx})=e^{at}\\text{M}_X(t)\\)\n\nTheorem 4 \\[\n\\text{M}_{aX}(t) =\\text{M}_X(at)\n\\]\n\nProof) \\(\\text{E}(e^{atx})=\\text{E}(e^{at(x)})=\\text{M}_X(at)\\)\n\nExample) when \\(X \\sim N(0,1)\\), the mgf of \\(Y=aX+b\\) is ?\n\n\\[\n\\begin{aligned}\n  \\text{M}_Y(t)&=\\text{M}_{aX+b}(t)\\\\\n               &=e^{bt}\\text{M}_{X}(at)\\\\\n               &=e^{bt}e^{\\frac{a^2t^2}{2}}\\\\\n               &=e^{bt+\\frac{a^2t^2}{2}}\n\\end{aligned}\n\\]\n\nTheorem 5 서로 독립인 확률 변수 \\(X_1,X_2, ..., X_n\\) 의 적률 생성 함수가 각 각 \\(\\text{M}_{X_1}(t), \\text{M}_{X_2}(t), ..., \\text{M}_{X_n}(t)\\) 일 때, 확률 변수 \\(Y=X_1+X_2+...+X_n\\) 의 적률 생성함수 \\(\\text{M}_Y(t)\\) 는 \\(\\text{M}_{X_1}(t)\\text{M}_{X_2}(t) \\dots \\text{M}_{X_n}(t)\\) 이다.\n\\[\n\\begin{aligned}\n  \\text{M}_Y(t) &= E(e^{Yt})\\\\\n                &= E(e^{t(X_1+X_2+ ...+ X_n)})\\\\\n                &= E(e^{tX_1}e^{tX_2}\\dots e^{tX_n})\\\\\n                &= E(e^{tX_1})E(e^{tX_2})\\dots E(e^{tX_n}) \\because X_i \\text{ are independent}\\\\\n                &= \\text{M}_{X_1}(t)\\text{M}_{X_2}(t) \\dots \\text{M}_{X_n}(t)\n\\end{aligned}\n\\]\n\n\nExample) \\(X_1, X_2, ..., X_n\\) 가 서로 독립이고 parameter 가 각 각 \\(\\lambda_1, \\lambda_2, ..., \\lambda_n\\) 인 poisson 분포를 따른다면 \\(Y=X_1+X_2+...+X_n\\) 의 mgf는\n\n$$\n\\[\\begin{aligned}\n  \\text{M}_X(t) &= e^{\\lambda(e^t-1)}\\\\\n  \\text{M}_Y(t) &= E(e^{Yt})\\\\\n                &= E(e^{t(X_1+X_2+ ...+ X_n)})\\\\\n                &= E(e^{tX_1}e^{tX_2}\\dots e^{tX_n})\\\\\n                &= E(e^{tX_1})E(e^{tX_2})\\dots E(e^{tX_n}) \\because X_i \\text{ are independent}\\\\\n                &= \\text{M}_{X_1}(t)\\text{M}_{X_2}(t) \\dots \\text{M}_{X_n}(t)\\\\\n                &= e^{\\lambda_1(e^t-1)}+e^{\\lambda_2(e^t-1)}+\\dots+e^{\\lambda_n(e^t-1)}\\\\\n                &= e^{(\\lambda_1+\\lambda_2+\\dots+\\lambda_n)(e^t-1)}\\\\\n                &= e^{\\sum_{i=1}^{n}\\lambda_i(e^t-1)}\\\\\n\\end{aligned}\\]\n$$\n이다. 즉, Y는 parameter가 \\(\\sum_{i=1}^{n}\\lambda_i\\) 인 poisson 분포를 따른다. 뿐만 아니라, 정규 분포, poisson 분포, 카이제곱 분포의 경우 독립 변수들의 합으로 만든 분포도 같은 종류의 분포를 따르게 된다.\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html",
    "title": "Poisson Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\nDefinition 1 모수 (parameter)가 단위 시간 또는 공간 당 평균 발생 횟수 \\(\\lambda\\) 일 때 주어진 단위 시간 또는 공간 내에 발생하는 사건의 횟수를 확률 변수 \\(X\\) 로 하는 분포를 Poisson Distribution이라 한다. \\(X\\) 의 probability mass function은 \\[\n\\begin{aligned}\n  f_X(x;\\lambda)&=\\frac{e^{-\\lambda}\\lambda^{x}}{x!} &(x=0,1,2, ..)\n\\end{aligned}\n\\] (\\(\\lambda\\) 는 단위 시간 또는 공간 당 평균 발생 횟수) 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x=0}^{\\infty}xf(x)\\\\\n             &=\\sum_{x=0}^{\\infty}x\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n             &=\\sum_{x=1}^{\\infty}x\\frac{e^{-\\lambda}\\lambda^{x}}{x!} &\\because x=0 \\rightarrow \\text{equation}=0\\\\\n             &=\\sum_{x=1}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x}}{(x-1)!}\\\\\n             &=\\lambda\\sum_{x-1=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x-1}}{(x-1)!}\\\\\n             &=\\lambda\\sum_{y=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{y}}{(y)!} &\\because x-1=y \\text{  }(y=0,1,2, ...)\\\\\n             &=\\lambda\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n    \\text{E}(X(X-1))&=\\sum_{x=0}^{\\infty}x(x-1)f(x)\\\\\n             &=\\sum_{x=0}^{\\infty}x(x-1)\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n             &=\\lambda^2\\sum_{x=2}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x-2}}{(x-2)!}\\\\\n             &=\\lambda^2\\sum_{y=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{y}}{(y)!} &\\because x-2=y \\text{  }(y=0,1,2, ...)\\\\\n             &=\\lambda^2\\\\\n    \\text{E}(X^2)&=\\lambda^2+\\lambda\\\\\n    \\text{Var}(X)&=\\lambda^2+\\lambda-\\lambda^2=\\lambda\\\\\n\\end{aligned}\n\\]\n\n\n\n어느 의료 장비 제조 업체의 의료 장비 불량률이 2% 라고 가정했을 때 임의로 100대의 의료 장비를 구매하여 제조 업체의 Quality Control (QC) guide line을 따라 Quality Control (QC)를 진행 했을 때 불량품이 하나도 발생하지 않을 확률은 다음과 같다.\n\\[\n\\begin{aligned}\n  \\lambda &= 100*0.02=2\\\\\n  f(x)&=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}=\\frac{2^{x}e^{-2}}{x!}\\\\\n  f(0)&=\\frac{2^{0}e^{-2}}{0!}=e^{-2}\n\\end{aligned}\n\\]\n\n\nDefinition 2 \\(X\\sim B(n,p)\\) 일 때 \\(p\\) 가 충분히 작고 \\(n \\rightarrow \\infty\\) 고 \\(np=\\lambda\\) 한다면 \\(x=0,1,2, ...\\) 에 대하여\n\\[\n\\begin{aligned}\n  \\lim_{n \\to \\infty}\\binom{n}{x}p^{x}(1-p)^{n-x}=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}\n\\end{aligned}\n\\] (\\(\\lambda\\) 는 단위 시간 또는 공간 당 평균 발생 횟수) 이다.\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/computing_networking.html",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/computing_networking.html",
    "title": "Computing and Networking",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nAWS global infrastructure – Region / Availability zone (AZ) / Data Center\nAWS region consideration : Compliance – Latency – Pricing – Service availability\nInteracting with AWS\n\nAWS Management console\nAWS Command Line Interface (CLI) - unified tool to manage AWS services,\n\nAWS Software Development Kits (SDKs) - executing code with programming languages.\n\nSecurity in the AWS Cloud\n\nResponsibility - Customer & AWS\n\nAWS – security of the cloud, responsible for protecting and securing the infrastructure that runs all the services\nCustomer – security in the cloud, responsible for properly configuring the service and your applications, as well as ensuring your data is secure.\n\nRoot user (unristricted access)\nMulti-Factor Authentication\n\nAWS Identity and Access Management\n\nIntroduction to AWS IAM (Identity and Access Management)\nAWS account나 resource에 대한 권한을 관리하게 해주는 서비스\nAuthentication (인증)\nAuthorization (인가): resource에 대한 권한\n\nEx) 회사의 AWS account가 존재한다면, 회사 구성원들은 account 에 접근하기 위해 모두 IAM user가 되고 (authentication), account 내부의 각 AWS resource에 대한 접근 권한을 부여받아야 한다. (authorization)\n\nIAM policy: Effect (Allow/Deny), Action, Resource, Condition\nIAM Group\nIAM roles : IAM user나 AWS resource에 권한을 부여하기 위한 권한 세부 조정\nreference\n\n\n\n\n\n\n\n\nComputing\n\nit gives our application power in the form of CPU, memory, and networking capacity so that we can process our users’ requests.\nCompute Solution Examples\n\nEC2\ncontainer services: Amazon ECS(Elastic Container Service) and Amazon ECS (Elastic Container Service)\nserverless options AWS Lambda\n\ncompare them to one another.\n\nwhen and how to apply each service, to different use cases.\n\n\nNetworking\n\nneeded for being hands-on with Amazon EC2\nlaunch an EC2 instance to host the employee directory application.\nHow to Launch? → put an EC2 instance in a network, VPCs on AWS\n\n\n\n\n\n\nApps requiring computing capacity for running such as\n\nweb servers, batch jobs, databases, HR software, machine learning, or something else\non-premises computing resources are costly and have many things to deal with\nAWS already is ready to be used\n\nbuilt the infrastructure\nsecured the data centers.\nbought the servers,\nracked and stacked them\nare already online,\n\n3 computing services for different use cases.\n\nEC2 (Elastsic Compute Cloud)\ncontainer services\nserverless compute.\n\nEC2 (Flexible and scalable)\n\nEC2 instances Characteristics\n\na lot of flexibility and control in the cloud\nconfigure them to meet your needs.\nPayment\n\nat the end of the billing cycle, you only pay for what you use, either per second or per hour, depending on the type of instance.\nterminate (stop) the instance and you will stop incurring charges.\n\na range of operating systems including\n\nLinux, Mac OS, Ubuntu, Windows, and more\nTo select the OS for your server, you must choose\n\nAMI (Amazon Machine Image)\n\ncan set several configurations according to users’ use case\n\nYou can launch one or many instances from a single AMI, which would create multiple instances that all have the same configurations.\n\n\nThe instance types are grouped for use cases like\n\ncompute optimized\nmemory optimized\nstorage optimized, and more.\nread AWS documentation\nFor example,\n\nthe G instance family (graphics-intensive applications)\nthe M5 general purpose EC2 instance family (balance of resources)\nThe T3 or A1 is the instance family (the blend of the hardware capabilities)\nThen there’s the size like small, medium, large. It goes down to nano and up to many, many extra large sizes, depending on the instance type.\n\nthis type of selection: you are no longer locked into hardware decisions up front.\n\nchoose an initial EC2 instance type→ evaluate its performance for your specific use case, → change to a different type\n\nEC2 is also resizable with\n\na few clicks in the console or\ncan be done programmatically through an API call\n\n\nEC2 Instance Lifecycle\n\nyou launch an EC2 instance from an AMI\n\nit enters a pending state (booting up).\nit enters the running state (start being charged for the EC2 instance)\n\nrunning options\n\nreboot the instance\nstop your instance (stopping phase like powering down your laptop)\nstop hibernate (the stopping phase - no boot sequence required)\nthe terminate (the shutting down phase - get rid of an instance)\ntermination protection (back up in persistent storage in EC2)\n\ncharged if you are in the running state or if you are in the stopping state, preparing to hibernate.\n\n\n\n\n\nContainer Services\n\nefficiency and portability: container orchestration tools in AWS\n\nContainer orchestration\n\nprocesses to start, stop, restart and monitor containers running across not just one EC2 instance, but a number of them together that we call a cluster of EC2 instances.\nhundreds or thousands of containers - hard to manage them\nOrchestration tools : run and manage containers.\nAmazon ECS (Elastic Container Service)\n\nECS is designed to help you manage your own container orchestration software.\n\nAmazon EKS (Elastic Kubernetes Service)\n\nThe way you interact with these container services\n\nthe orchestration tool’s API\nthe orchestration tool carries out the management tasks.\nautomate scaling of your cluster\nautomate hosting your containers\nautomate the scaling of the containers themselves.\n\nsuper fast response to increasing demand when compared to virtual machines.\nhosting options : either ECS or EKS.\n\n\nServerless Compute Platform\n\nan alternative to hosting containers\n\nWhen using Amazon EC2 or Container Services running on top of EC2 as a compute platform, you are required to set up and manage your fleet of instances. This means that you are responsible for patching your instances when new software packages come out or when new security updates come out.\n\nServerless meaning\n\ncan not see or access the underlying infrastructure or instances that are hosting your solution.\nInstead, the management of the underlying environment from a provisioning, scaling, fault-tolerance and maintenance perspective is taken care of for a user.\nAll you need to do is focus on your application.\nserverless offerings are very convenient to use.\n\nAWS Fargate (the container hosting platform)\n\nserverless compute platform for ECS or EKS.\nThe scaling or fault-tolerance, OS or environments are built in\nFor user to do just\n\ndefine your container\nhow you want your container to be run\nit scales on-demand.\n\n\nAWS Lambda (the serverless compute platform)\n\npackage and upload your code to the Lambda service, creating what’s called a Lambda function.\nLambda functions run in response to triggers.\nYou configure a trigger\ncommon examples of triggers for Lambda functions\n\nan HTTP request\nan upload of a file to the storage service, Amazon S3\nevents originating from other AWS services or\neven in-app activity from mobile devices\n\nWhen the trigger is detected, the code is automatically run in a managed environment.\nLambda is currently designed to run code that has a runtime of under 15 minutes.\n\nSo, this isn’t for long running processes like deep learning or batch jobs, you wouldn’t host something like a WordPress site on AWS Lambda.\nIt’s more suited for quick processing, like a web backend for handling requests or a backend report processing service.\n\nnot billed for code that isn’t running, you only get billed for the resources that you use, down to 100 millisecond intervals.\n\n\n\n\n\n\n\n\nthe network, or VPC\n\nto launch instances, you needed to select a network. Building a custom VPC for our application that is more secure and provides more granular access to the Internet than the default option we originally chose.\nNetworking on AWS is the basis of most architectures. In this section, geared towards EC2-related services.\na Lambda function\n\nmight not need a network at all\n\n\nCreation and Concept of VPC\n\nCreation and Concept of VPC\n\nConcept\n\nIt creates a boundary where your applications and resources are isolated from any outside movement.\nnothing comes into and comes out of the VPC without your explicit permission.\n\nCreation\n\nTo create a VPC, two specific settings to declare\n\nthe region you’re selecting (In this example, the Oregon region)\nthe IP range in the form of CIDR notation. (In this example, 10.1.0.0/16)\n\nthe VPC name: app-vpc\n\nDivide VPC space into subnets\n\nput your resources such as your EC2 Instances inside of these subnets.\nThe goal of these subnets is to provide more granular controls over access to your resources.\nWith public resources, put those resources inside a subnet with internet connectivity.\nWith private resources like a database, create another subnet and have different controls to keep those resources private.\nTo create a subnet, you need three main things,\n\nthe VPC your subnet to live in which is this one,\nthe AZ (example. AZ-A = US-West-2a) your subnet to live in,\nIP range for your subnet which must be a subset of the VPC IP range\n\n\ninternet gateway for public resource\n\nenable internet connectivity\nWhen you create an internet gateway, you then need to attach it to your VPC.\n\nVGW (Virtual Private Gateway) for private resource\n\ncreate a VPN connection between a private network like an On-premise data center or internal corporate network to your VPC.\nestablish an encrypted VPN connection to your private internal AWS resources.\n\nhaving high availability: one option to make VPC better\n\nWhat that means is if this AZ goes down for whatever reason, what happens to our resources in that AZ? They go down too. So ideally, we would have resources in another easy to take on the traffic coming to our application.\nTo do this, we’d need to duplicate the resources in this AZ into the second AZ. So that means we need to create two additional subnets each within another AZ, say AZ b. All right\n\n\n\nAmazon VPC Routing\n\nThe example has two additional subnets, one public, one private in a different AZ for a total of four subnets including an EC2 instance hosting our employee directory inside of the public subnet in AZ A.\nroute tables\n\nprovide a path for the internet traffic to enter the internet gateway and find the right subnet.\nA route table contains a set of rules, called routes\n\ndetermine where network traffic is directed\nThese route tables can be applied on either the subnet level or at the VPC level.\nWhen creating a brand new VPC, AWS creates a route table called the main route table and applies it to the entire VPC.\nAWS assumes that when you create a new VPC with subnets, you want traffic to flow between those subnets.\nThe default configuration of the main route table is to allow traffic between all subnets local to the VPC.\nthe main route table of the VPC can be created in console\n\nclick on route tables on the side panel\ncreate route table.\ngive it a name such as app-route-table-public\nchoose the app-vpc and then click create.\n\n\n\n\nSecure Network with Amazone VPC Security\n\nat the base level, any new VPC is isolated from internet traffic to prevent risk.\nwhen allowing internet traffic by opening up routes, you need two options to keep your network secure, network access control lists\n\nnetwork ACLs\n\na firewall at the subnet level\ncontrol what kind of traffic is allowed to enter, and leave, your subnet\nThe default network ACL allows all traffic in and out of your subnet.\nUsing this default configuration is a good starting place but if needed, you can change the configuration of your network ACLs to further lock down your subnets.\nFor example, if you only wanted to allow HTTPS traffic into my subnet, you can do that by creating an inbound rule and outbound rule in my ACL, that allows HTTPS traffic from anywhere on port 443 and denies everything else.\n\nsecurity groups\n\nfirewalls that exist at the EC2 instance level.\nAny time you create an EC2 instance, you’ll need to place that EC2 instance inside a security group that allows the appropriate kinds of traffic to flow to your application."
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#hypothesis-testing",
    "href": "docs/blog/posts/statistics/guide_map/index.html#hypothesis-testing",
    "title": "Content List, Statistics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n1111-11-11, Hypothesis Testing\n1111-11-11, Permutation Test\n\n\nMethods of Finding Tests\n\n1111-11-11, Likelihood Ratio Tests\n1111-11-11, Bayesian Tests\n1111-11-11, Union-Intersection and Intersection-Union Tets\n\n\n\nMethods of Evaluating Tests\n\n1111-11-11, Power\n1111-11-11, Error Proabilities and the Power Function\n1111-11-11, Most Powerful Tests\n2022-12-28, p-values\n1111-11-11, Loss Function Optimality\n1111-11-11, Multiple Testing\n1111-11-11, Sample Size Calculation\n1111-11-11, A/B Testing\n2023-01-07, ANOVA\n\n2023-01-27, ANCOVA\n2023-01-27, repeated measures ANOVA\n2023-01-28, MANOVA"
  },
  {
    "objectID": "docs/blog/posts/DL/2023-03-10_cnn/index.html",
    "href": "docs/blog/posts/DL/2023-03-10_cnn/index.html",
    "title": "CNN",
    "section": "",
    "text": "Affine Layer는 인접하는 Layers의 nodes가 모두 연결되고 출력의 수가 임의로 정해지는 특징을 갖는데 이 때 data shape가 무시가 되는 단점이 있다. 이미지 데이터는 보통 (weight, height, color channel) 형태의 shape를 갖지만 MLP에서 이 3차원 구조가 1차원으로 flatten된다. 다시 말해서, 3차원의 이미지 pixels이라는 여러 독립 변수의 위치적 상관성이 1차원화 되면서 무시가 된다.\n많은 일반 머신러닝 모델이나 통계 분석 모델은 독립 변수가 독립이라는 가정이 고려되어야 하지만 이미지의 독립 변수들이 서로 독립이 아니다. 픽셀값은 그 위치에 따라서 서로 상관성이 존재한다. 초기엔 독립 변수인 픽셀을 일렬로 늘어뜨려 input으로 사용했지만 위치 기반 픽셀의 상관성 정도를 자세히 반영하진 못했다. 이를 보완하기 위해 CNN에서는 region feature가 고안됐다. 이처럼, CNN은 이미지 인식 분야에서 독보적인 영역을 갖고 있다.\n\n\n\nRegion Feature 또는 Graphic Feature라고도 한다. 픽셀의 지역 정보를 학습할 수 있는 신경망 구조가 CNN이다."
  },
  {
    "objectID": "docs/blog/posts/DL/2023-03-10_cnn/index.html#convolution-layer-conv",
    "href": "docs/blog/posts/DL/2023-03-10_cnn/index.html#convolution-layer-conv",
    "title": "CNN",
    "section": "2.1 Convolution Layer (Conv)",
    "text": "2.1 Convolution Layer (Conv)\nkernel 또는 filter라고 불리는 특징 추출기(Feature detector)를 사용하여 데이터의 특징을 추출하는 CNN 모델의 핵심 부분이다. kernel 를 정의해 입력 층의 이미지의 feature를 추출한다. kernel는 region feature의 크기와 weight을 정의하게 된다. 예를 들어, kernel을 (3x3)으로 정하면 9칸에 가중치를 설정하여 이미지 픽셀값 (a part of input feature map)과 kernel의 weight의 선형결합으로 conv layer를 구성하는 하나의 값을 얻어낸다 (See Figure 1 의 노란색 사각형).\nconvolution layer의 input/output은 보통 feature map이라고 부르며 input data를 input feature map, output data를 output feature map로 부르기도 한다. 즉, feature map 과 input/output data는 같은 의미로 사용되고 feature map = input data + kernel (= receptive field = filter)로 구성된다.\n\n2.1.1 Convolution Operation\nConvolution Operation (합성곱 연산)은 filter operation (filter 연산)이라고도 불린다. Figure 2 을 보면 입력 데이터 (이미지의 pixels)와 filter의 가중치가 element-wise 별로 곱해져 더해진다. 이 연산을 fused-multiply-add (FMA) or multiply-accumulate operation 라고도 부른다. 예를 들어, input data의 4, 5, 2, 9, 6, 4, 2, 2, 5와 filter1 의 1, 1, 1, 0, 0, 0, -1, -1, -1 가 곱해지고 더해져 4, 5, 2, 0, 0, 0, -2, -2, -5의 결과가 Conv Layer의 output data의 한 칸을 구성하게 된다.\n\n\n\nFigure 2: Convolution Operation Example\n\n\n이렇게, 2차원 입력에 대한 convolution (conv) operation은 Figure 3 과 같이 동작한다. Sharpen filter라고 쓰여진 3x3 행렬은 kernel로서 입력 데이터의 특징을 추출하는데, 입력 데이터의 전체를 보는 것이 아닌 kernel size 만큼의 일부분만을 보며 특징을 추출한다. feature map은 kernel의 개수만큼 생성되는데 일반적으로 다양한 특징을 추출하기 위해 하나의 conv layer에서 여러개의 kernel을 사용한다. kernel size에 정해진 규칙은 없으나 주로 3*3 많이 사용하며 대게 conv layer마다 다른 kernel size를 적용한다. Fully connected layer에서의 weight는 CNN에서 filter의 weight과 대응되고 CNN에서의 bias는 항상 scalar로 주어진다.\n\n\n\nFigure 3: Convolution Operation Process Example\n\n\n\n\n2.1.2 합성곱 연산을 위한 설정 사항\n\npadding : 입력 데이터의 테두리를 0으로 채워 데이터의 크기를 늘려준다\n\n왜 padding을 사용하는가? padding이 없을 경우 합성곱은 입력 데이터의 1행 1열부터 시작된다. 그런데 합성곱은 입력 데이터에서 kernel size만큼의 영역을 하나로 축소하여 특징을 추출하기 때문에 이 경우 입력 데이터의 가장자리, edge 부분의 특징을 추출하기 어렵다. edge의 특징까지 추출하고자 하면 적어도 0행 0열부터 kernel을 적용해야 하는데 허공에서 element-wise 계산을 할 수 없으니 0을 추가해준다. 다시 말해서, padding은 output data size를 조정할 목적으로 사용된다. \n\nstride : kernel이 얼만큼씩 이동하면서 합성곱 계산을 할 것인지를 의미한다.\n\nstride를 키우게 되면 output data size가 작아지기 때문에 일반적으로 한 칸씩 이동한다. \n\nweight sharing:\n\nkernel size : kernel의 행과 열 개수\n\n사이즈가 작을수록 국소 단위의 특징을 추출한다.\n\nkernel 개수 또는 channel 개수 : 몇 개의 feature map을 추출하고 싶은지에 따라 kernel 개수를 정한다. \n이미지 데이터에서의 channel 예시\n\n고양이 이미지와 같이 컬러 이미지 데이터는 하나의 이미지에 대해 Red, Green, Blue (RGB) 3개의 색상으로 이루어져 있다  \n\n\n\n\n2.1.3 feature map의 shape 계산 방법\nfeature map은 다음 레이어의 입력 데이터가 되기 때문에 feature map의 shape을 계산할 수 있어야한다.\n\n2.1.3.1 2 Dimension Input Data Size\n다음과 같이 output data size계산을 위한 notation을 정의할 때,\n\ninput data size : \\((H,W)\\)\nfilter size : \\((FH, FW)\\)\noutput data size : \\((OH, OW)\\)\npadding : \\(P\\) (width number)\nstride : \\(S\\)\na function to make the calculation result an integer\n\nfloor function : \\(\\lfloor \\text{ } \\rfloor\\)\nceiling : \\(\\lceil \\text{ } \\rceil\\)\nrounding to the nearest integer: \\(\\lfloor \\text{ } \\rceil\\)\n\n\n\\[\n\\begin{aligned}\n  OH=&\\lfloor\\frac{H+2P-FH}{S}+1\\rfloor\\\\\n  OW=&\\lfloor\\frac{W+2P-FW}{S}+1\\rfloor\n\\end{aligned}\n\\]\n의 관계식을 따르게 된다.\n\n예시1- input size: (4x4), P:1, S:1, filter size : (3x3)일 때, \\((OH,OW)=(4,4)\\)\n예시2- input size: (7x7), P:0, S:2, filter size : (3x3)일 때, \\((OH,OW)=(3,3)\\)\n예시3- input size: (28x31), P:2, S:3, filter size : (5x5)일 때, \\((OH,OW)=(10,11)\\)\n\n\n\n2.1.3.2 3 Dimension Input Data Size\n길이 또는 채널 방향으로 feature map이 늘어나기 때문에 그 결과는 Figure 2 과 같이 나온다. 반드시 input data의 channel 수와 output data channel수가 같아야한다. 채널이 3개면 filter 당 3장의 feature map이 나오게 된다. filter의 종류의 수 weight의 종류의 수로 output data의 길이를 늘리려면 (즉, 다수의 채널로 만드려면) filter의 수 (=weight의 종류)를 늘리면 된다. FN: Flter Number일 때 filter의 가중치 데이터 크기는 (output data channel, input data channel, height, width)로 표현한다. Bias는 \\((FN,1,1)\\) 로 표현하여 채널 하나에 값 하나씩 할당되게 디자인한다. Output data size는 \\((FN,OH,OW)\\) 로 표현된다.\n참고) \\[\n(FN,1,1) + (FN,OH,OW) \\overset{\\text{broadcasting}} \\rightarrow (FN,OH,OW)\n\\]\n\n예시- 채널=3, (FH,FW)=(4,4), \\(FN=20\\) 이면 \\((20,3,4,4)\\) 로 표현"
  },
  {
    "objectID": "docs/blog/posts/DL/2023-03-10_cnn/index.html#pooling-layer",
    "href": "docs/blog/posts/DL/2023-03-10_cnn/index.html#pooling-layer",
    "title": "CNN",
    "section": "2.3 Pooling Layer",
    "text": "2.3 Pooling Layer"
  },
  {
    "objectID": "docs/blog/posts/DL/2023-03-10_cnn/index.html#fully-connected-layer",
    "href": "docs/blog/posts/DL/2023-03-10_cnn/index.html#fully-connected-layer",
    "title": "CNN",
    "section": "2.4 Fully Connected Layer",
    "text": "2.4 Fully Connected Layer"
  },
  {
    "objectID": "docs/blog/posts/DL/guide_map/index.html#introduction",
    "href": "docs/blog/posts/DL/guide_map/index.html#introduction",
    "title": "Content List, Deep Learning",
    "section": "Introduction",
    "text": "Introduction\n\n1111-11-11, Artificial Intelligence\n1111-11-11, Perceptron\n1111-11-11, Artificial Neural Netwroks (ANN)\n\n1111-11-11, activation functions\n1111-11-11, output layer design\n\n1111-11-11, loss function\n1111-11-11, numerical differentiation\n1111-11-11, gradient descent\n1111-11-11, backpropagation\n1111-11-11, optimizer\n\n1111-11-11, stochastic gradient descent\n1111-11-11, momentum\n1111-11-11, adaGrad\n1111-11-11, adam\n1111-11-11, weight initalization\n\n1111-11-11, batch normalization\n1111-11-11, dropout\n1111-11-11, tuning parameter\n1111-11-11, auto-encoder\n1111-11-11, stacked auto-encoder\n1111-11-11, denoising auto-encoder(DAE)\n\n\nConvolutional Neural Network (CNN)\n\n2023-03-10, CNN (1) - Concept\n2023-03-10, CNN (2) - Practice\n\n\n\nNatural Language Process (NLP)\n\n1111-11-11, word2vec\n1111-11-11, improved word2vec\n\n\n\nRecurrent Neural Network (RNN)\n\n\nGate Recurrent Unit (GRU)\n\n\nLong Short-Term Memory (LSTM)\n\n\nAttention (Transformer)\n\n\nBidirectional Encoder Representations from Transformers (BERT)\n\n\nGenerative Pre-training Transformer (GPT)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/epsilon_delta/index.html",
    "href": "docs/blog/posts/Mathmatics/epsilon_delta/index.html",
    "title": "\\(\\epsilon - \\delta\\) Method",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 Let \\(f\\) be a function defined on some open interval that contains the number \\(a\\), exccept possibly at \\(a\\) itself. Then, \\(f\\) is said to converge to the real number \\(L\\) provided that for every number \\(\\epsilon>0\\), there is a number \\(\\delta>0\\) such that \\[\n\\text{if } 0<|x-a|<\\delta \\text{ then } |f(x)-L|<\\epsilon\n\\]\nIf \\(f(x)\\) converges to \\(L\\), then \\(L\\) is called the limit of \\(f(x)\\) as \\(x\\) approaches \\(a\\) , and we write \\[\n\\lim_{x \\to a} f(x) =L\n\\]\nIf a function does not coverge to a real number, it is said to diverge.\n\n먼저, 위의 정의를 하나씩 곱 씹어보면,\n\n\\(\\text{if } 0<|x-a|<\\delta \\text{ then } |f(x)-L|<\\epsilon\\) 에서 \\(|x-a|\\) 와 \\(|f(x)-L|\\) 가 절대값으로 표기가 되어 있기 때문에 거리(distance)로 해석이 된다.\n또한, \\(|x-a|\\) 는 임의의 충분히 작은 수 \\(\\delta\\) 와 대응이 되고 \\(|f(x)-L|\\) 는 임의의 충분히 작은 수 \\(\\epsilon\\) 와 대응이 되는 것을 숙지해야한다.\n그리고 \\(\\epsilon - \\delta\\) Method 라는 표현에서도 이해에 대한 실마리를 얻을 수 있는데 \\(\\epsilon\\) 이 먼저 정해지면 \\(\\delta\\) 를 그 후에 결정할 수 있다. 좀 더 자세히 말하면, 궁극적으로 \\(\\delta\\) 를 \\(\\epsilon\\) 의 함수 \\(\\delta(\\epsilon)\\) 로 표현하여 함수의 수렴성을 증명하게 된다.\n\n그럼 본격적으로 limit의 notation, \\(\\lim_{x \\to a} f(x) =L\\) 을 해석하면,\n\n고등학교 때 우리는 이 표현을 \\(x\\) 가 \\(a\\) 로 한없이 가까워질 때 \\(f(x)\\) 의 limit은 \\(L\\) 이다 라고 배웠다.\n이를 조금 더 정밀하게 해석하면, \\(x\\) 와 \\(a\\) 사이의 거리가 만족할 만큼 충분히 작아질 때, (하지만 \\(x\\ne a\\)), \\(f(x)\\) 와 \\(L\\) 사이의 거리가 임의대로 작아 질수 있다는 것을 의미한다.\n\n하지만 두 번째 표현 역시 수학적이지 않다. 왜냐하면 위의 문장은 만족할 만큼 충분히 작아질 때 라는 표현 때문에 명제(statement)가 될 수 없기 때문이다. 사람의 주관마다 어떤 사람은 거리가 1 일 때 충분히 작다고 말 할 수 있고 좀 더 정밀한 사람의 경우 0.0001 이 충분히 작다고 말할 수 있기 때문이다. 또 어떤 사람은 100이 충분히 작은 거리라고 표현할 수 있는 모호성이 존재한다.\n여기서, \\(\\lim_{x \\to a} f(x) =L\\) 의 해석을 분석해보고 수식화 시켜보자.\n‘\\(x\\) 와 \\(a\\) 사이의 거리가 만족할 만큼 충분히 작아질 때, (하지만 \\(x\\ne a\\)), \\(f(x)\\) 와 \\(L\\) 사이의 거리가 임의대로 작아 질수 있다.’ 는\n\n조건절: ’\\(x\\) 와 \\(a\\) 사이의 거리가 만족할 만큼 충분히 작아질 때, (하지만 \\(x\\ne a\\)),\n결과절: \\(f(x)\\) 와 \\(L\\) 사이의 거리가 임의대로 작아 질수 있다.’\n\n와 같이 조건절과 결과절로 나눌 수 있다.\n이를 수식으로 표현하면, 거리의 의미는 수학에서 절대값으로 표현될 수 있기 때문에\n\n조건절: \\(|x-a|\\) 가 만족할 만큼 충분히 작아질 때, (하지만 \\(x\\ne a\\)),\n결과절: \\(|f(x)-L|\\) 가 임의대로 작아 질수 있다.’\n\n와 같이 표현 될 수 있다.\n위의 정의 Definition 1 의 \\(\\text{if } 0<|x-a|<\\delta \\text{ then } |f(x)-L|<\\epsilon\\) 를 유심히 보면 애매한 표현을 수학적으로 표현하기 위해 누구나 만족할만한 충분히 작은 수 를 임의의 양수 (every number \\(\\epsilon>0\\) or any number \\(\\epsilon>0\\))라고 표현하여 명제화 시키는 것을 볼 수 있다.\n부등식으로 표현된 명제, \\(\\text{if } 0<|x-a|<\\delta \\text{ then } |f(x)-L|<\\epsilon\\) 를 대수적으로 변형시켜 분석해보자.\n\n조건절: \\(\\text{if } 0<|x-a|<\\delta\\)\n\n\\[\n\\begin{aligned}\n    |&x-a|<\\delta \\\\\n    -\\delta<&x-a<\\delta \\text{  }(\\because \\delta>0)\\\\\n    a-\\delta<&x<a+\\delta\n\\end{aligned}\n\\tag{1}\\]\n\\[\n\\begin{aligned}\n    0<|&x-a|\\text{  }(\\because x \\ne a)\n\\end{aligned}\n\\tag{2}\\]\n\n결과절: \\(\\text{ then } |f(x)-L|<\\epsilon\\) \\[\n\\begin{aligned}\n  |&f(x)-L|<\\epsilon \\\\\n  -\\epsilon<&f(x)-L<\\epsilon \\text{  }(\\because \\epsilon>0)\\\\\n  L-\\epsilon<&f(x)<L+\\epsilon\\\\\n\\end{aligned}\n\\tag{3}\\]\n\n위의 간단한 부등식 조작으로 3가지 사실을 재정리했다.\n\n\\(x\\) 의 범위 (see Equation 1) : \\(a-\\delta<x<a+\\delta\\)\n\\(|x-a|\\) 의 범위 (see Equation 2): \\(0<|x-a|\\)\n\\(f(x)\\) 의 범위 (see Equation 3): \\(L-\\epsilon<f(x)<L+\\epsilon\\\\\\)\n\n이 3가지 사실을 기반으로 limit의 정의 (Definition 1)를 다시 해석해보면,\n\\(lim_{x\\to a}f(x)=L\\) 은 모든 임의의 양수 \\(\\epsilon>0\\) 에 대해서,\\(x \\in (a-\\delta,a-\\delta)\\) (i.e. \\(x\\) 가 \\((a-\\delta,a-\\delta)\\) 범위안에 있다) 이고 \\(x\\ne a\\) 라면 \\(f(x) \\in (L-\\epsilon,L+\\epsilon)\\) 을 만족시키는 임의의 양수 \\(\\delta>0\\) 가 존재한다\n라고하는 좀 더 쉬운 해석이 가능해진다. 다른 방식으로 표현하면,\n\\[\n\\begin{aligned}\n\\text{If } f(x) \\in (L-\\epsilon,L+\\epsilon), &\\text{then } \\exists \\text{ } x \\in (a-\\delta,a+\\delta)  \\ni \\\\\nf:(a-\\delta,a+\\delta) &\\rightarrow (L-\\epsilon,L+\\epsilon)\n\\end{aligned}\n\\]\n여기서, \\(f(x)\\in (L-\\epsilon,L+\\epsilon)\\) 은 \\(f(x)\\) 를 \\(L\\) 의 근방 \\((\\text{i.e. } L-\\text{neighborhood})\\) 으로 한정시켰다고 표현한다. 같은 방식으로, \\(x \\in (a-\\delta,a+\\delta)\\) 은 \\(x\\) 를 \\(a\\) 의 근방 \\((\\text{i.e. } a-\\text{neighborhood})\\) 한정시켰다고 표현한다. 여기서, neighborhood는 \\(\\epsilon\\) 과 \\(\\delta\\) 가 정해져야 결정될 수 있는 것을 볼 수 있다. 그리고 if 조건문에 의해 \\(\\epsilon\\) 에 의해 \\(\\delta\\) 가 정해진다는 것을 미루어 짐작할 수 있다.\n이를 또 다르게 해석할 수 있는데,\n\\[\n\\begin{aligned}\n\\text{If } \\lim_{x \\to a}f(x)=L, &\\text{ then } \\exists \\delta > 0 \\ni\\\\\n\\text{if } x \\in (a-\\delta,a+\\delta), &\\text{ then } f(x) \\in (L-\\epsilon,L+\\epsilon)\n\\end{aligned}\n\\]\n위의 표현을 해석해보면, \\(x\\) 가 \\(a\\) 로 한없이 다가가서 \\(L\\) 에 수렴한다면, \\(x\\) 를 \\(a\\) 근방에 한정시켜 \\(f(x)\\) 가 \\(L\\) 근방에 한정되는 임의의 양수 \\(\\delta\\) 가 존재한다 라고 해석할 수 있다.\n\n\n\nfind \\(\\delta\\) corresponding to \\(\\epsilon=0.5\\) in the definition of a limit for \\(f(x)=x+5\\) with \\(a=1\\) and \\(L=6\\). \\[\n\\text{if } |x-1|<\\delta \\text{ then} |(x+5)-6|<0.5\n\\]\n\nSolution)\n\\[\n\\begin{aligned}\n    -0.5<&(x+5)-6<0.5 \\\\\n    5.5 <&x+5<6.5 \\\\\n    0.5 <&x<1.5 \\\\\n\\text{If } 0.5 <x<1.5, &\\text{ then } 5.5 <x+5<6.5 \\\\\n\\therefore \\text{If }|x-1|<0.5, &\\text{ then } |(x+5)-6|<0.5 (\\because (0.5.1.5) \\text{ is symmetric about }x=1)\n\\end{aligned}\n\\]\nIf the interval of \\(x\\) is not symmetric about x=a, the smaller number is chosed as \\(\\delta\\). 만약 \\(x\\) 의 구간이 \\(a\\) 를 기준으로 대칭이 아니라면 더 짧은 근방을 \\(\\delta\\) 로 설정한다.\n\\(\\text{If }|x-1|<0.5, \\text{ then } |(x+5)-6|<0.5\\) 을 해석하면\n\\(a=1\\) 을 중심으로 \\(0.5(=\\delta)\\) 근방의 \\(x\\) 를 설정하면, \\(L=6\\) 을 중심으로 한 \\(0.5 (=\\epsilon)\\) 근방의 \\(f(x)\\) 를 얻을 수 있다.\n\nProve that \\(\\lim_{x \\to 3} (4x-5)=7\\)\n\nProof)\nThe 1st step is to find \\(\\delta\\) : \\[\n\\begin{aligned}\n    \\text{If } |x-3|<\\delta, &\\text{ then } |4x-5|-7<\\epsilon \\\\\n    |4x-5|-7&=|4x-12|=4|x-3| \\\\\n    4|x-3|<\\epsilon \\\\\n    \\text{If } |x-3|<\\delta, &\\text{ then } 4|x-3|<\\epsilon \\\\\n    \\text{If } |x-3|<\\delta, &\\text{ then } |x-3|<\\frac{\\epsilon}{4} \\\\\n    \\therefore \\delta = \\frac{\\epsilon}{4}\n\\end{aligned}\n\\]\nThe 2nd step is to prove that the \\(\\delta\\) works:\nGiven \\(\\epsilon>0\\), choose \\(\\delta=\\frac{\\epsilon}{4}\\) \\[\n\\begin{aligned}\n    \\text{If } 0<|x-3|<\\delta, &\\text{ then } 4|x-3|<4\\delta=4\\frac{\\epsilon}{4}=\\epsilon \\\\\n    \\text{Thus, } \\text{If } 0<|x-3|<\\delta, &\\text{ then } |(4x-5)-7|<\\epsilon \\\\\n    \\therefore \\lim_{x \\to 3}(4x-5)=7\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nreference : james steward, Calculus"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/taylor_series/index.html",
    "href": "docs/blog/posts/Mathmatics/taylor_series/index.html",
    "title": "Taylor’s Series",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 A sequence is a list of numbers written in a definite order: \\[\na_1, a_2, a_3, \\dots, a_n, a_{n+1} \\dots  \n\\]\nthe number \\(a_1\\), \\(a_2\\), and \\(a_n\\) are called the first term, the second term, the nth term. Since for every positive inter \\(n\\) there is a corresponding number \\(a_n\\), a sequence can be defined as a funtion with domain of the set of positive integers. The notation of the sequence function is \\(a_n\\) instead of \\(f(n)\\) in convention: \\[\n\\text{The sequence } \\{a_1,a_2,a_3,\\dots\\} \\text{ is also denoted by } \\{a_n\\} \\text{ or } \\{a_n\\}_{n=1}^{\\infty}\n\\] .\n\n\nDefinition 2 A series or infinite series is defined as a sum of the terms of an infinite sequence \\(\\{a_n\\}_{n=1}^{\\infty}\\): \\[\na_1+ a_2+ a_3+ \\dots+ a_n+ a_{n+1} \\dots  \n\\]\nThe notation of a series is: \\(\\sum_{n=1}^{\\infty} a_n\\) or \\(\\sum a_n\\).\n\n\nDefinition 3 A power series is a series of the following form: \\[\n\\sum_{n=0}^{\\infty} c_nx^n = c_0x^0+c_1x^1+c_2x^2+ \\dots+ c_nx^n+c_{n+1}x^{n+1}+\\dots  \n\\]\nwhere \\(x\\) is a variable and the \\(c_n\\)’s are constants called the coefficients of the series.\n\n\nDefinition 4 A power series centered at a is a series of the following form: \\[\n\\begin{aligned}\n    \\sum_{n=0}^{\\infty} c_n(x-a)^n &= c_0(x-a)^0+c_1(x-a)^1+c_2(x-a)^2+ \\dots+ c_n(x-a)^n+c_{n+1}(x-a)^{n+1}+\\dots  \\\\\n                                    &= c_0+c_1(x-a)^1+c_2(x-a)^2+ \\dots+ c_n(x-a)^n+c_{n+1}(x-a)^{n+1}+\\dots  \n\\end{aligned}\n\\]\nwhere \\(x\\) is a variable and the \\(c_n\\)’s are constants called the coefficients of the series.\n\n\nTheorem 1 \\(f\\) is said to be a expanded power series centered at a : \\[\n\\text{if }f(x)=\\sum_{n=0}^{\\infty} c_n(x-a)^n |x-a|<R, \\text{ then, its coefficients are given by the formula } c_n=\\frac{f^{(n)}(a)}{n!}\n\\]\nwhere \\(x\\) is a variable and the \\(c_n\\)’s are constants called the coefficients of the series.\n\nProof)\nLet \\(f\\) is any function that can be represented by a powerseries.\n$$\n\\[\\begin{aligned}\n    \n    f(x) &= c_0+c_1(x-a)^1+c_2(x-a)^2+ \\dots+ c_n(x-a)^n+c_{n+1}(x-a)^{n+1}+\\dots \\text{  }|x-a|<R\\\\\n    f(a) &= 0\\\\\n    f'(x) &= c_1+2c_2(x-a)+ 3c_3(x-a)^2+ \\dots \\text{  }|x-a|<R\\\\ \\\\\n    f'(a) &= c_1 \\\\\n    f''(x) &= 2c_2(x-a)+ 2\\times 3c_3(x-a)+ 3\\times 4c_4(x-a)^2+ \\dots \\text{  }|x-a|<R\\\\\n    f''(a) &= 2c_2 \\\\\n    f'''(x) &= 3!c_3 + 4! c_4(x-a)+3\\times 4 \\times 5 c_4(x-a)^2 \\dots \\text{  }|x-a|<R\\\\\n    f'''(a) &= 3!c_3 \\\\\n    \\vdots \\\\\n    f^{(n)}(a) &= n!c_n \\\\\n    c_n&=\\frac{f^{(n)}}{n!}\n\\end{aligned}\\]\n$$\n\nDefinition 5 \\(f\\) is said to be a Taylor’s series if f has a expanded power series at a with the following form: \\[\n\\begin{aligned}\n    f(x)&=\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n!}(x-a)^n \\\\\n        &= \\frac{f^{(0)}(a)}{0!}(x-a)^0+\\frac{f^{(1)}(a)}{1!}(x-a)^1+\\frac{f^{(2)}(a)}{2!}(x-a)^2+\\frac{f^{(0)}(a)}{0!}(x-a)^3 + \\dots \\\\\n        &= f(a)+\\frac{f^{(1)}(a)}{1!}(x-a)^1+\\frac{f^{(2)}(a)}{2!}(x-a)^2+\\frac{f^{(3)}(a)}{3!}(x-a)^3 + \\dots\n\\end{aligned}\n\\]\n\n\nDefinition 6 \\(f\\) is said to be a Maclaurin series if f has a Taylor’s series with the special case \\(a=0\\): \\[\n\\begin{aligned}\n    f(x)&=\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(0)}{n!}(x)^n \\\\\n        &= \\frac{f^{(0)}(0)}{0!}x^0+\\frac{f^{(1)}(0)}{1!}x^1+\\frac{f^{(2)}(0)}{2!}x^2+\\frac{f^{(3)}(0)}{3!}x^3 + \\dots \\\\\n        &= f(0)+\\frac{f^{(1)}(0)}{1!}x^1+\\frac{f^{(2)}(0)}{2!}x^2+\\frac{f^{(3)}(0)}{3!}x^3 + \\dots\n\\end{aligned}\n\\]\n\n\n\nCode\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndegrees = (1, 3, 5, 7)\nls = ('-', '--', '-.', ':')\n\ndef  taylor_e(x, a, n) :\n    \"\"\"\n    x* = a 에서 전개\n    f(x) = f(a) + f'(a)*(x-a) + (1/2!)f''(a)(x-a)^2 + ... + (1/k!)f^(k)(a)(x-a)^k + R_k\n    \"\"\"\n    signs  = (1, -1, -1, 1)\n    derivs = (np.cos, np.sin, np.cos, np.sin)\n\n    fx =  np.sin(a) \n    \n    for i in range(1, n+1) : \n        fx += (signs[(i%4)-1]*derivs[(i%4)-1](a)) / math.factorial(i)*(x-a)**i\n    \n    return fx\n    \nx = np.linspace(-10, 10, 100)\ny = np.sin(x)\n\nfig = plt.figure(figsize=(12,5))\nax = fig.add_subplot(1, 1, 1)\n\nax.xaxis.set_tick_params(labelsize=12)\nax.yaxis.set_tick_params(labelsize=12)\nax.set_xlabel(r'$x$', fontsize=15)\nax.set_ylabel(r'$x$', fontsize=15)\nax.grid(False)\n\ntaylors = (taylor_e(x, 0, i) for i in degrees)\nax.plot(x, y , lw=3, color='gray', \n        label=r\"sin(x)\")\n\nfor i, taylor in enumerate(taylors) :\n    ax.plot(x, taylor, lw=2, ls=ls[i], color='k', \n            label=\"degree {}\".format(degrees[i]))\n\n\nax.legend(fontsize=11)\nax.set_ylim([-5, 5])\n\n# plt.suptitle(\"Taylor series, order=1,2,3\", fontsize=15)\n\n\nplt.show()\n\n\n\n\n\n\nTheorem 2 If \\(f(x)\\) is differentiable (and therefore continuous) \\(n\\) times on \\([a,b]\\) , then there exists \\(c\\) such that \\[\n\\begin{aligned}\n    f(b)&=f(a)+f'(a)(b-a)+\\frac{f^{(2)}(a)^2}{2!}(b-a)^2+\\dots+\\\\\n    &\\frac{f^{(n-1)}(a)}{(n-1)!}(b-a)^{n-1} + \\frac{f^{(n)}(c)}{n!}(b-a)^{n}, a<c<b\n\\end{aligned}\n\\] .\n\nProof) find \\(k\\) such that \\(f(b)-(f(a)+f'(a)(b-a)+\\frac{f^{(2)}(a)^2}{2!}(b-a)^2+\\dots+\\frac{f^{(n-1)}(a)}{(n-1)!}(b-a)^{n-1} + k(b-a)^{n})=0\\).\n\\[\n\\begin{aligned}\n    \\text{Let }F(x)&=f(b)-(f(x)+f'(x)(b-x)+\\frac{f^{(2)}(a)^2}{2!}(b-x)^2+\\dots+\\\\\n    &\\frac{f^{(n-1)}(x)}{(n-1)!}(b-x)^{n-1} + k(b-x)^{n})\n\\end{aligned}\n\\]\nThen, \\(F(x)\\) is differentiable on \\([a,b]\\). In addition, since \\(F(a)=F(b)=0\\), there exists \\(c\\) such that \\(F'(c)=0\\), \\(a<c<b\\) by the Mean Value Theorem. Thus,\n\\[\n\\begin{aligned}\n  F'(x)&=-\\frac{f^{(n)}(x)}{(n-1)!}(b-x)^{n-1}+kn(b-x)^{n-1}\\\\\n  F'(c)&=-\\frac{f^{(n)}(c)}{(n-1)!}(b-c)^{n-1}+kn(b-c)^{n-1}=0\\\\\n  kn&=\\frac{f^{(n)}(c)}{(n-1)!}\\\\\n  \\therefore k&=\\frac{f^{(n)}(c)}{n!}\n\\end{aligned}\n\\]\n\nTheorem 3 If \\(f\\) is differentiable (and therefore continuous) \\(n\\) times on \\([a,b]\\) , \\(x^*,x \\in [a,b]\\), and \\(x\\ne x^*\\), there exists \\(\\theta\\) such that \\(0<\\theta<1\\) and\n\\[\n\\begin{aligned}\n    f(x)&=f(x^*)+f'(x^*)(x-x^*)+\\frac{f^{(2)}(x^*)^2}{2!}(x-x^*)^2+\\dots+\\\\\n    &\\frac{f^{(n-1)}(x^*)}{(n-1)!}(x-x^*)^{n-1} + \\frac{f^{(n)}(x^*+\\theta(x-x^*))}{n!}(x-x^*)^{n}\n\\end{aligned}\n\\] .\n\nIn the above expression (Theorem 3), \\(f(x)\\) can be expressed with finite terms because it is differentiable \\(n\\) times. The symbol \\(\\theta\\) represents a value between 0 and 1, and it is used in the context of Taylor’s theorem with remainder (see Definition 5). The general form of Taylor’s theorem with remainder is:\n\\[\nf(x) = f(x^*) + \\frac{f'(x^*)}{1!}(x-x^*) + \\frac{f''(x^*)}{2!}(x-x^*)^2 + ... + \\frac{f^{(n)}(x^*)}{n!}(x-x^*)^n + R_n(x)\n\\]\nwhere \\(R_n(x)\\) is the remainder term that involves the \\(n+1\\) th derivative of f evaluated at some point \\(c\\) between \\(x\\) and \\(x^*\\):\n\\[\n\\begin{aligned}\n  R_n(x)&=f(x)- (f(x^*) + \\frac{f'(x^*)}{1!}(x-x^*) + \\frac{f''(x^*)}{2!}(x-a)^2 + ... + \\frac{f^{(n)}(x^*)}{n!}(x-x^*)^n)\\\\\n  R_n(x)&= \\frac{f^{(n+1)}(c)}{(n+1)!}(x-x^*)^{n+1}\n\\end{aligned}\n\\]\nIn the given expression, \\(x^*+\\theta(x-x^*)\\) is the value of \\(c\\) that lies between \\(x\\) and \\(x^*\\), where \\(\\theta\\) is a scalar value between \\(0\\) and \\(1\\). In other words, \\(c\\) is an internally dividing point, \\(i\\) that divides the segment \\(\\overline{xx^*}\\) in the ratio \\(\\overline{x^*i}:\\overline{ix}=\\theta:(1-\\theta)\\) because \\(x^*+\\theta(x-x^*)=x^*(1-\\theta)+\\theta x\\) .Therefore, \\(x^*+\\theta(x-x^*)\\) of the remainder term in the expression is somewhere between \\(x and x^*\\):\n\\[\n\\frac{f^{(n)}(x^*+\\theta(x-x^*))}{(n)!}(x-x^*)^{n}\n\\]\n\n\n\\(f(x)=x^3-3x^2+4\\)\n\n\nCode\ndef f(x):\n    return x**3-3*x**2+4\ndef df(x):\n    return 3*x**2-6*x\ndef d2f(x):\n    return 6*x-6\ndef d3f(n):\n    return np.repeat(6,n)\n\n\n\n# Define the Taylor series expansion up to the 3rd order\ndef taylor(x):\n    return f(0) + df(0)*x + d2f(0)*(x**2)/2 + d3f(len(x))*(x**3)/6 \n\n# Create a range of x values\nx = np.linspace(-np.pi, np.pi, 100)\n\n# Calculate the function and its approximation using the Taylor series expansion\ny = f(x)\ny2 = df(x)\ny3 = d2f(x)\ny_approx = taylor(x)\n\n# Plot the function and its approximation\n\nplt.plot(x, y, '--',lw=5, label=r'$f(x)=x^3-3x^2+4$')\nplt.plot(x, df(x), label=r'$f(x)=3x^2-6x$')\nplt.plot(x, d2f(x), label=r'$f(x)=6x-6$')\nplt.plot(x, d3f(len(x)), label=r'$f(x)=6$')\nplt.plot(x, y_approx, label='Taylor Approximation')\nplt.legend()\nplt.show()\n\n\n\n\n\nIf \\(f(x)=x^3-3x^2+4\\), then \\(\\frac{df(x)}{dx}=3x^2-6x\\), \\(\\frac{d^2f(x)}{dx^2}=6x-6\\), \\(\\frac{d^3f(x)}{dx^3}=6\\), \\(\\frac{d^nf(x)}{dx^n}=0, n\\ge 4\\).\nFor \\(x\\ne x^*\\),\n\\[\n\\begin{aligned}\n  f(x)&=f(x^*)+f'(x^*)(x-x^*)+\\frac{f^{(2)}(x^*)^2}{2!}(x-x^*)^2+\\frac{f^{(3)}(x^*+\\theta(x-x^*))^3}{3!}(x-x^*)^3\\\\\n  f(x)&=f(x^*)+(3x^2-6x)(x-x^*)+\\frac{(6x-6)}{2!}(x-x^*)^2+\\frac{6}{3!}(x-x^*)^3\\\\\n  f(x)&=f(0)+(3x^2-6x)(x)+\\frac{(6x-6)}{2!}(x)^2+\\frac{6}{3!}(x)^3 \\text{ } (x^*=0)\\\\\n  f(x)&=x^3-3x^2+4\n\\end{aligned}     \n\\]\nIf f is differentated 3 times, f is expressed with \\(\\theta\\). But, if differentiated more than 4 times, the \\(\\theta\\) disappears.\n\n\n\nThe second derivative test is a method used to determine whether a critical point of a function is a local maximum, local minimum, or a saddle point. The test uses the sign of the second derivative of the function at the critical point to determine its nature. In other words, the second derivative test uses the sign of the second derivative to determine the concavity of the function at a critical point, which in turn determines whether the critical point is a local maximum, local minimum, or a saddle point.\n\nDefinition 7 Let \\(f\\) be a function with a critical point at \\(x^*\\). If \\(f\\) is twice differentiable at \\(x^*\\), then:\n\nIf \\(f''(x^*) > 0\\), then \\(f\\) has a local minimum at \\(x^*\\).\nIf \\(f''(x^*) < 0\\), then \\(f\\) has a local maximum at \\(x^*\\).\nIf \\(f''(x^*) = 0\\) and there exist values of \\(x\\) close to \\(x^*\\) such that \\(f''(x) < 0\\) and \\(f''(x) > 0\\), then \\(f\\) has a saddle point at \\(x^*\\).\nIf \\(f''(x^*) = 0\\) and there do not exist values of \\(x\\) close to \\(x^*\\) such that \\(f''(x) < 0\\) and \\(f''(x) > 0\\), then the test is inconclusive and we may need to use other methods to determine the nature of the critical point.\n\n\nThe second derivative test is a method used to determine the nature of a critical point of a function by examining the concavity of the function at that point.\nIf the second derivative of the function is positive at a critical point, then the function is concave up at that point, and the critical point is a local minimum. If the second derivative is negative, then the function is concave down, and the critical point is a local maximum. If the second derivative is zero, the test is inconclusive, and the other methods should be tried to determine the nature of the critical point. If there exist values of \\(x\\) close to \\(x^*\\) such that \\(f''(x) < 0\\) and \\(f''(x) > 0\\), then \\(f\\) has a saddle point at \\(x^*\\) (see Figure 1).\n\n\n\n\n\n\nWhat Is a Saddle Point?\n\n\n\nA saddle point is a type of critical point of a function where the first-order partial derivatives of the function are zero, but the behavior of the function around the point is neither a local maximum nor a local minimum. Instead, the behavior is like a saddle shape, hence the name “saddle point” (see Figure 1).\nAt a saddle point, the function changes concavity in different directions, meaning that the function is concave up in some directions and concave down in other directions. In other words, the Hessian matrix of the function (the matrix of second-order partial derivatives) evaluated at the saddle point has both positive and negative eigenvalues, indicating that the curvature of the function changes in different directions.\nSaddle points are important in optimization and machine learning because they can cause difficulties in finding the global minimum of a function. At a saddle point, gradient-based optimization algorithms can get stuck because the gradient is zero but the curvature of the function prevents the algorithm from moving in a direction that decreases the function value. This can result in slow convergence or even convergence to a suboptimal solution.\n\n\n\n\n\nFigure 1: Saddle Point Example\n\n\nSourced from Wiki By Nicoguaro - Own work, CC BY 3.0\nThus, for the problem of finding the maximum and minimum, it is usually sufficient for \\(f\\) to be differentiable twice in a taylor series.\n\\[\n\\begin{aligned}\n    f(x)&=f(x^*)+f'(x^*)(x-x^*)+\\frac{f^{(2)}(x^*+\\theta(x-x^*))}{2!}(x-x^*)^{2} ,\\text{ } 0<\\theta<1\n\\end{aligned}\n\\tag{1}\\]\nIt is the case that \\(f'(x^*)=0\\) to find \\(x^*\\) that makes the extrema (minimum or maximum) of \\(f\\). So, we can set \\(f'(x^*)\\) in Equation 1 as \\(0\\). Then,\n\\[\n\\begin{aligned}\n    f(x)&=f(x^*)+\\frac{f^{(2)}(x^*+\\theta(x-x^*))}{2!}(x-x^*)^{2} ,\\text{ } 0<\\theta<1\n\\end{aligned}\n\\tag{2}\\]\nIn Equation 2, we can make a certain conclusion on the second derivative test depending on the sign of the variable (not a constant because of \\(\\theta\\)), \\(f^{(2)}(x^*+\\theta(x-x^*))\\) because \\((x-x^*)^{2}>0\\):\n\nIf \\(f'(x^*)=0\\), \\(x \\in [a,b]\\), and \\(f''(x)>0\\), then \\(f(x)=f(x^*)+d, \\text{ } (d>0)\\)\n\\(\\therefore f(x)>f(x^*)\\), which means that \\(f(x)\\) has a minimum at \\(x^*\\).\nIf \\(f'(x^*)=0\\), \\(x \\in [a,b]\\), and \\(f''(x)<0\\), then \\(f(x)=f(x^*)-d, \\text{ } (d>0)\\)\n\\(\\therefore f(x)<f(x^*)\\), which means that \\(f(x)\\) has a maximum at \\(x^*\\).\n\n\n\nIf \\(f(x)=e^{x^2}, f'(x)=2xe^{x^2}, \\text{ and } f^{''}(x)=(2+4x^2)e^{x^2}\\), the case \\(f'(x^*)=0\\) is when \\(x^*=0\\). Since \\(f^{''}(x)>0\\) for all \\(x \\in \\mathbb{R}\\), \\(f(0)=1\\) is a minimum at \\(x^*\\).\n\\(f(x)=e^{x^2}\\)\n\n\nCode\ndef f(x):\n    return np.exp(x**2)\ndef df(x):\n    return 2*x*np.exp(x**2)\ndef d2f(x):\n    return (2+4*x**2)*np.exp(x**2)\n\n# Create a range of x values\nx = np.linspace(-1, 1, 100)\n\n# Calculate the function and its approximation using the Taylor series expansion\ny = f(x)\ny2 = df(x)\ny3 = d2f(x)\ny_approx = taylor(x)\n\n# Plot the function and its approximation\n\nplt.plot(x, y, label=r\"$f(x)=e^{x^2}$\")\nplt.plot(x, df(x), label=r\"$f'(x)=2xe^{x^2}$\")\nplt.plot(x, d2f(x), label=r\"$f^{''}(x)=(2+4x^2)e^{x^2}$\")\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\n\nplt.plot(x, y_approx, label='Taylor Approximation')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html",
    "href": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html",
    "title": "Categorical Data Analysis",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nVisualization Methods\n\nfor EDA: visualize patterns, trends, anomalies in data\nfor model diagnostic methods: visualize to assess violations of assumptions\nfor summary methods: visualize to provide an interpretable summary of data\n\napply theory to practice\n\nconert research questions into statistical hypotheses and models\nlook into the difference between non-parametric (ex. fisher exact test) vs parametric (ex. \\(\\chi^2 test for independence\\)) vs model-based methods (ex. logistic regression)\nfor summary methods: visualize to provide an interpretable summary of data\n\n\n\n\n\n\ncategorical (or frequency) data consist of a discrete set of categories, which may be ordered or unordered.\n\nunordered\n\ngener: {male, female, transgender}\nmarital status: {never married, married, separated, divorced, widowed}\nparty preference: {NDP, liberal, conservative, green}\ntreatment improvement: {none, some, marked}\n\nordered\n\nage group: {0s,10s,20s,30s, …}\nnumber of children: {0, 1 , 2 ,3, …} ## Structures\n\n\n\nCategorical data appears in various forms like:\n\ntables\n\none way\ntwo way\nthree way\n\nmatrices\narray\ndata frames\n\ncase form\nfrequency form"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#categorical-data-analysis",
    "href": "docs/blog/posts/statistics/guide_map/index.html#categorical-data-analysis",
    "title": "Content List, Statistics",
    "section": "Categorical Data Analysis",
    "text": "Categorical Data Analysis\n\n1111-11-11, Introduction\n1111-11-11,\n1111-11-11,\n2022-12-28,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n2023-01-07,\n2023-01-27,\n2023-01-27,\n2023-01-28,"
  },
  {
    "objectID": "docs/blog/posts/content_list.html",
    "href": "docs/blog/posts/content_list.html",
    "title": "Blog Content List",
    "section": "",
    "text": "Contents\n\nDeep Learning\nMachine Learning\nMathematics\nStatistics\nEngineering\nPatent\nLanguage\nSurveilance\n\n\n\nReference\n\nStatistics\n\nGeorge Casella & Rogeer L. Berger - Statistcal Inference, 2nd Edition\n슬기로운 통계생활 - https://www.youtube.com/@statisticsplaybook\n슬기로운 통계생활 - https://github.com/statisticsplaybook\nFast Campus, Coursera, Inflearn\n그 외 다수의 Youtube, and Documents from Googling\n\nMathematics\n\nJames Stewart - Calculus Early Transcedentals, 7th Eidition\nany James Stewart series\n임장환 - 머신러닝, 인공지능, 컴퓨터 비전 전공자를 위한 최적화 이론\nFast Campus, Coursera, Inflearn\n다수의 Youtube, and Documents from Googling\n\nMachine Learning\n\nGareth M. James, Daniela Witten, Trevor Hastie, Robert Tibshirani - An Introduction to Statistical Learning: With Applications in R 2nd Edition\nTrevor Hastie, Robert Tibshirani, Jerome H. Friedman - The Elements of Statistical Learning 2nd Edition\nFast Campus, Coursera, Inflearn\n다수의 Youtube, and Documents from Googling\n\n\nDeep Learning\n\nSaito Koki - Deep Learning from Scratch 1,2,3 (밑바닥부터 시작하는 딥러닝 1,2,3)\n조준우 - 머신러닝·딥러닝에 필요한 기초 수학 with 파이썬\n조준우 - https://github.com/metamath1/noviceml\n동빈나 - https://www.youtube.com/c/dongbinna\n혁펜하임 - https://www.youtube.com/channel/UCcbPAIfCa4q0x7x8yFXmBag\nFast Campus, Coursera, Inflearn\n다수의 Youtube, and Documents from Googling\n\nEngineering\n\nFast Campus, Coursera, Inflearn"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/differentiation/2023-03-18_higher_order_derivative.html",
    "href": "docs/blog/posts/Mathmatics/differentiation/2023-03-18_higher_order_derivative.html",
    "title": "Differentiation - Higher Order Derivative",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 미분 가능한 함수 \\(f(x)\\) 의 \\(f'(x)\\) 가 존재할 때, \\(f'(x)\\) 의 도함수 \\((f'(x))'\\) 를 \\[\nf^{''}(x), \\frac{d^2f(x)}{dx^2}, \\text{ or } \\frac{d^2y}{dx^2}\n\\] 라 표기하고 \\(f(x)\\) 의 2차 도함수 or second derivative라고 한다.\n같은 방법으로, \\(n\\) 차 도함수 \\(f^{(n)}(x)\\) or \\(\\frac{d^nf(x)}{dx^n}\\) 가 정의 된다.\n\n\n\n\n다음의 \\(n\\) 차 도함수\n\n\\(f(x)=x^{\\alpha} \\text{ } (x>0, \\alpha \\ne -1)\\)\n\n\\[\n\\begin{aligned}\n    f'(x)&=\\alpha x^{\\alpha-1}\\\\\n    f^{''}(x)&=\\alpha(\\alpha-1) x^{\\alpha-2}\\\\\n    &\\vdots\\\\\n    f^{n}(x)&=\\alpha(\\alpha-1)\\cdots(\\alpha-(n-1)) x^{\\alpha-n}\n\\end{aligned}\n\\]\n\n\\(f(x)=ln(1+x)\\)\n\n\\[\n\\begin{aligned}\n    f'(x)&=\\frac{1}{1+x}\\\\\n    f^{''}(x)&=-\\frac{1}{(1+x)^2}\\\\\n    f^{3}(x)&=(-1)^2\\frac{1 \\cdot 2 }{(1+x)^3}\\\\\n    &\\vdots\\\\\n    f^{n}(x)&=(-1)^{n-1}\\frac{(n-1)!}{(1+x)^n}\\\\\n\\end{aligned}\n\\]\n\n\\(f(x)=\\sin(x)\\)\n\n\\[\n\\begin{aligned}\n    f'(x)&=\\cos(x)=\\sin(x+1\\cdot\\frac{\\pi}{2})\\\\\n    f^{2}(x)&=(-1)\\sin(x)=\\sin(x+2\\cdot\\frac{\\pi}{2})\\\\\n    f^{3}(x)&=-\\cos(x)=\\sin(x+3\\cdot\\frac{\\pi}{2})\\\\\n    f^{4}(x)&=(-1)^2\\sin(x)=\\sin(x+4\\cdot\\frac{\\pi}{2})\\\\\n    &\\vdots\\\\\n    f^{n}(x)&=\\sin(x+n\\cdot\\frac{\\pi}{2})\n\\end{aligned}\n\\]\n\n\n\n\nTheorem 1 If A function \\(f(x)\\) is differentiable on \\(\\[a,b\\]\\), then there exists \\(c \\in (a,b)\\) such that \\[\n\\begin{aligned}\n    \\frac{f(b)-f(a)}{b-a} = f'(c), (a<c<b)\n\\end{aligned}\n\\]\n\n\n\n\nBy 4C - 자작, based on PNG version, CC BY-SA 3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html",
    "title": "FDA Software Validation Guidance Summary",
    "section": "",
    "text": "I am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto.\nThe FDA validation guidance document is a bit difficult to understand because its explanations provides abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\n\n\n2022-12-28, download this article as PDF\n2022-12-28, summary with diagrams\n\n\n\n\nFDA: General Principles of Software Validation\n\n\n\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\n\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#purpose",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#purpose",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.1 Purpose",
    "text": "2.1 Purpose\nThe purpose is to make a sketch of general validation principle of the validation of medical device software or software used to design or develop."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#scope",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#scope",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.2 Scope",
    "text": "2.2 Scope\nThe scope of this guidance is broad. The important activities for the software validation include at least:\n\nplanning,\nverfication,\ntesting,\ntraceability, and\nconfiguration management.\n\nAll of the activities above should be\n\nintegrated\nbe able to describe software life cycle management and\nbe able to describe software risk management.\n\nThe software validation and verification activities should be focused into the entire software life cycle. (It does not necessarily mean that the activies must follow any technical models.)\nThe guidance is applicable to any software related to a regulated medical device and anyone who is employed in a bio or medical industry.\n\n2.2.1 The Least Burdensome Approach\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\n2.2.2 Regulatory Requirements for Software Validation\n\nSoftware validation: a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997. (See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\nSpecific requirements for validation of device software are found in 21 CFR §820.30(g). Other design controls, such as planning, input, verification, and reviews, are required for medical device software. (See 21 CFR §820.30.)\ncomputer systems used to create, modify, and maintain electronic records and to manage electronic signatures are also subject to the validation requirements. (See 21 CFR §11.10(a).)\n\n\n2.2.2.1 Objective\nThe objective of software validation is to ensure:\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\n2.2.2.2 What to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\n\n2.2.3 Quality System Regulation vs Pre-market Submissions\nThis document does not address any specific requirements but general ones. Specific issues should be addressed to\n\nthe Office of Device Evaluation (ODE),\nCenter for Devices and Radiological Health (CDRH)\nthe Office of Blood Research and Review,\nCenter for Biologics Evaluation and Research (CBER). See the references in Appendix A for applicable FDA guidance documents for pre-market submissions."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#context-for-software-validation",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#context-for-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.3 Context for Software Validation",
    "text": "2.3 Context for Software Validation\n\nValidation elements that FDA expects to do for the Quality System regulation, using the principles and tasks are listed in Sections 4 and 5.\nAdditional specific information is available from many of the references listed in Appendix A\n\n\n2.3.1 Definition and Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology between the medical device Quality System regulation and the software industry:\n\nrequirements,\nspecification,\nverification, and\nvalidation.\n\n\n\n2.3.1.1 Requirements and Specifications\nThe Quality System regulation states\n\nthat design input requirements must be documented and\nthat specified requirements must be verified\n\nBut, the regulation does not further clarify the distinction between the terms “requirement” and “specification.”\n\nRequirement\n\ncan be any need or expectation for a system or for its software.\nreflects the stated or implied needs of the customer: requirements may be\n\nmarket-based,\ncontractual,\nstatutory, or\nan organization’s internal requirements.\n\nvarious examples of requirements\n\ndesign, functional, implementation, interface, performance, or physical requirements\n\nSoftware requirements derived from the system requirements for those aspects of system functionality\nSoftware requirements are typically stated in functional terms and are defined, refined, and updated as a development project progresses.\nSuccess in accurately and completely documenting software requirements is a crucial factor in successful validation of the resulting software.\n\nSpecification\n\ndefined as “a document that states requirements.” (See 21 CFR §820.3(y).)\nIt may refer to or include drawings, patterns, or other relevant documents\nIt usually indicates the means and the criteria whereby conformity with the requirement can be checked.\nVarious examples of written specifications\n\nsystem requirements specification,\nsoftware requirements specification,\nsoftware design specification,\nsoftware test specification,\nsoftware integration specification, etc.\n\nAll of these documents are design outputs for which various forms of verification are necessary.\n\n\n\n\n2.3.1.2 Verifiaction and Validation\nThe Quality System regulation is harmonized with ISO 8402:1994, which treats “verification” and “validation” as separate and distinct terms.\n\nSoftware verification\n\nIt provides objective evidence that the design outputs of a particular phase of the software development life cycle meet all of the specified requirements for that phase.\nIt looks for\n\nconsistency,\ncompleteness, and\ncorrectness of the software and its supporting documentation\n\nSoftware testing\n\nverification activities intended to confirm that software development output meets its input requirements.\n\nTypes of verification activities include\n\nvarious static and dynamic analyses,\ncode and document inspections,\nwalkthroughs, and other techniques.\n\n\nSoftware Validation\n\nConfirmation by examination and provision of the following objective evidence:\nEvidence 1: software specifications conform to user needs and intended uses, and\nEvidnece 2: the particular requirements implemented through software can be consistently fulfilled.\nEvidnece 3: all software requirements have been implemented correctly and completely and are traceable to system requirements.\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device.\n\nDifficulty in Software verification and validation\n\na developer cannot test forever, and\nit is difficult to know how much evidence is enough.\nIn large measure, software validation is a matter of developing a “level of confidence” that the device meets all requirements and user expectations for the software automated functions and features of the device.\nConsiderations for an acceptable level of confidence\n\nmeasures such as defects found in specifications documents,\nestimates of defects remaining,\ntesting coverage, and other techniques are all used to develop before shipping the product.\nHowever, a level of confidence varies depending upon the safety risk (hazard) posed by the automated functions of the device. (Info on safety risk is found in Section 4 and in the international standards ISO/IEC 14971-1 and IEC 60601-1-4 referenced in Appendix A).\n\n\n\n\n\n2.3.1.3 IQ/OQ/PQ\nIQ/OQ/PQ are the terminology related to user site software validation\n\nInstallation qualification (IQ)\nOperational qualification (OQ)\nPerformance qualification (PQ).\n\nDefinitions of these terms may be found in FDA’s Guideline on General Principles of Process Validation, dated May 11, 1987, and in FDA’s Glossary of Computerized System and Software Development Terminology, dated August 1995. Both FDA personnel and device manufacturers need to be aware of these differences in terminology as they ask for and provide information regarding software validation.\n\n\n\n2.3.2 Software Development as Part of System Design\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nthe user’s needs\nintended uses from which the product is developed.\n\nA primary goal of software validation is to then demonstrate that all completed software products comply with all documented software and system requirements.\n\n\n2.3.3 Software Is Different from Hardware\nSoftware engineering needs an even greater level of managerial scrutiny and control than does hardware engineering.\n\n\n2.3.4 Benefits of Software Validation\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nSoftware validation can also reduce long term costs by making it easier and less costly to reliably modify software and revalidate software changes.\n\n\n\n2.3.5 Design Review\nDesign reviews are documented, comprehensive, and systematic examinations of a design to evaluate\n\nthe adequacy of the design requirements,\nthe capability of the design to meet these requirements, and\nto identify problems.\n\nDesign review is a primary tool for managing and evaluating development projects.\n\nIt is strongly recommended that it should be formal design because it is more structured than the informal one.\nIt includes participation from others outside the development team.\nIt may review reference or include results from other formal and informal reviews.\nDesign reviews should include\n\nexamination of development plans,\nrequirements specifications,\ndesign specifications,\ntesting plans and procedures,\nall other documents and activities associated with the project,\nverification results from each stage of the defined life cycle, and\nvalidation results for the overall device.\n\nThe Quality System regulation requires that at least one formal design review be conducted during the device design process. However, it is recommended that multiple design reviews be conducted\n\n(e.g., at the end of each software life cycle activity, in preparation for proceeding to the next activity).\n\nFormal design reviews documented should include:\n\nthe appropriate tasks and expected results, outputs, or products been established for each software life cycle activity\ncorrectness, completeness, consistency, and accuracy\nsatisfaction for the standards, practices, and conventions of that activity\nestablishment of a proper basis for initiating tasks for the next software life cycle activity"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#principles-of-software-validation",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#principles-of-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.4 Principles of Software Validation",
    "text": "2.4 Principles of Software Validation\n\n2.4.1 Requirements\nA documented software requirements specification provides a baseline for both validation and verification. The software validation process must include an established software requirements specification (Ref: 21 CFR 820.3(z) and (aa) and 820.30(f) and (g)).\n\n\n2.4.2 Defect Prevention\nIn order to establish that confidence, software developers should use a mixture of methods and techniques to prevent software errors and to detect software errors that do occur.\n\n\n2.4.3 Time and Effort\nPreparation for software validation should begin early, i.e., during design and development planning and design input. The final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\n2.4.4 Software Life Cycle\n\nSoftware validation takes place within the environment of an established software life cycle.\nThe software life cycle contains software engineering tasks and documentation necessary to support the software validation effort.\nspecific verification and validation tasks need to be appropriate for the intended use of the software\n\n\n\n2.4.5 Plans\n\nThe software validation process is defined and controlled through the use of a plan.\nThe software validation plan defines “what” is to be accomplished through the software validation effort.\nSoftware validation plans specify areas such as\n\nscope,\napproach,\nresources,\nschedules and the types and extent of activities,\ntasks, and\nwork items.\n\n\n\n\n2.4.6 Procedures\nThe software validation process is executed through the use of procedures. These procedures establish “how” to conduct the software validation effort. The procedures should identify the specific actions or sequence of actions that must be taken to complete individual validation activities, tasks, and work items.\n\n\n2.4.7 Software Validation After a Change\n\nDue to the complexity of software, a small local change may have a significant global system impact.\nIf a change exists in the software, the whole validation status of the software needs to be re-established.\nneed to determine the extent and impact of that change on the entire software system.\nthe software developer should then conduct an appropriate level of software regression testing to show that unchanged but vulnerable portions of the system have not been adversely affected.\n\n\n\n2.4.8 Validation Coverage\n\nValidation coverage should be based on the software’s complexity and safety risk.\nThe selection of validation activities, tasks, and work items should be commensurate with the complexity of the software design and the risk associated with the use of the software for the specified intended use.\n\n\n\n2.4.9 Independence of Review\n\nValidation activities should be based on the basic quality assurance precept of “independence of review.”\nSelf-validation is extremely difficult.\nWhen possible, an independent evaluation is always better (like a contracted third-party independent verification and validation)\nAnother approach is to assign internal staff members that are not involved in a particular design or its implementation, but who have sufficient knowledge to evaluate the project and conduct the verification and validation activities.\n\n\n\n2.4.10 Flexibility and Responsibility\nThe device manufacturer has flexibility in choosing how to apply these validation principles, but retains ultimate responsibility for demonstrating that the software has been validated. FDA regulated medical device applications include software that:\n\nIs a component, part, or accessory of a medical device;\n\ncomponents: e.g., application software, operating systems, compilers, debuggers, configuration management tools, and many more\n\nIs itself a medical device; or\nIs used in manufacturing, design and development, or other parts of the quality system.\nNo matter how complex and disperse the software is, the manufacturer is in charge of responsibility for software validation."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#activities-and-tasks",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#activities-and-tasks",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.5 Activities and Tasks",
    "text": "2.5 Activities and Tasks\nSoftware validation is accomplished through a series of activities and tasks that are planned and executed at various stages of the software development life cycle. These tasks may be\n\none time occurrences\niterated many times\n\n\n2.5.1 Software Life Cycle Activities\n\nSoftware developers should establish a software life cycle model that is appropriate for their product and organization.\nThe selected software life cycle model should cover the software from its birth to its retirement.\nActivities in a typical software life cycle model:\n\nQuality Planning\nSystem Requirements Definition\nDetailed Software Requirements Specification\nSoftware Design Specification\nConstruction or Coding\nTesting\nInstallation\nOperation and Support\nMaintenance\nRetirement\n\nVerification, testing, and other tasks that support software validation occur during each of these activities.\nSeveral software life cycle models defined in FDA’s Glossary of Computerized System and Software Development\n\nTerminology dated August 1995:\n\nwaterfall\nspiral\nrapid prototyping\nincremental development, etc.\n\n\n\n2.5.2 Typical Tasks Supporting Validation\nthe software developer should at least consider each of the risk-related tasks and should define and document which tasks are or are not appropriate for their specific application.\n\n2.5.2.1 Quality Planning\nDesign and development planning should culminate in a plan that identifies\n\nnecessary tasks,\nprocedures for anomaly reporting and resolution,\nnecessary resources, and\nmanagement review requirements including formal design reviews.\n\nThe plan should include:\n\nThe specific tasks for each life cycle activity;\nEnumeration of important quality factors (e.g., reliability, maintainability, and usability);\nMethods and procedures for each task;\nTask acceptance criteria;\nCriteria for defining and documenting outputs in terms that will allow evaluation of their conformance to input requirements;\nInputs for each task;\nOutputs from each task;\nRoles, resources, and responsibilities for each task;\nRisks and assumptions; and\nDocumentation of user needs.\n\nThe plan should identify\n\nthe personnel,\nthe facility and equipment resources for each task, and\nthe role that risk (hazard) management will play.\n\nA configuration management plan should be developed that will guide and control multiple parallel development activities and ensure proper communications and documentation.\nControls are necessary to ensure positive and correct correspondence among all approved versions of the specifications documents, source code, object code, and test suites that comprise a software system. The controls also should ensure accurate identification of, and access to, the currently approved versions.\nProcedures should be created for reporting and resolving software anomalies found through validation or other activities.\nManagement should identify the reports and specify the contents, format, and responsible organizational elements for each report. Procedures also are necessary for the review and approval of software development results, including the responsible organizational elements for such reviews and approvals.\nTypical Tasks – Quality Planning\n\nRisk (Hazard) Management Plan\nConfiguration Management Plan\nSoftware Quality Assurance Plan\n\nSoftware Verification and Validation Plan\n\nVerification and Validation Tasks, and Acceptance Criteria\nSchedule and Resource Allocation (for software verification and validation activities)\nReporting Requirements\n\nFormal Design Review Requirements\nOther Technical Review Requirements\n\nProblem Reporting and Resolution Procedures\nOther Support Activities\n\n\n\n2.5.2.2 Requirements\nRequirements development includes the\n\nidentification,\nanalysis, and\ndocumentation of information about the device and its intended use.\n\nAreas of special importance include allocation of system functions to\n\nhardware/software,\noperating conditions,\nuser characteristics,\npotential hazards, and\nanticipated tasks.\n\nIn addition, the requirements should state clearly the intended use of the software. It is not possible to validate software without predetermined and documented software requirements. Typical software requirements specify the following:\n\nAll software system inputs;\nAll software system outputs;\nAll functions that the software system will perform;\nAll performance requirements that the software will meet, (e.g., data throughput, reliability, and timing);\nThe definition of all external and user interfaces, as well as any internal software-to-system interfaces;\nHow users will interact with the system;\nWhat constitutes an error and how errors should be handled;\nRequired response times;\nThe intended operating environment for the software, if this is a design constraint (e.g., hardware platform, operating system);\nAll ranges, limits, defaults, and specific values that the software will accept; and\nAll safety related requirements, specifications, features, or functions that will be implemented in software.\n\nSoftware requirement specifications should identify clearly the potential hazards that can result from a software failure in the system as well as any safety requirements to be implemented in software.\nThe consequences of software failure should be evaluated, along with means of mitigating such failures (e.g., hardware mitigation, defensive programming, etc.).\nThe Quality System regulation requires a mechanism for addressing incomplete, ambiguous, or conflicting requirements. (See 21 CFR 820.30(c).) Each requirement (e.g., hardware, software, user, operator interface, and safety) identified in the software requirements specification should be evaluated for accuracy, completeness, consistency, testability, correctness, and clarity.\nFor example, software requirements should be evaluated to verify that:\n\nThere are no internal inconsistencies among requirements;\nAll of the performance requirements for the system have been spelled out;\nFault tolerance, safety, and security requirements are complete and correct;\nAllocation of software functions is accurate and complete;\nSoftware requirements are appropriate for the system hazards; and\nAll requirements are expressed in terms that are measurable or objectively verifiable.\n\nA software requirements traceability analysis should be conducted to trace software requirements to (and from) system requirements and to risk analysis results. In addition to any other analyses and documentation used to verify software requirements, a formal design review is recommended to confirm that requirements are fully specified and appropriate before extensive software design efforts begin. Requirements can be approved and released incrementally, but care should be taken that interactions and interfaces among software (and hardware) requirements are properly reviewed, analyzed, and controlled.\nTypical Tasks – Requirements\n\nPreliminary Risk Analysis\nTraceability Analysis\n\nSoftware Requirements to System Requirements (and vice versa)\nSoftware Requirements to Risk Analysis\n\nDescription of User Characteristics\nListing of Characteristics and Limitations of Primary and Secondary Memory\nSoftware Requirements Evaluation\nSoftware User Interface Requirements Analysis\nSystem Test Plan Generation\nAcceptance Test Plan Generation\nAmbiguity Review or Analysis\n\n\n\n2.5.2.3 Design\nIn the design process, the software requirements specification is translated into a logical and physical representation of the software to be implemented. The software design specification is a description of what the software should do and how it should do it. The design specification may contain both a high level summary of the design and detailed design information. Human factors engineering should be woven into\n\nthe entire design and development process,\nthe device design requirements,\nanalyses, and\ntests.\n\nDevice safety and usability issues should be considered when developing\n\nflowcharts,\nstate diagrams,\nprototyping tools, and\ntest plans.\n\nAlso, task and function analyses, risk analyses, prototype tests and reviews, and full usability tests should be performed. Participants from the user population should be included when applying these methodologies.\nThe software design specification should include:\n\nSoftware requirements specification, including predetermined criteria for acceptance of the software;\nSoftware risk analysis;\nDevelopment procedures and coding guidelines (or other programming procedures);\nSystems documentation (e.g., a narrative or a context diagram) that describes the systems context in which the program is intended to function, including the relationship of hardware, software, and the physical environment;\nHardware to be used;\nParameters to be measured or recorded;\nLogical structure (including control logic) and logical processing steps (e.g., algorithms);\nData structures and data flow diagrams;\nDefinitions of variables (control and data) and description of where they are used;\nError, alarm, and warning messages;\nSupporting software (e.g., operating systems, drivers, other application software);\nCommunication links (links among internal modules of the software, links with the supporting software, links with the hardware, and links with the user);\nSecurity measures (both physical and logical security); and\nAny additional constraints not identified in the above elements.\n\nThe first four of the elements noted above usually are separate pre-existing documents that are included by reference in the software design specification. Software requirements specification was discussed in the preceding section, as was software risk analysis.\nSoftware design evaluations criteria:\n\ncomplete,\ncorrect,\nconsistent,\nunambiguous,\nfeasible,\nmaintainable,\nanalyses of control flow,\ndata flow,\ncomplexity,\ntiming,\nsizing,\nmemory allocation,\ncriticality analysis, and many other aspects of the design\n\nAppropriate consideration of software architecture (e.g., modular structure) during design can reduce the magnitude of future validation efforts when software changes are needed.\nA traceability analysis should be conducted to verify that the software design implements all of the software requirements. As a technique for identifying where requirements are not sufficient, the traceability analysis should also verify that all aspects of the design are traceable to software requirements.\nAn analysis of communication links should be conducted to evaluate the proposed design with respect to hardware, user, and related software requirements. At the end of the software design activity, a Formal Design Review should be conducted to verify that the design is correct, consistent, complete, accurate, and testable, before moving to implement the design.\nSeveral versions of both the software requirement specification and the software design specification should be maintained. All approved versions should be archived and controlled in accordance with established configuration management procedures.\nTypical Tasks – Design\n\nUpdated Software Risk Analysis\nTraceability Analysis - Design Specification to Software Requirements (and vice versa)\nSoftware Design Evaluation\nDesign Communication Link Analysis\nModule Test Plan Generation\nIntegration Test Plan Generation\nTest Design Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.4 Construction or Coding\nSoftware may be constructed either by coding. Coding is the software activity where the detailed design specification is implemented as source code. It is the last stage in decomposition of the software requirements where module specifications are translated into a programming language.\nCoding usually involves the use of a high-level programming language, but may also entail the use of assembly language (or microcode) for time-critical operations.\nA source code traceability analysis is an important tool to verify that all code is linked to established specifications and established test procedures. A source code traceability analysis should be conducted and documented to verify that:\n\nEach element of the software design specification has been implemented in code;\nModules and functions implemented in code can be traced back to an element in the software design specification and to the risk analysis;\nTests for modules and functions can be traced back to an element in the software design specification and to the risk analysis; and\nTests for modules and functions can be traced to source code for the same modules and functions.\n\nTypical Tasks – Construction or Coding\n\nTraceability Analyses\n\nSource Code to Design Specification (and vice versa)\nTest Cases to Source Code and to Design Specification\n\nSource Code and Source Code Documentation Evaluation\nSource Code Interface Analysis\nTest Procedure and Test Case Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.5 Testing by the Software Developer\nSoftware testing entails running software products under known conditions with defined inputs and documented outcomes that can be compared to their predefined expectations. It is a time consuming, difficult, and imperfect activity.\nAs such, it requires early planning in order to be effective and efficient. Test plans and test cases should be created as early in the software development process as feasible.\nThey should identify\n\nthe schedules,\nenvironments,\nresources (personnel, tools, etc.),\nmethodologies,\ncases (inputs, procedures, outputs, expected results),\ndocumentation, and\nreporting criteria.\n\nDescriptions of categories of software and software testing effort appear in the literature\n\nNIST Special Publication 500-235, Structured Testing: A Testing Methodology Using the Cyclomatic Complexity Metric;\nNUREG/CR-6293, Verification and Validation Guidelines for High Integrity Systems; and\nIEEE Computer Society Press, Handbook of Software Reliability Engineering.\n\nTesting of all program functionality does not mean all of the program has been tested. Testing of all of a program’s code does not mean all necessary functionality is present in the program. Testing of all program functionality and all program code does not mean the program is 100% correct! Software testing that finds no errors should not be interpreted to mean that errors do not exist in the software product; it may mean the testing was superficial.\nAn essential element of a software test case is the expected result. It is the key detail that permits objective evaluation of the actual test result. This necessary testing information is obtained from the corresponding, predefined definition or specification.\nA software testing process should be based on principles that foster effective examinations of a software product. Applicable software testing tenets include:\n\nThe expected test outcome is predefined;\nA good test case has a high probability of exposing an error;\nA successful test is one that finds an error;\nThere is independence from coding;\nBoth application (user) and software (programming) expertise are employed;\nTesters use different tools from coders;\nExamining only the usual case is insufficient;\nTest documentation permits its reuse and an independent confirmation of the pass/fail status of a test outcome during subsequent review.\n\nCode-based testing is also known as structural testing or “white-box” testing. It identifies test cases based on knowledge obtained from the source code, detailed design specification, and other development documents. Structural testing can identify “dead” code that is never executed when the program is run. Structural testing is accomplished primarily with unit (module) level testing, but can be extended to other levels of software testing.\nThe level of structural testing can be evaluated using metrics that are designed to show what percentage of the software structure has been evaluated during structural testing. These metrics are typically referred to as “coverage” and are a measure of completeness with respect to test selection criteria. The amount of structural coverage should be commensurate with the level of risk posed by the software. Use of the term “coverage” usually means 100% coverage. Common structural coverage metrics include:\n\nStatement Coverage – This criteria requires sufficient test cases for each program statement to be executed at least once; however, its achievement is insufficient to provide confidence in a software product’s behavior.\nDecision (Branch) Coverage – This criteria requires sufficient test cases for each program decision or branch to be executed so that each possible outcome occurs at least once. It is considered to be a minimum level of coverage for most software products, but decision coverage alone is insufficient for high-integrity applications.\nCondition Coverage – This criteria requires sufficient test cases for each condition in a program decision to take on all possible outcomes at least once. It differs from branch coverage only when multiple conditions must be evaluated to reach a decision.\nMulti-Condition Coverage – This criteria requires sufficient test cases to exercise all possible combinations of conditions in a program decision.\nLoop Coverage – This criteria requires sufficient test cases for all program loops to be executed for zero, one, two, and many iterations covering initialization, typical running and termination (boundary) conditions.\nPath Coverage – This criteria requires sufficient test cases for each feasible path, basis path, etc., from start to exit of a defined program segment, to be executed at least once. Because of the very large number of possible paths through a software program, path coverage is generally not achievable. The amount of path coverage is normally established based on the risk or criticality of the software under test.\nData Flow Coverage – This criteria requires sufficient test cases for each feasible data flow to be executed at least once. A number of data flow testing strategies are available.\n\nThe following types of functional software testing involve generally increasing levels of effort:\n\nNormal Case – Testing with usual inputs is necessary. However, testing a software product only with expected, valid inputs does not thoroughly test that software product. By itself, normal case testing cannot provide sufficient confidence in the dependability of the software product.\nOutput Forcing – Choosing test inputs to ensure that selected (or all) software outputs are generated by testing.\nRobustness – Software testing should demonstrate that a software product behaves correctly when given unexpected, invalid inputs. Methods for identifying a sufficient set of such test cases include Equivalence Class Partitioning, Boundary Value Analysis, and Special Case Identification (Error Guessing). While important and necessary, these techniques do not ensure that all of the most appropriate challenges to a software product have been identified for testing.\nCombinations of Inputs – The functional testing methods identified above all emphasize individual or single test inputs. Most software products operate with multiple inputs under their conditions of use. Thorough software product testing should consider the combinations of inputs a software unit or system may encounter during operation. Error guessing can be extended to identify combinations of inputs, but it is an ad hoc technique. Cause-effect graphing is one functional software testing technique that systematically identifies combinations of inputs to a software product for inclusion in test cases.\n\nFunctional and structural software test case identification techniques provide specific inputs for testing, rather than random test inputs. One weakness of these techniques is the difficulty in linking structural and functional test completion criteria to a software product’s reliability.\nAdvanced software testing methods, such as statistical testing, can be employed to provide further assurance that a software product is dependable. Statistical testing uses randomly generated test data from defined distributions based on an operational profile (e.g., expected use, hazardous use, or malicious use of the software product). Large amounts of test data are generated and can be targeted to cover particular areas or concerns, providing an increased possibility of identifying individual and multiple rare operating conditions that were not anticipated by either the software product’s designers or its testers. Statistical testing also provides high structural coverage. It does require a stable software product. Thus, structural and functional testing are prerequisites for statistical testing of a software product.\nAnother aspect of software testing is the testing of software changes. Changes occur frequently during software development. These changes are the result of\n\ndebugging that finds an error and it is corrected,\nnew or changed requirements (“requirements creep”), and\nmodified designs as more effective or efficient implementations are found.\n\nOnce a software product has been baselined (approved), any change to that product should have its own “mini life cycle,” including testing. Testing of a changed software product requires additional effort. It should demonstrate\n\nthat the change was implemented correctly, and\nthat the change did not adversely impact other parts of the software product.\n\nRegression analysis is the determination of the impact of a change based on review of the relevant documentation in order to identify the necessary regression tests to be run. Regression testing is the rerunning of test cases that a program has previously executed correctly and comparing the current result to the previous result in order to detect unintended effects of a software change. Regression analysis and regression testing should also be employed when using integration methods to build a software product to ensure that newly integrated modules do not adversely impact the operation of previously integrated modules.\nIn order to provide a thorough and rigorous examination of a software product, development testing is typically organized into levels: unit, integration, and system levels of testing.\n\nUnit (module or component) level testing focuses on the early examination of sub-program functionality and ensures that functionality not visible at the system level is examined by testing. Unit testing ensures that quality software units are furnished for integration into the finished software product.\nIntegration level testing focuses on the transfer of data and control across a program’s internal and external interfaces. External interfaces are those with\n\nother software (including operating system software),\nsystem hardware, and\nthe users and can be described as communications links.\n\nSystem level testing demonstrates that all specified functionality exists and that the software product is trustworthy. This testing verifies the as-built program’s functionality and performance with respect to the requirements for the software product as exhibited on the specified operating platform(s). System level software testing addresses functional concerns and the following elements of a device’s software that are related to the intended use(s):\n\nPerformance issues (e.g., response times, reliability measurements);\nResponses to stress conditions, e.g., behavior under maximum load, continuous use;\nOperation of internal and external security features;\nEffectiveness of recovery procedures, including disaster recovery;\nUsability; (Usability vs Utility??)\nCompatibility with other software products;\nBehavior in each of the defined hardware configurations; and\nAccuracy of documentation.\n\n\nControl measures (e.g., a traceability analysis) should be used to ensure that the intended coverage is achieved.\nSystem level testing also exhibits the software product’s behavior in the intended operating environment. The location of such testing is dependent upon the software developer’s ability to produce the target operating environment(s). Depending upon the circumstances, simulation and/or testing at (potential) customer locations may be utilized.\nTest plans should identify the controls needed to ensure\n\nthat the intended coverage is achieved and\nthat proper documentation is prepared when planned system level testing is conducted at sites not directly controlled by the software developer.\n\nTest procedures, test data, and test results\n\nshould be documented in a manner permitting objective pass/fail decisions to be reached.\nshould also be suitable for review and objective decision making subsequent to running the test,\nshould be suitable for use in any subsequent regression testing.\n\nErrors detected during testing should be\n\nlogged,\nclassified,\nreviewed, and\nresolved prior to release of the software.\n\nSoftware error data that is collected and analyzed during a development life cycle may be used to determine the suitability of the software product for release for commercial distribution. Test reports should comply with the requirements of the corresponding test plans.\nSoftware testing tools are frequently used to ensure consistency, thoroughness, and efficiency in the testing of such software products and to fulfill the requirements of the planned testing activities.\nAppropriate documentation providing evidence of the validation of these software tools for their intended use should be maintained (see section 6 of this guidance).\nTypical Tasks – Testing by the Software Developer\n\nTest Planning\nStructural Test Case Identification\nFunctional Test Case Identification\nTraceability Analysis - Testing\nUnit (Module) Tests to Detailed Design\nIntegration Tests to High Level Design\nSystem Tests to Software Requirements\nUnit (Module) Test Execution\nIntegration Test Execution\nFunctional Test Execution\nSystem Test Execution\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.6 User Site Testing\nTesting at the user site is an essential part of software validation. The Quality System regulation requires\n\ninstallation and\ninspection procedures (including testing where appropriate) as well as\ndocumentation of inspection and\ntesting to demonstrate proper installation. (See 21 CFR §820.170.)\n\nLikewise, manufacturing equipment must meet specified requirements, and automated systems must be validated for their intended use. (See 21 CFR §820.70(g) and 21 CFR §820.70(i) respectively.)\nTerminology regarding user site testing can be confusing. Terms such as\n\nbeta test,\nsite validation,\nuser acceptance test,\ninstallation verification, and\ninstallation testing have all been used to describe user site testing.\n\nFor the purposes of this guidance, the term “user site testing” encompasses all of these and any other testing that takes place outside of the developer’s controlled environment.\nThis testing should take place at a user’s site with the actual hardware and software that will be part of the installed system configuration. The testing is accomplished through either actual or simulated use of the software being tested within the context in which it is intended to function.\nTest planners should check with the FDA Center(s) with the corresponding product jurisdiction to determine whether there are any additional regulatory requirements for user site testing.\nUser site testing should follow a pre-defined written plan with\n\na formal summary of testing and\na record of formal acceptance.\n\nThe following documented evidence should be retained:\n\nall testing procedures,\ntest input data, and\ntest results\n\nThere should be evidence that hardware and software are installed and configured as specified. Measures should ensure that all system components are exercised during the testing and that the versions of these components are those specified. The testing plan should specify testing throughout the full range of operating conditions and should specify continuation for a sufficient time to allow the system to encounter a wide spectrum of conditions and events in an effort to detect any latent faults that are not apparent during more normal activities.\nSome of the evaluations of the system’s ability that have been performed earlier by the software developer at the developer’s site should be repeated at the site of actual use. These may include tests for:\n\na high volume of data,\nheavy loads or stresses,\nsecurity,\nfault testing (avoidance, detection, tolerance, and recovery),\nerror messages, and\nimplementation of safety requirements.\n\nThere should be an evaluation of the ability of the users of the system to understand and correctly interface with it.\nOperators should be able to perform the intended functions and respond in an appropriate and timely manner to all alarms, warnings, and error messages.\nRecords should be maintained of both proper system performance and any system failures that are encountered.\nThe revision of the system to compensate for faults detected during this user site testing should follow the same procedures and controls as for any other software change.\nThe developers of the software may or may not be involved in the user site testing.\n\nIf the developers are involved, they may seamlessly carry over to the user’s site the last portions of design-level systems testing.\nIf the developers are not involved, it is all the more important that the user have persons who understand the importance of careful test planning, the definition of expected test results, and the recording of all test outputs.\n\nTypical Tasks – User Site Testing\n\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.7 Maintenance and Software Changes\n\n2.5.2.7.1 Hardware vs Software\nHardware maintenance typically includes\n\npreventive hardware maintenance actions,\ncomponent replacement, and\ncorrective changes.\n\nSoftware maintenance includes\n\ncorrective,\nperfective, and\nadaptive maintenance\nbut does not include preventive maintenance actions or software component replacement.\n\n\n\n2.5.2.7.2 Maintenance Types\n\nCorrective maintenance: Changes made to correct errors and faults in the software.\nPerfective maintenance: Changes made to the software to improve the performance, maintainability, or other attributes of the software system .\nAdaptive maintenance: Changes to make the software system usable in a changed environment.\n\nSufficient regression analysis and testing should be conducted to demonstrate that portions of the software not involved in the change were not adversely impacted. When changes are made to a software system,\n\neither during initial development or\nduring post release maintenance,\n\nThis is in addition to testing that evaluates the correctness of the implemented change(s). The specific validation effort necessary for each software change is determined by\n\nthe type of change,\nthe development products affected, and the\nimpact of those products on the operation of the software.\n\n\n\n2.5.2.7.3 Factors of Limitting Validation Effort Needed When a Change Is Made\n\ncareful and complete documentation of the design structure and\ncareful and complete documentation of interrelationships of various modules,\ninterfaces, etc.\nFor example,\n\ntest documentation,\ntest cases, and\nresults of previous verification and validation testing All of them need to be archived if they are to be available for performing subsequent regression testing.\n\n\nThe following additional maintenance tasks should be addressed:\n\nSoftware Validation Plan Revision - For software that was previously validated, the existing software validation plan should be revised to support the validation of the revised software. If no previous software validation plan exists, such a plan should be established to support the validation of the revised software.\nAnomaly Evaluation – Software organizations frequently maintain documentation, such as software problem reports that describe software anomalies discovered and the specific corrective action taken to fix each anomaly.\n\nToo often, however, mistakes are repeated because software developers do not take the next step to determine the root causes of problems and make the process and procedural changes needed to avoid recurrence of the problem.\nSoftware anomalies should be evaluated in terms of their severity and their effects on system operation and safety,\nbut they should also be treated as symptoms of process deficiencies in the quality system.\nA root cause analysis of anomalies can identify specific quality system deficiencies.\nWhere trends are identified (e.g., recurrence of similar software anomalies), appropriate corrective and preventive actions must be implemented and documented to avoid further recurrence of similar quality problems. (See 21 CFR 820.100.)\n\nProblem Identification and Resolution Tracking - All problems discovered during maintenance of the software should be documented. The resolution of each problem should be tracked to ensure it is fixed, for historical reference, and for trending.\nProposed Change Assessment - All proposed modifications, enhancements, or additions should be assessed to determine the effect each change would have on the system. This information should determine the extent to which verification and/or validation tasks need to be iterated.\nTask Iteration - For approved software changes, all necessary verification and validation tasks should be performed to ensure that planned changes are implemented correctly, all documentation is complete and up to date, and no unacceptable changes have occurred in software performance.\nDocumentation Updating – Documentation should be carefully reviewed to determine which documents have been impacted by a change. All approved documents (e.g., specifications, test procedures, user manuals, etc.) that have been affected should be updated in accordance with configuration management procedures. Specifications should be updated before any maintenance and software changes are made."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.6 Validation of Automated Process Equipment and Quality System Software",
    "text": "2.6 Validation of Automated Process Equipment and Quality System Software\nThe Quality System regulation requires that “when computers or automated data processing systems are used as part of production or the quality system, the [device] manufacturer shall validate computer software for its intended use according to an established protocol.” (See 21 CFR §820.70(i)). This has been a regulatory requirement of FDA’s medical device Good Manufacturing Practice (GMP) regulations since 1978.\nComputer systems that implement part of a device manufacturer’s production processes or quality system (or that are used to create and maintain records required by any other FDA regulation) are subject to the Electronic Records; Electronic Signatures regulation. (See 21 CFR Part 11.) This regulation establishes additional security, data integrity, and validation requirements when records are created or maintained electronically. These additional Part 11 requirements should be carefully considered and included in system requirements and software requirements for any automated record keeping systems. System validation and software validation should demonstrate that all Part 11 requirements have been met.\nComputers and automated equipment are used extensively throughout all aspects of\n\nmedical device design,\nlaboratory testing and analysis,\nproduct inspection and acceptance,\nproduction and process control,\nenvironmental controls,\npackaging,\nlabeling,\ntraceability,\ndocument control,\ncomplaint management, and many other aspects of the quality system.\n\nIncreasingly, automated plant floor operations can involve extensive use of embedded systems in:\n\nprogrammable logic controllers;\ndigital function controllers;\nstatistical process control;\nsupervisory control and data acquisition;\nrobotics;\nhuman-machine interfaces;\ninput/output devices; and\ncomputer operating systems.\n\nAll software tools used for software design are subject to the requirement for software validation, but the validation approach used for each application can vary widely.\nValidation is typically supported by:\n\nverifications of the outputs from each stage of that software development life cycle; and\nchecking for proper operation of the finished software in the device manufacturer’s intended use environment.\n\n\n2.6.1 How Much Validation Evidence Is Needed?\nThe level of validation effort should be commensurate with\n\nthe risk posed by the automated operation,\nthe complexity of the process software,\nthe degree to which the device manufacturer is dependent upon that automated process to produce a safe and effective device\n\nDocumented requirements and risk analysis of the automated process help to define the scope of the evidence needed to show that the software is validated for its intended use. Without a plan, extensive testing may be needed for:\n\na plant-wide electronic record and electronic signature system;\nan automated controller for a sterilization cycle; or\nautomated test equipment used for inspection and acceptance of finished circuit boards in a lifesustaining / life-supporting device.\n\nHigh risk applications should not be running in the same operating environment with non-validated software functions, even if those software functions are not used. Risk mitigation techniques such as memory partitioning or other approaches to resource protection may need to be considered when high risk applications and lower risk applications are to be used in the same operating environment.\nWhen software is upgraded or any changes are made to the software, the device manufacturer should consider how those changes may impact the “used portions” of the software and must reconfirm the validation of those portions of the software that are used. (See 21 CFR §820.70(i).)\n\n\n2.6.2 Defined User Equipment\nA very important key to software validation is a documented user requirements specification that defines:\n\nthe “intended use” of the software or automated equipment; and\nthe extent to which the device manufacturer is dependent upon that software or equipment for production of a quality medical device.\n\nThe device manufacturer (user) needs to define the expected operating environment including any required hardware and software configurations, software versions, utilities, etc. The user also needs to:\n\ndocument requirements for system performance, quality, error handling, startup, shutdown, security, etc.;\nidentify any safety related functions or features, such as sensors, alarms, interlocks, logical processing steps, or command sequences; and\ndefine objective criteria for determining acceptable performance.\n\nThe validation must be conducted in accordance with a documented protocol, and the validation results must also be documented. (See 21 CFR §820.70(i).) Test cases should be documented that will exercise the system to challenge its performance against the pre-determined criteria, especially for its most critical parameters.\nTest cases should address\n\nerror and alarm conditions,\nstartup, shutdown,\nall applicable user functions and operator controls,\npotential operator errors,\nmaximum and minimum ranges of allowed values, and\nstress conditions applicable to the intended use of the equipment.\n\nThe test cases should be executed and the results should be recorded and evaluated to determine whether the results support a conclusion that the software is validated for its intended use.\nA device manufacturer may conduct a validation using their own personnel or may depend on a third party such as the equipment/software vendor or a consultant. In any case, the device manufacturer retains the ultimate responsibility for ensuring that the production and quality system software:\n\nis validated according to a written procedure for the particular intended use; and\nwill perform as intended in the chosen application.\n\nThe device manufacturer should have documentation including:\n\ndefined user requirements;\nvalidation protocol used;\nacceptance criteria;\ntest cases and results; and\na validation summary that objectively confirms that the software is validated for its intended use.\n\n\n\n2.6.3 Validation of Off-The-Shelf Software and Automated Equipment\nMost of the automated equipment and systems used by device manufacturers are supplied by thirdparty vendors and are purchased off-the-shelf (OTS). The device manufacturer is responsible for ensuring that the product development methodologies used by the OTS software developer are appropriate and sufficient for the device manufacturer’s intended use of that OTS software.\nWhere possible and depending upon the device risk involved, the device manufacturer should consider auditing the vendor’s design and development methodologies used in the construction of the OTS software and should assess the development and validation documentation generated for the OTS software. Such audits can be conducted by the device manufacturer or by a qualified third party.\nThe audit should demonstrate that the vendor’s procedures for and results of the verification and validation activities performed the OTS software are appropriate and sufficient for the safety and effectiveness requirements of the medical device to be produced using that software."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "",
    "text": "Notice\nLast Update\nIntroduction\n\nDefinition of SW Validation\nSome Terminology\nRationale\nObjective of SW Validation\nWhat to validate\nMain Institutions\n\nQuality System Regulation\nVerification\nValidation\nBenefits and Difficulty in SW V&V\nSW Development as Part of System Design\n\nOverview\nDesign Reveiw\n\n\n\n\nValidation Pinciples\n\nOverview\nConditions\nPlanning\nAfter SW Change\nSW Lifecycle\n\nSW Lifecycle Tasks\n\nOverview\nQuality Planning\nConfiguration Management\nTask Requirements\nDesign Overview\n\nDesign Consideration\nDesign Specification\nDesign Activity and Task\n\n\n\n\n\nTesting Tasks\n\nOverview\nConsideration Before Testing Tasks\nCode Based Testing\nSolution to White Box Testing\nDevelopment Testing\nUser Site Testing\n\nOverview\nTesting\n\n\nMaintenance and SW Changes\nValidation of Quality System SW\n\nOverview\nFactors in Validation"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#notice",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#notice",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Notice",
    "text": "Notice\n\nI am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto. (it seems that Quarto system has some issues on mermaid diagrams.)\nThe FDA validation guidance document is a bit difficult to understand because its explanations provide abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\nLast Update\n\n2022-12-28, Summary of Document"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#introduction",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#introduction",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Introduction",
    "text": "Introduction\n\nDefinition of Software Validation\nSoftware Validation is a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997.\n(See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\n\n\nSome Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology: requirements, specification, verification, and validation.\n\n\n\nRationale\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\nObjective of SW validation is to ensure\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\nWhat to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\nMain Institutions\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#quality-system-regulation",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#quality-system-regulation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Quality System Regulation",
    "text": "Quality System Regulation\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\n\n\nflowchart TB\n    subgraph Quality_System_Regulation\n        direction LR\n        subgraph Requirement\n            direction TB\n            user_requirements\n        end\n        subgraph Specification\n           direction TB\n           document_user_requirements \n        end \n        subgraph Verification\n           direction TB\n           verify_spacified_requirements\n        end\n        subgraph Validation\n           direction TB\n           Confirmation_by_Examinations\n           Provision_of_objective_3evidences\n        end\n        Requirement--> Specification --> Verification --> Validation                    \n    end\n    subgraph First_Detail\n        direction TB\n        subgraph User_Requirement\n            direction TB\n            any_need_for_customer---\n            any_need_for_system---\n            any_need_for_software\n        end\n            subgraph Document_User_Requirement\n            direction TB\n            define_means_for_requirements---\n          define_criteria_for_requirements\n        end         \n        subgraph Verify_Spacified_Requirement\n            direction TB\n            Objective_Evidence--->|needs|Software_Testing\n        end\n        subgraph SW_Validation\n            direction TB\n            subgraph Confirmation_by_Examination\n            direction TB\n                subgraph Examination_List_of_SW_LifeCycle\n                    direction TB\n                    comprehensiveness_of_software_testing---\n                    inspection_verification_test---\n                    analysis_verification_test---\n                    other_varification_tests    \n                end \n            end             \n            subgraph Provision_of_Objective_3evidences\n                direction TB\n                Software_specifications_conformity---\n                Consistent_SW_Implementation---\n                Correctness_Completeness_Traceability\n            end\n        end\n        Requirement---User_Requirement\n        Specification---Document_User_Requirement\n        Verification---Verify_Spacified_Requirement\n        Confirmation_by_Examinations---Confirmation_by_Examination\n        Provision_of_objective_3evidences---Provision_of_Objective_3evidences             \n    end"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#verification",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#verification",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Verification",
    "text": "Verification\n\n\n\n\nflowchart LR\n    subgraph Objective_Evidence\n        direction LR\n        subgraph Design_Outputs_of_SW_life_cycle_for_Specified_Requirements\n            direction TB\n            Consistency---\n            Completeness---\n            Correctness---\n            Documentation\n        end       \n        subgraph Software_Testing\n            direction LR\n            subgraph Testing_Environments\n                direction TB\n                satisfaction_for_input_requirements\n                satisfaction_for_input_requirements---Simulated_Use_Environment\n                subgraph User_Site_Testing\n                    direction TB                            \n                    Installation_Qualification---\n                    Operational_Qualification---\n                    Performance_Qualification\n                end\n            end\n            satisfaction_for_input_requirements---User_Site_Testing\n            subgraph Testing_Activities\n                direction TB\n                static_analyses---\n                dynamic_analyses---\n                code_and_document_inspections---\n                walkthroughs\n            end \n        Testing_Environments-->Testing_Activities\n        end\n    Design_Outputs_of_SW_life_cycle_for_Specified_Requirements-->Software_Testing-->Testing_Activities\nend    \n\n\n\n\n\n\n\n\n\n\nInstallation_Qualification (IQ): documentation of correct installations according to requirements, specifications, vendor’s recommendations, and the FDA’s guidance for all hardware, software, equipment and systems.\nOperational_Qualification (OQ): establishment of confidence that the software shows constant performances according to specified requirements.\nPerformance_Qualification (PQ): confirmation of the performance in the intended use according to the specified requirements for functionality and safety throughout the SW life cycle."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation",
    "text": "Validation\n\n\n\n\nflowchart LR\n    subgraph Validation\n    direction LR\n        subgraph Confirmation_by_Examination\n            direction TB\n            subgraph Examination_List_at_each_stage_of_SW_Life_Cycle\n                direction TB\n                comprehensiveness_of_software_testing---\n                inspection_verification_test---\n                analysis_verification_test---\n                other_varification_tests    \n            end \n        end\n        subgraph Provision_of_objective_3evidences\n            direction TB\n            subgraph Software_specifications_conform_to\n                direction TB\n                user_needs \n                intended_uses\n            end\n            subgraph Consistent_SW_Implementation\n                direction TB\n                particular_requirements\n            end\n            subgraph Correctness_Completeness_Traceability\n                direction TB\n                correct_complete_implementation_by_all_SW_requirements---\n                traceable_to_system_requirements\n            end\n            Software_specifications_conform_to---\n            Consistent_SW_Implementation---\n            Correctness_Completeness_Traceability\n        end\n        Confirmation_by_Examination-->\n        Provision_of_objective_3evidences\n    end\n\n\n\n\n\n\n\n\n\n\n\n\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#benefits-and-difficulty-of-sw-vv",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#benefits-and-difficulty-of-sw-vv",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Benefits and Difficulty of SW V&V",
    "text": "Benefits and Difficulty of SW V&V\n\nBenefits of SW V&V\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nReduce long term costs by making V&V easier and less costly to reliably modify software and revalidate software changes.\n\n\n\nDifficulty in SW V&V\n\na developer cannot test forever, and\n\nit is difficult to know how much evidence is enough.\na matter of developing a level of confidence that the device meets all requirements\n\nConsiderations for an acceptable level of confidence\nmeasures and estimates such as defects found in specifications documents\ntesting coverage, and other techniques are all used before shipping the product.\na level of confidence varies depending upon the safety risk (hazard) of a SW or device"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-development-as-part-of-system-design",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-development-as-part-of-system-design",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Development as Part of System Design",
    "text": "SW Development as Part of System Design\n\nOverview\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        purpose_design_review---\n        design_review_types---\n        design_review_requirements---\n        design_review_outputs\n    end\n\n\n\n\n\n\n\n\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nuser’s needs\nintended uses from which the product is developed.\n\nA primary goal of SW validation is to demonstrate that all completed SW products comply with all documented requirements.\n\n\nDesign Review\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        subgraph Purpose_Design_Review\n            direction TB\n            documented_structured_comprehensive_systematic_examinations---\n            adequacy_of_design_requirements---\n            capability_of_design_for_requirements---\n            identification_of_problem   \n        end\n        subgraph Design_Reivew_Types\n            direction TB\n            subgraph Formal_Design_Review\n                direction TB\n                3rd_parties_outside_development_team\n            end\n            subgraph Informal_Design_Review\n                direction TB\n                within_development_team\n            end\n        Formal_Design_Review---Informal_Design_Review    \n        end\n        subgraph Design_Review_Requirements\n            direction TB\n               necessary_at_least_one_formal_design_review---\n               optinal_informal_design_review---\n               recommended_multiple_design_reviews\n        end\n        subgraph Formal_Design_Review_Outputs\n            direction TB\n            more_than_10_outputs\n        end\n        Purpose_Design_Review--> Design_Reivew_Types--> Design_Review_Requirements\n        Design_Review_Requirements-->Formal_Design_Review_Outputs\n    end\n\n\n\n\n\n\n\n\n\n\nDesign review is a primary tool for managing and evaluating development projects.\nAt least one formal design review must be conducted during the device design process.\nIt is recommended that multiple design reviews be conducted.\nProblems found at this point can\n\nbe resolved more easily,\nsave time and money, and\nreduce the likelihood of missing a critical issue."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-principles",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-principles",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation Principles",
    "text": "Validation Principles\n\nOverview\n\n\n\n\nflowchart LR\n  subgraph Validation_Principles\n        direction LR\n        subgraph Validation_Starting_Point\n            direction TB\n            during_design_planning---\n            during_development_planning---\n            all_results_should_be_supported_by_evidence_collected_from_planning_SW_lifecylce\n        end\n        subgraph Validation_Conditions\n            direction TB\n            Requirements---Estabilishment_Confidence---SW_Lifecycle\n        end\n\n        subgraph Validation_Planning\n            direction TB\n            Specify_Areas\n            subgraph Validation_Coverage\n                direction TB\n            end\n            subgraph Validation_Process_Establishment\n                direction TB\n            end\n        Specify_Areas---Validation_Coverage---Validation_Process_Establishment\n        end\n\n        subgraph After_Self_Validation\n            direction TB\n            subgraph Validation_After_SW_Change\n        direction TB\n        end\n\n        subgraph Independence_of_Review\n        direction TB\n\n        end\n        Validation_After_SW_Change---Independence_of_Review\n        end\n            Validation_Starting_Point-->Validation_Conditions-->Validation_Planning-->\nAfter_Self_Validation\n    end\n\n\n\n\n\n\n\n\n\nPreparation for software validation should begin as early as possible because the final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\n\nConditions\n\n\n\n\nflowchart LR\n\nsubgraph Validation_Conditions\n    direction LR\n    subgraph SW_Requirments\n        direction TB\n        subgraph Documented_SW_Requirments_Specification\n            direction TB\n            Baseline_Provision_for_V&V---\n            establishment_of_software_requirements_specification\n        end\n    end\n    subgraph Estabilishment_Confidence\n        direction TB\n            mixture_of_methods_techinques---\n            preventing_SW_errors---\n            detecting_SW_errors                 \n    end\n    subgraph SW_Lifecycle\n        direction TB\n        validation_must_be_conducted_within_established_environment_across_lifecycle---\n        lifecycle_contains_SW_engineering_tasks_and_documentation---\n        V&V_tasks_must_reflect_intended_use\n    end\nend\nSW_Requirments---Estabilishment_Confidence---SW_Lifecycle\n\n\n\n\n\n\n\n\n\n\nPlanning\n\n\n\n\nflowchart LR\n    subgraph Validation_Planning\n        direction LR\n        define_what_to_accomplish\n        subgraph Specify_Areas\n            direction TB\n            scope---\n            approach---\n            resources---\n            schedules_activities---\n            types_activitieis---\n            extent_of_activities---\n            tasks---\n            work_items\n        end\n            define_what_to_accomplish-->Specify_Areas\n        subgraph Validation_Coverage\n               direction TB\n            depending_on_SW_complexity_of_SW_design---\n            depending_on_safety_risk_for_specified_intended_use---\n            select_activities_tasks_work_items_for_complexity_safety_risk\n        end\n        subgraph Validation_Process_Establishment\n            direction TB\n            establish_how_to_conduct-->\n            identify_sequence_of_specific_actions-->\n            identify_specific_activitieis-->\n            identify_specific_tasks-->\n            identify_specific_work_items\n        end\n    Specify_Areas-->Validation_Coverage-->Validation_Process_Establishment\n    end\n\n\n\n\n\n\n\n\n\n\nAfter SW Change\n\n\n\n\nflowchart LR\n\nsubgraph After_Self_Validation\n    direction LR\n    subgraph Validation_After_SW_Change\n        direction TB\n        determine_extent_of_change_on_entire_SW_system-->\n        determine_impact_of_change_on_entire_SW_system-->\n        conduct_SW_regression_testing_on_unchanged_but_vulnerable_modules\n    end\n    subgraph Independence_of_Review\n        direction TB\n        follow_basic_quality_assurance_precept_of_independence_of_review---\n        avoid_self_validation---\n        should_conduct_contracted_3rd_party_independent_V&V---\n        or_conduct_blind_test_with_internal_staff\n    end\n    Validation_After_SW_Change---Independence_of_Review\nend\n    \n\n\n\n\n\n\n\n\n\n\nSW Lifecycle\n\n\n\n\nflowchart LR\nsubgraph SW_Lifecycle\n    direction TB\n    validation_must_be_conducted_within_the_established_environment_across_lifecycle---\n    lifecycle_contains_SW_engineering_tasks_and_documentation---\n    V&V_tasks_must_reflect_intended_use\nend\n\nsubgraph SW_Lifecycle_Activities\n    direction TB\n    subgraph should_establish_lifecycle_model\n        direction TB\n        subgraph SW_Lifecycle_Model_List_Defined_in_FDA\n            direction TB\n            waterfall---\n            spiral---\n            rapid_prototyping---\n            incremental_development---\n            etc\n        end     \n    end\n    subgraph should_cover_SW_birth_to_retirement\n        direction TB\n        subgraph Lifecycle_Activities\n            direction TB\n            Quality_Plan-->\n            System_Requirements_Definition-->\n            Detailed_Software_Requirements_Specification-->\n            Software_Design_Specification-->\n            Construction_or_Coding-->\n            Testing-->\n            Installation-->\n            Operation_and_Support-->\n            Maintenance-->\n            Retirement\n        end\n    end\n    should_establish_lifecycle_model-->should_cover_SW_birth_to_retirement\n    should_cover_SW_birth_to_retirement-->Lifecycle_Activities\nend\nsubgraph SW_Lifecycle_Tasks\n    direction TB\n    should_define_and_document_risk_related_tasks---\n    should_define_and_document_which_tasks_are_appropriate_in_vice_versa---\n    Quality_Planning---\n    Quality_Planning_Tasks---\n    Inclusion_Task_List_for_Plan---\n    Identification_Task_List_for_Plan---\n    Configuration_Management---\n    Control---\n    Management---\n    Procedures---\n    ensure_proper_communications_and_documentation---\n    Task_Requirements\nend\nSW_Lifecycle-->SW_Lifecycle_Activities-->SW_Lifecycle_Tasks"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-lifecycle-tasks",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-lifecycle-tasks",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Lifecycle Tasks",
    "text": "SW Lifecycle Tasks\n\nOverview\n\n\n\n\n \nflowchart TB\n\nsubgraph SW_Lifecycle_Tasks\n    direction LR\n    subgraph Define_and_Document_List\n        direction TB\n        risk_related_tasks---\n        whether_or_not_tasks_are_appropriate\n    end\n    \n    subgraph Quality_Planning\n        direction TB\n        subgraph Quality_Planning_Tasks\n            direction TB\n        \n        end\n        subgraph Inclusion_List_for_Plan\n            direction TB\n            \n        end\n        subgraph Identification_List_for_Plan\n            direction TB\n            \n        end\n    Quality_Planning_Tasks-->Inclusion_List_for_Plan-->Identification_List_for_Plan\n    end\n    \n    subgraph Configuration_Management\n        direction TB\n        subgraph Control\n            direction TB\n            \n        end\n        subgraph Management\n            direction TB\n        end\n        subgraph Procedures\n            direction TB\n        end\n        ensure_proper_communications_and_documentation\n        Control-->Management-->Procedures-->ensure_proper_communications_and_documentation \n    end\n    subgraph Task_Requirements\n        direction TB\n        identification---\n        analysis---\n        predetermined_documentation_about_device_its_intended_use---\n        Requirements_Specification_List---\n        Verfification_List_by_Evaluation---\n        Requirements_Tasks    \n    end\nDefine_and_Document_List-->Quality_Planning-->Configuration_Management-->Task_Requirements\nend     \n\n\n\n\n\n\n\n\n\n\nQuality Planning\n\n\n\n\nflowchart TB\nsubgraph Quality_Planning\n    direction LR\n    subgraph Quality_Planning_Tasks\n        direction TB\n        Risk_Hazard_Management_Plan---\n        Configuration_Management_Plan---\n        Software_Quality_Assurance_Plan---\n        Software_Verification_and_Validation_Plan---\n        Verification_and_Validation_Tasks---\n        Acceptance_Criteria---\n        Schedule_and_Resource_Allocation_for_V&V_activities---\n        Reporting_Requirements---\n        Formal_Design_Review_Requirements---\n        Other_Technical_Review_Requirements---\n        Problem_Reporting_and_Resolution_Procedures---\n        Other_Support_Activities\n    end\n    subgraph Inclusion_List_for_Plan\n        direction TB\n        specific_tasks_for_each_life_cycle_activity---\n        Enumeration_of_important_quality_factors--- \n        like_reliability_maintainability_usability---\n        Methods_and_procedures_for_each_task---\n        Task_acceptance_criteria---\n        Criteria_for_defining_and_documenting_outputs_for_input_requirements---\n        Inputs_for_each_task---\n        Outputs_from_each_task---\n        Roles_resources_and_responsibilities_for_each_task---\n        Risks_and_assumptions---\n        Documentation_of_user_needs    \n    end\n    subgraph Identification_List_for_Plan\n        direction TB\n        personnel---\n        facility_and_equipment_resources_for_each_task---\n        role_that_risk_hazard_management        \n    end\nQuality_Planning_Tasks-->Inclusion_List_for_Plan-->Identification_List_for_Plan\nend\n\n\n\n\n\n\n\n\n\n\nConfiguration Management\n\n\n\n\nflowchart LR\nsubgraph Configuration_Management\n    direction LR\n    subgraph Control\n        direction TB\n        control_multiple_parallel_development_activities---\n        ensure_positive_and_correct_correspondence_of---\n        specifications_documents---\n        source_code---\n        object_code---\n        test_suites---\n        ensure_accurate_identification_of_approved_versions---\n        ensure_access_to_approved_versions---\n        create_procedures_for_reporting---\n        create_procedures_for_resolving_SW_anomalies                            \n    end\n    subgraph Management\n        direction TB\n        identify_reports---\n        specify_contents---\n        specify_format---\n        specify_responsible_organizational_elements_for_each_report\n    end\n    subgraph Procedures\n        direction TB\n        necessary_for_review_of_SW_development_results---\n        necessary_for_approval_of_SW_development_results\n    end\n    ensure_proper_communications_and_documentation\n    Control-->Management-->Procedures-->ensure_proper_communications_and_documentation \nend\n\n\n\n\n\n\n\n\n\n\nTask Requirements\n\n\n\n\n\nflowchart TB\n    subgraph Task_Requirements\n        direction LR\n        subgraph group\n            direction TB\n            identification---\n            analysis---\n            predetermined_documentation_about_device_its_intended_use\n        end\n        \n        subgraph Requirements_Specification_List\n            direction TB\n            All_software_system_inputs---\n            All_software_system_outputs---\n            All_functions_that_software_system_will_perform---\n            All_performance_requirements_that_software_will_meet---\n            requirement_example_data_throughput_reliability_timing---\n            definition_of_all_external_and_user_interfaces---\n            any_internal_software_to_system_interfaces---\n            How_users_will_interact_with_system---\n            What_constitutes_error---\n            how_errors_should_be_handled---\n            Required_response_times---\n            Intended_operating_environment_for_software---\n            All_acceptable_ranges_limits_defaults_specific_values---\n            All_safety_related_requirements_that_will_be_implemented_in_SW---\n            All_safety_related_specifications_that_will_be_implemented_in_SW---\n            All_safety_related_features_that_will_be_implemented_in_SW---\n            All_safety_related_functions_that_will_be_implemented_in_SW---\n            clearly_identify_potential_hazards---\n            risk_evaluation_for_accuracy---\n            risk_evaluation_for_completeness---\n            risk_evaluation_for_consistency---\n            risk_evaluation_for_testability---\n            risk_evaluation_for_correctness---\n            risk_evaluation_for_clarity\n        end\n        subgraph Verfification_List_by_Evaluation\n            direction TB\n            no_internal_inconsistencies_among_requirements---\n            All_of_performance_requirements_for_system---\n            Complete_correct_Fault_tolerance_safety_security_requirements---\n            Accurate_Complete_Allocation_of_software_functions---\n            Appropriate_Software_requirements_for_system_hazards---\n            mesurable_requirements---\n            objectively_verifiable_requirements---\n            traceable_requirements\n        end\n        subgraph Requirements_Tasks\n            direction TB\n            Preliminary_Risk_Analysis---\n            Traceability_Analysis---\n            ex_Software_Requirements_to_System_Requirements_vice_versa---\n            ex_Software_Requirements_to_Risk_Analysis---\n            Description_of_User_Characteristics---\n            Listing_of_Characteristics_and_Limitations_of_Memory---\n            Software_Requirements_Evaluation---\n            Software_User_Interface_Requirements_Analysis---\n            System_Test_Plan_Generation---\n            Acceptance_Test_Plan_Generation---\n            Ambiguity_Review_or_Analysis\n        end\n    group-->Requirements_Specification_List \n    Verfification_List_by_Evaluation-->Requirements_Tasks\n    end\n\n\n\n\n\n\n\n\n\n\nDesign Overview\n\n\n\n\nflowchart TB\n    subgraph Deign_Task\n        direction LR\n    subgraph Design_Consideration_List\n        direction TB\n        subgraph Description\n                    direction TB\n                end\n        subgraph Human_Factors_Engineering\n          direction TB\n    \n        end\n        subgraph Safety_Usability_Issues_Conisderation\n            direction TB\n\n            end\n        Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n    subgraph Design_Specificiation\n        direction TB\n        subgraph Performing_List\n            direction TB\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n        end\n    Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design \n    end\n    subgraph Design_Activity_and_Task_List\n        direction TB\n        subgraph Final_Design_activity\n            direction TB\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n            end\n            subgraph Coding_Tasks\n                direction TB\n            end\n        traceability_analysis-->Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end\n    Design_Consideration_List---Design_Specificiation---Design_Activity_and_Task_List\n\n    end\n\n\n\n\n\n\n\n\n\nDesign Consideration\n\n\n\n\nflowchart TB\nsubgraph Design_Consideration_List\n    direction LR\n        subgraph Requirement_Specification\n            direction TB\n            logical_representation---\n            physical_representation\n        end\n    subgraph Description\n            direction TB\n            what_to_do---\n            how_to_do                   \n        end\n    subgraph Human_Factors_Engineering\n      direction TB\n            entire_design_and_development_process---\n            device_design_requirements---\n            analyses---\n            tests\n    end\n    subgraph Safety_Usability_Issues_Conisderation\n        direction TB\n                flowcharts--- \n                state_diagrams--- \n                prototyping_tools---\n                test_plans\n        end\n        Requirement_Specification---Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n\n\n\n\n\n\n\n\n\n\nDesign Specification\n\n\n\n\nflowchart TB\nsubgraph Design_Specificiation\n        direction LR\n        subgraph Conceptual_Specification\n            direction TB\n            requirements_specification---\n            predetermined_criteria---\n            Software_risk_analysis---\n            Development_procedures---\n            coding_guidelines\n        end\n        subgraph Performing_List\n            direction TB\n            task---\n            function_analyses---\n            risk_analyses---\n            prototype_tests_and_reviews---\n            full_usability_tests\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n            SW_requirements_specification---\n            predetermined_criteria_for_SW_acceptance---\n            SW_risk_analysis---\n            Development_procedure_list---\n            coding_guidance---\n            Systems_documentation---\n            Hardware_to_be_used---\n            Parameters_to_be_measured---\n            Logical_structure---\n            Control_logic---\n            logical_processing_steps_aka_algorithms---\n            Data_structures_diagram---\n            data_flow_diagrams---\n            Definitions_of_variables---\n            description_of_where_they_are_used---\n            Error_alarm_and_warning_messages---\n            Supporting_software---\n            internal_modules_Communication_links---\n            supporting_sw_links---\n            link_with_hardware---\n            link_with_user---\n            physical_Security_measures---\n            logical_security_measures\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n            complete--- \n            correct---\n            consistent--- \n            unambiguous--- \n            feasible---\n            maintainable---\n            analyses_of_control_flow---\n            data_flow--- \n            complexity--- \n            timing--- \n            sizing--- \n            memory_allocation---\n            module_architecture---\n            traceability_analysis_of_modules--- \n            criticality_analysis\n        end\n    Conceptual_Specification---Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design  \n    end\n\n\n\n\n\n\n\n\n\n\nDesign Activity and Task\n\n\n\n\n\nflowchart TB\nsubgraph Design_Activity_and_Task_List\n        direction LR\n        subgraph Final_Design_activity\n            direction TB\n            Formal_Design_Review_Before_Design_Implementation---\n            correct_consistent_complete_accurate_testable\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n            Updated_Software_Risk_Analysis---\n            Traceability_Analysis---\n            Software_Design_Evaluation---\n            Design_Communication_Link_Analysis---\n            Module_Test_Plan_Generation---\n            Integration_Test_Plan_Generation---\n            module_Test_Design_Generation---\n            integration_Test_Design_Generation---\n            system_Test_Design_Generation---\n            acceptance_Test_Design_Generation   \n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n                each_element_implementation---\n                each_module_implementation_to_element_and_risk_analysis---\n                each_functions_implemented_to_element_and_risk_analysis---\n                Tests_for_modules_to_element_and_risk_analysis--- \n                Tests_for_functions_to_element_and_risk_analysis---\n                Tests_for_modules_to_source_code---\n                Tests_for_functions_to_source_code\n            end\n            subgraph Coding_Tasks\n                direction TB\n                Traceability_Analyses---\n                Source_Code_to_Design_Specification_and_vice_versa---\n                Test_Cases_to_Source_Code_and_to_Design_Specification---\n                Source_Code_and_Source_Code_Documentation_Evaluation---\n                Source_Code_Interface_Analysis---\n                Test_Procedure_and_Test_Case_Generation \n            end\n        traceability_analysis-->Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#testing-task",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#testing-task",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Testing Task",
    "text": "Testing Task\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction TB\n            subgraph Test_Plans\n                direction TB\n            end\n            subgraph Conditions\n                direction TB\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n        subgraph Code_Based_Testing\n            direction TB\n            subgraph white_box_testing\n                direction TB\n            end\n            subgraph Evaluation_of_level_of_white_box_testing\n                direction TB\n            end\n            subgraph Coverage_Metrics_of_White_Box_Testing\n                direction TB\n            end\n        white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n        end\n        subgraph Alternatives_to_White_Box_Testing\n            direction TB\n            subgraph Types_of_Functional_Software_Testing_Increasing_Cost\n                direction TB\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n            end\n            subgraph Change_in_SW\n                direction TB    \n            end\n        Types_of_Functional_Software_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW\n        end\n        \n\n        subgraph Development_Testing\n            direction TB\n            subgraph unit_level_testing\n                direction TB    \n            end\n            subgraph integration_level_testing\n                direction TB\n            end\n            subgraph system_level_testing\n                direction TB\n            end\n            subgraph Error_Detected\n                direction TB        \n            end\n        unit_level_testing-->integration_level_testing-->system_level_testing-->Error_Detected\n        end\n\n        subgraph Testing_Tasks\n            direction TB\n        end\n        subgraph User_Site_Testing\n            direction TB\n            subgraph Quality_System_Rregulation\n                direction TB\n            end\n            subgraph Understand_Terminology\n                direction TB\n            end\n            subgraph Testing\n                direction TB\n            end\n            Quality_System_Rregulation---Understand_Terminology---Testing\n        end\nConsideration_Before_Testing_Tasks---Code_Based_Testing---Alternatives_to_White_Box_Testing\nDevelopment_Testing---Testing_Tasks---User_Site_Testing\n    end\n\n\n\n\n\n\n\n\n\n\nConsideration Before Testing Tasks\n\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction LR\n            subgraph Test_Plans\n                direction TB\n                should_identify_control_measures_like_traceability_analysis---\n                ensure_that_intended_coverage_is_achieved---\n                ensure_that_proper_documentation_is_prepared---\n                conduct_tests_not_by_SW_developers_but_in_other_sites\n            end\n            subgraph Conditions\n                direction TB\n                use_defined_inputs---\n                documented_outcomes---\n                gonnabe_time_consuming_activity---\n                gonnabe_difficult_activity---\n                gonnabe_imperfect_activity---\n                testing_all_program_functionality---\n                does_not_mean_100_prcnt_correction_perfection---\n                make_detailed_objective_evaluation---\n                requires_sophisticated_definition_specificiation---\n                all_test_procedures_data_results_are_documented---\n                all_test_procedures_data_results_are_suitable_for_review---\n                all_test_procedures_data_results_are_suitable_for_objective_decision_making---\n                all_test_procedures_data_results_are_suitable_for_subsequent_regression_testing\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n                make_test_plans---\n                make_test_cases---\n                plan_schedules---\n                plan_environments---\n                plan_resources_of_personnel_tools---\n                plan_methodologies---\n                plan_inputs_procedures_outputs_expected_results---\n                plan_documentation---\n                plan_reporting_criteria\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n                expected_test_outcome_is_predefined---\n                good_test_case_has_high_probability_of_exposing_errors---\n                successful_test_is_one_that_finds_errors---\n                There_is_independence_from_coding---\n                Both_application_for_user_and_SW_for_programming_expertise_are_employed---\n                Testers_use_different_tools_from_coders---\n                Examining_only_the_usual_case_is_insufficient---\n                Test_documentation_permits_its_reuse---\n                Test_documentation_permits_independent_confirmation_---\n                of_pass/fail_test_outcome_during_subsequent_review\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n\nend\n\n\n\n\n\n\n\n\n\n\nCode Based Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n            subgraph Code_Based_Testing\n                direction LR\n                subgraph white_box_testing\n                    direction TB\n                    identify_dead_code_never_executed---\n                    conduct_unit_test---\n                    conduct_other_level_tests\n                end\n                subgraph Evaluation_of_level_of_white_box_testing\n                    direction TB\n                    use_coverage_metrics---\n                    metrics_of_completeness_of_test_selection_criteria---\n                    coverage_should_be_commensurate_with_level_of_SW_risk---\n                    coverage_means_100_prcnt_coverage\n                end\n                subgraph Coverage_Metrics_of_White_Box_Testing\n                    direction TB\n                    Statement_Coverage---\n                    Decision_or_Branch_Coverage---\n                    Condition_Coverage---\n                    Multi_Condition_Coverage\n                    Loop_Coverage---\n                    Path_Coverage---\n                    Data_Flow_Coverage\n                end\n            white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n            end\nend\n\n\n\n\n\n\n\n\n\n\nSolution to White Box Testing\n\n\n\n\n\n \nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Alternatives_to_White_Box_Testing\n            direction LR\n            subgraph Types_of_Testing_Increasing_Cost\n                direction TB\n                    Normal_Case---\n                    Output_Forcing---\n                    Robustness---\n                    Combinations_of_Inputs\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n                difficulty_in_linking_---\n                tests_completion_criteria_to_SW_reliability\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n                statistical_testing---\n                provide_further_assurance_of_reliability---\n                generate_randomly_test_data_from_defined_distributions---\n                distribution_defined_by_expected_use---\n                distribution_defined_by_hazardous_use---\n                distribution_defined_by_malicious_use---\n                large_test_data_cover_particular_areas_or_concerns---\n                statistical_testing_provides_high_structural_coverage---\n                statistical_testing_requires_stable_system---\n                structural_and_functional_testing_are_prerequisites_for_statistical_testing\n            end\n            subgraph Change_in_SW\n                direction TB\n                conduct_regression_analysis_and_testing---\n                should_demonstrate_correct_implementation---\n                should_demonstrate_no_adverse_impact_on_other_modules   \n            end\n            subgraph Testing_Tasks\n                direction TB\n                Test_Planning---\n                Structural_Test_Case_Identification---\n                Functional_Test_Case_Identification---\n                Traceability_Analysis_Testing---\n                Unit_Tests_to_Detailed_Design---\n                Integration_Tests_to_High_Level_Design---\n                System_Tests_to_Software_Requirements---\n                Unit_Test_Execution---\n                Integration_Test_Execution---\n                Functional_Test_Execution---\n                System_Test_Execution---\n                Acceptance_Test_Execution---\n                Test_Results_Evaluation---\n                Error_Evaluation_Resolution---\n                Final_Test_Report\n            end\n        Types_of_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW---Testing_Tasks\n        end\nend\n\n\n\n\n\n\n\n\n\n\nDevelopment Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Development_Testing\n            direction LR\n            subgraph unit_level_testing\n                direction TB    \n                focus_on_early_examination_of_sub_program_functionality---\n                ensure_functionality_invisible_at_system_level_examined---\n                ensure_quality_software_units_furnished_for_integration\n            end\n            subgraph integration_level_testing\n                direction TB\n                focuses_on_transfer_of_data---\n                focuses_on_control_across_program's_internal_and_external_interfaces\n            end\n            subgraph system_level_testing\n                direction TB\n                demonstrate_all_specified_functionality_exists---\n                demonstrate_SW_is_trustworthy---\n                verifies_as_built_program's_functionality_and_performance_on_requirements---\n                addresses_functional_concerns_and_intended_uses---\n                like_Performance_issues---\n                like_Responses_to_stress_conditions---\n                like_Operation_of_internal_and_external_security_features---\n                like_Effectiveness_of_recovery_procedures---\n                like_disaster_recovery---\n                like_Usability---\n                like_Compatibility_with_other_SW---\n                like_Behavior_in_each_of_the_defined_hardware_configurations---\n                like_Accuracy_of_documentation\n            end\n            subgraph Error_Detected\n                direction TB        \n                should_be_logged---\n                should_be_classified---\n                should_be_reviewed---\n                should_be_resolved_before_SW_release\n            end\n        unit_level_testing-->integration_level_testing-->system_level_testing-->Error_Detected\n        end\nend\n\n\n\n\n\n\n\n\n\n\nUser Site Testing\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph User_Site_Testing\n            direction LR\n            subgraph Quality_System_Rregulation\n                direction TB\n                installation---\n                inspection_procedures---\n                testing_appropriateness---\n                documentation_of_inspection---\n                testing_to_demonstrate_proper_installation\n            end\n            subgraph Understand_Terminology\n                direction TB\n                beta_test---\n                site_validation---\n                user_acceptance_test---\n                installation_verification---\n                installation_testing\n            end\n            subgraph Testing\n                direction TB\n                subgraph Requirements\n                    direction TB\n                    either_actual_or_simulated_use---\n                    verification_of_intended_functionality---\n                    constant_contact_FDA_center\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n    \n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_System_Ability\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_User_Ability\n                        direction TB\n        \n                    end \n                    subgraph Evaluation_of_Operator_Ability\n                        direction TB\n        \n                    end\n                constant_contact_FDA_center-->Follow_Predefiened_Plan-->Documented_Evidence-->\n            Evaluation_of_System_Ability-->Evaluation_of_User_Ability-->Evaluation_of_Operator_Ability    \n                end \n                        \n            \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements-->User_Site_Testing_Task\n        end\n        Quality_System_Rregulation-->    Understand_Terminology-->Testing-->User_Site_Testing_Task\n        end\nend\n\n\n\n\n\n\n\n\n\n\nTesting\n\n\n\n\nflowchart TB\n            subgraph Testing\n                direction LR\n                subgraph Requirements\n                    direction LR\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n                        either_actual_or_simulated_use---\n                        verification_of_intended_functionality---\n                        constant_contact_FDA_center---\n                        formal_summary_of_testing---\n                        record_of_formal_acceptance\n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n                        testing_plan_of_full_range_of_operating_conditions---\n                        testing_plan_to_detect_any_latent_faults---\n                        all_testing_procedures---\n                        test_input_data---\n                        test_results---\n                        hardware_installation_and_configuration---\n                        software_installation_and_configuration---\n                        exercising_measure_of_all_system_components---\n                        versions_of_all_system_components           \n                    end\n                    subgraph Evaluation\n                        direction TB\n                      subgraph Evaluation_of_System_Ability\n                            direction TB\n                            high_volume_of_data---\n                            heavy_loads_or_stresses---\n                            security\n                            subgraph fault_testing\n                                direction TB\n                                avoidance---\n                                detection---\n                                tolerance---\n                                recovery\n                            end\n                        security---fault_testing---\n                        error_message---\n                        implementation_of_safety_requirements\n                        end\n                      subgraph Evaluation_of_User_Ability\n                            direction TB\n                            ability_to_understand_system---\n                            ability_to_interface_with_system\n                        end \n                        subgraph Evaluation_of_Operator_Ability\n                            direction TB\n                            ability_to_perform_intended_functions---\n                            ability_to_respond_in_alarms---\n                            ability_to_respond_in_warnings---\n                            ability_to_respond_in_error_messages\n                        end\n\n                    end\n            Follow_Predefiened_Plan-->Documented_Evidence-->\n            Evaluation_of_System_Ability-->Evaluation_of_User_Ability-->Evaluation_of_Operator_Ability    \n            end     \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements-->User_Site_Testing_Task\n        end"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#maintenance-and-software-changes",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#maintenance-and-software-changes",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Maintenance and Software Changes",
    "text": "Maintenance and Software Changes\n\n\n\n\nflowchart LR\n    subgraph Hardware_VS_Software\n        direction LR\n        subgraph HW_maintenance_Inclusion\n            direction TB\n            preventive_hardware_maintenance_actions--- \n            component_replacement---\n            corrective_changes\n        end\n        subgraph SW_maintenance_Inclusion\n            direction TB\n            corrective---\n            perfective---\n            adaptive_maintenance---\n            not_include_preventive_maintenance_actions---\n            not_include_software_component_replacement\n        end\n    end\n    subgraph Maintenance_Type\n        direction TB\n        Corrective_maintenance---\n        Perfective_maintenance---\n        Adaptive_maintenance---\n        Sufficient_regression_analysis---\n        Sufficient_regression_testing\n    end\n    subgraph Factors_of_Validation_for_SW_change\n        direction TB\n        type_of_change---\n        development_products_affected---\n        impact_of_those_products_on_operation\n    end\n    subgraph Factors_of_Limitting_Validation_Effort\n        direction TB\n        documentation_of_design_structure---\n        documentation_of_interrelationships_of_modules---\n        documentation_of_interrelationships_of_interfaces---\n        test_documentation---\n    test_cases---\n        results_of_previous_verification_and_validation_testing\n    end\n    subgraph Maintenance_tasks\n        direction TB\n        Software_Validation_Plan_Revision---\n        Anomaly_Evaluation---\n        Problem_Identification_and_Resolution_Tracking---\n        Proposed_Change_Assessment---\n        Task_Iteration---\n        Documentation_Updating\n    end\nHardware_VS_Software-->Maintenance_Type-->Factors_of_Validation_for_SW_change-->\nFactors_of_Limitting_Validation_Effort-->Maintenance_tasks"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-of-quality-system-software",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-of-quality-system-software",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation of Quality System Software",
    "text": "Validation of Quality System Software\n\nOverview\n\n\n\n\nflowchart LR\n    subgraph Use_of_Computers_and_automated_equipment\n        direction TB\n        medical_device_design---\n        laboratory_testing_and_analysis---\n        product_inspection_and_acceptance---\n        production_and_process_control---\n        environmental_controls---\n        packaging---\n        labeling---\n        traceability---\n        document_control---\n        complaint_management---\n        programmable_logic_controllers---\n        digital_function_controllers---\n        statistical_process_control---\n        supervisory_control_and_data_acquisition---\n        robotics---\n        human_machine_interfaces---\n        input_output_devices---\n        computer_operating_systems\n    end\n    subgraph Factors_in_Validation\n        direction TB\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System-->Validation_Supporting_Factors-->Factors_of_Validation_Evidence_Level-->\nFactors_of_Easing_Validation_Effort\n    end\n    subgraph Documented_User_Requirements\n        direction TB\n        intended_use_of_software_or_automated_equipment---\n      level_of_dependency_on_software_or_equipment\n    end\n    subgraph List_That_Must_Be_Defined_by_User\n        direction TB\n        \n    end\n    subgraph Documentation_List\n        direction TB\n        documented_protocol---\n        documented_validation_results\n        subgraph Documented_Test_Cases\n            direction TB\n        \n        end\n        documented_validation_results---Documented_Test_Cases\n    end\n\n    subgraph Manufaturer's_Responsbility\n        direction TB\n        \n    end\nUse_of_Computers_and_automated_equipment---Factors_in_Validation---Documented_User_Requirements---\nList_That_Must_Be_Defined_by_User---Documentation_List---Manufaturer's_Responsbility\n\n\n\n\n\n\n\n\n\n\nFactors in Validation\n\n\n\n\nflowchart LR\n    subgraph Factors_in_Validation\n        direction LR\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n                electronic_records_regulation---\n                electronic_signatures_regulation---\n                regulations_establishment---\n                security---\n                data_integrity---\n                validation_requirements \n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n            verifications_of_outputs_from_each_stage--- \n            verifications_of_outputs_throught_SW_life_cycle---\n            checking_for_proper_operation_in_intended_use_environment\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n            risk_posed_by_automated_operation---\n            complexity_of_process_software---\n            degree_of_dependence_on_automated_process\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System-->Validation_Supporting_Factors-->Factors_of_Validation_Evidence_Level-->\nFactors_of_Easing_Validation_Effort\n    end"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html",
    "title": "Content List, Validation",
    "section": "",
    "text": "0000-00-00, EN62304"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#fda",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#fda",
    "title": "Content List, Validation",
    "section": "2 FDA",
    "text": "2 FDA\n\n2023-01-27, General Principles of SW Validation\n2023-01-27, General Principles of SW Validation - Diagram Summary\n1111-11-11, Guidance for the Content of Premarket Submissions for Software Contained in Medical Devices"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#dhf",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#dhf",
    "title": "Content List, Validation",
    "section": "3 DHF",
    "text": "3 DHF"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#public-health",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#public-health",
    "title": "Content List, Validation",
    "section": "4 Public Health",
    "text": "4 Public Health"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#wet-lab",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#wet-lab",
    "title": "Content List, Validation",
    "section": "5 Wet Lab",
    "text": "5 Wet Lab\n\n0000-00-00, PCR (Polymerase Chain Reaction) Experiment"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html",
    "title": "Storage and Database",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\nStorage Types \n\nBlock Storage: split into fixed size chunk of data\n\neasy to change small parts: change only block/piece of the data\nfor frequently modified data, or data with a high trasaction rate (like app or system files)\n\nObject Storage: each file = single unit of data\n\nto change even small parts: need to update the entire file\nfor WORM(write once, read many) model\n\naccessed often, but modified rarely (like photo)\n\n\nFile Storage: tree-like hierarchy (folders → subfolders)\n\nsimilar to windows file explorer or MacOS Finder\nfor files shared/managed by multiple host computers\n\nuser home directories, developmental environments\n\n\n\n\n\n\n\n\n\nblock storage\n\nboot volume for OS or data volume\n\nblock storages for EC2 instances\n\n\n\n\nblock storages for EC2 instances with EBS\n\n\n\nEC2 instance store: internal storage, ephemeral storage\n\ndirectly(physically) attached: fast, quick response\nlife cycle is tied to the instance: lose data when stop/terminate the instance\n\nAmazon Elastic Block Store(Amazon EBS): external storage, persistent storage\n\nseparate from EC2, user-configured size\none-to-one with EC2 instances (can’t be shared/attached to multiple instances): secure communication\n\ncan be detached → attached to another instance in the same AZ\nfor multiple attachements: need to use Amazon EBS Multi-Attach\n\nTypes of EBS\n\nSSD backed volumes: for random I/O\nHDD backed volumes: for sequential I/O\n\nbacking up data: (EBS)Snapshot\n\n\n\n\n\n\nfor employee photos: can’t use amazon EBS\n\ncan’t be attached to instances when the app scales\nhas size limitations\n\namazon simple storage service(Amazon S3)\n\nstandalone, don’t mount onto EC2 instances\naccess data through URL: storage for the internet\n\nsize limit for a single object: 5Tb\nflat structure: use unique identifiers(?)\ndistributed storage: store data across multiple different facilities within one AWS region\n\ndurable storage system\n\n\nS3 buckets: objects is stored in buckets\n\nneed to create buckets first\ncan make folders inside\nregion specific\nname: should be globally unique across AWS accounts, DNS compliant (no special characters, spaces, etc.)\n\nURL will be constructed using the name → should be reached with HTTP/HTTPS\n\nbucket URL → append object key to bucket URL\n\n\n\nAccesss control\n\ndefault: everything in S3 is private → can give public access\n\nto make object publically access, need to change bucket settings\n\npolicies \n\nIAM policies\nS3 bucket policies\n\nJSON format (like IAM policies)\nonly placed on buckets (can apply for another AWS accounts or anonymous users)\n\nnot for folders/objects\n\n\n\n\n\n\n\n\nRelational database(RDB): data를 table 형태로 저장, 서로 다른 table간 data는 relationship으로 연결됨\n\n\n\nRDB\n\n\n\nTable은 row와 column으로 구성\nrow는 record라고도 부르며 특정 개체에 관련된 모든 정보를 담고 있음\ncolumn은 attribute라고도 부르며 개체의 각 속성을 나타냄\n\nSchema: 각 table 별 관계 및 column에 들어갈 data type 등을 정의한 것\n\nschema는 한 번 설정하고 나면 변경하기 어려움\n예시: MySQL, PostgresQL, Oracle, SQL server, Amazon Aurora\n일반적으로 SQL query를 사용해서 data 접근 및 수정\n\n장점\n\nJoins: table을 join하여 data간 관계를 쉽게 이해 가능\nReduced redundancy: 일부 attribute만 다른 경우 table을 나누어 중복 정보가 저장되는 것을 방지할 수 있음\nFamiliarity: 오래된 기술이기 때문에 자료가 많아서 적응하기 쉬움\nAccuracy: data의 안정성 및 결과 보장 참고\n\n\n사용처\n\nSchema가 거의 변경되지 않는 application들\n작업 및 data의 안정성이 필요한 분야 전반\nEnterprise Resource Planning (ERP) applications\nCustomer Relationship Management (CRM) applications\nCommerce and financial applications\n\nRDB on AWS\n\nManaged database: EC2 or AWS database service\n\n\n\nEC2 or AWS database service\n\n\n\nAmazon RDS: AWS에서 제공하는 RDB service\n\nCommercial: Oracle, SQL Server\nOpen Source: MySQL, PostgreSQL, MariaDB\nCloud Native: Amazon Aurora Note: The cloud native option, Amazon Aurora, is a MySQL and PostgreSQL-compatible database built for the cloud. It is more durable, more available, and provides faster performance than the Amazon RDS version of MySQL and PostgreSQL\n\nDB instance type - 아래 type 내에서 size 별 선택지 존재\n\nStandard, which include general-purpose instances\nMemory Optimized, which are optimized for memory-intensive applications\nBurstable Performance, which provides a baseline performance level, with the ability to burst to full CPU usage.\n\nDB storage - the DB instance uses Amazon Elastic Block Store (EBS) volumes as its storage layer\n\n용량: 20~65536Gb\nGeneral purpose (SSD)\nProvisioned IOPS (SSD)\nMagnetic storage (not recommended)\n\nDB subnet group\n\nDB를 사용하기 위해서 VPC 및 subnet 설정 필요 => availability zone 내 subnet 지정 필요\nDB subnet은 private해야 됨 - gateway에 직접 연결 금지 for 보안\n보안의 경우 ACL 및 security group으로 통제 가능 - network section 참고\n\nIAM policy\n\nDB subnet group은 traffic을 조절\nIAM policy는 data와 table에 대한 접근 및 수정 권한을 조절\n\nBackup\n\nAutomatic\n\ndefault로 설정\nlog 및 DB instance 자체를 백업\n주기: 0~35일 0일의 경우 automatic 백업을 disable, 기존 backup도 삭제됨\n방식: point-in-time => 특정 기간 내 일어난 transaction에 대해서 recovery\n\nManual snapshot\n\n35일보다 긴 기간에 대해 backup할 때 사용\n\nBackup recovery: 새 instance를 생성\n\nRedundancy\n\nMulti-AZ를 허용할 경우, Amazon RDS가 다른 AZ에 redundant copy 생성\nPrimary copy: 평소에 사용하는 copy\nStandby copy: primary copy에 접근이 불가한 경우 사용하는 copy\n두 copy간 싱크로는 자동 유지\nDB instance 생성시 DNS를 설정하면 AWS가 이를 인식하여 자동으로 failover 수행\nRedundant copy는 subnet에 존재해야 됨\n\nAmazon DynamoDB\n\nFully managed NoSQL database service: provides fast and predictable performance with seamless scalability\nServerless\n\nRDB와 달리 size 제한 없음\n자동 scale 조절\nSSD에 자동 저장되며 replication 또한 자동 수행\nNo schema\n\n\n저장된 데이터 양과 접근 횟수에 따라 과금\n구성 요소\n\nTable: RDB와 유사하게 item의 집합으로 구성\nItem: 다른 item과 unique하게 구분가능한 data, 개수 제한 없음, attribute의 조합으로 구성됨, RDB와 달리 각 item의 attribute 개수가 다를 수 있음 RDB의 row에 대응\nAttribute: RDB와 달리 같은 attribute라도 다양한 type의 정보를 저장할 수 있음? RDB의 column에 대응\n\nAWS Database Services \n\n\n\n\n\n\n\n\n\nDatabase Type\nUse Cases\nAWS Service\n\n\n\n\nRelational\nTraditional applications, ERP, CRM, e-commerce\nAmazon RDS, Amazon Aurora, Amazon Redshift\n\n\nKey-value\nHigh-traffic web apps, e-commerce systems, gaming applications\nAmazon DynamoDB\n\n\nIn-memory\nCaching, session management, gaming leaderboards, geospatial applications\nAmazon ElastiCache for Memcached, Amazon ElastiCache for Redis\n\n\nDocument\nContent management, catalogs, user profiles\nAmazon DocumentDB (with MongoDB compatibility)\n\n\nWide column\nHigh-scale industrial apps for equipment maintenance, fleet management, and route optimization\nAmazon Keyspaces (for Apache Cassandra)\n\n\nGraph\nFraud detection, social networking, recommendation engines\nAmazon Neptune\n\n\nTime series\nIoT applications, DevOps, industrial telemetry\nAmazon Timestream\n\n\nLedger\nSystems of record, supply chain, registrations, banking transactions\nAmazon QLDB\n\n\n\n\n선택 기준\n\nRDB: 데이터 간 관계가 복잡하고 별도 관리가 필요한 경우에 사용 복잡도에 의해 overhead가 발생하기 때문\nKey-value DB: Large scale, low latency 보장, 단순 데이터 저장 및 조회 목적으로 적합 => RDB에서는 여러 table에 나누어 저장해야 되는 정보를 한 table에 저장 가능\nGraph: SNS와 같은 관계형 자료구조에 적합\nLedger: 금융과 같은 안정성, 변경 불가가 필요한 자료를 저장하는 경우에 적합\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Blog Content List"
  },
  {
    "objectID": "docs/blog/posts/DL/2023-03-10_cnn/index.html#batch-processing",
    "href": "docs/blog/posts/DL/2023-03-10_cnn/index.html#batch-processing",
    "title": "CNN",
    "section": "2.2 Batch Processing",
    "text": "2.2 Batch Processing\n데이터를 (데이터수, 채널 수, 높이, 너비) \\(= (N,C,H,W)\\) 순으로 저장하여 처리하여 NN에 4차원 데이터가 하나가 흐를 때마다 데이터 N개의 합성곱 연산이 발생한다. N번의 처리를 한번에 수행한다."
  },
  {
    "objectID": "docs/projects/LLFS/statistical_approach.html",
    "href": "docs/projects/LLFS/statistical_approach.html",
    "title": "Statistical Approach",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 Go to Project Content List\nProject Content List\n\n\n2 Go to Blog Content List\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/variables/index.html",
    "href": "docs/blog/posts/Mathmatics/variables/index.html",
    "title": "Variable Types",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nVariable types can be classified with various perspectives depending on research purpose:\n\nFrom the perspective of a data type, variable types are largely divided into two categories:\n\nquantitative variable: a variable containing quantitative data that represents quantity\ncategorical variable: a variable containing qualitative data that represents groups\n\nFrom the standpoint of modeling or experiment designs, variable types are largely divided into 3 categories:\n\nindependent variable: a variable (cause) that might have an effect on a dependent variabe (result).\ndependent variable: a variable (result) that might be influenced by independent variables (cause).\ncontrol variable: a variable that is fixed to look into a relation between an independent variable in your interest and dependent variable.\n\nFrom the point of mathmatical view, variable types are categorized largely into 4 categories:\n\nunivariable: each subject gives rise to a single measurement of independent variable termed exploratory variable.\nunivariate: each subject gives rise to a single measurement of dependent variables termed response.\nmultivariate: each subject gives rise to a vector of measurements of independent variables termed exploratory variables.\nmultivariable: each subject gives rise to a vector of measurements of dependent variables termed responses.\n\n\n\n\n\n\nthe values of the quantitative variables with which you can conduct arithematic operations. There are two types of quantitative variables: discrete and continuous.\n\n\n\nAs integer, count valuess of individual items.\nex: number of people, number of different events, etc.\n\n\n\n\n\nAs real number, measurement values of continuous or uncountable values.\nex: height, weight, distance, volume, age, etc.\n\n\n\n\n\nCategorical variables contain grouping values representing categories rather than quantity. There are three types of categorical variables: binary, nominal, and ordinal variables.\n\n\n\nBinary variables a.k.a dichotomous variables contain two types of values, true or false, 1 or 0\n\nex: disease/non-disease, heads/tails in flipping a coin, win/lose in a game\n\n\n\n\n\ncatogories with no rank or order among them.\nex: gender, races, colors, brands, company names\n\n\n\n\n\ncatogories ranked in a specific order\nex: ranks in a game, places in a line, rating scale responses in a movie review\n\n\n\n\n\n\nExperiments or models are usually designed or built to discover what effect one variable has on another.\n\n\n\nIndependent variable is a variable you can set to observe an effect on the outcome of an experiment.\n\nBy many people, independent Variables are also commonly called predictors, explanatory variables, treatment variables, features, etc.\n\n\n\n\n\nDependent variable is a variable that represents the outcome of the experiment.\nBy many people, dependent variables are also commonly called outcome variables, response variables, targets, etc.\n\n\n\n\n\nControl variable is a variable that is held fixed throughout the experiment.\nPositive control: a variable that is set for showing effect on the dependent variable.\nNegative control: a variable that is set for showing no effect on the dependent variable.\nInternal control: a variable that is set for showing effect on the dependent variable with a researcher’s certain intention.\n\n\n\n\n\n\n\nFYI\n\n\n\nStrictly speaking, the synonyms of independent and dependent variables are all slightly different for the different purpose.\n\nIn association research, the use of the terms “dependent” and “independent” should be avoided because the research does not focus on causality between one another.\n\nWhen the before-and-after relationship is clear, there might be cases where one variable clearly precedes the other\n\nfor example, rainfall leads to mud, rather than the other way around.\nIn these cases, you may call the rainfall a predictor and the mud an outcome variable.\n\n\n\n\n\n\n\n\nThere are largely 3 types of variables: confounders, latent variables, and composite variables\n\n\n\nConfounding variables or confounders\n\nConfounder is a variable that hides the true effect of another variable in an experiment by confounding the association between independent and dependent variables. This can happen when the 3rd variable has effect on both independent variable and dependent variable but the 3rd variable has not been controlled in your experiment. Confounders run a high risk of introducing a variety of research biases to your analysis result, particularly omitted variable bias.\n\nex: When conducting a study on muscle mass increase for dumbbells in a gym, if gender is not included in the research model, gender is a confounder. This is because men and women have different innate muscle mass and baseline for lifting dumbbells.\n\n\n\n\n\nLatent variable is a variable that can’t be measured directly but indirectly via a proxy.\nex: lactose tolerance of a person cannot be measured directly but indirectly inferred from measurements of a person’s can be inferred from measurements of digestion ability with biochemical metrics in a certain designed experiment.\n\n\n\n\n\nComposite variable is a variable made by combining multiple variables of your data. These variables are created not when you measure it but when you analyze data,\nex: When your academic performance is measured with math, physics, literature, and writing composition, your numerical academic performance can be measured by combining math with physics, and your language academic performance by combining literature with writing composition.\n\n\n\n\n\n\nunivariable: each subject gives rise to a single measurement of independent variable termed exploratory variable.\nunivariate: each subject gives rise to a single measurement of dependent variables termed response.\nmultivariate: each subject gives rise to a vector of measurements of independent variables termed exploratory variables.\nmultivariable: each subject gives rise to a vector of measurements of dependent variables termed responses.\n\n\n\n\n\nData types can also be classified with various perspectives depending on research purpose:\n\nFrom the perspective of programming or computer science data type\n\n\n\n\n\n\n\n\n\nData Type\nDefinition\nExamples\n\n\n\n\nInteger (int)\nNumeric data type for numbers without fractions\n-707, 0, 707\n\n\nFloating Point (float)\nNumeric data type for numbers with fractions\n707.07, 0.7, 707.00\n\n\nCharacter (char)\nSingle letter, digit, punctuation mark, symbol, or blank space\na, 1, !\n\n\nString (str or text)\nSequence of characters, digits, or symbols—always treated as text\nhello, +1-999-666-3333\n\n\nBoolean (bool)\nTrue or false values\n0 (false), 1 (true)\n\n\nEnumerated type (enum)\nSmall set of predefined unique values (elements or enumerators) that can be text-based or numerical\nrock (0), jazz (1)\n\n\nArray\nList with a number of elements in a specific order—typically of the same type\nrock (0), jazz (1), blues (2), pop (3)\n\n\nDate\nDate in the YYYY-MM-DD format (ISO 8601 syntax)\n2021-09-28\n\n\nTime\nTime in the hh:mm:ss format for the time of day, time since an event, or time interval between events\n12:00:59\n\n\nDatetime\nDate and time together in the YYYY-MM-DD hh:mm:ss format\n2021-09-28 12:00:59\n\n\nTimestamp\nNumber of seconds that have elapsed since midnight (00:00:00 UTC), 1st January 1970 (Unix time)\n1632855600\n\n\n\n\nFrom the perspective of data measurement\n\nlongitudinal (or repeated) data: Each subject gives rise to a vector of measurements, but these represent the same response measured at a sequence of observation times\ncross-sectional data : Outcome variable(s) and covariates that are measured at a single time point\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScribbr\n\n\n\n\n\n\n\n\n\n\n\n\nProject Content List\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/statistics/LDA/intro.html",
    "href": "docs/blog/posts/statistics/LDA/intro.html",
    "title": "LDA (1) - Introduction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nIndividuals are measured repeatedly over time\nThe time when the measurements are taken is not of primary interest and is often considered fixed by design.\nSmall number of observations per subject but relatively large number of subjects.\nThe variability of observed data can be divided into three components:\n\nHeterogeneity between individuals.\nSerial correlation, measurements closely spaced are more similar.\nMeasurement error.\n\n\n\n\n\n\nLongitudinal data analysis (LDA) focuses on\n\nchanges over time within individuals\ndifferences among people in their baseline levels\n\nTypes of LDA\n\nTime series studies\nPanel studies (sociology and economics)\nProspective studies (clinical trials)\n\n\n\n\n\n\n\n\nLongitudinal Study vs Cross-Sectional Study Example\n\n\n\nA cross-sectional study found that older people smoke more.\nPossible explanations:\n\nPeople tend to smoke more when they get older.\nOlder people grew up in an environment where the harm of smoking was less widely accepted. In other words, when older people were younger, smoking was more socially acceptable and its harmful effects were not well-known or well-publicized. As a result, they may have started smoking earlier in life and developed a stronger habit or addiction. This explanation implies that younger people today may be less likely to smoke because they are growing up in an environment where smoking is less socially acceptable and the risks are more widely known.\n\nLDA can distinguish the effect due to aging (i.e., changes over time within subject) from cohort effects (i.e., difference between subjects at baseline). Cross-sectional study cannot.\n\n\n\n\n\n\nEach subject can serve as his/her own control. Influence of genetic make-up, environmental exposures, and maybe unmeasured characteristics tend to persist over time.\n\nin certain types of studies or experiments, individuals can be used as their own comparison group. This means that the same person is tested or measured at different points in time, and the differences observed can be attributed to changes over time rather than to differences between individuals.\nFor example, in a study looking at the effect of a new medication on blood pressure, each participant’s blood pressure before and after taking the medication would be compared to determine if there was a change. By using the same participant as their own control, the effects of genetic factors, environmental exposures, and other individual differences that might influence blood pressure would be minimized.\nHowever, even when using this approach, some individual differences that are not directly measured or controlled for may persist over time and influence the results. These could include factors such as diet, stress levels, or other lifestyle habits that could impact the outcome being measured.\n\nDistinguish the degree of variation in \\(Y\\) across time within a subject from the variation in \\(Y\\) between subjects. With repeated values, one can borrow strength across time for the person of interest as well as across people.\n\nwhen you have repeated measurements of a variable (Y) for the same person over time, you can use the information from those repeated measurements to better estimate the true value of Y for that person at any given time point. This is known as borrowing strength across time.\nAdditionally, you can also use the information from the repeated measurements of Y across different people to better estimate the true value of Y for a particular time point across the population. This is known as borrowing strength across people.\nBy doing both, you can distinguish the degree of variation in Y across time within a subject (i.e., how much Y varies for a particular person over time) from the variation in Y between subjects (i.e., how much Y varies between different people at a particular time point).\n\nIncreased power, by repeated measurements. The repeated measurements from the same subject are rarely perfectly correlated. Hence, longitudinal studies are more powerful than cross-sectional studies.\n\nLongitudinal studies are more powerful than cross-sectional studies because they allow researchers to directly model and account for the within-subject correlation among repeated measurements. In other words, longitudinal studies take advantage of the fact that individuals are their own controls by measuring outcomes at multiple time points, which allows for a more accurate estimation of the true effect of an exposure or intervention.\nIn contrast, cross-sectional studies only measure outcomes at a single time point, which makes it difficult to distinguish between within-subject variability and between-subject variability. In a cross-sectional study, any observed differences between groups may be due to differences in the underlying populations, or due to differences in the timing of the outcome measurement, or both.\nFurthermore, longitudinal studies can also provide information on the rate of change in the outcome over time, which can be important in understanding disease progression, treatment effects, or the impact of other time-dependent factors.\n\nTherefore, because longitudinal studies allow for a more accurate estimation of the true effect of an exposure or intervention and provide more information about disease progression, they are generally considered more powerful than cross-sectional studies.\n\n\n\nLDA requires special statistical methods because the set of observations on one subject tends to be inter-correlated.\nCopied from Diggle et al. (2002, page 2).\n\nConsider the example, variation of readability of child as they get aged.\n\nAssume this is a longitudinal study with two measurements per child at two age or time points.\nThe two measurements per subject may be highly correlated.\nIf we use cross-sectional methods to analyze the data, we may not be able to distinguish changes over time within individual and difference among people in their baseline levels.\n\nOnly plot (a) is from cross sectional study. Not using connected lines might mislead the time trend within subjects.\n\n\nIn general, repeated observations \\(y_{i1}, \\dots , y_{in_i}\\) for subject \\(i\\) are likely to be correlated, so the independence assumption is violated.\nThe standard regression methods (ignoring correlation) may lead to\n\nIncorrect inference\n\nthe violation of the independence assumption means that the errors in the model are correlated across observations, and this correlation can bias the estimated coefficients.\nThe errors in the model are correlated across observations when there is some form of dependence or clustering in the data. This means that the error term in one observation is related to the error terms in other observations, either through some underlying factor or because of the way the data is collected.\nWhen the errors are correlated across observations, the estimated coefficients from standard regression methods may be biased because the regression model assumes that the errors are independent.\nThe bias in the estimated coefficients can arise in several ways:\n\nThe standard errors of the estimated coefficients will be too small, which can lead to overconfidence in the results.\nThe estimated coefficients may not reflect the true relationships between the dependent variable and the independent variables, as the correlation between the errors can distort the estimates.\nThe estimates of the standard errors will be biased, leading to incorrect inference in hypothesis testing and confidence interval construction.\n\nTo sum up, failing to account for the correlation between errors can lead to incorrect and imprecise estimates of the coefficients and standard errors in a regression model.\n\nInefficient estimates of \\(\\beta\\)\n\nthe estimates are less precise than they could be if the correlation between observations were taken into account.\nThe standard errors of the estimates will be too large, which means that confidence intervals will be wider and hypothesis tests will have less power.\n\nOversight of important correlation structure\n\nthe standard regression methods may miss important patterns of correlation in the data that could be used to improve the accuracy and precision of the estimates.\nFor example, if there is a time trend in the data that is not accounted for, the standard errors of the estimates may be too large, and the estimates may not capture the true effect of the independent variables.\nAccounting for the correlation structure in the data can lead to more accurate and precise estimates, and can also help identify interesting patterns and relationships that might otherwise be missed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Content List\n\n\n\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#longitudinal-data-analysis",
    "href": "docs/blog/posts/statistics/guide_map/index.html#longitudinal-data-analysis",
    "title": "Content List, Statistics",
    "section": "Longitudinal Data Analysis",
    "text": "Longitudinal Data Analysis\n\n2023-03-23, LDA (1) - Intro\n2023-03-23, LDA (2) - Concepts\n2023-03-25, LDA (3) - CD4+ Cell Numbers Data Example\n2023-03-25, LDA (4) - Respiratory Infection Data Example\n2023-03-28, LDA (5) - Epileptic Seizures Data Example\n\n\nMixed Models\n\n1111-11-11, Linear Mixed Models"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/optimization/terminology.html",
    "href": "docs/blog/posts/Mathmatics/optimization/terminology.html",
    "title": "Minimizer & Maximizer",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\nA minimizer or a maximizer refer to a point in the domain of a function where the function achieves its minimum or maximum value. More formally, let \\(f: X → \\mathbb R\\) be a real-valued function defined on a set \\(X \\subset \\mathbb R\\).\n\nDefinition 1 \\(f(x^*)\\) with a point \\(x^* \\in X\\) is called a minimum if \\(f(x^*) \\le f(x)\\) for all \\(x \\in X\\). \\[\n\\min_{x\\in X}{f(x)}\n\\] , which means a minimum, \\(f(x)\\).\n\n\nDefinition 2 \\(f(x^*)\\) with a point \\(x^* \\in X\\) is called a maximum if \\(f(x^*) \\ge f(x)\\) for all \\(x \\in X\\). \\[\n\\max_{x\\in X}{f(x)}\n\\] , which means a maximum, \\(f(x)\\).\n\n\nDefinition 3 A point \\(x^* \\in X\\) is called a minimizer of \\(f\\) if \\(f(x^*) \\le f(x)\\) for all \\(x \\in X\\). \\[\n\\arg\\min_{x\\in X}{f(x)}\n\\] , which means \\(x\\) that mainimizes \\(f(x)\\).\n\n\nDefinition 4 A point \\(x^* \\in X\\) is called a maximizer of \\(f\\) if \\(f(x^*) \\ge f(x)\\) for all \\(x \\in X\\). \\[\n\\arg\\max_{x\\in X}{f(x)}\n\\] , which means \\(x\\) that maximizes \\(f(x)\\).\n\n\nDefinition 5 A point \\(x^* \\in X\\) is called a global mainimizer of \\(f\\) if \\(f(x^*) \\ge f(x)\\) for all \\(x \\in \\mathbb R\\). \\[\n\\arg\\min_{x\\in \\mathbb R}{f(x)}\n\\] , which means \\(x\\) that minimizes \\(f(x)\\).\n\n\nDefinition 6 A point \\(x^* \\in X\\) is called a global maximizer of \\(f\\) if \\(f(x^*) \\ge f(x)\\) for all \\(x \\in \\mathbb R\\). \\[\n\\arg\\max_{x\\in \\mathbb R}{f(x)}\n\\] , which means \\(x\\) that maximizes \\(f(x)\\).\n\n\nDefinition 7 A point \\(x^* \\in X\\) is called a strict global maximizer of \\(f\\)\nif \\(f(x^*) > f(x)\\) for all \\(x \\in \\mathbb R\\).\n\n\nDefinition 8 A point \\(x^* \\in X\\) is called a strict global miniimizer of \\(f\\)\nif \\(f(x^*) < f(x)\\) for all \\(x \\in \\mathbb R\\).\n\n\nDefinition 9 A local minimizer of \\(f\\) is a point \\(x* \\in X\\) such that there exists a radius, \\(\\delta>0\\) of a neighborhood of \\(x^*\\) such that \\(f(x*) \\le f(x)\\) for all \\(x \\in (x^*-\\delta,x^*+\\delta)\\) or \\(|x-x^*|<\\delta\\) . A local minimizer is at least best possible solution among nearby points.\n\n\nDefinition 10 A local maximizer of \\(f\\) is a point \\(x* \\in X\\) such that there exists a radius, \\(\\delta>0\\) of a neighborhood of \\(x^*\\) such that \\(f(x*) \\ge f(x)\\) for all \\(x \\in (x^*-\\delta,x^*+\\delta)\\) or \\(|x-x^*|<\\delta\\) . A local maximizer is at least best possible solution among nearby points.\n\n\nDefinition 11 A strict local minimizer of \\(f\\) is a point \\(x* \\in X\\) such that there exists a radius, \\(\\delta>0\\) of a neighborhood of \\(x^*\\) such that \\(f(x*) < f(x)\\) for all \\(x \\in (x^*-\\delta,x^*+\\delta)\\) or \\(|x-x^*|<\\delta\\) . A local minimizer is at least best possible solution among nearby points.\n\n\nDefinition 12 A strict local maximizer of \\(f\\) is a point \\(x* \\in X\\) such that there exists a radius, \\(\\delta>0\\) of a neighborhood of \\(x^*\\) such that \\(f(x*) > f(x)\\) for all \\(x \\in (x^*-\\delta,x^*+\\delta)\\) or \\(|x-x^*|<\\delta\\) . A local maximizer is at least best possible solution among nearby points.\n\n\nDefinition 13 It is said to be a critical point if \\(f'(x)\\) exists and \\(f'(x^*)=0\\) for \\(x^* \\in X\\).\n\nNote that a function may have multiple minimizers, and some functions may not have a minimizer at all. In addition, if a function has multiple minimizers, they may be either global or local minimizers.\n\nExample\n\nIf \\(f(x)=2(x-3)^2+8\\), the vertex is \\((2,8)\\), the global minimizer is \\(2\\), the global minimum is \\(8\\).\nIf \\(f(x)=x^3-3x^2+4\\), then \\(f'(x)=3x^2-6x=3x(x-2)\\) and the critical points are \\((0,4), (2,0)\\)\nIf \\(f(x)=7x^5-35x+4\\), then \\(f'(x)=35x^4-35=35(x^2+1)(x-1)(x+1)\\) critical points are \\((-1,0), (1,32)\\)\nIf \\(f(x)=\\frac{2x}{x^2+1}\\), then \\(f'(x)=\\frac{2(1-x)(1+x)}{(1+x^2)^2}\\) the critical points are \\((-1,-1), (1,1)\\).\n\n\n\nTheorem 1 For \\(f:X \\rightarrow \\mathbb R\\), let \\(f(x)\\), \\(f'(x)\\), and \\(f''(x)\\) be all continuous. For \\(x^* \\in X\\), \\(f'(x^*)=0\\)\n\nIf \\(f''(x)\\ge 0\\) for all \\(x \\in X\\), \\(x^*\\) is a global minimizer\nIf \\(f''(x)> 0\\) for all \\(x \\in X\\), \\(x^*\\) is a strict global minimizer\nIf \\(f''(x^*)> 0\\), \\(x^*\\) is a strict local minimizer\n\n\nthe reverse of the stament 1 in Theorem 1 is not true.\n\n\n\n\n\n\n\n\n1 Go to Blog Content List\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/optimization/minimizer.html",
    "href": "docs/blog/posts/Mathmatics/optimization/minimizer.html",
    "title": "Minimizer & Maximizer",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\nA minimizer or a maximizer refer to a point in the domain of a function where the function achieves its minimum or maximum value. More formally, let \\(f: X → \\mathbb R\\) be a real-valued function defined on a set \\(X \\subset \\mathbb R\\).\n\nDefinition 1 \\(f(x^*)\\) with a point \\(x^* \\in X\\) is called a minimum if \\(f(x^*) \\le f(x)\\) for all \\(x \\in X\\). \\[\n\\min_{x\\in X}{f(x)}\n\\] , which means a minimum, \\(f(x)\\).\n\n\nDefinition 2 \\(f(x^*)\\) with a point \\(x^* \\in X\\) is called a maximum if \\(f(x^*) \\ge f(x)\\) for all \\(x \\in X\\). \\[\n\\max_{x\\in X}{f(x)}\n\\] , which means a maximum, \\(f(x)\\).\n\n\nDefinition 3 A point \\(x^* \\in X\\) is called a minimizer of \\(f\\) if \\(f(x^*) \\le f(x)\\) for all \\(x \\in X\\). \\[\n\\arg\\min_{x\\in X}{f(x)}\n\\] , which means \\(x\\) that mainimizes \\(f(x)\\).\n\n\nDefinition 4 A point \\(x^* \\in X\\) is called a maximizer of \\(f\\) if \\(f(x^*) \\ge f(x)\\) for all \\(x \\in X\\). \\[\n\\arg\\max_{x\\in X}{f(x)}\n\\] , which means \\(x\\) that maximizes \\(f(x)\\).\n\n\nDefinition 5 A point \\(x^* \\in X\\) is called a global mainimizer of \\(f\\) if \\(f(x^*) \\ge f(x)\\) for all \\(x \\in \\mathbb R\\). \\[\n\\arg\\min_{x\\in \\mathbb R}{f(x)}\n\\] , which means \\(x\\) that minimizes \\(f(x)\\).\n\n\nDefinition 6 A point \\(x^* \\in X\\) is called a global maximizer of \\(f\\) if \\(f(x^*) \\ge f(x)\\) for all \\(x \\in \\mathbb R\\). \\[\n\\arg\\max_{x\\in \\mathbb R}{f(x)}\n\\] , which means \\(x\\) that maximizes \\(f(x)\\).\n\n\nDefinition 7 A point \\(x^* \\in X\\) is called a strict global maximizer of \\(f\\)\nif \\(f(x^*) > f(x)\\) for all \\(x \\in \\mathbb R\\).\n\n\nDefinition 8 A point \\(x^* \\in X\\) is called a strict global miniimizer of \\(f\\)\nif \\(f(x^*) < f(x)\\) for all \\(x \\in \\mathbb R\\).\n\n\nDefinition 9 A local minimizer of \\(f\\) is a point \\(x* \\in X\\) such that there exists a radius, \\(\\delta>0\\) of a neighborhood of \\(x^*\\) such that \\(f(x*) \\le f(x)\\) for all \\(x \\in (x^*-\\delta,x^*+\\delta)\\) or \\(|x-x^*|<\\delta\\) . A local minimizer is at least best possible solution among nearby points.\n\n\nDefinition 10 A local maximizer of \\(f\\) is a point \\(x* \\in X\\) such that there exists a radius, \\(\\delta>0\\) of a neighborhood of \\(x^*\\) such that \\(f(x*) \\ge f(x)\\) for all \\(x \\in (x^*-\\delta,x^*+\\delta)\\) or \\(|x-x^*|<\\delta\\) . A local maximizer is at least best possible solution among nearby points.\n\n\nDefinition 11 A strict local minimizer of \\(f\\) is a point \\(x* \\in X\\) such that there exists a radius, \\(\\delta>0\\) of a neighborhood of \\(x^*\\) such that \\(f(x*) < f(x)\\) for all \\(x \\in (x^*-\\delta,x^*+\\delta)\\) or \\(|x-x^*|<\\delta\\) . A local minimizer is at least best possible solution among nearby points.\n\n\nDefinition 12 A strict local maximizer of \\(f\\) is a point \\(x* \\in X\\) such that there exists a radius, \\(\\delta>0\\) of a neighborhood of \\(x^*\\) such that \\(f(x*) > f(x)\\) for all \\(x \\in (x^*-\\delta,x^*+\\delta)\\) or \\(|x-x^*|<\\delta\\) . A local maximizer is at least best possible solution among nearby points.\n\n\nDefinition 13 It is said to be a critical point if \\(f'(x)\\) exists and \\(f'(x^*)=0\\) for \\(x^* \\in X\\).\n\nNote that a function may have multiple minimizers, and some functions may not have a minimizer at all. In addition, if a function has multiple minimizers, they may be either global or local minimizers.\nExample\n\nIf \\(f(x)=2(x-3)^2+8\\), then\nthe vertex is \\((2,8)\\), the global minimizer is \\(2\\), the global minimum is \\(8\\).\nIf \\(f(x)=x^3-3x^2+4\\), then\n\\(f'(x)=3x^2-6x=3x(x-2)\\) and the critical points are \\((0,4), (2,0)\\).\nIf \\(f(x)=7x^5-35x+4\\), then\n\\(f'(x)=35x^4-35=35(x^2+1)(x-1)(x+1)\\) critical points are \\((-1,0), (1,32)\\).\nIf \\(f(x)=\\frac{2x}{x^2+1}\\), then\n\\(f'(x)=\\frac{2(1-x)(1+x)}{(1+x^2)^2}\\) the critical points are \\((-1,-1), (1,1)\\).\nIf \\(f(x)=4x^5-\\frac{20}{3}x^3+4\\), then\n\\(f'(x)=20x^4-20x^2=20(x^2)(x-1)(x+1)\\) critical points are \\((-1,\\frac{20}{3}), (1,\\frac{4}{3})\\).\nIf \\(f(x)=\\frac{(x^2-1)}{(x-2)}\\), then\n\\(f'(x)=\\frac{(x^2-4x+1)}{(x-2)^2}\\), critical points are \\((-1,\\frac{20}{3}), (1,\\frac{4}{3})\\).\nIf \\(f(x)=\\mathrm{e}^{\\sin\\left(x^2+1\\right)}\\), then\n\\(f'(x)=2x\\mathrm{e}^{\\sin\\left(x^2+1\\right)}\\cos\\left(x^2+1\\right)\\) , \\(f''(x)=-2\\mathrm{e}^{\\sin\\left(x^2+1\\right)}\\cdot\\left(2x^2\\sin\\left(x^2+1\\right)-2x^2\\cos^2\\left(x^2+1\\right)-\\cos\\left(x^2+1\\right)\\right)\\), critical points are \\((0,e^{\\sin(1)}), (\\pm\\sqrt{(2n-1)\\frac{\\pi}{2}-1} )\\) where \\(n=1,2,\\dots\\).\nIf \\(f(x)=3x^4-4x^3+1\\), then\n\\(f'(x)=12x^3-12x^2=12x^2(x-1)\\) , \\(f''(x)=36x^2-24x=12x(3x-2)\\), critical points are \\((0,1), (1,0)\\).\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    return 2*(x-3)**2+8\ndef f2(x):\n    return x**3-3*x**2+4\ndef f3(x):\n    return 7*x**5-35*x+4\ndef f4(x):\n    return 2*x/(x**2+1)\ndef f5(x):\n    return 4*x**5-(20/3)*x**3+4\ndef f6(x):\n    return 3*x**4-4*x**3+1\n\ndef df(x):\n    return 4*(x-3)\ndef df2(x):\n    return 3*x**2-6*x\ndef df3(x):\n    return 35*x**4-35\ndef df4(x):\n    return 2*(1-x)*(1+x)/(1+x**2)**2\ndef df5(x):\n    return 20*x**4-20*x**2\ndef df6(x):\n    return 12*x**2*(x-1)\n\ndef ddf(n):\n    return np.repeat(4,n)\ndef ddf2(x):\n    return 6*x-6\ndef ddf3(x):\n    return 140*x**3\ndef ddf4(x):\n    return (4*x*(x**2-3))/(x**2+1)**3\ndef ddf5(x):\n    return 80*x**3-40*x\ndef ddf6(x):\n    return 12*x*(3*x-2)\n\n\n\n\nCode\n# Create a range of x values\nx = np.linspace(-2, 8, 1000)\n\n# Plot the function\nplt.plot(x, f(x), label=r'$f(x)=2(x-3)^2+8$')\nplt.plot(x, df(x), label=r'$df(x)=4(x-3)$')\nplt.plot(x, ddf(len(x)), label=r'$d^2f(x)=4$')\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nx = np.linspace(-1, 4, 1000)\n\nplt.plot(x, f2(x), label=r'$f(x)=x^3-3x^2+4$')\nplt.plot(x, df2(x), label=r'$df(x)=3x^2-6x=3x(x-2)$')\nplt.plot(x, ddf2(x), label=r'$d^2f(x)=6x-6=6(x-1)$')\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\n# Create a range of x values\nx = np.linspace(-1.2, 1.2, 1000)\n\n# Plot the function\nplt.plot(x, f3(x), label=r'$f(x)=7x^5-35x+4$')\nplt.plot(x, df3(x), label=r'$df(x)=35x^4-35=35(x^2+1)(x-1)(x+1)$')\nplt.plot(x, ddf3(x), label=r'$d^2f(x)=120x^3$')\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nx = np.linspace(-2, 4, 1000)\n\nplt.plot(x, f4(x), label=r'$\\frac{2x}{x^2+1}$')\nplt.plot(x, df4(x), label=r'$df(x)=\\frac{2(1-x)(1+x)}{(1+x^2)^2}$')\nplt.plot(x, ddf4(x), label=r'$d^2f(x)=\\frac{4x(x^2-3)}{(1+x^2)^3}$')\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nx = np.linspace(-2, 2, 1000)\n\n# Plot the function\nplt.plot(x, f5(x), label=r'$f(x)=4x^5-\\frac{20}{3}x^3+4$')\nplt.plot(x, df5(x), label=r'$df(x)=20x^4-20x^2=20(x^2)(x-1)(x+1)$')\nplt.plot(x, ddf5(x), label=r'$d^2f(x)=80x^3-40x=40x(x-1)(x+1)$')\n\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\nplt.axis([-2, 2, -20, 20])\nplt.legend()\nplt.show()\n\n\n\n\n\n\nI felt too annoyed to type this latex and make the function…\n\nTheorem 1 For \\(f:X \\rightarrow \\mathbb R\\), let \\(f(x)\\), \\(f'(x)\\), and \\(f''(x)\\) be all continuous. For \\(x^* \\in X\\), \\(f'(x^*)=0\\)\n\nIf \\(f''(x)\\ge 0\\) for all \\(x \\in X\\), \\(x^*\\) is a global minimizer\nIf \\(f''(x)> 0\\) for all \\(x \\in X\\), \\(x^*\\) is a strict global minimizer\nIf \\(f''(x^*)> 0\\), \\(x^*\\) is a strict local minimizer\n\n\nthe reverse of the stament 1 in Theorem 1 is not true.\nCounter Example\nIf \\(f(x)=3x^4-4x^3+1\\), then\n\\(f'(x)=12x^3-12x^2=12x^2(x-1)\\) , \\(f''(x)=36x^2-24x=12x(3x-2)\\), critical points are \\((0,1), (1,0)\\).\n\\(x^*=1\\) is a global minimizer. For \\(x\\in \\mathbb R\\), \\(f''(x)\\ge 0\\) is not true.\n\n\nCode\nx = np.linspace(-1, 1.5, 1000)\n\n# Plot the function\nplt.plot(x, f6(x), label=r'$f(x)=3x^4-4x^3+1$')\nplt.plot(x, df6(x), label=r'$df(x)=12x^3-12x^2=12x^2(x-1)$')\nplt.plot(x, ddf6(x), label=r'$d^2f(x)=36x^2-24x=12x(3x-2)$')\n\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\nplt.axis([-1, 1.5, -5, 5])\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 Go to Blog Content List\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/Language/pipeline.html",
    "href": "docs/blog/posts/Language/pipeline.html",
    "title": "Tensorflow - Data Input Pipeline",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nTensorflow 공식 문서 중 Dataset에 대한 기능 및 성능에 대한 비교 자료\n\n\n\n\n\n\nNote\n\n\n\nThe tf.data API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations. The tf.data API introduces a tf.data.Dataset abstraction that represents a sequence of elements, in which each element consists of one or more components. For example, in an image pipeline, an element might be a single training example, with a pair of tensor components representing the image and its label. Source: https://www.tensorflow.org/guide/data\n\n\n\n\n\n어떠한 데이터의 형태가 오더라도 Dataset object 자체가 iterative한 interface를 제공해서 for loop 등의 iteration을 이용하여 데이터의 입력 형태가 변경되어도 코드의 일관성을 유지할 수 있음\n\nTo create an input pipeline, you must start with a data source. For example, to construct a Dataset from data in memory, you can use tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices(). Alternatively, if your input data is stored in a file in the recommended TFRecord format, you can use tf.data.TFRecordDataset().\n\n\nCode\nimport tensorflow as tf\nimport pathlib\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nnp.set_printoptions(precision=4)\n\ndataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\ndataset\n\n\n<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n\n\nOnce you have a Dataset object, you can transform it into a new Dataset by chaining method calls on the tf.data.Dataset object. For example, you can apply per-element transformations such as Dataset.map, and multi-element transformations such as Dataset.batch. Refer to the documentation for tf.data.Dataset for a complete list of transformations. The Dataset object is a Python iterable. This makes it possible to consume its elements using a for loop:\n\n\nCode\nfor z in dataset:\n    print(z.numpy())\n\n\n8\n3\n0\n8\n2\n1\n\n\n\n\n\n\n\nDataset을 사용은 prefetch 기능과 interleaving 기능에 의한 연산 속도를 향상 시킬수있다.\n\n사용할 수 없는 (또는 사용하지 말아야 할) 특별한 이유가 없다면 필수로 사용해야 할 듯\n\nPipeline을 쓸 때 연산 처리 속도가 빨리지는 이유 -> cpu architecture 관련\n\nX86, ARM 프로세서\n\n실행을 위해 D램 올라감 -> cpu에서 하나의 명령이 거치는 step들 fetch(cpu에 로드), decode(해석), execution(수행) -> load (다시 메모리 이동)\n각 단계에서 수행 소요 시간 존재\n\n\n파이프라인 구조를 가지면, 한 스텝 수행 시 다음 데이터가 다른 스텝 수행 가능 -> 시간 단축\n\nTensor Flow Pipeline\n\n\n앞쪽 데이터 트레이닝 동안 다음 데이터를 미리 읽어옴\n\n\n\n어떤 작업이 끝나기 전에 dependency 없는 다른 작업 수행(async 하게)\n\n\n\n같은 데이터를 반복적으로 사용시 한번 읽은 데이터를 계속 메모리에 가지고 있음\n\n\n\n\n\n\nfloating point 로 인한 error 누적\n\n\n\nTensorflow와 Numpy로 구현한 값에 오차가 발생하는 이유는 Tensorflow를 어떻게 compile 하였느냐에 따른 차이.\n\nFloating point (IEEE-754)에서는 값의 표현에 대한 정의만 있고 실제 연산은 processor vendor마다 다르므로 약간의 오차가 있을 수 있으며, 부동소수점 연산기능을 지원하는 명령들 중 어떠한 명령을 사용하도록 compile하였느냐에 따라 계산값에 차이 발생 가능하고 대부분 무시하지만 오차가 누적이 되면 error rate에 영향이 있을 수 있음.\n부동소수점의 precision이 낮을수록 overfitting 가능성 저하되어 오히려 학습이 잘될 수도 있으며 모델을 경량화 할 수 있음 \\(\\rightarrow\\) bfloat16 type이 생기게 된 이유.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/statistics/LDA/CD4_plus.html",
    "href": "docs/blog/posts/statistics/LDA/CD4_plus.html",
    "title": "LDA (2) - CD4+ Cell Depletion",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\nGoals\n\nCharacterize the time course of CD4+ cell depletion.\nIdentify factors which predict CD4+ cell changes.\nCharacterize the degree of heterogeneity across men in the rate of progression. Data are highly unbalanced with irregular observation times and numbers.\n\nIn a usual regression analysis,\n\\[\n\\begin{aligned}\n    \\mathbf Y&=\\mathbf X\\mathbf\\beta+\\mathbf\\epsilon\\\\\n    y_{ij}&=\\beta x_{ij}+\\epsilon_{ij}\n\\end{aligned}\n\\] where \\(i\\) = the \\(i\\) th person, \\(j\\)= the \\(j\\) th observation, and \\(\\epsilon_{ij}\\)= the \\(j\\) th measurement error of the \\(i\\) th person.\nSince repeated measurements on a subject are likely correlated, the assumption of independence is likely to be violated.\nTo see the correlation structure within each subject, * First regress \\(y_{ij}\\) onto \\(x_{ij}\\) using the ordinary least-squares (OLS) and obtain the residuals \\(r_{ij} = y_{ij} - \\hat{\\beta_{ij}}x_{ij}\\) or \\[\\mathbf R= \\mathbf Y - \\mathbf X\\hat{\\mathbf\\beta } \\].\n* Then create the scatter plots of residuals at different time points (or time intervals if the time points are not regular).\n\n\nCode\n# this file contains code for linear marginal models for longitudinal data\n# We test different covariance patterns and show how to fit model with WLS and REML\n# By Gen Li, 1/1/2018\n# also check lme4\n\n\nlibrary(nlme)\nlibrary(tidyverse)\nopposites <- read.table(\"https://stats.idre.ucla.edu/stat/r/examples/alda/data/opposites_pp.txt\",header=TRUE,sep=\",\")\nhead(opposites)\n\n# spaghetti plot\np = ggplot(opposites, aes(time, opp, group=id)) + geom_line()\nprint(p)\n\n\n# Fit different cov model with REML\n###################################################\n# unstructured covariance for marginal model\nunstruct <- gls(opp~time*ccog,opposites, correlation=corSymm(form = ~ 1 |id),  weights=varIdent(form = ~ 1| wave),method=\"REML\")\n\n# corSymm(form = ~ 1 |id) : the same covariance across different measurement, same correlation matrix for different subjects\n# check ?gls ?corClasses ?corSymm\nsummary(unstruct) # focus on corr, var, (weight)\nunstruct$modelStruct$corStruct # corr\nunstruct$modelStruct$varStruct # variance:weight\nunstruct$sigma # standard deviation\ncov2cor(unstruct$varBeta)\n\n\n# compound symmetry\ncomsym <- gls(opp~time,opposites, correlation=corCompSymm(form = ~ 1|id),   weights=varIdent(form = ~ 1| wave), method=\"REML\")\nsummary(comsym)\ncorMatrix(comsym$modelStruct$corStruct)[[1]]\n\n\n\n# AR(1)\nauto1 <- gls(opp~time ,opposites, correlation=corAR1(form = ~ 1 |id), method=\"REML\")\nsummary(auto1)\ncorMatrix(auto1$modelStruct$corStruct)[[1]]\n\n\n\n\n\n\n\n\n\n\n1 Go to Project Content List\nProject Content List\n\n\n2 Go to Blog Content List\nBlog Content List"
  }
]