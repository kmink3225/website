<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kwangmin Kim">
<meta name="dcterms.date" content="2023-03-30">
<meta name="description" content="template">

<title>Kwangmin Kim - Basics (1) - Vector Operations</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../../">
<script src="../../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../../.././images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../../../index.html">
    <span class="navbar-title">Kwangmin Kim</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../docs/CV/index.html">
 <span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../docs/projects/index.html">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../docs/blog/index.html">
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../about.html">
 <span class="menu-text">Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kmink3225"><i class="bi bi-github" role="img" aria-label="Github">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kwangmin-kim-a5241b200/"><i class="bi bi-linkedin" role="img" aria-label="Linkedin">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../../docs/blog/index.html"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-linear-algebra-to-deep-learning" id="toc-introduction-linear-algebra-to-deep-learning" class="nav-link active" data-scroll-target="#introduction-linear-algebra-to-deep-learning"><span class="toc-section-number">1</span>  Introduction Linear Algebra to Deep Learning</a>
  <ul class="collapse">
  <li><a href="#scalar" id="toc-scalar" class="nav-link" data-scroll-target="#scalar"><span class="toc-section-number">1.1</span>  Scalar</a></li>
  <li><a href="#vector" id="toc-vector" class="nav-link" data-scroll-target="#vector"><span class="toc-section-number">1.2</span>  Vector</a>
  <ul class="collapse">
  <li><a href="#plotting-vectors-on-the-coordinate-plane" id="toc-plotting-vectors-on-the-coordinate-plane" class="nav-link" data-scroll-target="#plotting-vectors-on-the-coordinate-plane"><span class="toc-section-number">1.2.1</span>  Plotting Vectors on the Coordinate Plane</a></li>
  </ul></li>
  <li><a href="#matrix" id="toc-matrix" class="nav-link" data-scroll-target="#matrix"><span class="toc-section-number">1.3</span>  Matrix</a></li>
  <li><a href="#basic-vector-operations" id="toc-basic-vector-operations" class="nav-link" data-scroll-target="#basic-vector-operations"><span class="toc-section-number">1.4</span>  Basic Vector Operations</a>
  <ul class="collapse">
  <li><a href="#addition-of-vectors" id="toc-addition-of-vectors" class="nav-link" data-scroll-target="#addition-of-vectors"><span class="toc-section-number">1.4.1</span>  Addition of Vectors</a></li>
  <li><a href="#subtraction-of-vectors" id="toc-subtraction-of-vectors" class="nav-link" data-scroll-target="#subtraction-of-vectors"><span class="toc-section-number">1.4.2</span>  Subtraction of Vectors</a></li>
  <li><a href="#scalar-multiplication-of-vectors" id="toc-scalar-multiplication-of-vectors" class="nav-link" data-scroll-target="#scalar-multiplication-of-vectors"><span class="toc-section-number">1.4.3</span>  Scalar Multiplication of Vectors</a></li>
  <li><a href="#dot-product-of-vectors" id="toc-dot-product-of-vectors" class="nav-link" data-scroll-target="#dot-product-of-vectors"><span class="toc-section-number">1.4.4</span>  Dot Product of Vectors</a></li>
  <li><a href="#cross-product-of-vectors" id="toc-cross-product-of-vectors" class="nav-link" data-scroll-target="#cross-product-of-vectors"><span class="toc-section-number">1.4.5</span>  Cross Product of Vectors</a></li>
  </ul></li>
  <li><a href="#column-vector-row-vector" id="toc-column-vector-row-vector" class="nav-link" data-scroll-target="#column-vector-row-vector"><span class="toc-section-number">1.5</span>  Column Vector &amp; Row Vector</a></li>
  <li><a href="#basic-matrix-operations" id="toc-basic-matrix-operations" class="nav-link" data-scroll-target="#basic-matrix-operations"><span class="toc-section-number">1.6</span>  Basic Matrix Operations</a>
  <ul class="collapse">
  <li><a href="#matrix-addition" id="toc-matrix-addition" class="nav-link" data-scroll-target="#matrix-addition"><span class="toc-section-number">1.6.1</span>  Matrix addition</a></li>
  <li><a href="#scalar-multiplication" id="toc-scalar-multiplication" class="nav-link" data-scroll-target="#scalar-multiplication"><span class="toc-section-number">1.6.2</span>  Scalar multiplication</a></li>
  <li><a href="#matrix-multiplication" id="toc-matrix-multiplication" class="nav-link" data-scroll-target="#matrix-multiplication"><span class="toc-section-number">1.6.3</span>  Matrix multiplication</a></li>
  <li><a href="#transpose" id="toc-transpose" class="nav-link" data-scroll-target="#transpose"><span class="toc-section-number">1.6.4</span>  Transpose</a></li>
  <li><a href="#determinant" id="toc-determinant" class="nav-link" data-scroll-target="#determinant"><span class="toc-section-number">1.6.5</span>  Determinant</a></li>
  <li><a href="#inverse" id="toc-inverse" class="nav-link" data-scroll-target="#inverse"><span class="toc-section-number">1.6.6</span>  Inverse</a></li>
  <li><a href="#rank" id="toc-rank" class="nav-link" data-scroll-target="#rank"><span class="toc-section-number">1.6.7</span>  Rank</a></li>
  <li><a href="#trace" id="toc-trace" class="nav-link" data-scroll-target="#trace"><span class="toc-section-number">1.6.8</span>  Trace</a></li>
  <li><a href="#eigenvalues-and-eigenvectors" id="toc-eigenvalues-and-eigenvectors" class="nav-link" data-scroll-target="#eigenvalues-and-eigenvectors"><span class="toc-section-number">1.6.9</span>  Eigenvalues and Eigenvectors</a></li>
  <li><a href="#singular-value-and-singluar-vectors" id="toc-singular-value-and-singluar-vectors" class="nav-link" data-scroll-target="#singular-value-and-singluar-vectors"><span class="toc-section-number">1.6.10</span>  Singular value and Singluar Vectors</a></li>
  </ul></li>
  <li><a href="#sec-matmul" id="toc-sec-matmul" class="nav-link" data-scroll-target="#sec-matmul"><span class="toc-section-number">1.7</span>  행렬의 곱셈 (Matrix Multiplication)</a></li>
  <li><a href="#열공간column-space" id="toc-열공간column-space" class="nav-link" data-scroll-target="#열공간column-space"><span class="toc-section-number">1.8</span>  열공간(Column Space)</a></li>
  <li><a href="#선형-독립linear-independent" id="toc-선형-독립linear-independent" class="nav-link" data-scroll-target="#선형-독립linear-independent"><span class="toc-section-number">1.9</span>  선형 독립(Linear Independent)</a></li>
  <li><a href="#기저basis" id="toc-기저basis" class="nav-link" data-scroll-target="#기저basis"><span class="toc-section-number">1.10</span>  기저(basis)</a></li>
  <li><a href="#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix" id="toc-항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix" class="nav-link" data-scroll-target="#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix"><span class="toc-section-number">1.11</span>  항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)</a>
  <ul class="collapse">
  <li><a href="#identity-matrix항등행렬" id="toc-identity-matrix항등행렬" class="nav-link" data-scroll-target="#identity-matrix항등행렬"><span class="toc-section-number">1.11.1</span>  Identity matrix(항등행렬)</a></li>
  <li><a href="#sec-inv" id="toc-sec-inv" class="nav-link" data-scroll-target="#sec-inv"><span class="toc-section-number">1.11.2</span>  Inverse matrix(역행렬)</a></li>
  <li><a href="#diagonal-matrix대각행렬" id="toc-diagonal-matrix대각행렬" class="nav-link" data-scroll-target="#diagonal-matrix대각행렬"><span class="toc-section-number">1.11.3</span>  Diagonal Matrix(대각행렬)</a></li>
  <li><a href="#orthogonal-matrix직교행렬-orthonomal-matrix" id="toc-orthogonal-matrix직교행렬-orthonomal-matrix" class="nav-link" data-scroll-target="#orthogonal-matrix직교행렬-orthonomal-matrix"><span class="toc-section-number">1.11.4</span>  Orthogonal matrix(직교행렬, orthonomal matrix)</a></li>
  </ul></li>
  <li><a href="#계수rank" id="toc-계수rank" class="nav-link" data-scroll-target="#계수rank"><span class="toc-section-number">1.12</span>  계수(Rank)</a></li>
  <li><a href="#영공간null-space" id="toc-영공간null-space" class="nav-link" data-scroll-target="#영공간null-space"><span class="toc-section-number">1.13</span>  영공간(Null space)</a></li>
  <li><a href="#ax-b의-해의-수" id="toc-ax-b의-해의-수" class="nav-link" data-scroll-target="#ax-b의-해의-수"><span class="toc-section-number">1.14</span>  Ax = b의 해의 수</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Basics (1) - Vector Operations</h1>
<p class="subtitle lead">template</p>
  <div class="quarto-categories">
    <div class="quarto-category">Mathematics</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>template</p>
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kwangmin Kim </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 30, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="introduction-linear-algebra-to-deep-learning" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction Linear Algebra to Deep Learning</h1>
<p>Deep learning is a pile of neural networks that are made up of layers of interconnected nodes or neurons, and the weights of the connections between them are learned through a process called backpropagation.</p>
<p>Linear algebra is fundamental to deep learning because many of the computations involved in training neural networks can be expressed as linear algebra operations. For example, matrix multiplication is used to compute the output of each layer in a neural network, and the gradients of the loss function with respect to the weights are computed using the chain rule of calculus, which involves matrix multiplication and vector operations.</p>
<p>In addition to matrix multiplication, other linear algebra concepts such as eigenvectors, eigenvalues, and singular value decomposition (SVD) are also important in deep learning. For example, SVD can be used to reduce the dimensionality of a dataset or to compute principal components, which are useful for data visualization and feature extraction.</p>
<p>Linear algebra libraries such as Numpy, Scipy, and PyTorch provide efficient implementations of these operations, which are essential for training large-scale neural networks on GPUs. Without these libraries, implementing deep learning algorithms would be much more difficult and time-consuming.</p>
<p><a href="https://nbviewer.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb">Reference: Motivation to Learn Linear Algebra</a></p>
<section id="scalar" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="scalar"><span class="header-section-number">1.1</span> Scalar</h2>
<p>A scalar is a single mathematical quantity, usually a real number, which can be represented by a single value. Scalars are typically denoted by lowercase letters, such as <span class="math inline">\(a, b, c,\)</span> and so on.</p>
</section>
<section id="vector" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="vector"><span class="header-section-number">1.2</span> Vector</h2>
<p>A vector <span class="math inline">\(\textbf{v}\)</span> is a mathematical object that represents a quantity with both a magnitude and a direction. In <span class="math inline">\(n\)</span>-dimensional Euclidean space <span class="math inline">\(\mathbb{R}^n\)</span>, a vector <span class="math inline">\(\textbf{v}\)</span> is typically represented as an ordered list of <span class="math inline">\(n\)</span> real numbers:</p>
<ul>
<li>the magnitude of <span class="math inline">\(\mathbf{v} =\begin{bmatrix} x\\ y \end{bmatrix}\)</span> is <span class="math inline">\(||\mathbf{v}|| = \sqrt{x^{2} + y^{2}}\)</span></li>
<li>the direction of it is the angle with the x axis, <span class="math inline">\(tan^{-1}(\frac{y}{x})\)</span></li>
<li>If magnitude and vector are equal, then they are equal vectors</li>
</ul>
<p><span class="math display">\[
\textbf{v}=
\begin{bmatrix}
  v_1 \\
  v_2 \\
  \vdots \\
  v_n
\end{bmatrix}
\]</span></p>
<p>where <span class="math inline">\(v_1, v_2, \ldots, v_n\)</span> are the components of the vector <span class="math inline">\(\textbf{v}\)</span>.</p>
<section id="plotting-vectors-on-the-coordinate-plane" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="plotting-vectors-on-the-coordinate-plane"><span class="header-section-number">1.2.1</span> Plotting Vectors on the Coordinate Plane</h3>
<p>Example</p>
<p>Map <span class="math inline">\(\begin{bmatrix} 3\\ 2 \end{bmatrix}\)</span> into <span class="math inline">\(x=3\)</span>, <span class="math inline">\(y=2\)</span> on the Coordinate Plane</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">2</span>, color<span class="op">=</span><span class="st">'b'</span>, angles<span class="op">=</span><span class="st">'xy'</span>, scale_units<span class="op">=</span><span class="st">'xy'</span>, scale<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">3.5</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">2.5</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="1.basic_files/figure-html/cell-2-output-1.png" width="592" height="416"></p>
</div>
</div>
<p><a href="http://immersivemath.com/ila/ch02_vectors/ch02.html#auto_label_33">Reference: Read This Article with Interactive Visualization - Points and Vectors</a></p>
</section>
</section>
<section id="matrix" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="matrix"><span class="header-section-number">1.3</span> Matrix</h2>
<p>A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If <span class="math inline">\(\mathbf{A}\)</span> is an <span class="math inline">\(m \times n\)</span> matrix, it can be represented as: <span class="math display">\[
\begin{bmatrix}
  a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
  a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{bmatrix}
\]</span> where <span class="math inline">\(a_{ij}\)</span> is the element in the <span class="math inline">\(i\)</span>-th row and <span class="math inline">\(j\)</span>-th column of the matrix <span class="math inline">\(\mathbf{A}\)</span>.</p>
</section>
<section id="basic-vector-operations" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="basic-vector-operations"><span class="header-section-number">1.4</span> Basic Vector Operations</h2>
<section id="addition-of-vectors" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="addition-of-vectors"><span class="header-section-number">1.4.1</span> Addition of Vectors</h3>
<p>The addition of two vectors is the process of adding their corresponding components. If <span class="math inline">\(\textbf{A}\)</span> and <span class="math inline">\(\textbf{B}\)</span> are two vectors of the same dimension, then their sum <span class="math inline">\(\textbf{C} = \textbf{A} + \textbf{B}\)</span> is a vector whose <span class="math inline">\(i\)</span>-th component is the sum of the <span class="math inline">\(i\)</span>-th components of <span class="math inline">\(\textbf{A}\)</span> and <span class="math inline">\(\textbf{B}\)</span>.</p>
<p><span class="math display">\[
\begin{align*}
  \textbf{C}&amp;=\textbf{A}+\textbf{B}\\
  C_i &amp;= A_i + B_i
\end{align*}
\]</span></p>
<p>For example, if <span class="math inline">\(\textbf{A} = [1, 2, 3]\)</span> and <span class="math inline">\(\textbf{B} = [4, 5, 6]\)</span>, then their sum <span class="math inline">\(\textbf{C} = [5, 7, 9]\)</span>.</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/chap02_01.PNG" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/chap02_02.PNG" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="images/chap02_03.PNG" class="img-fluid"></p>
</div>
</div>
</div>
</section>
<section id="subtraction-of-vectors" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="subtraction-of-vectors"><span class="header-section-number">1.4.2</span> Subtraction of Vectors</h3>
<p>The subtraction of two vectors is the process of subtracting their corresponding components. If <span class="math inline">\(\textbf{A}\)</span> and <span class="math inline">\(\textbf{B}\)</span> are two vectors of the same dimension, then their difference <span class="math inline">\(\textbf{C} = \textbf{A} - \textbf{B}\)</span> is a vector whose <span class="math inline">\(i\)</span>-th component is the difference between the <span class="math inline">\(i\)</span>-th components of <span class="math inline">\(\textbf{A}\)</span> and <span class="math inline">\(\textbf{B}\)</span>. The formal definition is:</p>
<p><span class="math display">\[
\begin{align*}
  \textbf{C}&amp;=\textbf{A} - \textbf{B}\\
  C_i &amp;= A_i - B_i
\end{align*}
\]</span></p>
<p>For example, if <span class="math inline">\(\textbf{A} = [1, 2, 3]\)</span> and <span class="math inline">\(\textbf{B} = [4, 5, 6]\)</span>, then their difference <span class="math inline">\(\textbf{C} = [-3, -3, -3]\)</span>.</p>
</section>
<section id="scalar-multiplication-of-vectors" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="scalar-multiplication-of-vectors"><span class="header-section-number">1.4.3</span> Scalar Multiplication of Vectors</h3>
<p>The scalar multiplication of a vector is the process of multiplying each component of the vector by a scalar. If <span class="math inline">\(\textbf{A}\)</span> is a vector and <span class="math inline">\(k\)</span> is a scalar, then the scalar multiple <span class="math inline">\(\textbf{C} = k\textbf{A}\)</span> is a vector whose <span class="math inline">\(i\)</span>-th component is <span class="math inline">\(k\)</span> times the <span class="math inline">\(i\)</span>-th component of <span class="math inline">\(\textbf{A}\)</span>. The formal definition is:</p>
<p><span class="math display">\[
\begin{align*}
  \textbf{C}&amp;=k\textbf{A}\\
  C_i &amp;= kA_i
\end{align*}
\]</span></p>
<p>For example, if <span class="math inline">\(\textbf{A} = [1, 2, 3]\)</span> and <span class="math inline">\(k = 2\)</span>, then their scalar multiple <span class="math inline">\(\textbf{C} = [2, 4, 6]\)</span>.</p>
<p><a href="http://immersivemath.com/ila/ch02_vectors/ch02.html#sec_vec_arithmetic">Reference: Read This Article with Interactive Visualization - Properties of Vector Arithmetic</a></p>
</section>
<section id="dot-product-of-vectors" class="level3" data-number="1.4.4">
<h3 data-number="1.4.4" class="anchored" data-anchor-id="dot-product-of-vectors"><span class="header-section-number">1.4.4</span> Dot Product of Vectors</h3>
<p>The dot product of two vectors is the sum of the products of their corresponding components (a.k.a dot product &amp; scalar product). If <span class="math inline">\(\textbf{A}\)</span> and <span class="math inline">\(\textbf{B}\)</span> are two vectors of the same dimension, then their dot product <span class="math inline">\(\textbf{C} = \textbf{A} \cdot \textbf{B}\)</span> is a scalar given by the formula:</p>
<p><span class="math display">\[
\begin{align*}
  \textbf{C}&amp;=\textbf{A}\textbf{B}\\
  &amp;= \sum_{i=1}^{n}a_ib_i
\end{align*}
\]</span></p>
<ul>
<li>Dot product can be used to measure the similarity between two vectors.</li>
<li>For the two vectors, <span class="math inline">\(\mathbf{a} = [a_1, a_2, \cdots a_n]\)</span> , <span class="math inline">\(\mathbf{b} = [b_1, b_2, \cdots b_n]\)</span>, dot product can be defined as <span class="math display">\[
\mathbf{a} \cdot \mathbf{b} = \mathbf{a}^{T} \mathbf{b} = ||\mathbf{a}|| ||\mathbf{b}|| \cos \theta
\]</span></li>
<li>When two vectors are orthogonal, <span class="math inline">\(\cos 90^{\circ} = 0\)</span>, the similarity of the two vectors is 0.</li>
<li>In the Euclidean space, doct product is often called inner product (inner product is a generalization of dot product)</li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Inner Product vs Dot Product
</div>
</div>
<div class="callout-body-container callout-body">
<p>In general, an inner product is a mathematical operation that takes two vectors and produces a scalar. It satisfies certain properties, such as being linear in the first argument, conjugate linear in the second argument, and positive-definite.</p>
<ul>
<li>“Linear in the first argument” means that for any fixed vector <span class="math inline">\(\mathbf u\)</span>, the function <span class="math inline">\(f\)</span> defined by <span class="math inline">\(f(\mathbf v) = &lt;\mathbf u, \mathbf v&gt;\)</span> is a linear function of <span class="math inline">\(\mathbf v\)</span>, i.e., <span class="math inline">\(f(a\mathbf x + b\mathbf y) = af(\mathbf x) + bf(\mathbf y)\)</span> for any scalars <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and vectors <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>.</li>
<li>“Conjugate linear in the second argument” means that for any fixed vector <span class="math inline">\(\mathbf v\)</span>, the function <span class="math inline">\(g\)</span> defined by <span class="math inline">\(g(\mathbf u) = &lt;\mathbf u,\mathbf v&gt;\)</span> is a conjugate linear function of <span class="math inline">\(\mathbf u\)</span>, i.e., <span class="math inline">\(g(a \mathbf x + b \mathbf y) = \bar{a} g(\mathbf x) + \bar{b} * g(\mathbf y)\)</span> for any scalars <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and vectors <span class="math inline">\(\mathbf x\)</span>, <span class="math inline">\(\mathbf y\)</span>, where <span class="math inline">\(\bar{a}\)</span> denotes the complex conjugate of <span class="math inline">\(a\)</span>.</li>
<li>“Positive-definite” means that for any nonzero vector v, the inner product <span class="math inline">\(&lt;\mathbf v,\mathbf v&gt;\)</span> is a positive real number. In other words, the inner product of a vector with itself is always positive, except when the vector is the zero vector.</li>
</ul>
<p>A dot product is a specific type of inner product that is defined for Euclidean spaces, which are spaces with a notion of distance or length. The dot product of two vectors is defined as the sum of the products of their corresponding components. In other words, if <span class="math inline">\(\mathbf a = [a_1, a_2, ..., a_n]\)</span> and <span class="math inline">\(\mathbf b = [b_1, b_2, ..., b_n]\)</span> are two vectors in <span class="math inline">\(\mathbb R^n\)</span>, then their dot product is given by:</p>
<p><span class="math display">\[
\mathbf a \cdot \mathbf b = a_1b_1 + a_2b_2 + ... + a_nb_n
\]</span></p>
<p>The dot product satisfies some of the properties of an inner product, such as being linear in the first argument and symmetric. However, it is not conjugate linear in the second argument, and it is not positive-definite in general.</p>
<p>So, while a dot product is a specific type of inner product, not all inner products are dot products.</p>
</div>
</div>
<p>For example, if <span class="math inline">\(\textbf{A} = [1, 2, 3]\)</span> and <span class="math inline">\(\textbf{B} = [4, 5, 6]\)</span>, then their dot product <span class="math inline">\(\textbf{C} = 1\cdot 4 + 2\cdot 5 + 3\cdot 6 = 32\)</span>.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Norm
</div>
</div>
<div class="callout-body-container callout-body">
<p>The norm of a vector <span class="math inline">\(\mathbf{x}\)</span> is a non-negative scalar value that represents the size or length of the vector. The norm is denoted by <span class="math inline">\(|\mathbf{x}|\)</span> and satisfies the following properties:</p>
<ul>
<li>Non-negativity: <span class="math inline">\(|\mathbf{x}|\geq 0\)</span>, with equality if and only if <span class="math inline">\(\mathbf{x}=\mathbf{0}\)</span>.</li>
<li>Homogeneity: <span class="math inline">\(|\alpha\mathbf{x}|=|\alpha||\mathbf{x}|\)</span> for any scalar <span class="math inline">\(\alpha\)</span>.</li>
<li>Triangle Inequality: <span class="math inline">\(|\mathbf{x}+\mathbf{y}|\leq |\mathbf{x}|+|\mathbf{y}|\)</span>.</li>
</ul>
<p>Here is an example of finding the Euclidean norm of a vector: Suppose we have a vector <span class="math inline">\(\mathbf{x}=\begin{bmatrix}1 \\ -2 \\ 2\end{bmatrix}\)</span>. We can find its norm as follows:</p>
<p><span class="math display">\[
||\mathbf x||=\sqrt{1^2+(-2)^2+2^2}=\sqrt{9}=3
\]</span></p>
<p>Therefore, the norm of <span class="math inline">\(\mathbf{x}\)</span> is 3.</p>
<p>There are several types of norms:</p>
<ul>
<li><p>Manhattan Norm or Absolute Norm or <span class="math inline">\(l_1\)</span>-norm <span class="math display">\[
\begin{equation*}
|\mathbf{x}|_{l_1} = \sum_{i=1}^{n} |x_i|
\end{equation*}
\]</span> where <span class="math inline">\(\mathbf{x}\)</span> is a vector of length <span class="math inline">\(n\)</span>. Example: For <span class="math inline">\(\mathbf{x} = [1, -2, 3]\)</span>, <span class="math inline">\(|\mathbf{x}|_{l_1} = |1| + |-2| + |3| = 6\)</span>.</p></li>
<li><p>Euclidean Norm or <span class="math inline">\(l_2\)</span>-norm <span class="math display">\[
\begin{equation*}
||\mathbf{x}||_{l_2} = \sqrt{\sum_{i=1}^{n} x_i^2}
\end{equation*}
\]</span> where <span class="math inline">\(\mathbf{x}\)</span> is a vector of length <span class="math inline">\(n\)</span>. Example: For <span class="math inline">\(\mathbf{x} = [1, 2, 3]\)</span>, <span class="math inline">\(|\mathbf{x}|_{l_2} = \sqrt{1^2 + 2^2 + 3^2} = \sqrt{14}\)</span>.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/chap02_05.PNG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><span class="math inline">\(l_2\)</span>-norm</figcaption><p></p>
</figure>
</div>
<ul>
<li>p-norm(<span class="math inline">\(l_2\)</span>-norm)</li>
</ul>
<p>For <span class="math inline">\(p \geq 1\)</span>, <span class="math display">\[
\begin{equation*}
||\mathbf{x}||_p = (\sum_{i=1}^n |x_i|^p)^{\frac{1}{p}}
\end{equation*}
\]</span> where <span class="math inline">\(\mathbf{x}\)</span> is a vector of length <span class="math inline">\(n\)</span>. Example: For <span class="math inline">\(\mathbf{x} = [1, 2, 3]\)</span>, <span class="math inline">\(|\mathbf{x}|_{l_p} = \sqrt{1^p + 2^p + 3^p}\)</span>.</p>
<ul>
<li>Maximum Norm <span class="math display">\[
\begin{equation*}
|\mathbf{x}|_{\infty} = \max_{1 \leq i \leq n} |x_i|
\end{equation*}
\]</span> where <span class="math inline">\(\mathbf{x}\)</span> is a vector of length <span class="math inline">\(n\)</span>. Example: For <span class="math inline">\(\mathbf{x} = [1, -2, 3]\)</span>, <span class="math inline">\(|\mathbf{x}|_{l_\infty} = \max{1, |-2|, 3} = 3\)</span>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/chap02_06.PNG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><span class="math inline">\(l_1\)</span>-norm vs <span class="math inline">\(l_1\)</span>-norm vs <span class="math inline">\(\max\)</span>-norm</figcaption><p></p>
</figure>
</div>
<ul>
<li>Frobenius Norm: <span class="math display">\[
\begin{equation*}
|\mathbf{A}|_{F} = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} |a_{ij}|^2}
\end{equation*}
\]</span> where <span class="math inline">\(\mathbf{A}\)</span> is an <span class="math inline">\(m \times n\)</span> matrix. Example: For <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span>, <span class="math inline">\(|\mathbf{A}|_{F} = \sqrt{1^2 + 2^2 + 3^2 + 4^2} = \sqrt{30}\)</span>.</li>
</ul>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Projection
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(\mathbf{u}\)</span> and <span class="math inline">\(\mathbf{v}\)</span> be two vectors. The projection of <span class="math inline">\(\mathbf{u}\)</span> onto <span class="math inline">\(\mathbf{v}\)</span> is defined as the vector:</p>
<p>This vector is the closest vector to <span class="math inline">\(\mathbf{u}\)</span> that lies on the line spanned by <span class="math inline">\(\mathbf{v}\)</span>. <span class="math display">\[
\text{proj}_{\mathbf v}\mathbf u =\frac{\mathbf u \mathbf v}{||\mathbf v||^2} \mathbf v
\]</span> <img src="images/chap02_04.PNG" class="img-fluid" alt="Projection"></p>
<ul>
<li><span class="math inline">\(\mathbf{w} = ||\mathbf{w}||\mathbf{v} = ||\mathbf{u}|| \cos \theta \mathbf{v}\)</span></li>
<li><span class="math inline">\(\mathbf{u}^T \mathbf{u} = ||\mathbf{u}|| ||\mathbf{u}|| = ||\mathbf{u}||^2\)</span></li>
<li>the magnitude of <span class="math inline">\(\mathbf{u}\)</span> = <span class="math inline">\(||\mathbf{u}|| = \sqrt{\mathbf{u}^T \mathbf{u}}\)</span></li>
<li>unit vector: a normalized vector by dividing it by its magnitude, so the magnitude of a unit vector is 1 <span class="math display">\[
\hat{\mathbf{u}} = \frac{\mathbf{u}}{||\mathbf{u}||} = \frac{\mathbf{u}}{\sqrt{\mathbf{u}^T \mathbf{u}}}
\]</span></li>
<li>Projected vector, <span class="math inline">\(\mathbf{w}\)</span>
<ul>
<li>the product of <span class="math inline">\(\frac{\mathbf{u}^T \mathbf{v}}{||\mathbf{v}||}\)</span> and a unit vector of <span class="math inline">\(\mathbf{v}\)</span> <span class="math display">\[
\frac{\mathbf{u}^T \mathbf{v}}{||\mathbf{v}||} \frac{\mathbf{v}}{||\mathbf{v}||} = \frac{\mathbf{u}^T \mathbf{v}}{||\mathbf{v}||^2}\mathbf{v}
\]</span></li>
</ul></li>
</ul>
<p>For example, let <span class="math inline">\(\mathbf{u} = \begin{bmatrix}2 \ 3\end{bmatrix}\)</span> and <span class="math inline">\(\mathbf{v} = \begin{bmatrix}1 \\ 1\end{bmatrix}\)</span>. Then, the projection of <span class="math inline">\(\mathbf{u}\)</span> onto <span class="math inline">\(\mathbf{v}\)</span> is:</p>
<p><span class="math display">\[
\text{proj}_{\mathbf v}\mathbf u =\frac{\mathbf u \mathbf v}{||\mathbf v||^2} \mathbf v =\frac{\begin{bmatrix}2 \\ 3\end{bmatrix}\begin{bmatrix}1 \\ 1\end{bmatrix}}{\bigg{|}\bigg{|}\begin{bmatrix}1 \\ 1\end{bmatrix}\bigg{|}\bigg{|}^2}=\frac{5}{2}\begin{bmatrix}1 \\ 1\end{bmatrix}=\begin{bmatrix}5 \\ 2\end{bmatrix}
\]</span></p>
<p>This vector is the closest vector to <span class="math inline">\(\mathbf{u}\)</span> that lies on the line spanned by <span class="math inline">\(\mathbf{v} = \begin{bmatrix}1 \\ 1\end{bmatrix}\)</span>.</p>
<p><a href="http://immersivemath.com/ila/ch03_dotproduct/ch03.html#auto_label_107">Reference: Read This Article with Interactive Visualization - Projection</a></p>
</div>
</div>
</section>
<section id="cross-product-of-vectors" class="level3" data-number="1.4.5">
<h3 data-number="1.4.5" class="anchored" data-anchor-id="cross-product-of-vectors"><span class="header-section-number">1.4.5</span> Cross Product of Vectors</h3>
<p>The cross product of two vectors is a vector that is perpendicular to both of them. If <span class="math inline">\(\textbf{A}\)</span> and <span class="math inline">\(\textbf{B}\)</span> are two vectors in <span class="math inline">\(\mathbb{R}^3\)</span>, then their cross product <span class="math inline">\(\textbf{C} = \textbf{A} \times \textbf{B}\)</span> is a vector given by the formula</p>
<p><span class="math display">\[
$\textbf{C} = \textbf{A} \times \textbf{B}$ = [A_2B_3 - A_3B_2, A_3B_1 - A_1B_3, A_1B_2 - A_2B_1]
\]</span></p>
<p>For example, if <span class="math inline">\(\textbf{A} = [1, 2, 3]\)</span> and <span class="math inline">\(\textbf{B} = [4, 5, 6]\)</span>, then their cross product <span class="math inline">\(\textbf{C} = [-3, 6, -3]\)</span>.</p>
</section>
</section>
<section id="column-vector-row-vector" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="column-vector-row-vector"><span class="header-section-number">1.5</span> Column Vector &amp; Row Vector</h2>
<p><span class="math display">\[
\begin{bmatrix}
  a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
  a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{bmatrix}
\]</span></p>
<p>A column vector <span class="math inline">\(\mathbf{x}\)</span> with <span class="math inline">\(n\)</span> elements is an <span class="math inline">\(m \times 1\)</span> matrix, which can be represented as: <span class="math display">\[
\mathbf{x} =
\begin{bmatrix}
x_{1} \\
x_{2} \\
\vdots \\
x_{m}
\end{bmatrix}
\]</span></p>
<p>where <span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i\)</span>-th element of the column vector <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(m\)</span> is the number of rows in the matrix. A row vector <span class="math inline">\(\mathbf{y}\)</span> with <span class="math inline">\(m\)</span> elements is a <span class="math inline">\(1 \times n\)</span> matrix, which can be represented as: <span class="math display">\[
\mathbf{y} =
\begin{bmatrix}
y_{1} &amp; y_{2} &amp; \cdots &amp; y_{n}
\end{bmatrix}
\]</span></p>
<p>where <span class="math inline">\(y_i\)</span> is the <span class="math inline">\(i\)</span>-th element of the row vector <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(n\)</span> is the number of columns in the matrix.</p>
</section>
<section id="basic-matrix-operations" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="basic-matrix-operations"><span class="header-section-number">1.6</span> Basic Matrix Operations</h2>
<section id="matrix-addition" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" class="anchored" data-anchor-id="matrix-addition"><span class="header-section-number">1.6.1</span> Matrix addition</h3>
<p>The sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.</p>
<p>Given two <span class="math inline">\(m \times n\)</span> matrices <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span>, their sum <span class="math inline">\(\mathbf{C} = \mathbf{A} + \mathbf{B}\)</span> is defined by:</p>
<p><span class="math display">\[
c_{i,j}=a_{i,j}+b_{i,j}​
\]</span> for <span class="math inline">\(1 \leq i \leq m\)</span> and <span class="math inline">\(1 \leq j \leq n\)</span>.</p>
<p><span class="math display">\[
\begin{bmatrix}
  1 &amp; 2 \\
  3 &amp; 4 \\
  5 &amp; 6
\end{bmatrix} +
\begin{bmatrix}
  -1 &amp; 0 \\
  2 &amp; -3 \\
  -5 &amp; 4
\end{bmatrix} =
\begin{bmatrix}
  0 &amp; 2 \\
  5 &amp; 1 \\
  0 &amp; 10
\end{bmatrix}
\]</span></p>
</section>
<section id="scalar-multiplication" class="level3" data-number="1.6.2">
<h3 data-number="1.6.2" class="anchored" data-anchor-id="scalar-multiplication"><span class="header-section-number">1.6.2</span> Scalar multiplication</h3>
<p>The product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.</p>
<p>Given a scalar <span class="math inline">\(k\)</span> and an <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>, their product <span class="math inline">\(k\mathbf{A}\)</span> is defined by: <span class="math display">\[
(k\mathbf{A})_{i,j} = k(a_{i,j})
\]</span> for <span class="math inline">\(1 \leq i \leq m\)</span> and <span class="math inline">\(1 \leq j \leq n\)</span>.</p>
<p>Example: <span class="math display">\[
2\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4 \\
5 &amp; 6
\end{bmatrix} =
\begin{bmatrix}
2 &amp; 4 \\
6 &amp; 8 \\
10 &amp; 12
\end{bmatrix}
\]</span></p>
</section>
<section id="matrix-multiplication" class="level3" data-number="1.6.3">
<h3 data-number="1.6.3" class="anchored" data-anchor-id="matrix-multiplication"><span class="header-section-number">1.6.3</span> Matrix multiplication</h3>
<p>The product of two matrices <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> is a matrix obtained by multiplying the rows of <span class="math inline">\(\mathbf{A}\)</span> by the columns of <span class="math inline">\(\mathbf{B}\)</span>.</p>
<p>Given two matrices <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> with dimensions <span class="math inline">\(m \times n\)</span> and <span class="math inline">\(n \times p\)</span>, respectively, their product <span class="math inline">\(\mathbf{C} = \mathbf{AB}\)</span> is an <span class="math inline">\(m \times p\)</span> matrix defined by: <span class="math display">\[
c_{i,j} = \sum_{k=1}^n a_{i,k}b_{k,j}
\]</span> for <span class="math inline">\(1 \leq i \leq m\)</span> and <span class="math inline">\(1 \leq j \leq p\)</span>.</p>
<p>Example: <span class="math display">\[
\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4 \\
5 &amp; 6
\end{bmatrix} \begin{bmatrix}
-1 &amp; 0 &amp; 2 \\
2 &amp; -3 &amp; 1
\end{bmatrix} =
\begin{bmatrix}
3 &amp; -6 &amp; 4 \\
5 &amp; -12 &amp; 10 \\
7 &amp; -18 &amp; 16
\end{bmatrix}
\]</span></p>
</section>
<section id="transpose" class="level3" data-number="1.6.4">
<h3 data-number="1.6.4" class="anchored" data-anchor-id="transpose"><span class="header-section-number">1.6.4</span> Transpose</h3>
<p>The transpose of an <span class="math inline">\(m x n\)</span> matrix A, denoted by <span class="math inline">\(A^T\)</span>, is the <span class="math inline">\(n x m\)</span> matrix obtained by interchanging the rows and columns of A. Formally, if <span class="math inline">\(\mathbf{A} = [a_{ij}]\)</span> is an m x n matrix, then its transpose <span class="math inline">\(\mathbf{A}^T = [b_{ij}]\)</span> is an <span class="math inline">\(n x m\)</span> matrix where <span class="math inline">\(b_{ij}\)</span> = <span class="math inline">\(a_{ji}\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. In other words, the element in the <span class="math inline">\(i\)</span> th row and <span class="math inline">\(j\)</span> th column of <span class="math inline">\(A^T\)</span> is equal to the element in the <span class="math inline">\(j\)</span> th row and ith column of <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>Given an <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>, its transpose <span class="math inline">\(\mathbf{A}^T\)</span> is an <span class="math inline">\(n \times m\)</span> matrix defined by: $$ <em>{i,j}^T = </em>{j,i}</p>
<p>$$</p>
<ul>
<li>When <span class="math inline">\(\mathbf{A}\)</span> is transposed, diagnoal entries(<span class="math inline">\(a_{ii}\)</span>) do not change but off-diagnoal elements(<span class="math inline">\(a_{ij} \; i \neq j\)</span>) change.</li>
<li>A column vector is tranposed into a row vector, and vice versa.</li>
<li>symmetric matrix: <span class="math inline">\(\mathbf{A} = \mathbf{A}^T\)</span></li>
</ul>
<p>Example:</p>
<p>Let A be the matrix <span class="math display">\[
\mathbf{A} = \begin{bmatrix}
1 &amp; 2 &amp; 3\\
4 &amp; 5 &amp; 6
\end{bmatrix}
\]</span> The transpose of A, denoted by A^T, is the matrix <span class="math display">\[
\mathbf{A}^T = \begin{bmatrix}
1 &amp; 4\\
2 &amp; 5\\
3 &amp; 6
\end{bmatrix}
\]</span></p>
</section>
<section id="determinant" class="level3" data-number="1.6.5">
<h3 data-number="1.6.5" class="anchored" data-anchor-id="determinant"><span class="header-section-number">1.6.5</span> Determinant</h3>
<p>Let <span class="math inline">\(\mathbf{A}\)</span> be an <span class="math inline">\(n \times n\)</span> square matrix. The determinant of <span class="math inline">\(\mathbf{A}\)</span>, denoted by <span class="math inline">\(|\mathbf{A}|\)</span> or <span class="math inline">\(\det(\mathbf{A})\)</span>, is a scalar value calculated as the sum of the products of the elements in any row or column of <span class="math inline">\(\mathbf{A}\)</span> with their corresponding cofactors, that is,</p>
<p><span class="math display">\[
|\mathbf{A}|=\sum_{i=1}^{n}a_{ij}C_{ij}=\sum_{j=1}^{n}a_{ij}C_{ij}
\]</span></p>
<p>where <span class="math inline">\(a_{ij}\)</span> is the element of <span class="math inline">\(\mathbf{A}\)</span> in the <span class="math inline">\(i\)</span>-th row and <span class="math inline">\(j\)</span>-th column, and <span class="math inline">\(C_{ij}\)</span> is the cofactor of <span class="math inline">\(a_{ij}\)</span>. The cofactor of <span class="math inline">\(a_{ij}\)</span>, denoted by <span class="math inline">\(C_{ij}\)</span>, is given by <span class="math inline">\((-1)^{i+j}\)</span> times the determinant of the <span class="math inline">\((n-1) \times (n-1)\)</span> matrix obtained by deleting the <span class="math inline">\(i\)</span>-th row and <span class="math inline">\(j\)</span>-th column of <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>The determinant of an <span class="math inline">\(n x n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> is a scalar value denoted as <span class="math inline">\(|\mathbf{A}|\)</span>. It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a <span class="math inline">\(3 x 3\)</span> matrix <span class="math inline">\(\mathbf{A} =\)</span> <span class="math display">\[
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33} \\
\end{bmatrix}
\]</span></p>
<p>is given by:</p>
<p><span class="math display">\[
|\mathbf{A}| = a_{11}
\begin{vmatrix}
a_{22} &amp; a_{23} \\
a_{32} &amp; a_{33}
\end{vmatrix}
- a_{12}
\begin{vmatrix}
a_{21} &amp; a_{23} \\
a_{31} &amp; a_{33}
\end{vmatrix}
+ a_{13}
\begin{vmatrix}
a_{21} &amp; a_{22} \\
a_{31} &amp; a_{32}
\end{vmatrix}
\]</span></p>
<p>For example, consider the <span class="math inline">\(3 \times 3\)</span> matrix <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{bmatrix}\)</span>. We can calculate the determinant of <span class="math inline">\(\mathbf{A}\)</span> using any row or column. Let’s use the first column:</p>
<p><span class="math display">\[
|\mathbf{A}| = 1
\begin{vmatrix}
5 &amp; 6 \\
8 &amp; 9
\end{vmatrix}
- 4
\begin{vmatrix}
2 &amp; 3\\
8 &amp; 9
\end{vmatrix}
+ 7
\begin{vmatrix}
2 &amp; 5 \\
3 &amp; 6
\end{vmatrix} = 0
\]</span></p>
<p>Therefore, the determinant of <span class="math inline">\(\mathbf{A}\)</span> is zero.</p>
</section>
<section id="inverse" class="level3" data-number="1.6.6">
<h3 data-number="1.6.6" class="anchored" data-anchor-id="inverse"><span class="header-section-number">1.6.6</span> Inverse</h3>
<p>The inverse of a square matrix <span class="math inline">\(A\)</span> of size <span class="math inline">\(n\)</span> is a matrix <span class="math inline">\(A^{-1}\)</span> such that the product of <span class="math inline">\(A\)</span> and <span class="math inline">\(A^{-1}\)</span> is the identity matrix <span class="math inline">\(I_n\)</span>, i.e.&nbsp;<span class="math inline">\(A \times A^{-1} = I_n\)</span>. If such a matrix exists, then <span class="math inline">\(A\)</span> is said to be invertible or non-singular.</p>
<p>The inverse of a square matrix <span class="math inline">\(\mathbf{A}\)</span> is denoted by <span class="math inline">\(\mathbf{A}^{-1}\)</span> and is defined as the unique matrix that satisfies the following equation: <span class="math display">\[
\mathbf{A}^{-1}\mathbf{A} = \mathbf{A}\mathbf{A}^{-1} = \mathbf{I}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{I}\)</span> is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.</p>
<p>For example, consider the <span class="math inline">\(2\times 2\)</span> matrix <span class="math display">\[
\mathbf{A} =
\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}.
\]</span> The inverse of <span class="math inline">\(\mathbf{A}\)</span> is given by:</p>
<p><span class="math display">\[
\mathbf{A}^{-1} =
\frac{1}{-2}
\begin{bmatrix}
4 &amp; -2 \\
-3 &amp; 1
\end{bmatrix} =
\begin{bmatrix}
-2 &amp; 1 \\
\frac{3}{2} &amp; -\frac{1}{2}
\end{bmatrix}
\]</span></p>
<p>We can verify that <span class="math inline">\(\mathbf{A}\mathbf{A}^{-1} = \mathbf{A}^{-1}\mathbf{A} = \mathbf{I}\)</span> by computing:</p>
<p><span class="math display">\[
\mathbf{A}\mathbf{A}^{-1} =
\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}
\begin{bmatrix}
-2 &amp; 1 \\
\frac{3}{2} &amp; -\frac{1}{2}
\end{bmatrix} =
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{bmatrix} = \mathbf{I}
\]</span></p>
<p><span class="math display">\[
\mathbf{A}^{-1}\mathbf{A} =
\begin{bmatrix} -2 &amp; 1 \\
\frac{3}{2} &amp; -\frac{1}{2}
\end{bmatrix}
\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix} =
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{bmatrix} = \mathbf{I}
\]</span></p>
<p>Let me give another example and A be a <span class="math inline">\(3x3\)</span> square matrix:</p>
<p><span class="math display">\[
\begin{equation*}
\mathbf{A} = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33} \\
\end{bmatrix}
\end{equation*}
\]</span> Then, the inverse of <span class="math inline">\(\mathbf{A}\)</span>, denoted as <span class="math inline">\(\mathbf{A}^{-1}\)</span>, is given by:</p>
<p><span class="math display">\[
\begin{equation*}
\mathbf{A}^{-1} = \frac{1}{\text{det}(\mathbf{A})}\begin{bmatrix}
a_{22}a_{33}-a_{23}a_{32} &amp; a_{13}a_{32}-a_{12}a_{33} &amp; a_{12}a_{23}-a_{13}a_{22} \\
a_{23}a_{31}-a_{21}a_{33} &amp; a_{11}a_{33}-a_{13}a_{31} &amp; a_{13}a_{21}-a_{11}a_{23} \\
a_{21}a_{32}-a_{22}a_{31} &amp; a_{12}a_{31}-a_{11}a_{32} &amp; a_{11}a_{22}-a_{12}a_{21} \\
\end{bmatrix}
\end{equation*}
\]</span></p>
<p>where <span class="math inline">\(det(\mathbf{A})\)</span> is the determinant of <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>For example, let:</p>
<p><span class="math display">\[
\begin{equation*}
\mathbf{A} = \begin{bmatrix}
1 &amp; 2 &amp; 3 \\
0 &amp; 1 &amp; 4 \\
5 &amp; 6 &amp; 0 \\
\end{bmatrix}
\end{equation*}
\]</span> Then, <span class="math inline">\(det(A)\)</span> = -57, and the inverse of <span class="math inline">\(\mathbf{A}\)</span> is:</p>
<p><span class="math display">\[
\begin{equation*}
\mathbf{A}^{-1} = \frac{1}{-57}\begin{bmatrix}
-24 &amp; 18 &amp; 5 \\
20 &amp; -15 &amp; -4 \\
-3 &amp; 2 &amp; 1 \\
\end{bmatrix}
\end{equation*}
\]</span></p>
<p>There are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.</p>
</section>
<section id="rank" class="level3" data-number="1.6.7">
<h3 data-number="1.6.7" class="anchored" data-anchor-id="rank"><span class="header-section-number">1.6.7</span> Rank</h3>
<p>The rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by <span class="math inline">\(\text{rank}(\mathbf{A})\)</span>.</p>
<p>For example, consider the following matrix:</p>
<p><span class="math display">\[
\begin{equation*}
\mathbf{A} = \begin{bmatrix}
1 &amp; 2 &amp; 3\\
4 &amp; 5 &amp; 6 \\
7 &amp; 8 &amp; 9\\
\end{bmatrix}
\end{equation*}
\]</span></p>
<p>The columns of <span class="math inline">\(\mathbf{A}\)</span> are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of <span class="math inline">\(\mathbf{A}\)</span> is 2.</p>
</section>
<section id="trace" class="level3" data-number="1.6.8">
<h3 data-number="1.6.8" class="anchored" data-anchor-id="trace"><span class="header-section-number">1.6.8</span> Trace</h3>
<p>The trace of a square matrix <span class="math inline">\(\mathbf{A}\)</span>, denoted by <span class="math inline">\(\mathrm{tr}(\mathbf{A})\)</span>, is defined as the sum of the diagonal elements of <span class="math inline">\(\mathbf{A}\)</span>. In other words, if <span class="math inline">\(\mathbf{A}\)</span> is an <span class="math inline">\(n \times n\)</span> matrix, then its trace is given by:</p>
<p><span class="math display">\[
\mathrm{tr}(\mathbf{A})=\sum_{i=1}^{n}a_{ij}
\]</span></p>
<p>where <span class="math inline">\(a_{ii}\)</span> denotes the <span class="math inline">\(i\)</span> th diagonal element of <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>For example, let <span class="math display">\[
\begin{bmatrix}
2 &amp; 3 &amp; 1 \\
0 &amp; 5 &amp; 2 \\
1 &amp; 1 &amp; 4
\end{bmatrix}
\]</span> Then, the trace of <span class="math inline">\(\mathbf{A}\)</span> is <span class="math inline">\(\mathrm{tr}(\mathbf{A}) = 2 + 5 + 4 = 11\)</span></p>
</section>
<section id="eigenvalues-and-eigenvectors" class="level3" data-number="1.6.9">
<h3 data-number="1.6.9" class="anchored" data-anchor-id="eigenvalues-and-eigenvectors"><span class="header-section-number">1.6.9</span> Eigenvalues and Eigenvectors</h3>
<p>Let A be an <span class="math inline">\(n × n\)</span> square matrix. A scalar <span class="math inline">\(\lambda\)</span> is called an eigenvalue of <span class="math inline">\(\mathbf A\)</span> if there exists a non-zero vector <span class="math inline">\(\mathbf{v}\)</span> such that <span class="math display">\[
\mathbf{Av}=\lambda\mathbf{v}
\]</span></p>
<p>Such a vector <span class="math inline">\(\mathbf{v}\)</span> is called an eigenvector corresponding to <span class="math inline">\(\lambda\)</span>.</p>
<p>Example:</p>
<p>Let <span class="math inline">\(\mathbf A\)</span> be the matrix</p>
<p>To find the eigenvalues of <span class="math inline">\(\mathbf A\)</span>, we solve the characteristic equation <span class="math inline">\(\text{det}(\mathbf A - \lambda \mathbf I ) = 0\)</span>, where I is the n × n identity matrix.</p>
<p><span class="math display">\[
\begin{align*}
\text{det}(\mathbf A - \lambda \mathbf I )
&amp;=
\begin{vmatrix}
3 - \lambda &amp; 1 \\
1 &amp; 3 - \lambda
\end{vmatrix} \\
&amp;=
(3 - \lambda)(3 - \lambda) - 1 \\
&amp;= \lambda^2 - 6\lambda + 8 = 0
\end{align*}
\]</span></p>
<p>Solving this quadratic equation gives us the eigenvalues of <span class="math inline">\(\mathbf A\)</span>: <span class="math inline">\(\lambda_1 = 2\)</span> and <span class="math inline">\(\lambda_2 = 4\)</span>.</p>
<p>To find the eigenvectors corresponding to <span class="math inline">\(\lambda_1 = 2\)</span>, we solve the equation <span class="math inline">\((\mathbf A - 2 \mathbf I)\mathbf{v} = \mathbf{0}\)</span>, where <span class="math inline">\(\mathbf I\)</span> is the <span class="math inline">\(2 \times 2\)</span> identity matrix.</p>
<p><span class="math display">\[
\begin{align*}
(\mathbf A - 2 \mathbf I)\mathbf{v} = \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{bmatrix}
\begin{bmatrix} x \ y \end{bmatrix} = \begin{bmatrix} 0 \ 0 \end{bmatrix}
\end{align*}
\]</span></p>
<p>Solving this system of equations gives us the eigenvectors corresponding to <span class="math inline">\(\lambda_1 = 2\)</span>: <span class="math inline">\(\mathbf{v_1} = \begin{bmatrix} -1 \\ 1 \end{bmatrix}\)</span></p>
<p>Similarly, for <span class="math inline">\(\lambda_2 = 4\)</span>, we solve the equation <span class="math inline">\((\mathbf A - 4\mathbf I)\mathbf{v}\)</span> = <span class="math inline">\(\mathbf{0}\)</span> to get the eigenvectors corresponding to <span class="math inline">\(\lambda_2 = 4\)</span>: <span class="math inline">\(\mathbf{v_2} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span></p>
</section>
<section id="singular-value-and-singluar-vectors" class="level3" data-number="1.6.10">
<h3 data-number="1.6.10" class="anchored" data-anchor-id="singular-value-and-singluar-vectors"><span class="header-section-number">1.6.10</span> Singular value and Singluar Vectors</h3>
<p>The singular value decomposition (SVD) of a matrix <span class="math inline">\(\mathbf A\)</span> is a factorization of <span class="math inline">\(\mathbf A\)</span> into the product of three matrices as follows:</p>
<p><span class="math display">\[
\mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T
\]</span> where <span class="math inline">\(\mathbf{U}\)</span> is an <span class="math inline">\(m \times m\)</span> orthogonal matrix, <span class="math inline">\(\mathbf{\Sigma}\)</span> is an <span class="math inline">\(m \times n\)</span> rectangular diagonal matrix with non-negative real numbers on the diagonal, and <span class="math inline">\(\mathbf{V}\)</span> is an <span class="math inline">\(n \times n\)</span> orthogonal matrix.</p>
<p>The diagonal entries of <span class="math inline">\(\mathbf{\Sigma}\)</span> are called the singular values of <span class="math inline">\(\mathbf{A}\)</span>, denoted as <span class="math inline">\(\sigma_1, \sigma_2, \ldots, \sigma_r\)</span> (where <span class="math inline">\(r\)</span> is the rank of <span class="math inline">\(\mathbf{A}\)</span>), and are arranged in descending order. The columns of <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> are called the left and right singular vectors of <span class="math inline">\(\mathbf{A}\)</span>, respectively, and are orthonormal vectors.</p>
<p>For example, let <span class="math inline">\(\mathbf{A}\)</span> be a 3 by 2 matrix given by:</p>
<p><span class="math display">\[
\begin{equation*}
\mathbf{A} = \begin{bmatrix}
1 &amp; 2\\
3 &amp; 4\\
5 &amp; 6
\end{bmatrix}
\end{equation*}
\]</span></p>
<p>The SVD of A is given by:</p>
<p><span class="math display">\[
\begin{equation*}
\mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T =
\begin{bmatrix}
-0.23 &amp; -0.53 &amp; -0.81\\
-0.53 &amp; -0.72 &amp; 0.45\\
-0.81 &amp; 0.45 &amp; -0.38
\end{bmatrix}
\begin{bmatrix}
9.53 &amp; 0\\
0 &amp; 0.90\\
0 &amp; 0
\end{bmatrix}
\begin{bmatrix}
-0.62 &amp; -0.78\\
-0.78 &amp; 0.62
\end{bmatrix}^T
\end{equation*}
\]</span></p>
<p>where the left singular vectors of <span class="math inline">\(\mathbf{A}\)</span> are the columns of <span class="math inline">\(\mathbf{U}\)</span>, the right singular vectors of <span class="math inline">\(\mathbf{A}\)</span> are the columns of <span class="math inline">\(\mathbf{V}\)</span>, and the singular values of <span class="math inline">\(\mathbf{A}\)</span> are the diagonal entries of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>.</p>
</section>
</section>
<section id="sec-matmul" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="sec-matmul"><span class="header-section-number">1.7</span> 행렬의 곱셈 (Matrix Multiplication)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/Lo8FsB1anzQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li>두 개의 연립 방정식을 행렬의 곱으로 나타내보기 <span class="math display">\[\begin{matrix}x_1+2y_1=4\\2x_1+5y_1=9\end{matrix} \quad \quad \quad \begin{matrix}x_2+2y_2=3\\2x_2+5y_2=7\end{matrix}\]</span> <span class="math display">\[ \begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 5 \end{bmatrix} \begin{bmatrix} x_1 &amp; x_2 \\ y_1 &amp; y_2 \end{bmatrix} = \begin{bmatrix} 4 &amp; 9 \\ 3 &amp; 7 \end{bmatrix}\]</span></li>
<li>중요한 사실(….당연한 사실?)
<ul>
<li>곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함
<ul>
<li><span class="math inline">\(A_{m \times n} \times B_{o \times p}\)</span> 에서 <span class="math inline">\(n = o\)</span> 여야 곱셈 성립</li>
</ul></li>
<li>곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 <span class="math inline">\(\times\)</span> 곱셈의 오른쪽 행렬의 열 수
<ul>
<li><span class="math inline">\(A_{m \times n} \times B_{o \times p} = C_{m \times p}\)</span></li>
</ul></li>
<li>교환법칙(Commutative property)이 성립하지 않음
<ul>
<li><span class="math inline">\(AB \neq BA\)</span></li>
</ul></li>
</ul></li>
<li>행렬 곱셈의 여러가지 관점
<ul>
<li>내적으로 바라보기 <span class="math display">\[ A = \begin{bmatrix} \mathbf{a_1^T} \\ \mathbf{a_2^T} \\ \vdots \\ \mathbf{a_m^T} \end{bmatrix} \]</span> <span class="math display">\[ AB = \begin{bmatrix} \mathbf{a_1^T} \\ \mathbf{a_2^T} \\ \vdots \\ \mathbf{a_m^T} \end{bmatrix} \begin{bmatrix} \mathbf{b_1} &amp; \mathbf{b_2} &amp; \cdots &amp; \mathbf{b_m} \end{bmatrix} = \begin{bmatrix} \mathbf{a_1^T b_1} &amp; \mathbf{a_1^T b_2} &amp; \cdots &amp; \mathbf{a_1^T b_m} \\ \mathbf{a_2^T b_1} &amp; \mathbf{a_2^T b_2} &amp; \cdots &amp; \mathbf{a_2^T b_m} \\ \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\ \mathbf{a_m^T b_1} &amp; \mathbf{a_m^T b_2} &amp; \cdots &amp; \mathbf{a_m^T b_m} \end{bmatrix}\]</span></li>
<li>rank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) <span class="math display">\[AB = \begin{bmatrix} \mathbf{a_1} &amp; \mathbf{a_2} &amp; \cdots &amp; \mathbf{a_m} \end{bmatrix} \begin{bmatrix} \mathbf{b_1^T} \\ \mathbf{b_2^T} \\ \vdots \\ \mathbf{b_m^T} \end{bmatrix} = \mathbf{a_1 b_1^T} + \mathbf{a_2 b_2^T} + \cdots + \mathbf{a_m b_m^T}\]</span></li>
<li>column space로 바라보기 <span class="math display">\[A\mathbf{x} = \begin{bmatrix} \mathbf{a_1} &amp; \mathbf{a_2} &amp; \cdots &amp; \mathbf{a_m} \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_m \end{bmatrix} = \mathbf{a_1} x_1 + \mathbf{a_2} x_2 + \cdots + \mathbf{a_m} x_m \]</span> (스칼라배의 합)
<ul>
<li><span class="math inline">\(A = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span> 는 2차원 좌표평면의 모든 점을, <span class="math inline">\(A=\begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\)</span>은 3차원 좌표평면의 모든 점 표현 가능</li>
<li><span class="math inline">\(AB = A \begin{bmatrix} \mathbf{b_1} &amp; \mathbf{b_2} &amp; \cdots &amp; \mathbf{b_m} \end{bmatrix} = \begin{bmatrix} A \mathbf{b_1} &amp; A \mathbf{b_2} &amp; \cdots &amp; A \mathbf{b_m} \end{bmatrix}\)</span></li>
<li>column space: A의 column vector로 만들 수 있는 부분 공간</li>
</ul></li>
<li>row space로 바라보기 <span class="math display">\[\mathbf{x^T}A = \begin{bmatrix} x_1 &amp; x_2 &amp; \cdots &amp; x_m \end{bmatrix} \begin{bmatrix} \mathbf{a_1^T} \\ \mathbf{a_2^T} \\ \vdots \\ \mathbf{a_m^T} \end{bmatrix} = x_1 \mathbf{a_1^T} + x_2 \mathbf{a_2^T} + \cdots + x_m \mathbf{a_m^T} \]</span></li>
</ul></li>
</ul>
</section>
<section id="열공간column-space" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="열공간column-space"><span class="header-section-number">1.8</span> 열공간(Column Space)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/g0eaDeVRdZk" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li>column space: column vector 들이 span 하는 space
<ul>
<li><span class="math inline">\(A\)</span>의 column space = <span class="math inline">\(C(A)\)</span> 또는 <span class="math inline">\(range(A)\)</span></li>
</ul></li>
<li>span: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합
<ul>
<li>vector에 따라, 점일수도 선일수도 평면일 수도 있음</li>
<li>vector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space</li>
</ul></li>
<li>vector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것
<ul>
<li><span class="math inline">\(\mathbf{v_1}\)</span> 과 <span class="math inline">\(\mathbf{v_2}\)</span>의 linear combination으로 2차원 좌표평면 나타내기 <img src="images/chap02_09.PNG" class="img-fluid"></li>
</ul></li>
</ul>
</section>
<section id="선형-독립linear-independent" class="level2" data-number="1.9">
<h2 data-number="1.9" class="anchored" data-anchor-id="선형-독립linear-independent"><span class="header-section-number">1.9</span> 선형 독립(Linear Independent)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/mOOI4-BfjGQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>…and also see</p>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/9F4PZ_1orF0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li>선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌</li>
<li>orthogonal 하면 independent
<ul>
<li>but independent해도 항상 orthogonal하지는 않음 (Independent &gt; Orthogonal)</li>
</ul></li>
<li>definition: <span class="math inline">\(a_1 \mathbf{v_1} + a_2 \mathbf{v_2} + a_3 \mathbf{v_3} \cdots a_n \mathbf{v_n} = \mathbf{0}\)</span> 를 만족하는 <span class="math inline">\(a_1, a_2, a_3, \cdots a_n\)</span> 이 <span class="math inline">\(a_1 = a_2 = a_3 = \cdots = a_n = 0\)</span> 밖에 없을때
<ul>
<li><span class="math inline">\(\mathbf{0}\)</span>는 모든 elements가 <span class="math inline">\(0\)</span>인 벡터</li>
<li>예: <span class="math inline">\(\begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span>, <span class="math inline">\(\begin{bmatrix} 2 \\ 2 \end{bmatrix}\)</span> 는 <span class="math inline">\(-2 \begin{bmatrix} 1 \\ 1 \end{bmatrix} + 1 \begin{bmatrix} 2 \\ 2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\)</span> 이 되므로, linearly independent 하지 않음</li>
<li>independent한 vector 들의 수 = 표현할 수 있는 차원의 dimension</li>
</ul></li>
</ul>
</section>
<section id="기저basis" class="level2" data-number="1.10">
<h2 data-number="1.10" class="anchored" data-anchor-id="기저basis"><span class="header-section-number">1.10</span> 기저(basis)</h2>
<ul>
<li>주어진 vector space를 span하는 linearly independent한 vectors</li>
<li>어떤 공간을 이루는 필수적인 구성요소</li>
<li>orthogonal 하면 orthogonal basis</li>
<li>예: 2차원 좌표평면에 대해
<ul>
<li><span class="math inline">\(\begin{bmatrix} 1 \\ 0 \end{bmatrix}\)</span>, <span class="math inline">\(\begin{bmatrix} 0 \\ 1 \end{bmatrix}\)</span> : orthogonal basis</li>
<li><span class="math inline">\(\begin{bmatrix} 1 \\ 0 \end{bmatrix}\)</span>, <span class="math inline">\(\begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span> : orthogonal 하지 않은 basis</li>
<li><span class="math inline">\(\begin{bmatrix} 1 \\ 0 \end{bmatrix}\)</span>, <span class="math inline">\(\begin{bmatrix} 2 \\ 0 \end{bmatrix}\)</span> : linearly independent 하지 않으므로 basis 아님</li>
</ul></li>
</ul>
</section>
<section id="항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix" class="level2" data-number="1.11">
<h2 data-number="1.11" class="anchored" data-anchor-id="항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix"><span class="header-section-number">1.11</span> 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/XqOvyfMUAwA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="identity-matrix항등행렬" class="level3" data-number="1.11.1">
<h3 data-number="1.11.1" class="anchored" data-anchor-id="identity-matrix항등행렬"><span class="header-section-number">1.11.1</span> Identity matrix(항등행렬)</h3>
<ul>
<li>항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)
<ul>
<li>실수에서 곱셈의 항등원은 1</li>
</ul></li>
<li>행렬의 항등원: 항등행렬(<span class="math inline">\(I\)</span>) <span class="math display">\[I = \begin{bmatrix} 1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; 1 \end{bmatrix}\]</span>
<ul>
<li><span class="math inline">\(A_{m \times n} \times I_{n \times n = n} = A_{m \times n}\)</span></li>
<li><span class="math inline">\(I_{m \times m = m} \times A_{m \times n} = A_{m \times n}\)</span></li>
</ul></li>
</ul>
</section>
<section id="sec-inv" class="level3" data-number="1.11.2">
<h3 data-number="1.11.2" class="anchored" data-anchor-id="sec-inv"><span class="header-section-number">1.11.2</span> Inverse matrix(역행렬)</h3>
<ul>
<li>역원: 연산 결과 항등원이 나오게 하는 연소
<ul>
<li>실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): <span class="math inline">\(a \times a^{-1} = 1\)</span></li>
</ul></li>
<li>행렬의 역원: 역행렬(<span class="math inline">\(A^{-1}\)</span>) <span class="math display">\[A \times A^{-1} = I , A^{-1} \times A = I\]</span>
<ul>
<li>존재하지 않는 경우도 있음</li>
<li>존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림
<ul>
<li>존재하지 않으면 singular, degenerate라고 불림</li>
</ul></li>
<li>square matrix(정사각행렬, <span class="math inline">\(m = n\)</span>)은 특수한 경우를 제외하면 역행렬이 항상 존재
<ul>
<li>역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우</li>
</ul></li>
<li><span class="math inline">\(m \neq n\)</span>인 행렬의 경우에는 역행렬이 존재하지 않음
<ul>
<li>다만, 경우에 따라 <span class="math inline">\(A \times A^{-1} = I\)</span> 를 만족하거나(right inverse), <span class="math inline">\(A^{-1} \times A = I\)</span>를 만족하는(left inverse)는 <span class="math inline">\(A^{-1}\)</span>이 존재함</li>
</ul></li>
<li>연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 <span class="math display">\[A\mathbf{x} = \mathbf{b} \Rightarrow A^{-1}A\mathbf{x} = A^{-1}\mathbf{b} \Rightarrow I\mathbf{x} = A^{-1}\mathbf{b} \Rightarrow \mathbf{x} = A^{-1}\mathbf{b}\]</span></li>
</ul></li>
</ul>
</section>
<section id="diagonal-matrix대각행렬" class="level3" data-number="1.11.3">
<h3 data-number="1.11.3" class="anchored" data-anchor-id="diagonal-matrix대각행렬"><span class="header-section-number">1.11.3</span> Diagonal Matrix(대각행렬)</h3>
<ul>
<li>diagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix <span class="math display">\[ D = Diag(\mathbf{a}) = \begin{bmatrix} a_{1,1} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; a_{2,2} &amp; \cdots &amp; 0 \\ \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; a_{n,n} \end{bmatrix}\]</span>
<ul>
<li>identity matrix는 diagonal matrix</li>
<li>diagnomal matrix는 symmetric matrix 이기도 함</li>
<li>보통은 square matrix에서 주로 사용됨
<ul>
<li>square matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="orthogonal-matrix직교행렬-orthonomal-matrix" class="level3" data-number="1.11.4">
<h3 data-number="1.11.4" class="anchored" data-anchor-id="orthogonal-matrix직교행렬-orthonomal-matrix"><span class="header-section-number">1.11.4</span> Orthogonal matrix(직교행렬, orthonomal matrix)</h3>
<ul>
<li>행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) <span class="math display">\[A A^T = A^T A = I\]</span></li>
<li>identity matrix는 orthogonal matrix</li>
<li>square matrix에서만 정의됨</li>
<li>Orthogonal matrix인 <span class="math inline">\(A\)</span>이면 <span class="math inline">\(A^{-1} = A^{T}\)</span>
<ul>
<li>각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임</li>
</ul></li>
<li>complex matrix(복소수 행렬)에서는 unitary matrix라고 부름</li>
</ul>
</section>
</section>
<section id="계수rank" class="level2" data-number="1.12">
<h2 data-number="1.12" class="anchored" data-anchor-id="계수rank"><span class="header-section-number">1.12</span> 계수(Rank)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/HMST0Yc7EXE" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li>rank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension</li>
<li>independent한 column의 수 = independent한 행의 수: <span class="math inline">\(rank(A) = rank(A^T)\)</span>
<ul>
<li>proof: <a href="https://en.wikipedia.org/wiki/Rank_%28linear_algebra%29#Proofs_that_column_rank_=_row_rank">Wikipedia</a></li>
</ul></li>
<li>예: <span class="math display">\[\begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} \Rightarrow rank=1\]</span> <span class="math display">\[\begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \Rightarrow rank=2\]</span></li>
<li><span class="math inline">\(A_{m \times n}\)</span> 의 최대 랭크는 <span class="math inline">\(min\{m,n\}\)</span>
<ul>
<li><span class="math inline">\(rank(A) &lt; min\{m,n\}\)</span> 면 rank-deficient, <span class="math inline">\(rank(A) = min\{m,n\}\)</span>면 full (row/column) rank</li>
</ul></li>
</ul>
</section>
<section id="영공간null-space" class="level2" data-number="1.13">
<h2 data-number="1.13" class="anchored" data-anchor-id="영공간null-space"><span class="header-section-number">1.13</span> 영공간(Null space)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/Eizc9TSRYMQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li><span class="math inline">\(A\mathbf{x}= \mathbf{0}\)</span> 을 만족하는 <span class="math inline">\(\mathbf{x}\)</span>의 집합
<ul>
<li>column space 관점에서 보기: <span class="math inline">\(A\mathbf{x} = x_1 \mathbf{a_1} + x_2 \mathbf{a_2} + \cdots + x_n \mathbf{a_n} = \mathbf{0}\)</span></li>
<li>null space에 항상 들어가는 <span class="math inline">\(\mathbf{x} = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}\)</span> : trivial solution
<ul>
<li>모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 <span class="math inline">\(\mathbf{x}=\mathbf{0}\)</span>하나 밖에 없음</li>
</ul></li>
</ul></li>
<li><span class="math inline">\(\mathbf{x}=\mathbf{0}\)</span> 가 아닌 vector가 null space에 있으면, 스칼라배(constant <span class="math inline">\(c\)</span>에 대해 <span class="math inline">\(c \mathbf{x}\)</span>) 역시 null space에 포함됨</li>
<li>혼동 주의! null space는 column space의 일부가 아님
<ul>
<li>row vector의 차원이 null space가 존재하는 공간</li>
</ul></li>
<li>rank와 null space의 dimension의 합은 항상 matrix의 column의 수
<ul>
<li><span class="math inline">\(A_{m \times n}\)</span>에 대해, <span class="math inline">\(dim(N(A)) = n - r\)</span>
<ul>
<li>모든 columns이 다 lienarly independent 하면 null space는 0차원(점)</li>
</ul></li>
<li>null space는 row space와 수직한 space
<ul>
<li><span class="math inline">\(A\mathbf{x}= \mathbf{0}\)</span> : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0</li>
<li>rank는 row space의 dimension → row space의 dimension(<span class="math inline">\(dim(R(A))\)</span>)과 null space의 dimension(<span class="math inline">\(dim(N(A))\)</span>)의 합이 <span class="math inline">\(n\)</span></li>
<li><span class="math inline">\(\mathbb{R^n}\)</span> 공간에 표현: <img src="images/chap02_10.PNG" class="img-fluid">
<ul>
<li>겹친 점: 영벡터</li>
</ul></li>
</ul></li>
</ul></li>
<li>left null space: <span class="math inline">\(\mathbf{x^T} A = \mathbf{0^T}\)</span> 인 <span class="math inline">\(\mathbf{x}\)</span>
<ul>
<li>위의 성질을 row에 대해 적용
<ul>
<li>m 차원에 놓인 벡터</li>
<li>dimension: <span class="math inline">\(dim(N_L(A)) = m - r\)</span></li>
<li>column space와 수직: <span class="math inline">\(dim(N_L(A)) +dim(C(A)) = m\)</span></li>
</ul></li>
</ul></li>
<li><span class="math inline">\(R(A)\)</span>에 있는 vector <span class="math inline">\(\mathbf{x_r}\)</span> 와 <span class="math inline">\(N(A)\)</span>에 있는 vector <span class="math inline">\(\mathbf{x_n}\)</span>에 대해:
<ul>
<li><span class="math inline">\(\mathbf{x_r}\)</span>에 <span class="math inline">\(A\)</span>를 곱하면 column space로 감</li>
<li><span class="math inline">\(\mathbf{x_n}\)</span>에 <span class="math inline">\(A\)</span>를 곱하면 $</li>
<li><span class="math inline">\(A(\mathbf{x_r} +\mathbf{x_n}) = A\mathbf{x_r} + A\mathbf{x_n} = A\mathbf{x_r} = \mathbf{b}\)</span></li>
</ul></li>
</ul>
</section>
<section id="ax-b의-해의-수" class="level2" data-number="1.14">
<h2 data-number="1.14" class="anchored" data-anchor-id="ax-b의-해의-수"><span class="header-section-number">1.14</span> Ax = b의 해의 수</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/nNI2TlD598c" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li><p>full column rank 일때</p>
<ul>
<li><span class="math inline">\(\mathbf{b}\)</span>가 column space(<span class="math inline">\(C(A)\)</span>)안에 있으면 해가 하나</li>
<li><span class="math inline">\(\mathbf{b}\)</span>가 column space(<span class="math inline">\(C(A)\)</span>)안에 없으면 해가 없음 <img src="images/chap02_11.PNG" class="img-fluid"></li>
</ul></li>
<li><p>full row rank 일때</p>
<ul>
<li><span class="math inline">\(\mathbf{b}\)</span>는 항상 column space 안에 있음: 무한의 해를 가짐</li>
<li>임의의 특정한 해(particular solution) <span class="math inline">\(\mathbf{x_p}\)</span>와 null space의 vector <span class="math inline">\(\mathbf{x_n}\)</span>에 대해, <span class="math inline">\(A(\mathbf{x_p} +\mathbf{x_n})=\mathbf{b}\)</span>
<ul>
<li>즉, <span class="math inline">\(\mathbf{x_p} +\mathbf{x_n}\)</span> 도 해가 됨: complete solution
<ul>
<li>null space는 무한하므로, 해도 무한함</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>full rank 일때(square matrix): 해가 하나 존재 (<span class="math inline">\(\mathbf{x} = A^{-1}\)</span>$)</p></li>
<li><p>rank-deficient 일때</p>
<ul>
<li><span class="math inline">\(\mathbf{b}\)</span>가 column space(<span class="math inline">\(C(A)\)</span>)안에 있으면 무한한 해를 가짐</li>
<li><span class="math inline">\(\mathbf{b}\)</span>가 column space(<span class="math inline">\(C(A)\)</span>)안에 없으면 해가 없음</li>
</ul></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>