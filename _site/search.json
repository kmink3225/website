[
  {
    "objectID": "about.html#enthusiastic-data-scientist",
    "href": "about.html#enthusiastic-data-scientist",
    "title": "Kwangmin Kim",
    "section": "Enthusiastic Data Scientist",
    "text": "Enthusiastic Data Scientist\n\nInterests\nData Modeling, Statistics, Machine Learning, Deep Learning, Optimization"
  },
  {
    "objectID": "docs/blog/index.html",
    "href": "docs/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Blog Content List\n\n\nGuide Map of Blog Contents\n\n\n\n\nAll List\n\n\n\n\nAs the number of blog topics increased, it became difficult to organize the contents. In order to increase content accessibility and to look at the relevance of blogs, links to the content list by topic are listed.\n\n\n\n\n\n\nJan 1, 3000\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Statistics\n\n\nGuide Map of Blogs in Statistics Section\n\n\n\n\nStatistics\n\n\n\n\nJust enumerating a list of the contents in the the statistics section\n\n\n\n\n\n\nMay 1, 2100\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Machine Learning\n\n\nGuide Map of Blogs in Machine Learning (ML) Section\n\n\n\n\nML\n\n\n\n\nJust enumerating a list of the contents in the ML section\n\n\n\n\n\n\nApr 1, 2100\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Mathematics\n\n\nGuide Map of Blogs in Mathematics Section\n\n\n\n\nMathematics\n\n\n\n\nRather than studying pure mathematics, I focus on studying and organizing mathematical concepts by filling out this mathematics blog section with some mathematics stuff for deep learning\n\n\n\n\n\n\nMar 1, 2100\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Deep Learning\n\n\nGuide Map of Blogs in Deep Learning Section\n\n\n\n\nDL\n\n\n\n\nJust enumerating a list of the contents in the Deep Learning (DL) section\n\n\n\n\n\n\nJan 1, 2100\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Engineering\n\n\nEngineering for Python\n\n\n\n\nEngineering\n\n\n\n\nEngineering for Data Science\n\n\n\n\n\n\nJan 1, 2100\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Language\n\n\nContants of Blogs in Language Section\n\n\n\n\nLanguage\n\n\n\n\nJust enumerating a list of the contents in the Language section\n\n\n\n\n\n\nJan 1, 2100\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Patent\n\n\nGuide Map of Blogs\n\n\n\n\nPatent\n\n\n\n\nJust enumerating a list of the contents in the the statistics section\n\n\n\n\n\n\nJan 1, 2100\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent List, Validation\n\n\nVerification & Validation on Software\n\n\n\n\nSurveilance\n\n\n\n\nAny business that directly or indirectly affects human health or life must comply with regulations regarding inspection, testing, verification and validation. It is necessary to systematically manage and document risks by arranging regulatory policy data for the medical and IT industry. These materials are rigorous and conservative, so there are various documents for each case, but the underlying principles have the same root. This blog section summarises and organizes documents with fundamental explanations of regulation for each area. 사람의 건강이나 생명에 직 간접적으로 영향을 미치는 어떠한 비즈니스는 검사, 테스트, 검증 및 인증에 관한 규정을 준수해야한다. 의료분야와 IT 분야에 대한 규정 방침 자료를 정리하여 체계적인 위험 관리를 해야한다. 이러한 자료들은 엄격하고 보수적이어서 각 사례마다 다양한 문서들이 존재하지만 그 근본 원리는 같다. 이 블로그에서는 각 영역마다 근본이 되는 문서들을 요약 및 정리한다.\n\n\n\n\n\n\nJan 1, 2090\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommon Sense in Data Sense\n\n\nStatistics, Data Science, Data Analystics, Machine Learning, Deep Learning, MISCELLANEOUS\n\n\n\n\nCommon Sense\n\n\n\n\ntemplate\n\n\n\n\n\n\nApr 22, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nVector Space\n\n\nvector space, basis vector, susbspace, dimension, rank, column space, row space, null space\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nApr 10, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfrastructure Security\n\n\nWeek2\n\n\n\n\nEngineering\n\n\n\n\nAWS\n\n\n\n\n\n\nApr 5, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Transformation (4) - Biinear Form\n\n\nLinear Regression, Fully Connected layers, Neural Networks, Linear Classifiers\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nApr 2, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Transformation (3) - Linear Form\n\n\nLinear Regression, Fully Connected layers, Neural Networks, Linear Classifiers\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nApr 2, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Calculus (1) - Matrix to Vector Derivatives\n\n\nthe sum of squares, covariance matrix, and correlation matrix\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nApr 2, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Calculus (1) - Matrix to Vector Derivatives\n\n\nthe sum of squares, covariance matrix, and correlation matrix\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nApr 2, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Transformation (5) - Quadratic Form\n\n\nthe sum of squares, covariance matrix, and correlation matrix\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nApr 2, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasics (2) - Matrix Operations\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasics (3) - Special Matrices\n\n\nSquare Matrix, Diagnogal Matrix, Identity Matrix, Symmetric Matrix, Idempotent Matrix, Oncs Matrix, Centering Matrix, Covariance Matrix, Positive Definite Matrix\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasics (4) - Tensor\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Equations\n\n\nVectors and Linear Equations, Elimination, Rules for Matrix Operations, Inverse Matrices, Factorization, Transposes and Permutations\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Equations\n\n\nVectors and Linear Equations, Elimination, Rules for Matrix Operations, Inverse Matrices, Factorization, Transposes and Permutations\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Equations\n\n\nVectors and Linear Equations, Elimination, Rules for Matrix Operations, Inverse Matrices, Factorization, Transposes and Permutations\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Equations\n\n\nVectors and Linear Equations, Elimination, Rules for Matrix Operations, Inverse Matrices, Factorization, Transposes and Permutations\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasics (3) - Special Matrices\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasics (3) - Special Matrices\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasics (3) - Special Matrices\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasics (3) - Special Matrices\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasics (3) - Special Matrices\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasics (3) - Special Matrices\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasics (3) - Special Matrices\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasics (3) - Special Matrices\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasics (3) - Special Matrices\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasics (3) - Special Matrices\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasics (3) - Special Matrices\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasics (3) - Special Matrices\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasics (1) - Vector Operations\n\n\nMotivation, Scalr, Vector, Addition, Scalar Multiplication, Inner Product, Dot Product, Linear Transformation, Hermitian Transpose, Norm, Unit Vector, Projection, Cross Product, Column Vector, Row Vector, Linear Combination of Vectors, Outer Product\n\n\n\n\nMathematics\n\n\n\n\nBasic Linear Algebra\n\n\n\n\n\n\nMar 30, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation, Statistical Bias, and Point Estimation\n\n\nOverview\n\n\n\n\nStatistics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 29, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensorflow - Data Input Pipeline\n\n\ntf.data, optimize pipeline performance, analyze pipeline performance\n\n\n\n\nLanguage\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 24, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLDA (2) - Concept & Covariance Models\n\n\nOverview\n\n\n\n\nStatistics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 24, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nMinimizer & Maximizer\n\n\nLocal & Global Minimizer and Maximizer, Critical points\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 23, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Types\n\n\ntemplate\n\n\n\n\ntemplate\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 23, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nStorage and Database\n\n\nWeek3\n\n\n\n\nEngineering\n\n\n\n\nAWS\n\n\n\n\n\n\nMar 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nStorage and Database\n\n\nWeek3\n\n\n\n\nEngineering\n\n\n\n\nAWS\n\n\n\n\n\n\nMar 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nDifferentiation - Higher Order Derivative\n\n\nHigher Order Derivative\n\n\n\n\nMathematics\n\n\n\n\nTo solve optimization problems, it is required to know about derivatives because derivatives are mostly used 최적화 문제를 풀기위해 미분이 항상 사용되기 떄문에 미분에 대해서 알 필요가 있다.\n\n\n\n\n\n\nMar 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategorical Data Analysis\n\n\ntemplate\n\n\n\n\ntemplate\n\n\n\n\ntemplate\n\n\n\n\n\n\nMar 17, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nTaylor’s Series\n\n\nTaylor’s Series and Second Derivative Test\n\n\n\n\nMathematics\n\n\n\n\n\n\n\n\n\n\n\nMar 16, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\epsilon - \\delta\\) Method\n\n\nThe Precise definition of a Limit\n\n\n\n\nMathematics\n\n\n\n\nPre-requisite for convergence in probability and convergence in distribution.\n\n\n\n\n\n\nMar 14, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nCNN\n\n\n\n\n\n\n\nDL\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRNN Concept\n\n\n\n\n\n\n\nDL\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAN Concept\n\n\n\n\n\n\n\nDL\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputing and Networking\n\n\nWeek2\n\n\n\n\nEngineering\n\n\n\n\nAWS\n\n\n\n\n\n\nMar 9, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMomment Generating Function\n\n\nMoment\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric Distribution\n\n\nExponential Family\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransformation of Random Variables\n\n\nTransformation of Functions\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransofrmations of Functions\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nFeb 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifferentiation - Chain Rule & Partial Derivative\n\n\nDerivative of Multivariable Composite Function\n\n\n\n\nMathematics\n\n\n\n\nTo solve optimization problems, it is required to know about derivatives because derivatives are mostly used 최적화 문제를 풀기위해 미분이 항상 사용되기 떄문에 미분에 대해서 알 필요가 있다.\n\n\n\n\n\n\nFeb 10, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNaive Bayes Classification\n\n\nBayes’ Rule, Supervised Learning, Classification, Regression\n\n\n\n\ntemplate\n\n\n\n\ntemplate\n\n\n\n\n\n\nFeb 6, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nBayes’ Rule\n\n\nBayesean Statistics, Frequentist Statistics, Deductive Method, Inductive Method, Proof by Contratiction, Hypothetical Deductive Method, Total Probability Rule, Naive Bayes\n\n\n\n\nStatistics\n\n\n\n\nProbability for statistics, machine learning and deep learning. Studying conditional probability is fundamental to stochastic processes, reinforcement learning, and naive Bayes classification, so it’s important to understand the concept.\n\n\n\n\n\n\nFeb 5, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConditional Probability\n\n\nConditional Probability\n\n\n\n\nStatistics\n\n\n\n\nProbability for statistics, machine learning and deep learning. Studying conditional probability is fundamental to stochastic processes, reinforcement learning, and naive Bayes classification, so it’s important to understand the concept.\n\n\n\n\n\n\nFeb 5, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability\n\n\nSet Thoery, Theory Errors, Calculus, Real Analysis, Measure Thoery\n\n\n\n\nStatistics\n\n\n\n\nProbability for statistics, machine learning and deep learning.\n\n\n\n\n\n\nFeb 5, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nDifferentiation - Univariabe Function\n\n\nUnivariable Differentiation\n\n\n\n\nMathematics\n\n\n\n\nTo solve optimization problems, it is required to know about derivatives because derivatives are mostly used 최적화 문제를 풀기위해 미분이 항상 사용되기 떄문에 미분에 대해서 알 필요가 있다.\n\n\n\n\n\n\nFeb 4, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (9) Priority Queue\n\n\nPython List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nFeb 3, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytorch Introduction\n\n\noverview, object creation, indexing, concatenating, casting, shape, transpose, arithematic operations, matrix multiplication, mean, max, argmax, dimension manipulation, automatic differenctiation\n\n\n\n\nML\n\n\n\n\nLearn how to manipulate Pytorch, one of the most commonly used Python frameworks to implement machine learning algorithms using Python. 파이썬을 이용하여 머신러닝 알고리즘을 구현하기 위해 가장 대표적으로 쓰이는 파이썬 package중 하나인 Tensor flow조작법에 대해 알아본다.\n\n\n\n\n\n\nFeb 3, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensor Introduction\n\n\noverview, object creation, indexing, concatenating, casting, shape, transpose, arithematic operations, matrix multiplication, mean, max, argmax, dimension manipulation, automatic differenctiation\n\n\n\n\nML\n\n\n\n\nLearn how to manipulate Tensor flow, one of the most commonly used Python frameworks to implement machine learning algorithms using Python. 파이썬을 이용하여 머신러닝 알고리즘을 구현하기 위해 가장 대표적으로 쓰이는 파이썬 package중 하나인 Tensor flow조작법에 대해 알아본다.\n\n\n\n\n\n\nFeb 3, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComposite Function\n\n\nDraft\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nJan 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nJan 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction - Multivariable Scalar Function\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nJan 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nFunction - Multivariable Vector Function\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nJan 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction - Univariable Scalar Function\n\n\ntemplate\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nJan 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction - Univariable Vector Function\n\n\nOne to Many\n\n\n\n\nMathematics\n\n\n\n\ntemplate\n\n\n\n\n\n\nJan 31, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nData Structure (8) Binary Search Tree\n\n\nPython List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANCOVA\n\n\nANOVA, one-way ANOVA, Two-way ANOVA, ANCOVA, repeated measures ANOVA, MANOVA, MANCOVA\n\n\n\n\nStatistics\n\n\n\n\nThe analysis of variance (ANOVA) is one of the most widely used statistical techniques. When we conduct a comparison testing of multiple groups such as A, B, and C on each with numeric data, the statistical test for a significant difference among the groups is called analysis of variance, or ANOVA.\n\n\n\n\n\n\nJan 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepeated Meausres ANOVA\n\n\nANOVA, one-way ANOVA, Two-way ANOVA, ANCOVA, repeated measures ANOVA, MANOVA, MANCOVA\n\n\n\n\nStatistics\n\n\n\n\n.\n\n\n\n\n\n\nJan 27, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (7) Deque\n\n\nPython List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 26, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (10) Graph\n\n\nPython List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 20, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (6) Queue\n\n\nPython List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 19, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (5) Stack\n\n\nPython List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 19, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (2) Array\n\n\nArray\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (3) Linked List\n\n\nLinked List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 18, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nData Structure (1) Overview\n\n\nOverview\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 17, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure (4) Python List\n\n\nPython List\n\n\n\n\nEngineering\n\n\n\n\nData Structure for Data Science\n\n\n\n\n\n\nJan 17, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntemplate\n\n\ntemplate\n\n\n\n\ntemplate\n\n\n\n\ntemplate\n\n\n\n\n\n\nJan 17, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormality Check\n\n\nShapiro-Wilk Test, Anderson–Darling test, Kolmogorov–Smirnov test, and Lilliefors test, qqplot\n\n\n\n\nStatistics\n\n\n\n\nnormality check\n\n\n\n\n\n\nJan 16, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA\n\n\nANOVA, one-way ANOVA, Two-way ANOVA, ANCOVA, repeated measures ANOVA, MANOVA\n\n\n\n\nStatistics\n\n\n\n\nThe analysis of variance (ANOVA) is one of the most widely used statistical techniques. When we conduct a comparison testing of multiple groups such as A, B, and C on each with numeric data, the statistical test for a significant difference among the groups is called analysis of variance, or ANOVA.\n\n\n\n\n\n\nJan 7, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMANOVA\n\n\nANOVA, one-way ANOVA, Two-way ANOVA, ANCOVA, repeated measures ANOVA, MANOVA\n\n\n\n\nStatistics\n\n\n\n\nThe analysis of variance (ANOVA) is one of the most widely used statistical techniques. When we conduct a comparison testing of multiple groups such as A, B, and C on each with numeric data, the statistical test for a significant difference among the groups is called analysis of variance, or ANOVA.\n\n\n\n\n\n\nJan 7, 2023\n\n\nKwangmin Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFDA Software Validation Guidance Presentation\n\n\nSource: General Principles of Software Validation\n\n\n\n\nSurveilance\n\n\n\n\nThe purpose of this article is to help understand the summary of the ‘General Principles of the ’Software Validation; Final Guidance for Industry and FDA Staff’ document issued on 2002-01-11. This article provides short sentences with many diagrams for intuitive understanding.\n\n\n\n\n\n\nDec 28, 2022\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\np-values\n\n\nStatistical Hypothesis Test\n\n\n\n\nStatistics\n\n\n\n\np-value is one of the most commonly used statistcal index to show significance level of a hypothesis testing result of your experiment.\n\n\n\n\n\n\nDec 15, 2022\n\n\nKwangmin Kim\n\n\n\n\n\n\n  \n\n\n\n\nFDA Software Validation Guidance Summary\n\n\nDcoument: General Principles of Software Validation\n\n\n\n\nSurveilance\n\n\n\n\nThe purpose of this blog is to get a rough concept of the FDA approval process by making a summary of the ‘General Principles of the ’Software Validation; Final Guidance for Industry and FDA Staff’ document issued on 2002-01-11. So far, the document seems to be still valid taking into account that its guidance for the FDA approval are broad, general, and comprehensive, and that many recent FDA documents supplement it.\n\n\n\n\n\n\nDec 15, 2022\n\n\nKwangmin Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/blog/posts/content_list.html",
    "href": "docs/blog/posts/content_list.html",
    "title": "Blog Content List",
    "section": "",
    "text": "Note\n\n\n\n\nScalars are denoted with a lower-case letter (ex a ) or a non-bolded lower-case Greek letter (ex \\(\\alpha\\) ).\nVectors are denoted using a bold-faced lower-case letter (ex \\(\\mathbf a\\)).\nMatrices are denoted using a bold-faced upper-case letter (ex \\(\\mathbf A\\), \\(\\mathbf \\phi\\)) or a bold-faced upper-case Greek letter (ex \\(\\mathbf \\Phi\\)).\nTensors are denoted using a bold-faced upper-case letter with multiple subscripts or superscripts, indicating the number of indices and the dimensions of the tensor along each axis.\n\nA second-order tensor (also known as a matrix) \\(\\mathbf A\\) with dimensions \\(n \\times m\\) can be represented as: \\(\\mathbf A_{ij}\\) where \\(i = 1,\\dots,m\\) and \\(j = 1,\\dots,n\\), which are the indices that run over the rows and columns of the matrix, respectively.\nA third-order tensor \\(T\\) with dimensions \\(n \\times m \\times p\\) can be represented as: \\(\\mathbf A_{ijk}\\) where \\(i = 1,\\dots,m\\), \\(j = 1,\\dots,n\\), which are \\(i\\), and \\(k = 1,\\dots,p\\) \\(j\\), and \\(k\\), which are the indices that run over the three dimensions of the tensor.\n\n\n\n\n\nContents\n\nDeep Learning\nMachine Learning\nMathematics\nStatistics\nEngineering\nPatent\nLanguage\nSurveilance\n\n\n\nReference\n\nStatistics\n\nGeorge Casella & Rogeer L. Berger - Statistcal Inference, 2nd Edition\nDobson and Barnett (2008) An Introduction to Generalized Linear Model. 3rd Ed. Chapman & Hall.\nFitzmaurice, Laird and Ware (2011) Applied Longitudinal Analysis. 2nd Ed. Wiley.\nHosmer, Lemeshow and May (2008) Applied Survival Analysis. 2nd Ed. Wiley.\n슬기로운 통계생활 - https://www.youtube.com/@statisticsplaybook\n슬기로운 통계생활 - https://github.com/statisticsplaybook\nFast Campus, Coursera, Inflearn\n그 외 다수의 Youtube, and Documents from Googling\n\nMathematics\n\nJames Stewart - Calculus Early Transcedentals, 7th Eidition & any James Stewart series\nGILBERT STRANG - Introduction to Linear Algebra, 4th Edition.\n임장환 - 머신러닝, 인공지능, 컴퓨터 비전 전공자를 위한 최적화 이론\nFast Campus, Coursera, Inflearn\n8일간의 선형대수학 기초(이상준 경희대 교수)\nLinear Algebra(Prof. Gilbert Strang, MIT Open Courseware)\nComputational Linear Algebra for Coders\nImmersive linear Algebra\n3blue1brown\n그 외 다수의 Youtube, and Documents from Googling\n\nMachine Learning\n\nGareth M. James, Daniela Witten, Trevor Hastie, Robert Tibshirani - An Introduction to Statistical Learning: With Applications in R 2nd Edition\nTrevor Hastie, Robert Tibshirani, Jerome H. Friedman - The Elements of Statistical Learning 2nd Edition\nFast Campus, Coursera, Inflearn\n그 외 다수의 Youtube, and Documents from Googling\n\nDeep Learning\n\nSaito Koki - Deep Learning from Scratch 1,2,3 (밑바닥부터 시작하는 딥러닝 1,2,3)\n조준우 - 머신러닝·딥러닝에 필요한 기초 수학 with 파이썬\n조준우 - https://github.com/metamath1/noviceml\n동빈나 - https://www.youtube.com/c/dongbinna\n혁펜하임 - https://www.youtube.com/channel/UCcbPAIfCa4q0x7x8yFXmBag\nFast Campus, Coursera, Inflearn\n다수의 Youtube, and Documents from Googling\n\nEngineering\n\nFast Campus, Coursera, Inflearn\n그 외 다수의 Youtube, and Documents from Googling"
  },
  {
    "objectID": "docs/blog/posts/DL/2023-03-10_cnn/index.html",
    "href": "docs/blog/posts/DL/2023-03-10_cnn/index.html",
    "title": "CNN",
    "section": "",
    "text": "Affine Layer는 인접하는 Layers의 nodes가 모두 연결되고 출력의 수가 임의로 정해지는 특징을 갖는데 이 때 data shape가 무시가 되는 단점이 있다. 이미지 데이터는 보통 (weight, height, color channel) 형태의 shape를 갖지만 MLP에서 이 3차원 구조가 1차원으로 flatten된다. 다시 말해서, 3차원의 이미지 pixels이라는 여러 독립 변수의 위치적 상관성이 1차원화 되면서 무시가 된다.\n많은 일반 머신러닝 모델이나 통계 분석 모델은 독립 변수가 독립이라는 가정이 고려되어야 하지만 이미지의 독립 변수들이 서로 독립이 아니다. 픽셀값은 그 위치에 따라서 서로 상관성이 존재한다. 초기엔 독립 변수인 픽셀을 일렬로 늘어뜨려 input으로 사용했지만 위치 기반 픽셀의 상관성 정도를 자세히 반영하진 못했다. 이를 보완하기 위해 CNN에서는 region feature가 고안됐다. 이처럼, CNN은 이미지 인식 분야에서 독보적인 영역을 갖고 있다.\n\n\n\nRegion Feature 또는 Graphic Feature라고도 한다. 픽셀의 지역 정보를 학습할 수 있는 신경망 구조가 CNN이다."
  },
  {
    "objectID": "docs/blog/posts/DL/2023-03-10_cnn/index.html#convolution-layer-conv",
    "href": "docs/blog/posts/DL/2023-03-10_cnn/index.html#convolution-layer-conv",
    "title": "CNN",
    "section": "2.1 Convolution Layer (Conv)",
    "text": "2.1 Convolution Layer (Conv)\nkernel 또는 filter라고 불리는 특징 추출기(Feature detector)를 사용하여 데이터의 특징을 추출하는 CNN 모델의 핵심 부분이다. kernel 를 정의해 입력 층의 이미지의 feature를 추출한다. kernel는 region feature의 크기와 weight을 정의하게 된다. 예를 들어, kernel을 (3x3)으로 정하면 9칸에 가중치를 설정하여 이미지 픽셀값 (a part of input feature map)과 kernel의 weight의 선형결합으로 conv layer를 구성하는 하나의 값을 얻어낸다 (See Figure 1 의 노란색 사각형).\nconvolution layer의 input/output은 보통 feature map이라고 부르며 input data를 input feature map, output data를 output feature map로 부르기도 한다. 즉, feature map 과 input/output data는 같은 의미로 사용되고 feature map = input data + kernel (= receptive field = filter)로 구성된다.\n\n2.1.1 Convolution Operation\nConvolution Operation (합성곱 연산)은 filter operation (filter 연산)이라고도 불린다. Figure 2 을 보면 입력 데이터 (이미지의 pixels)와 filter의 가중치가 element-wise 별로 곱해져 더해진다. 이 연산을 fused-multiply-add (FMA) or multiply-accumulate operation 라고도 부른다. 예를 들어, input data의 4, 5, 2, 9, 6, 4, 2, 2, 5와 filter1 의 1, 1, 1, 0, 0, 0, -1, -1, -1 가 곱해지고 더해져 4, 5, 2, 0, 0, 0, -2, -2, -5의 결과가 Conv Layer의 output data의 한 칸을 구성하게 된다.\n\n\n\nFigure 2: Convolution Operation Example\n\n\n이렇게, 2차원 입력에 대한 convolution (conv) operation은 Figure 3 과 같이 동작한다. Sharpen filter라고 쓰여진 3x3 행렬은 kernel로서 입력 데이터의 특징을 추출하는데, 입력 데이터의 전체를 보는 것이 아닌 kernel size 만큼의 일부분만을 보며 특징을 추출한다. feature map은 kernel의 개수만큼 생성되는데 일반적으로 다양한 특징을 추출하기 위해 하나의 conv layer에서 여러개의 kernel을 사용한다. kernel size에 정해진 규칙은 없으나 주로 3*3 많이 사용하며 대게 conv layer마다 다른 kernel size를 적용한다. Fully connected layer에서의 weight는 CNN에서 filter의 weight과 대응되고 CNN에서의 bias는 항상 scalar로 주어진다.\n\n\n\nFigure 3: Convolution Operation Process Example\n\n\n\n\n2.1.2 합성곱 연산을 위한 설정 사항\n\npadding : 입력 데이터의 테두리를 0으로 채워 데이터의 크기를 늘려준다\n\n왜 padding을 사용하는가? padding이 없을 경우 합성곱은 입력 데이터의 1행 1열부터 시작된다. 그런데 합성곱은 입력 데이터에서 kernel size만큼의 영역을 하나로 축소하여 특징을 추출하기 때문에 이 경우 입력 데이터의 가장자리, edge 부분의 특징을 추출하기 어렵다. edge의 특징까지 추출하고자 하면 적어도 0행 0열부터 kernel을 적용해야 하는데 허공에서 element-wise 계산을 할 수 없으니 0을 추가해준다. 다시 말해서, padding은 output data size를 조정할 목적으로 사용된다. \n\nstride : kernel이 얼만큼씩 이동하면서 합성곱 계산을 할 것인지를 의미한다.\n\nstride를 키우게 되면 output data size가 작아지기 때문에 일반적으로 한 칸씩 이동한다. \n\nweight sharing:\n\nkernel size : kernel의 행과 열 개수\n\n사이즈가 작을수록 국소 단위의 특징을 추출한다.\n\nkernel 개수 또는 channel 개수 : 몇 개의 feature map을 추출하고 싶은지에 따라 kernel 개수를 정한다. \n이미지 데이터에서의 channel 예시\n\n고양이 이미지와 같이 컬러 이미지 데이터는 하나의 이미지에 대해 Red, Green, Blue (RGB) 3개의 색상으로 이루어져 있다  \n\n\n\n\n2.1.3 feature map의 shape 계산 방법\nfeature map은 다음 레이어의 입력 데이터가 되기 때문에 feature map의 shape을 계산할 수 있어야한다.\n\n2.1.3.1 2 Dimension Input Data Size\n다음과 같이 output data size계산을 위한 notation을 정의할 때,\n\ninput data size : \\((H,W)\\)\nfilter size : \\((FH, FW)\\)\noutput data size : \\((OH, OW)\\)\npadding : \\(P\\) (width number)\nstride : \\(S\\)\na function to make the calculation result an integer\n\nfloor function : \\(\\lfloor \\text{ } \\rfloor\\)\nceiling : \\(\\lceil \\text{ } \\rceil\\)\nrounding to the nearest integer: \\(\\lfloor \\text{ } \\rceil\\)\n\n\n\\[\n\\begin{aligned}\n  OH=&\\lfloor\\frac{H+2P-FH}{S}+1\\rfloor\\\\\n  OW=&\\lfloor\\frac{W+2P-FW}{S}+1\\rfloor\n\\end{aligned}\n\\]\n의 관계식을 따르게 된다.\n\n예시1- input size: (4x4), P:1, S:1, filter size : (3x3)일 때, \\((OH,OW)=(4,4)\\)\n예시2- input size: (7x7), P:0, S:2, filter size : (3x3)일 때, \\((OH,OW)=(3,3)\\)\n예시3- input size: (28x31), P:2, S:3, filter size : (5x5)일 때, \\((OH,OW)=(10,11)\\)\n\n\n\n2.1.3.2 3 Dimension Input Data Size\n길이 또는 채널 방향으로 feature map이 늘어나기 때문에 그 결과는 Figure 2 과 같이 나온다. 반드시 input data의 channel 수와 output data channel수가 같아야한다. 채널이 3개면 filter 당 3장의 feature map이 나오게 된다. filter의 종류의 수 weight의 종류의 수로 output data의 길이를 늘리려면 (즉, 다수의 채널로 만드려면) filter의 수 (=weight의 종류)를 늘리면 된다. FN: Flter Number일 때 filter의 가중치 데이터 크기는 (output data channel, input data channel, height, width)로 표현한다. Bias는 \\((FN,1,1)\\) 로 표현하여 채널 하나에 값 하나씩 할당되게 디자인한다. Output data size는 \\((FN,OH,OW)\\) 로 표현된다.\n참고) \\[\n(FN,1,1) + (FN,OH,OW) \\overset{\\text{broadcasting}} \\rightarrow (FN,OH,OW)\n\\]\n\n예시- 채널=3, (FH,FW)=(4,4), \\(FN=20\\) 이면 \\((20,3,4,4)\\) 로 표현"
  },
  {
    "objectID": "docs/blog/posts/DL/2023-03-10_cnn/index.html#batch-processing",
    "href": "docs/blog/posts/DL/2023-03-10_cnn/index.html#batch-processing",
    "title": "CNN",
    "section": "2.2 Batch Processing",
    "text": "2.2 Batch Processing\n데이터를 (데이터수, 채널 수, 높이, 너비) \\(= (N,C,H,W)\\) 순으로 저장하여 처리하여 NN에 4차원 데이터가 하나가 흐를 때마다 데이터 N개의 합성곱 연산이 발생한다. N번의 처리를 한번에 수행한다."
  },
  {
    "objectID": "docs/blog/posts/DL/2023-03-10_cnn/index.html#pooling-layer",
    "href": "docs/blog/posts/DL/2023-03-10_cnn/index.html#pooling-layer",
    "title": "CNN",
    "section": "2.3 Pooling Layer",
    "text": "2.3 Pooling Layer"
  },
  {
    "objectID": "docs/blog/posts/DL/2023-03-10_cnn/index.html#fully-connected-layer",
    "href": "docs/blog/posts/DL/2023-03-10_cnn/index.html#fully-connected-layer",
    "title": "CNN",
    "section": "2.4 Fully Connected Layer",
    "text": "2.4 Fully Connected Layer"
  },
  {
    "objectID": "docs/blog/posts/DL/GAN/gan_concept.html",
    "href": "docs/blog/posts/DL/GAN/gan_concept.html",
    "title": "RNN Concept",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n(The Very First Draft)\n\n\nGNN stands for Generative Adversarial Network\n\nGenerator 와 Discriminator가 서로 Adversarial\n\n서로 동시에 학습 불가\n\nG : D를 속이고 싶음. → D에서 G의 출력 값이 정답으로 나와야 함.\nD : G가 만든 출력과 real 데이터를 구분하고 싶음. →\nG 학습 → D 학습 → G 학습 → D 학습 … 반복\nGAN 모델이 잘 만들어지면 D는 G의 출력과 real 데이터를 잘 구분하지 못함. → 0.5, 0.5\n\n입력\n출력\n\nG의 결과\n\n\n\n그림1과 같이 RNN에서는 hidden layer에서 activation function을 통해 output을 출력하는 노드를 cell이라고 표현한다. 이는 cell이 이전 시점의 값을 기억하는 일종의 메모리 역할을 수행하기 때문이며 memory cell 또는 RNN cell이라고 부르기도 한다.\n<그림2>\nRNN 모델에서 cell은 이전 시점에 출력된 값을 현 시점의 입력으로 사용하는 재귀적 활동을 한다. 그림2의 왼쪽 그림은 그림1을 세로로 표현한 버전으로 \\(t\\)는 현재 시점을 의미한다. 그림2의 오른쪽 그림은 time step에 따라 cell의 output이 어떻게 다음 시점의 입력이 되는지를 보여준다.\n이 때, 현재 시점(\\(t\\))에서 cell의 입력으로 사용되는 과거 시점(\\(t-1\\))의 cell의 output 값을 hidden state라고 부른다.\n\n\nRNN 모델은 FeedForward network와 달리 해결하고 싶은 문제에 따라 입력과 출력의 길이를 다양하게 조절할 수 있다.\n<그림3>\n\n\n하나의 입력에 대해 하나의 결과를 출력\n\n\n\n하나의 입력에 대해 여러 개의 결과를 출력\n예시\n\n이미지 캡셔닝 (Image Captioning) : 이미지 입력 시 이미지의 제목(단어의 나열)을 출력\n\n\n\n\n여러 개의 입력에 대해 하나의 결과를 출력\n예시\n\n감성 분류 (sentiment classification) : 문장의 긍부정 판별\n\n\n\n\n여러 개의 입력에 대해 여러 개의 결과를 출력\n예시\n\nchatbot : 사용자가 문장을 입력하면 문장으로 답변.\n번역 : 영어 문장을 입력하면 한국어로 번역한 문장 출력.\n개체명 인식 :\n품사 태깅 :\n\n\n\n\n\nRNN 설명이 gif로 잘 정리되어 있음\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Content List\n\n\n\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/DL/guide_map/index.html",
    "href": "docs/blog/posts/DL/guide_map/index.html",
    "title": "Content List, Deep Learning",
    "section": "",
    "text": "(Draft)"
  },
  {
    "objectID": "docs/blog/posts/DL/guide_map/index.html#introduction",
    "href": "docs/blog/posts/DL/guide_map/index.html#introduction",
    "title": "Content List, Deep Learning",
    "section": "Introduction",
    "text": "Introduction\n\n1111-11-11, Artificial Intelligence\n1111-11-11, Perceptron\n1111-11-11, Artificial Neural Netwroks (ANN)\n\n1111-11-11, activation functions\n1111-11-11, output layer design\n\n1111-11-11, loss function\n1111-11-11, numerical differentiation\n1111-11-11, gradient descent\n1111-11-11, backpropagation\n1111-11-11, optimizer\n\n1111-11-11, stochastic gradient descent\n1111-11-11, momentum\n1111-11-11, adaGrad\n1111-11-11, adam\n1111-11-11, weight initalization\n\n1111-11-11, batch normalization\n1111-11-11, dropout\n1111-11-11, tuning parameter\n1111-11-11, auto-encoder\n1111-11-11, stacked auto-encoder\n1111-11-11, denoising auto-encoder(DAE)\n\n\nConvolutional Neural Network (CNN)\n\n2023-03-10, CNN (1) - Concept\n2023-03-10, CNN (2) - Practice\n\n\n\nNatural Language Process (NLP)\n\n1111-11-11, word2vec\n1111-11-11, improved word2vec\n\n\n\nRecurrent Neural Network (RNN)\n\n\nGate Recurrent Unit (GRU)\n\n\nLong Short-Term Memory (LSTM)\n\n\nAttention (Transformer)\n\n\nBidirectional Encoder Representations from Transformers (BERT)\n\n\nGenerative Pre-training Transformer (GPT)"
  },
  {
    "objectID": "docs/blog/posts/DL/RNN/rnn_concept.html",
    "href": "docs/blog/posts/DL/RNN/rnn_concept.html",
    "title": "GAN Concept",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n(The Very First Draft)\n\n\nRNN stands for Recurrent Neural Network\nSequence 모델로 hidden layer의 결과를 출력층 방향 + 다음 계산의 입력으로 사용하는 모델\n주로 sequential data 또는 time series data에 사용된다.\n<그림1>\n그림1과 같이 RNN에서는 hidden layer에서 activation function을 통해 output을 출력하는 노드를 cell이라고 표현한다. 이는 cell이 이전 시점의 값을 기억하는 일종의 메모리 역할을 수행하기 때문이며 memory cell 또는 RNN cell이라고 부르기도 한다.\n<그림2>\nRNN 모델에서 cell은 이전 시점에 출력된 값을 현 시점의 입력으로 사용하는 재귀적 활동을 한다. 그림2의 왼쪽 그림은 그림1을 세로로 표현한 버전으로 \\(t\\)는 현재 시점을 의미한다. 그림2의 오른쪽 그림은 time step에 따라 cell의 output이 어떻게 다음 시점의 입력이 되는지를 보여준다.\n이 때, 현재 시점(\\(t\\))에서 cell의 입력으로 사용되는 과거 시점(\\(t-1\\))의 cell의 output 값을 hidden state라고 부른다.\n\n\nRNN 모델은 FeedForward network와 달리 해결하고 싶은 문제에 따라 입력과 출력의 길이를 다양하게 조절할 수 있다.\n<그림3>\n\n\n하나의 입력에 대해 하나의 결과를 출력\n\n\n\n하나의 입력에 대해 여러 개의 결과를 출력\n예시\n\n이미지 캡셔닝 (Image Captioning) : 이미지 입력 시 이미지의 제목(단어의 나열)을 출력\n\n\n\n\n여러 개의 입력에 대해 하나의 결과를 출력\n예시\n\n감성 분류 (sentiment classification) : 문장의 긍부정 판별\n\n\n\n\n여러 개의 입력에 대해 여러 개의 결과를 출력\n예시\n\nchatbot : 사용자가 문장을 입력하면 문장으로 답변.\n번역 : 영어 문장을 입력하면 한국어로 번역한 문장 출력.\n개체명 인식 :\n품사 태깅 :\n\n\n\n\n\nRNN 설명이 gif로 잘 정리되어 있음\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Content List\n\n\n\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-17_data_structure_overview/index.html",
    "title": "Data Structure (1) Overview",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n딥러닝은 다양한 알고리즘의 조합으로 수행되기 때문에 다양한 알고리즘을 정확하게 작성하기 위해서는 다수의 다양한 자료(data)를 담기 위해 사용되는 자료 구조를 이해할 필요가 있다. 즉, 자료구조는 정확한 알고리즘을 구현하기 위해 다수의 자료(data)를 담기 위한 구조이다.\n\n딥러닝 유저들간에도 자료구조를 이해하는 것에 대한 의견이 분분하지만\n올바른 자료구조를 사용하는 것은 프로그램을 조직적으로 만들 수 있는 능력을 키울 수 있다.\n데이터의 수가 많아질수록 효율적인 자료구조가 필요하다.\n예시) 학생 수가 1,000,000명 이상인 학생 관리 프로그램\n\n매일 자료 조회가 1억번 이상 발생한다면 더 빠르게 동작하는 자료 구조를 사용해야 프로그램의 효율성을 올릴 수 있다.\n\n\n\n\n\n자료구조의 필요성에 대해서 이해할 필요가 있다.\n성능 비교: 자료구조/알고리즘의 성능 측정 방법에 대해 이해할 필요가 있다.\n\nA: 적당한 속도의 삽입 & 적당한 속도의 추출 (삽입: \\(O (log N)\\) / 추출: \\(O(log N)\\))\nB: 느린 삽입 & 빠른 추출 (삽입: \\(O (N)\\) / 추출: \\(O (1)\\))\nA vs B? 상황에 따라 A를 만들지 B를 만들지 선택해야 한다. 삽입 연산이 많으면 A를, 추출 연산이 많으면 B를 택해야 한다. (속도 비교: \\(O (N) < O (log N)< O (1)\\))\n하지만, 실무적으로 많은 개발자들이 A를 택한다. 왜냐면 log 복잡도는 상수 복잡도와 속도가 비슷하기 때문\n\n\n\n\n\n\n\n이처럼 상황에 맞게 알고리즘의 연산 속도를 결정해야 하므로 데이터를 효과적으로 저장하고, 처리하는 방법에 대해 바르게 이해할 필요가 있다.\n자료구조를 제대로 이해해야 불필요하게 메모리와 계산을 낭비하지 않는다.\nC언어를 기준으로 정수(int) 형식의 데이터가 100만 개가량이 존재한다고 가정하자.\n해당 프로그램을 이용하면, 내부적으로 하루에 데이터 조회가 1억 번 이상 발생한다.\n이때 원하는 데이터를 가장 빠르게 찾도록 해주는 자료구조는 무엇일까?\n\n트리(tree)와 같은 자료구조를 활용할 수 있다.\n\n\n\n\n\n\n선형 자료 구조(linear data structure) 선형 자료구조는 하나의 데이터 뒤에 다른 데이터가 하나 존재하는 자료구조이며 데이터가 일렬로 연속적으로(순차적으로) 연결되어 있다.\n\n배열(array)\n연결 리스트(linked list)\n스택(stack)\n큐(queue)\n\n비선형 자료 구조(non-linear data structure)\n비선형 자료구조는 하나의 데이터 뒤에 다른 데이터가 여러 개 올 수 있는 자료구조이며 데이터가 일직선상으로 연결되어 있지 않아도 된다.\n\n트리(tree)\n그래프(graph)\n\n\n\n\n\n\n효율적인 자료구조 설계를 위해 알고리즘 지식이 필요하다.\n효율적인 알고리즘을 작성하기 위해서 문제 상황에 맞는 적절한 자료구조가 사용되어야 한다.\n프로그램을 작성할 때 자료구조와 알고리즘 모두 고려해야 한다.\n\n\n\n\n\n시간 복잡도(time complexity): 알고리즘에 사용되는 연산 횟수를 측정 (시간 측정)\n공간 복잡도(space complexity): 알고리즘에 사용되는 메모리의 양을 측정 (공간 측정)\n공간을 많이 사용하는 대신 시간을 단축하는 방법이 흔히 사용된다.\n프로그램의 성능 측정 방법: Big-O 표기법\n\n복잡도를 표현할 때는 Big-O 표기법을 사용한다.\n\n특정한 알고리즘이 얼마나 효율적인지 수치적으로 표현할 수 있다.\n가장 빠르게 증가하는 항만을 고려하는 표기법이다.\n\n아래의 알고리즘은 \\(O(n)\\) 의 시간 복잡도를 가진다. 왜냐면, n에 따라 summary += i의 연산 횟수가 정해지기 때문이다.\n\n\n\n\nCode\nn = 10\nsummary = 0\nfor i in range(n):\n    summary += i\nprint(summary)\n\n\n45\n\n\n\n다음 알고리즘은 \\(O (n^2)\\) 의 시간 복잡도를 가진다. 2 중 for loop은 i와 j가 n에 따라 각 각 n 번씩 연산되기때문에 \\(n \\times n\\) 회 만큼 연산된다.\n\n\n\nCode\nn = 3\nfor i in range(1, n + 1):\n    for j in range(1, n + 1):\n        print(f\"{i} X {j} = {i * j}\")\n\n\n1 X 1 = 1\n1 X 2 = 2\n1 X 3 = 3\n2 X 1 = 2\n2 X 2 = 4\n2 X 3 = 6\n3 X 1 = 3\n3 X 2 = 6\n3 X 3 = 9\n\n\n\n일반적으로 연산 횟수가 10억 (\\(1.0 \\times 10^9\\))을 넘어가면 1초 이상의 시간이 소요된다.\n[예시] n이 1,000일 때를 고려해 보자.\n\n\\(O(n)\\): 약 1,000번의 연산\n\\(O(nlogn )\\): 약 10,000번의 연산 (약 \\(log10=10\\))\n\\(O(n^2)\\): 약 1,000,000번의 연산\n\\(O(n^3)\\): 약 1,000,000,000번의 연산\n\n그러므로, 알고리즘 짤 때 코딩 레벨로 연산 횟수를 계산해서 연산 시간을 어림잡아 추정할 수 있다.\n시간 복잡도 속도 비교\n By Cmglee - Own work, CC BY-SA 4.0\nBig-O 표기법으로 시간 복잡도를 표기할 때는 가장 영향력이 큰 항만을 표시한다.\n\n\\(O(3n^2 + n) = O(n^2)\\)\n현실 세계에서는 동작 시간이 1초 이내인 알고리즘을 설계할 필요가 있다.\n실무적으로 프로그램 동작 시간이 1초 이상이면 매우 느린 것으로 간주.\n\n공간 복잡도를 나타낼 때는 MB 단위로 표기한다.\nint a[1000]: 4KB int a[1000000]: 4MB int a[2000][2000]: 16MB\n자료구조를 적절히 활용하기\n\n자료구조의 종류로는 스택, 큐, 트리 등이 있다.\n프로그램을 작성할 때는 자료구조를 적절히 활용하여 시간 복잡도를 최소화하여야 한다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_array/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-18_array/index.html",
    "title": "Data Structure (2) Array",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n가장 기본적인 자료구조다.\n여러 개의 변수를 담는 공간으로 이해할 수 있다.\ndata가 연속적으로 들어가는 형태여서 배열은 인덱스(index)가 존재하며, 인덱스는 0부터 시작한다.\n특정한 인덱스에 직접적으로 접근 가능하여 수행 시간은 빠른 속도인 \\(O(1)\\) 이다.\n\n\n\n\n\n컴퓨터의 메인 메모리에서 배열의 공간은 연속적으로 할당된다.\n장점: Cache memory(속도면에서, \\(RAM<Cache<CPU\\), 공간면에서, \\(CPU<RAM<Cache\\), CPU옆에 위치) 히트(RAM에 있는 data를 Cache에 일부 옮기는 현상) 가능성이 높으며, 조회가 빠르다. 배열 같은 경우는 공간적으로 또는 연속적으로 붙어 있기때문에 cache memory 묶어서 옮길 수 있다.\n\nCache Hit: 원하는 data가 Cache Memory존재하는 것을 의미.\n특정 index에 접근하는 속도가 매우 빠르다, \\(O(1)\\).\n\n단점: 배열의 크기를 미리 지정해야 하는 것이 일반적이므로, 데이터의 추가 및 삭제에 한계가 있다.\n\n\n\n\n\n컴퓨터의 메인 메모리(RAM)상에서 주소가 연속적이지 않다.\n배열과 다르게 크기가 정해져 있지 않고, 리스트의 크기는 동적으로 변경 가능하다.\n장점: 포인터(pointer)를 통해 다음 데이터의 위치를 가리킨다는 점에서 삽입과 삭제가 간편하다.\n단점: 원소를 검색할 때는 포인터가 앞에서부터 원소를 찾아야 하므로, 데이터 검색 속도가 느리다.\n\n\n\n\n파이썬의 리스트(List) 자료형\n\n파이썬에서는 리스트 자료형을 제공한다. (컴퓨터 공학에서의 연결 리스트와는 다른 의미)\n일반적인 프로그래밍 언어에서의 배열로 이해할 수 있다. 그러므로, 파이썬의 리스트는 배열이라고 생각해야한다.\n\n파이썬의 리스트는 배열처럼 임의의 인덱스를 이용해 직접적인 접근이 가능하다.\n\n파이썬의 리스트 자료형은 동적 배열이다.\n\nappend를 이용해 데이터를 삽입할 때 배열의 용량이 가득 차면, 자동으로 크기를 증가시킨다.\n\n내부적으로 포인터(pointer)를 사용하여, 연결 리스트의 장점도 가지고 있다.\n배열(array) 혹은 스택(stack)의 기능이 필요할 때 리스트 자료형을 그대로 사용할 수 있다.\n큐(queue)의 기능을 제공하지 못한다. (비효율적)\n\n\n\n\n\n파이썬에서는 임의의 크기를 가지는 배열을 만들 수 있다.\n일반적으로 리스트를 초기화할 때 리스트 컴프리헨션(list comprehension)이 자주 사용된다. (매우 편리)\n크기가 N인 1차원 배열을 만드는 방법은 다음과 같다.\n\n\n\nCode\n# [0, 0, 0, 0, 0]\nn = 5\narr = [0] * n\nprint(arr)\n\n# [0, 1, 2, 3, 4]\nn = 5\narr = [i for i in range(n)]\nprint(arr)\n\n\n[0, 0, 0, 0, 0]\n[0, 1, 2, 3, 4]\n\n\n\n크기가 \\(N \\times M\\) 인 2차원 리스트(배열) 만들기 1\n\n2차원 배열이 필요할 때는 다음과 같이 초기화한다.\n\n\n\n\nCode\nn = 3\nm = 5\narr = [[0] * m for i in range(n)]\nprint(arr)\n\n\n[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\n\n\n\n크기가 \\(N \\times M\\) 인 2차원 리스트(배열) 만들기 2\n\n\n\nCode\nn = 3\nm = 5\narr = [[i * m + j for j in range(m)] for i in range(n)]\nprint(arr)\n\n\n[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]\n\n\n\n\n\n\n리스트는 기본적으로 메모리 주소를 반환한다.\n따라서 단순히 [[0]∗m]∗n 형태로 배열을 초기화하면 안 된다.\n이렇게 초기화를 하게 되면, n개의 [0]∗m 리스트는 모두 같은 객체로 인식된다.\n다시 말해, 같은 메모리를(동일한 리스트를) 가리키는 n개의 원소를 담는 리스트가 된다.\n2차원 배열을 초기화할 때는 리스트 컴프리헨션을 이용하는 것이 일반적이다.\n\n\n\nCode\nn = 3\nm = 5\narr1 = [[0] * m] * n # 잘못된 방식\narr2 = [[0] * m for i in range(n)] # 옳은 방식\n\narr1[1][3] = 7\narr2[1][3] = 7\n\nprint(arr1)\nprint(arr2)\n\n\n[[0, 0, 0, 7, 0], [0, 0, 0, 7, 0], [0, 0, 0, 7, 0]]\n[[0, 0, 0, 0, 0], [0, 0, 0, 7, 0], [0, 0, 0, 0, 0]]\n\n\n\n\n위의 결과를 보면, 잘못된 방식으로 초기화된 배열 arr1은 [[0, 0, 0, 7, 0], [0, 0, 0, 7, 0], [0, 0, 0, 7, 0]]와 같이 7의 삽입이 모든 행에 걸쳐서 적용됐다. 반면에, 올바른 방식으로 초기화된 arr2는 [[0, 0, 0, 0, 0], [0, 0, 0, 7, 0], [0, 0, 0, 0, 0]]는 의도된 대로 하나의 element가 [1][3] index에 삽입이 된 것을 볼 수 있다.\n\n\n\n\n\n\n자신이 원하는 임의의 값을 넣어 곧바로 사용할 수 있다.\n\n\n\nCode\narr = [0, 1, 2, 3, 4, 5, 6, 7, 8]\nprint(arr)\n\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_linked_list/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-18_linked_list/index.html",
    "title": "Data Structure (3) Linked List",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n연결 리스트는 각 노드가 한 줄로 연결되어 있는 자료 구조다.\n각 노드는 (데이터, 포인터) 형태를 가진다.\n포인터: 다음 노드의 메모리 주소를 가리키는 목적으로 사용된다.\n연결성: 각 노드의 포인터는 다음 혹은 이전 노드를 가리킨다.\n\n연결 리스트를 이용하면 다양한 자료구조를 구현할 수 있다.\n\n예시) 스택, 큐 등을 구현 가능\n\nPython은 연결 리스트를 활용하는 자료구조를 제공한다.\n연결 리스트를 실제 구현해야 하는 경우는 적지만, 그 원리 이해는 자료 구조와 클래스를 작성하는데 도움이 된다.\n\n\n\n\n\n연결 리스트와 배열(array)을 비교하여 장단점을 이해할 필요가 있다.\n특정 위치의 데이터를 삭제할 때, 일반적인 배열에서는 \\(O(N)\\) 만큼의 시간이 소요된다.\n하지만, 연결 리스트를 이용하면 단순히 연결만 끊어주면 된다.\n따라서 삭제할 위치를 정확히 알고 있는 경우 \\(O(1)\\) 의 시간이 소요된다.\n하지만 삭제할 위치를 정확히 알아내기 위해 앞의 코드를 자세히 보게 되는 소요 시간이 증가할 수 있다.\n\n\n\n\n배열에 새로운 원소를 삽입할 때, 최악의 경우 시간 복잡도를 계산하여라.\n예시) 배열에서 인덱스 3에 원소 “59”를 삽입할 경우, 인덱스 4 이후의 공간에 있는 데이터를 한칸씩 밀어내는 \\(O(n)\\) 만큼 소요\n\n\n\n\n\n배열에 존재하는 원소를 삭제할 때, 최악의 경우 시간 복잡도를 계산하여라.\n예시) 배열에서 인덱스 3에 해당하는 원소를 삭제한 후 데이터를 한칸 씩 당겨 이동 시키는 \\(O(n)\\) 만큼 소요\n따라서, 최악의 경우 시간 복잡도는 \\(O(N)\\) 이다.\n\n\n\n\n\n삽입할 위치를 알고 있다면, 물리적인 위치를 한 칸씩 옮기지 않아도 삽입할 수 있다.\n인덱스 2의 위치에 원소를 삽입할 경우 인덱스 1의 Node에서 인덱스 2에 위치할 데이터를 가리키고 인덱스 2의 node가 인덱스 3의 node를 가리키도록 만들면 된다.\n\n\n\n\n\n삭제할 위치를 알고 을 경우 연결 리스트 사용\n인덱스 2의 위치에 원소를 삭제할 경우 인덱스 1의 Node가 인덱스 3의 node를 가리키게 만들면 됨\n\n\n\n\n\n뒤에 붙일 때는 남는 공간에 마지막 노드의 다음 위치에 원소를 할당 시키면 된다.\n마지막 위치에 새로운 원소를 추가\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data # 데이터 할당\n        self.next = None # 다음 노드\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head = None # 첫 번째  node\n\n    # 가장 뒤에 노드 삽입\n    def append(self, data):\n        \n        if self.head == None: # 헤드(head)가 비어있는 경우\n            self.head = Node(data)\n            return\n        \n        currrent = self.head # 그렇지 않다면 마지막 노드에 새로운 노드 추가\n\n        while currrent.next is not None: # 다음 노드가 없을 때까지  \n            currrent = currrent.next # 다음 원소로 넘어감\n        currrent.next = Node(data) # 다음 노드가 없으면 새로운 데이터를 추가 \n\n    # 모든 노드를 하나씩 출력\n    def show(self):\n        currrent = self.head\n        while currrent is not None:\n            print(currrent.data, end=\" \")\n            currrent = currrent.next\n\n    # 특정 인덱스(index)의 노드 찾기\n    def search(self, index):\n        node = self.head\n        for _ in range(index):\n            node = node.next\n        return node\n\n    # 특정 인덱스(index)에 노드 삽입\n    def insert(self, index, data):\n        new = Node(data)\n        # 첫 위치에 추가하는 경우\n        if index == 0:\n            new.next = self.head\n            self.head = new\n            return\n        # 삽입할 위치의 앞 노드\n        node = self.search(index - 1)\n        next = node.next\n        node.next = new\n        new.next = next\n\n    # 특정 인덱스(index)의 노드 삭제\n    def remove(self, index):\n        # 첫 위치를 삭제하는 경우\n        if index == 0:\n            self.head = self.head.next\n            return\n        # 삭제할 위치의 앞 노드\n        front = self.search(index - 1)\n        front.next = front.next.next\n\n\nlinked_list = LinkedList()\ndata_list = [3, 5, 9, 8, 5, 6, 1, 7]\n\nfor data in data_list:\n    linked_list.append(data)\n\nprint(\"전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\nlinked_list.insert(4, 4)\nprint(\"\\n전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\nlinked_list.remove(7)\nprint(\"\\n전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\nlinked_list.insert(7, 2)\nprint(\"\\n전체 노드 출력:\", end=\" \")\nlinked_list.show()\n\n\n전체 노드 출력: 3 5 9 8 5 6 1 7 \n전체 노드 출력: 3 5 9 8 4 5 6 1 7 \n전체 노드 출력: 3 5 9 8 4 5 6 7 \n전체 노드 출력: 3 5 9 8 4 5 6 2 7"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-18_python_list/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-18_python_list/index.html",
    "title": "Data Structure (4) Python List",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\nTable 1: a list of the list functions in Python\n\n\n\n\n\n\n\n\n\nNumber\nMethods\nTime Complexity\nExamples\nDescription\n\n\n\n\n1\nIndexing\n\\(O(1)\\)\narr[i]\n특정 i th 인덱스의 값 반환\n\n\n2\nStoring\n\\(O(1)\\)\narr[i] = 1\n특정 i th 인덱스에 값 (=1) 할당\n\n\n3\nAppend\n\\(O(1)\\)\narr.append(5)\n리스트의 가장 뒤에 데이터 추가\n\n\n4\nPop\n\\(O(1)\\)\narr.pop()\n리스트의 가장 뒤에서 원소 꺼내기\n\n\n5\nLength\n\\(O(1)\\)\nlen(arr)\n리스트의 길이 얻기\n\n\n6\nClear\n\\(O(1)\\)\narr.clear()\n리스트 내 모든 원소 제거하기\n\n\n7\nSlicing\n\\(O(b-a)\\)\narr[a:b]\n리스트에서 인덱스 a부터 b-1까지의 원소만 꺼내 새 리스트 만들기\n\n\n8\nExtend\n\\(O(len(other))\\)\narr.extend(list2)\n기존 리스트, list1에 다른 리스트, list2를 이어 붙이기\n\n\n9\nInsertion\n\\(O(N)\\)\narr.insert(index, x)\n특정 인덱스에 데이터 x를 삽입하기, 즉 i th index를 뒤로 밀고 추가\n\n\n10\nDelete\n\\(O(N)\\)\ndel arr[index]\n특정 인덱스의 데이터 삭제하기\n\n\n11\nConstruction\n\\(O(len(other))\\)\narr = list(other)\n다른 자료구조의 원소들을 이용해 리스트로 만들기\n\n\n12\nIn\n\\(O(N)\\)\nx in arr\n데이터 x가 리스트에 존재하는지 확인\n\n\n13\nNot in\n\\(O(N)\\)\nx not in arr\n데이터 x가 리스트에 존재하지 않는지 확인\n\n\n14\nPop\n\\(O(N)\\)\narr.pop(index)\n특정 인덱스의 데이터를 꺼내기 / 단, 가장 뒤 원소를 꺼내는 경우 O(1)\n\n\n15\nRemove\n\\(O(N)\\)\narr.remove(x)\n리스트 내에 존재하는 데이터 x를 삭제\n\n\n16\nCopy\n\\(O(N)\\)\narr.copy()\n리스트를 복제\n\n\n17\nMin\n\\(O(N)\\)\nmin(arr)\n리스트 내에 존재하는 가장 작은 원소\n\n\n18\nMax\n\\(O(N)\\)\nmax(arr)\n리스트 내에 존재하는 가장 큰 원소\n\n\n19\nIteration\n\\(O(N)\\)\nfor x in arr\n리스트 내에 존재하는 모든 원소 순회\n\n\n20\nMultiply\n\\(O(k*N)\\)\narr * k\n리스트를 k번 반복하여 길게 만들기\n\n\n21\nSort\n\\(O(NlogN)\\)\narr.sort()\n리스트 내 존재하는 원소를 정렬\n\n\n\n\nSee Table 1.\n\n1~6: 파이썬의 list는 동적 배열의 특징이 있다. 시간 복잡도는 모두 \\(O(1)\\) 이다.\n\n3~4: 사실상 stack의 기능과 동일\n\n\n\n\nCode\narr = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(arr[4]) # 인덱싱(indexing)\n\n# 저장(storing)\narr[7] = 10\n\n# 뒤에 붙이기(append)\narr.append(10)\nprint(arr)\n\n# 뒤에서 꺼내기(pop)\narr.pop()\nprint(arr)\n\n# 길이(length)\nprint(len(arr))\n\n# 배열 비우기(clear)\narr.clear()\nprint(arr)\n\n\n4\n[0, 1, 2, 3, 4, 5, 6, 10, 8, 9, 10]\n[0, 1, 2, 3, 4, 5, 6, 10, 8, 9]\n10\n[]\n\n\n\n7~11\n\n\n\nCode\narr = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nnew_arr = arr[2:7] # 슬라이싱(slicing)\nprint(new_arr)\n\narr1 = [0, 1, 2, 3, 4]\narr2 = [5, 6, 7, 8, 9]\narr1.extend(arr2) # 확장(extend)\nprint(arr1)\n\narr = [0, 1, 2, 3, 4]\narr.insert(3, 7) # 삽입(insertion)\nprint(arr)\n\ndel arr[3] # 삭제(delete)\nprint(arr)\n\ndata = {7, 8, 9}\narr = list(data) # 다른 자료구조로 리스트 만들기\nprint(arr)\n\n\n[2, 3, 4, 5, 6]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[0, 1, 2, 7, 3, 4]\n[0, 1, 2, 3, 4]\n[8, 9, 7]\n\n\n\n12~16\n\n\n\nCode\narr = [0, 1, 2, 3, 4]\n\nprint(3 in arr) # 존재 여부(in)\nprint(7 not in arr) # 비존재 여부(not in)\n\narr.pop(1) # 인덱스 1에 해당하는 원소 꺼내기(pop)\nprint(arr)\n\narr.remove(3) # 리스트의 특정 원소 삭제(remove)\nprint(arr)\n\nnew_arr = arr.copy() # 복제(copy)\nprint(new_arr)\n\n\nTrue\nTrue\n[0, 2, 3, 4]\n[0, 2, 4]\n[0, 2, 4]\n\n\n\n17~21\n\n\n\nCode\narr = [3, 5, 4, 1, 2]\n\nprint(min(arr)) # 최소(min)\nprint(max(arr)) # 최대(max)\n\nfor x in arr: # 원소 순회(iteration)\n    print(x, end=\" \")\nprint()\n\nprint(arr * 2) # 리스트 반복하여 곱하기(multiply)\n\narr.sort() # 정렬(sorting)\nprint(arr)\n\n\n1\n5\n3 5 4 1 2 \n[3, 5, 4, 1, 2, 3, 5, 4, 1, 2]\n[1, 2, 3, 4, 5]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_deque/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-19_deque/index.html",
    "title": "Data Structure (7) Deque",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n덱은 스택(stack)과 큐(queue)의 기능을 모두 가지고 있다.\n그래서, 스택과 큐대신 덱을 사용해도 괜찮음\n다만, 포인터 변수가 더 많이 필요하기 때문에, 메모리는 상대적으로 더 많이 필요하다.\nPython에서는 큐(queue)의 기능이 필요할 때 간단히 덱(deque)을 사용한다.\n데이터의 삭제와 삽입 모두에서 \\(O(1)\\) 의 시간 복잡도가 소요된다.\n덱에 여러 개의 데이터를 삽입하고 삭제하는 예시를 확인해 보자.\n\n[12개의 전체 연산]\n\n좌측으로부터 삽입 연산이 가능\n우측으로부터 삽입 연산이 가능\n삭제 연산시 우측/좌측 선택적 삭제가 가능\n\n\n\n\n• 데이터의 삭제와 삽입 모두에서 \\(O(1)\\) 의 시간 복잡도가 소요된다.\n\n\nTable 1: a list of the deque functions in Python\n\n\n\n\n\n\n\n\nNumber\nMethods\nTime Complexity\nDescription\n\n\n\n\n1\nappend left\n\\(O(1)\\)\n덱의 가장 왼쪽에 새 데이터를 삽입\n\n\n2\npop left\n\\(O(1)\\)\n덱의 가장 왼쪽에서 데이터를 추출\n\n\n3\nappend right\n\\(O(1)\\)\n덱의 가장 오른쪽에 새 데이터를 삽입\n\n\n4\npop right\n\\(O(1)\\)\n덱의 가장 오른쪽에서 데이터를 추출\n\n\n\n\nSee Table 1.\n\n\n\n\nPython에서는 덱(deque) 라이브러리를 사용할 수 있다.\n아래의 모든 메서드는 최악의 경우 시간 복잡도 O 1 을 보장한다.\n우측 삽입: append()\n좌측 삽입: appendleft()\n우측 추출: pop()\n좌측 추출: popleft()\n\n\n\nCode\nfrom collections import deque\n\n\nd = deque()\narr = [5, 6, 7, 8] \nfor x in arr:\n    d.append(x) # 오른쪽 삽입\narr = [4, 3, 2, 1]\nfor x in arr:\n    d.appendleft(x) # 좌측 삽입\nprint(d)\n\nwhile d:\n    print(d.popleft()) # 좌측 삭제\n\narr = [1, 2, 3, 4, 5, 6, 7, 8]\nfor x in arr:\n    d.appendleft(x)\nprint(d)\n\nwhile True:\n    print(d.pop())\n    if not d:\n        break\n    print(d.popleft())\n    if not d:\n        break\n\n\ndeque([1, 2, 3, 4, 5, 6, 7, 8])\n1\n2\n3\n4\n5\n6\n7\n8\ndeque([8, 7, 6, 5, 4, 3, 2, 1])\n1\n8\n2\n7\n3\n6\n4\n5\n\n\n\n\n\n기본적인 Python의 리스트 자료형은 큐(queue)의 기능을 제공하지 않는다.\n가능하다면 Python에서 제공하는 덱(deque) 라이브러리를 사용한다.\n큐(queue)의 기능이 필요할 때는 덱 라이브러리를 사용하는 것을 추천한다.\n삽입과 삭제에 대하여 모두 시간 복잡도 \\(O(1)\\) 이 요구된다.\n\n\n\n\n\n\n덱(deque)을 연결 리스트로 구현하면, 삽입과 삭제에 있어서 O 1 을 보장할 수 있다.\n연결 리스트로 구현할 때는 앞(front)과 뒤(rear) 두 개의 포인터를 가진다.\n앞(front): 가장 좌측에 있는 데이터를 가리키는 포인터\n뒤(rear): 가장 우측에 있는 데이터를 가리키는 포인터\n삽입과 삭제의 구현 방법은 스택 및 큐와 유사하다.\n앞(front)과 뒤(rear)에 대하여 대칭적으로 로직이 구현될 수 있다.\n\n\n\n\n좌측 삽입할 때는 앞(front) 위치에 데이터를 넣는다.\n새로운 데이터가 삽입되었을 때 front data와 연결이 먼저 된 후 front data의 이전 노드가 새로운 데이터가 되도록 설정\n\n\n\n\n\n삭제할 때는 앞(front) 위치에서 데이터를 꺼낸다. 즉, 그냥 front를 그 다음 데이터로 설정하면 됨\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.prev = None\n        self.next = None\n\n\nclass Deque:\n    def __init__(self):\n        self.front = None\n        self.rear = None\n        self.size = 0\n\n    def appendleft(self, data):\n        node = Node(data)\n        if self.front == None:\n            self.front = node\n            self.rear = node\n        else:\n            node.next = self.front\n            self.front.prev = node\n            self.front = node\n        self.size += 1\n\n    def append(self, data):\n        node = Node(data)\n        if self.rear == None:\n            self.front = node\n            self.rear = node\n        else:\n            node.prev = self.rear\n            self.rear.next = node\n            self.rear = node\n        self.size += 1\n\n    def popleft(self):\n        if self.size == 0:\n            return None\n        # 앞에서 노드 꺼내기\n        data = self.front.data\n        self.front = self.front.next\n        # 삭제로 인해 노드가 하나도 없는 경우\n        if self.front == None:\n            self.rear = None\n        else:\n            self.front.prev = None\n        self.size -= 1\n        return data\n\n    def pop(self):\n        if self.size == 0:\n            return None\n        # 뒤에서 노드 꺼내기\n        data = self.rear.data\n        self.rear = self.rear.prev\n        # 삭제로 인해 노드가 하나도 없는 경우\n        if self.rear == None:\n            self.front = None\n        else:\n            self.rear.next = None\n        self.size -= 1\n        return data\n\n    def front(self):\n        if self.size == 0:\n            return None\n        return self.front.data\n\n    def rear(self):\n        if self.size == 0:\n            return None\n        return self.rear.data\n\n    # 앞에서부터 원소 출력\n    def show(self):\n        cur = self.front\n        while cur:\n            print(cur.data, end=\" \")\n            cur = cur.next\n\n\nd = Deque()\narr = [5, 6, 7, 8]\nfor x in arr:\n    d.append(x)\narr = [4, 3, 2, 1]\nfor x in arr:\n    d.appendleft(x)\nd.show()\n\nprint()\nwhile d.size != 0:\n    print(d.popleft())\n\narr = [1, 2, 3, 4, 5, 6, 7, 8]\nfor x in arr:\n    d.appendleft(x)\nd.show()\n\nprint()\nwhile True:\n    print(d.pop())\n    if d.size == 0:\n        break\n    print(d.popleft())\n    if d.size == 0:\n        break\n\n\n1 2 3 4 5 6 7 8 \n1\n2\n3\n4\n5\n6\n7\n8\n8 7 6 5 4 3 2 1 \n1\n8\n2\n7\n3\n6\n4\n5"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_queue/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-19_queue/index.html",
    "title": "Data Structure (6) Queue",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n큐(queue)는 먼저 삽입된 데이터가 먼저 추출되는 자료구조(data structure)다. (First-In First-Out)\n딥러닝 모델에 들어가는 데이터 순서대로 들어가는데 먼저 들어간 데이터는 먼저 나오게 할때 사용되는 자료 구조이다.\n\n\n\n\n\n큐를 연결 리스트로 구현하면, 삽입과 삭제에 있어서 \\(O(1)\\) 을 보장할 수 있다.\n연결 리스트로 구현할 때는 머리(head)와 꼬리(tail) 두 개의 포인터를 가진다.\n머리(head): 남아있는 원소 중 가장 먼저 들어 온 데이터를 가리키는 포인터\n꼬리(tail): 남아있는 원소 중 가장 마지막에 들어 온 데이터를 가리키는 포인터\n\n\n\n\n삽입할 때는 꼬리(tail) 위치에 데이터를 넣는다.\n값으로 8을 갖는 새로운 데이터가 삽입되었을 때 예시)\n\n\n\n\n\n삭제할 때는 머리(head) 위치에서 데이터를 꺼낸다.\n하나의 데이터를 삭제할 때의 예시)\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\n\nclass Queue:\n    def __init__(self):\n        self.head = None\n        self.tail = None\n\n    def enqueue(self, data):\n        node = Node(data)\n        if self.head == None:\n            self.head = node\n            self.tail = node\n        # 꼬리(tail) 위치에 새로운 노드 삽입\n        else:\n            self.tail.next = node\n            self.tail = self.tail.next\n\n    def dequeue(self):\n        if self.head == None:\n            return None\n\n        # 머리(head) 위치에서 노드 꺼내기\n        data = self.head.data\n        self.head = self.head.next\n\n        return data\n\n    def show(self):\n        cur = self.head\n        while cur:\n            print(cur.data, end=\" \")\n            cur = cur.next\n\n\nqueue = Queue()\ndata_list = [3, 5, 9, 8, 5, 6, 1, 7]\n\nfor data in data_list:\n    queue.enqueue(data)\n\nprint(\"\\n전체 노드 출력:\", end=\" \")\nqueue.show()\n\nprint(\"\\n[원소 삭제]\")\nprint(queue.dequeue())\nprint(queue.dequeue())\nprint(queue.dequeue())\n\nprint(\"[원소 삽입]\")\nqueue.enqueue(2)\nqueue.enqueue(5)\nqueue.enqueue(3)\n\nprint(\"전체 노드 출력:\", end=\" \")\nqueue.show()\n\n\n\n전체 노드 출력: 3 5 9 8 5 6 1 7 \n[원소 삭제]\n3\n5\n9\n[원소 삽입]\n전체 노드 출력: 8 5 6 1 7 2 5 3 \n\n\n\n\n\n\n다수의 데이터를 삽입 및 삭제할 때에 대하여, 수행 시간을 측정할 수 있다.\n단순히 Python의 리스트 자료형을 이용할 때보다 수행 시간 관점에서 효율적이다.\n\n\n\nCode\nimport time\n\ndata_list = [i for i in range(100000)]\n\nstart_time = time.time()\n\nqueue = []\nfor data in data_list:\n    queue.append(data)\nwhile queue:\n    queue.pop(0)\n\nprint(f\"Elapsed time: {time.time() - start_time} seconds.\")\nprint(queue)\n\nstart_time = time.time()\n\nqueue = Queue()\nfor data in data_list:\n    queue.enqueue(data)\nwhile queue.head != None:\n    queue.dequeue()\n\nprint(f\"Elapsed time: {time.time() - start_time} seconds.\")\nqueue.show()\n\n\nElapsed time: 1.0510380268096924 seconds.\n[]\n\n\nElapsed time: 0.25047826766967773 seconds."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-19_stack/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-19_stack/index.html",
    "title": "Data Structure (5) Stack",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n다양한 알고리즘과 프로그램에서 사용됨\n스택: 먼저 들어온 데이터가 나중에 나가는 자료구조\n흔히 박스가 쌓인 형태를 스택(stack)이라고 한다. 예) ‘Deep Learning 알고리즘의 구조가 stacked 되어 있는 구조다’ 라고 표현\n\n우리가 박스를 쌓은 뒤에 꺼낼 때는, 가장 마지막에 올렸던 박스부터 꺼내야 한다.\n\n새로운 원소를 삽입할 때는 마지막 위치에 삽입한다. (가장 최근에 삽입된 원소가 가장 끝에 위치)\n새로운 원소를 삭제할 때는 마지막 원소가 삭제된다. (가장 최근에 삽입된 원소가 제거됨)\nhead = 최상위 원소 = 가장 최근에 삽입이된 원소\n\n\n\n\n\n스택은 굉장히 기본적인 자료구조이다.\n기계 학습 분야뿐 아니라 다양한 프로그램을 개발할 때 빠지지 않고 사용된다.\n\n\n\n\n\n스택은 여러 가지 연산을 제공한다.\n\n\n\nTable 1: a list of the stack functions in Python\n\n\n\n\n\n\n\n\nNumber\nMethods\nTime Complexity\nDescription\n\n\n\n\n1\n삽입(Push)\n\\(O(1)\\)\n스택에 원소를 삽입하는 연산\n\n\n2\n추출(Pop)\n\\(O(1)\\)\n스택에서 원소를 추출하는 연산\n\n\n3\n최상위 원소 (Top)\n\\(O(1)\\)\n스택의 최상위 원소(마지막에 들어온 원소) 를 확인(조회)하는 연산\n\n\n4\nEmpty\n\\(O(1)\\)\n스택이 비어 있는지 확인하는 연산\n\n\n\n\nSee Table 1.\n\n\n\n\n파이썬의 기본적인 리스트 자료형은 다음의 두 가지 메서드를 제공한다.\nappend() 메서드: 마지막 위치에 원소를 삽입하며, 시간 복잡도는 \\(O(1)\\) 이다.\npop() 메서드: 마지막 위치에서 원소를 추출하며, 시간 복잡도는 \\(O(1)\\) 이다.\n따라서 일반적으로 스택을 구현할 때, 파이썬의 리스트(list) 자료형을 사용한다.\n\n\n\nCode\nclass Stack:\n    def __init__(self):\n        self.stack = []\n\n    def push(self, data):\n        # 마지막 위치에 원소 삽입\n        self.stack.append(data)\n\n    def pop(self):\n        if self.is_empty():\n            return None\n        # 마지막 원소 추출\n        return self.stack.pop()\n\n    def top(self):\n        if self.is_empty():\n            return None\n        # 마지막 원소 반환\n        return self.stack[-1]\n\n    def is_empty(self):\n        return len(self.stack) == 0\n\n\nstack = Stack()\narr = [9, 7, 2, 5, 6, 4, 2]\nfor x in arr:\n    stack.push(x)\n\nwhile not stack.is_empty():\n    print(stack.pop())\n\n\n2\n4\n6\n5\n2\n7\n9\n\n\n\n\n\n\n스택을 연결 리스트로 구현하면, 삽입과 삭제에 있어서 \\(O(1)\\) 을 보장한다.\n연결 리스트로 구현할 때는 머리(head)를 가리키는 하나의 포인터만 가진다.\n머리(head): 남아있는 원소 중 가장 마지막에 들어 온 데이터를 가리키는 포인터\n\n\n\n\n삽입할 때는 기존의 머리 뒤에 데이터가 들어가고 포인터가 가장 최근에 삽입된 데이터를 가리키도록 머리(head) 위치를 바꿔준다.\n삭제할 때는 머리(head) 위치에서 데이터를 꺼낸다.\n\n즉, 포인터를 삭제할 데이터에 앞에 있는 데이터로 머리 위치를 바꾸는 것만으로 삭제는 이루어진다.\n\n\n\n\n\n\n삭제할 때는 머리(head) 위치에서 데이터를 꺼낸다.\n\n\n\nCode\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\n\nclass Stack:\n    def __init__(self):\n        self.head = None\n\n    # 원소 삽입\n    def push(self, data):\n        node = Node(data)\n        node.next = self.head\n        self.head = node\n\n    # 원소 추출하기\n    def pop(self):\n        if self.is_empty():\n            return None\n\n        # 머리(head) 위치에서 노드 꺼내기\n        data = self.head.data\n        self.head = self.head.next\n\n        return data\n\n    # 최상위 원소(top)\n    def top(self):\n        if self.is_empty():\n            return None\n        return self.head.data\n\n    # 먼저 추출할 원소부터 출력\n    def show(self):\n        cur = self.head\n        while cur:\n            print(cur.data, end=\" \")\n            cur = cur.next\n\n    # 스택이 비어있는지 확인\n    def is_empty(self):\n        return self.head is None\n\n\nstack = Stack()\narr = [9, 7, 2, 5, 6, 4, 2]\nfor x in arr:\n    stack.push(x)\nstack.show()\nprint()\n\nwhile not stack.is_empty():\n    print(stack.pop())\n\n\n2 4 6 5 2 7 9 \n2\n4\n6\n5\n2\n7\n9"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-20_binary_search_tree/index.html",
    "title": "Data Structure (8) Binary Search Tree",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n트리는 가계도와 같이 계층적인 구조를 표현할 때 사용할 수 있는 자료구조다.\n나무(tree)의 형태를 뒤집은 것과 같이 생겼다.\n다수의 데이터를 관리하기에 적합한 트리 자료 구조의 가장 기본적인 형태\n\n\n\n\n\n루트 노드(root node): 부모가 없는 최상위 노드\n단말 노드(leaf node): 자식이 없는 노드\n트리(tree)에서는 부모와 자식 관계가 성립한다 (직계).\n형제 관계 (sibling, 방계): 부모 node로 부터 왼쪽 자식과 오른쪽 자식과의 관계\n깊이(depth): 루트 노드에서의 길이(length), 루트 노드로부터 손자까지의 depth=2\n\n이때, 길이란 출발 노드에서 목적지 노드까지 거쳐야 하는 간선의 수를 의미한다.\n\n트리의 높이(height)은 루트 노드에서 가장 깊은 노드까지의 길이를 의미한다.\n\n\n\n\n\n이진 트리는 최대 2개의 자식을 가질 수 있는 트리를 말한다.\n\n\n\n\n\n다수의 데이터를 관리(조회, 저장, 삭제)하기 위한 가장 기본적인 자료구조 중 하나다.\n이진 탐색 트리의 성질: 순서가 있음\n\n왼쪽 자식 노드 < 부모 노드 < 오른쪽 자식 노드\n루트 노드 기준 모든 왼쪽 노드들은 루트 노드보다 작음\n루트 노드 기준 모든 오른쪽 노드들은 루트 노드보다 큼\n2진 탐색을 가능하게 하는 구조\n\n\n\n\n\n특정한 노드의 키(key) 값보다 그 왼쪽 자식 노드의 키(key) 값이 더 작다.\n특정한 노드의 키(key) 값보다 그 오른쪽 자식 노드의 키(key) 값이 더 크다.\n특정한 노드의 왼쪽 서브 트리, 오른쪽 서브 트리 모두 이진 탐색 트리다.\nworst case: 찾는게 없을 때 혹은 찾고자 하는 데이터가 가장 마지막에 있을 때\n\n탐색시 재귀적으로 중앙값을 기준으로 오른쪽만 찾음\n매 실행마다 데이터의 개수가 절반씩 줄어듬\n그러면, 몇 번만에 사이즈가 1이 되는가?\n수식 유도, input size를 N이라고 가정했을때\n\\(N \\times {(\\frac{1}{2})}^{k}=1 \\rightarrow N=2^k \\rightarrow k = log_2N\\)\n위의 수식을 점근적 표기법으로 표현하면 \\(\\Theta(logN)\\)\n\nbest case: 한번에 찾았을 때\n\n\\(\\Theta(1)\\)\n\n그러므로, lower bound = \\(\\Theta(1)\\), upper bound = \\(O(logN)\\)\n\n\n\n\n\n루트 노드에서 출발하여 아래쪽으로 내려오면서, 삽입할 위치를 찾는다.\n\n삽입할 노드의 키(key)가 작으면 왼쪽으로,\n삽입할 노드의 키(key)가 크면 오른쪽으로 삽입\n\n삽입할 노드 목록 예시: [7,4,5,9,6,2,3,2,8]으로 트리 생성해보기\n\n\n\n\nBinary Tree\n\n\nSorcue: 코딩 테스트를 위한 트리(Tree) 자료구조 10분 핵심 요약 By 동빈나\n\n\n\n\n루트 노드에서 출발하여 아래쪽으로 내려오면서, 찾고자 하는 원소를 조회한다. 삽입 연산과 같은 로직을 따름\n1 삽입할 노드의 키(key)가 작으면 왼쪽으로, 2 삽입할 노드의 키(key)가 크면 오른쪽으로 조회\n조회할 노드 목록 예시: 5번 노드\n\n\n\n\n\n루트 노드에서 출발하여 아래쪽으로 내려오면서, 삭제할 원소에 접근한다.\n삭제할 노드 목록 예시: 7번 노드\n\nCase #1 왼쪽 자식이 없는 경우 → 오른쪽 자식으로 대체\nCase #2 오른쪽 자식이 없는 경우 → 왼쪽 자식으로 대체\nCase #3 왼쪽, 오른쪽이 모두 있는 경우 → 오른쪽 서브\n\n트리에서 가장 작은 노드로 대체\n삭제할 노드 목록 예시: 4번 노드\n\n\n\n\nBinary Tree Deletion\n\n\nSorcue: 코딩 테스트를 위한 트리(Tree) 자료구조 10분 핵심 요약 By 동빈나\n\n\n\n\n\n트리에 포함되어 있는 정보를 모두 출력하고자 할 때, 어떤 방식을 사용할 수 있을까?\n바로 순회(traversal)를 사용할 수 있다.\n트리의 모든 노드를 특정한 순서(조건)에 따라서 방문하는 방법을 순회(traversal)라고 한다.\n\n\n전위 순회(pre-order traverse): 루트 방문 → 왼쪽 자식 방문 → 오른쪽 자식 방문\n중위 순회(in-order traverse): 왼쪽 자식 방문 → 루트 방문 → 오른쪽 자식 방문\n후위 순회(post-order traverse): 왼쪽 자식 방문 → 오른쪽 자식 방문 → 루트 방문\n\n\n\n\n전위 순회(pre-order traverse): A → B → D → E → C → F → G\n중위 순회(in-order traverse): D → B → E → A → F → C → G\n후위 순회(post-order traverse): D → E → B → F → G → C → A\n\n\n\n\nBinary Tree Traverse\n\n\nSorcue: 코딩 테스트를 위한 트리(Tree) 자료구조 10분 핵심 요약 By 동빈나\n\n\n\n• 방문 방법: 현재 노드 → 왼쪽 자식 노드 → 오른쪽 자식 노드\n\n\nCode\ndef _preorder(self, node):\n  if node:\n    print(node.key, end=' ')\n    self._preorder(node.left)\n    self._preorder(node.right)\n\n\n\n\n\n\n방문 방법: 왼쪽 자식 노드 → 현재 노드 → 오른쪽 자식 노드\n\n\n\nCode\ndef _inorder(self, node):\n  if node:\n    self._inorder(node.left)\n    print(node.key, end=' ')\n    self._inorder(node.right)\n\n\n\n\n\n\n방문 방법: 왼쪽 자식 노드 → 오른쪽 자식 노드 → 현재 노드\n\n\n\nCode\ndef _postorder(self, node):\n  if node:\n    self._postorder(node.left)\n    self._postorder(node.right)\n    print(node.key, end=' ')\n\n\n\n\n\n\n낮은 레벨(루트)부터 높은 레벨까지 순차적으로 방문한다.\n단순히 루트 노드에서부터 너비 우선 탐색(BST)를 진행하면 된다.\n레벨 순회 순회(level-order traverse): A → B → C → D → E → F → G\n\n\n\n\n\n\n다른 메서드 안에서 사용되는 메서드는 이름 앞에 언더바(_) 기호를 붙인다.\n\n\n\nCode\ndef search(self, node, key):\n  return self._search(self.root, key) # search: recursively 조회\n\ndef _search(self, node, key):\n  if node is None or node.key == key:\n    return node\n\n  # 현재 노드의 key보다 작은 경우\n  if node.key > key:\n    return self._search(node.left, key)\n\n  # 현재 노드의 key보다 큰 경우\n  elif node.key < key:\n    return self._search(node.right, key)\n\n\n\n\n\n편향 이진 트리는 다음의 두 가지 속성을 가진다.\n\n\n같은 높이의 이진 트리 중 최소 개수의 노드 개수를 가진다.\n왼쪽 혹은 오른쪽으로 한 방향에 대한 서브 트리를 가진다.\n\n\n\n\n\n노드의 개수가 N개일 때, 시간 복잡도는 다음과 같다.\n트리의 높이(height)을 H라고 할 때, 엄밀한 시간 복잡도는 \\(O(H)\\) 다.\n이상적인 경우 H = log2 N로 볼 수 있다.\n하지만 최악의 경우(편향된 경우) H = N로 볼 수 있다.\n\n\n\nTable 1: a list of the time complexity of the binary search trees in Python\n\n\n\n\n\n\n\n\n\n\nNumber\nMethods\n조회\n삽입\n삭제\n수정\n\n\n\n\n1\n균형 잡힌 이진 탐색 트리\n\\(O(logN)\\)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\n\n2\n편향 이진 탐색 트리\n\\(O(N)\\)\n\\(O(N)\\)\n\\(O(N)\\)\n\\(O(N)\\)\n\n\n\n\nSee Table 1.\n\n\n\n\nAVL stands for Adelson-Velsky and Landis\n이진 탐색 트리는 편향 트리가 될 수 있으므로, 최악의 경우 \\(O(N)\\) 을 요구한다.\n반면에 AVL 트리는 균형이 갖춰진 이진 트리다.\n간단한 구현 과정으로 완전 이진 트리에 가까운 형태를 유지하도록 한다.\n\n\n\nCode\nfrom collections import deque\n\n\nclass Node:\n    def __init__(self, key):\n        self.key = key\n        self.left = None\n        self.right = None\n\n\nclass BinarySearchTree:\n    def __init__(self):\n        self.root = None\n\n    def search(self, node, key):\n        return self._search(self.root, key)\n\n    def _search(self, node, key):\n        if node is None or node.key == key:\n            return node\n\n        # 현재 노드의 key보다 작은 경우\n        if node.key > key:\n            return self._search(node.left, key)\n        # 현재 노드의 key보다 큰 경우\n        elif node.key < key:\n            return self._search(node.right, key)\n\n    def insert(self, key):\n        self.root = self._insert(self.root, key)\n\n    def _insert(self, node, key):\n        if node is None:\n            return Node(key)\n\n        # 현재 노드의 key보다 작은 경우\n        if node.key > key:\n            node.left = self._insert(node.left, key)\n        # 현재 노드의 key보다 큰 경우\n        elif node.key < key:\n            node.right = self._insert(node.right, key)\n\n        return node\n\n    def delete(self, key):\n        self.root = self._delete(self.root, key)\n\n    def _delete(self, node, key):\n        if node is None:\n            return None\n\n        # 현재 노드의 key보다 작은 경우\n        if node.key > key:\n            node.left = self._delete(node.left, key)\n        # 현재 노드의 key보다 큰 경우\n        elif node.key < key:\n            node.right = self._delete(node.right, key)\n        # 삭제할 노드를 찾은 경우\n        else:\n            # 왼쪽 자식이 없는 경우\n            if node.left is None:\n                return node.right\n            # 오른쪽 자식이 없는 경우\n            elif node.right is None:\n                return node.left\n            # 왼쪽과 오른쪽 자식 모두 있는 경우\n            node.key = self._get_min(node.right)\n            node.right = self._delete(node.right, node.key)\n\n        return node\n\n    def _get_min(self, node):\n        key = node.key\n        while node.left:\n            key = node.left.key\n            node = node.left\n        return key\n\n    def preorder(self):\n        self._preorder(self.root)\n\n    def _preorder(self, node):\n        if node:\n            print(node.key, end=' ')\n            self._preorder(node.left)\n            self._preorder(node.right)\n\n    def inorder(self):\n        self._inorder(self.root)\n\n    def _inorder(self, node):\n        if node:\n            self._inorder(node.left)\n            print(node.key, end=' ')\n            self._inorder(node.right)\n\n    def postorder(self):\n        self._postorder(self.root)\n\n    def _postorder(self, node):\n        if node:\n            self._postorder(node.left)\n            self._postorder(node.right)\n            print(node.key, end=' ')\n\n    def levelorder(self):\n        return self._levelorder(self.root)\n\n    def _levelorder(self, node):\n        if node is None:\n            return\n\n        result = []\n\n        queue = deque()\n        queue.append((0, node))  # (level, node)\n\n        while queue:\n            level, node = queue.popleft()\n            if node:\n                result.append((level, node.key))\n                queue.append((level + 1, node.left))\n                queue.append((level + 1, node.right))\n\n        for level, key in result:\n            print(f\"level: {level}, key: {key}\")\n\n    def to_list(self):\n        return self._to_list(self.root)\n\n    def _to_list(self, node):\n        if node is None:\n            return []\n        return self._to_list(node.left) + [node.key] + self._to_list(\n            node.right)\n\n\narr = [7, 4, 5, 9, 6, 3, 2, 8]\nbst = BinarySearchTree()\nfor x in arr:\n    bst.insert(x)\nprint('전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nbst.delete(7)\nprint('\\n전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nbst.delete(4)\nprint('\\n전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nbst.delete(3)\nprint('\\n전위 순회:', end=' ')\nbst.preorder()\nprint('\\n중위 순회:', end=' ')\nbst.inorder()\nprint('\\n후위 순회:', end=' ')\nbst.postorder()\nprint('\\n[레벨 순회]')\nbst.levelorder()\n\nprint(bst.to_list())\n\n\n전위 순회: 7 4 3 2 5 6 9 8 \n중위 순회: 2 3 4 5 6 7 8 9 \n후위 순회: 2 3 6 5 4 8 9 7 \n[레벨 순회]\nlevel: 0, key: 7\nlevel: 1, key: 4\nlevel: 1, key: 9\nlevel: 2, key: 3\nlevel: 2, key: 5\nlevel: 2, key: 8\nlevel: 3, key: 2\nlevel: 3, key: 6\n\n전위 순회: 8 4 3 2 5 6 9 \n중위 순회: 2 3 4 5 6 8 9 \n후위 순회: 2 3 6 5 4 9 8 \n[레벨 순회]\nlevel: 0, key: 8\nlevel: 1, key: 4\nlevel: 1, key: 9\nlevel: 2, key: 3\nlevel: 2, key: 5\nlevel: 3, key: 2\nlevel: 3, key: 6\n\n전위 순회: 8 5 3 2 6 9 \n중위 순회: 2 3 5 6 8 9 \n후위 순회: 2 3 6 5 9 8 \n[레벨 순회]\nlevel: 0, key: 8\nlevel: 1, key: 5\nlevel: 1, key: 9\nlevel: 2, key: 3\nlevel: 2, key: 6\nlevel: 3, key: 2\n\n전위 순회: 8 5 2 6 9 \n중위 순회: 2 5 6 8 9 \n후위 순회: 2 6 5 9 8 \n[레벨 순회]\nlevel: 0, key: 8\nlevel: 1, key: 5\nlevel: 1, key: 9\nlevel: 2, key: 2\nlevel: 2, key: 6\n[2, 5, 6, 8, 9]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_graph/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-20_graph/index.html",
    "title": "Data Structure (10) Graph",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n• 그래프(graph)란 사물을 정점(vertex)과 간선(edge)으로 나타내기 위한 도구다.\n• 그래프는 두 가지 방식으로 구현할 수 있다.\n\n인접 행렬(adjacency matrix): 2차원 배열을 사용하는 방식\n인접 리스트(adjacency list): 연결 리스트를 이용하는 방식\n\n\n\n\n• 인접 행렬(adjacency matrix)에서는 그래프를 2차원 배열로 표현한다.\n\n\n• 모든 간선이 방향성을 가지지 않는 그래프를 무방향 그래프라고 한다.\n• 모든 간선에 가중치가 없는 그래프를 무가중치 그래프라고 한다.\n• 무방향 무가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 행렬로 출력할 수 있다.\n\n\n\n• 모든 간선이 방향을 가지는 그래프를 방향 그래프라고 한다.\n• 모든 간선에 가중치가 있는 그래프를 가중치 그래프라고 한다.\n• 방향 가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 행렬로 출력할 수 있다.\n\n\n\n\n• 인접 리스트(adjacency list)에서는 그래프를 리스트로 표현한다.\n\n\n• 모든 간선이 방향성을 가지지 않는 그래프를 무방향 그래프라고 한다.\n• 모든 간선에 가중치가 없는 그래프를 무가중치 그래프라고 한다.\n• 무방향 무가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 리스트로 출력할 수 있다.\n\n\n\n• 모든 간선이 방향을 가지는 그래프를 방향 그래프라고 한다.\n• 모든 간선에 가중치가 있는 그래프를 가중치 그래프라고 한다.\n• 방향 가중치 그래프가 주어졌을 때 연결되어 있는 상황을 인접 리스트로 출력할 수 있다.\n\n\n\n\n\n인접 행렬: 모든 정점들의 연결 여부를 저장해 O V\n\n2 의 공간을 요구한다.\n• 공간 효율성이 떨어지지만, 두 노드의 연결 여부를 O 1 에 확인할 수 있다.\n\n인접 리스트: 연결된 간선의 정보만을 저장하여 O V + E 의 공간을 요구한다.\n\n• 공간 효율성이 우수하지만, 두 노드의 연결 여부를 확인하기 위해 O V 의 시간이 필요하다.\n\n\nTable 1: a list of the stack functions in Python\n\n\n\n\n\n\n\n\nNumber\nCategory\n필요한 메모리\n연결 여부 확인\n\n\n\n\n1\n인접 행렬\n\\(O(V^2)\\)\n\\(O(1)\\)\n\n\n2\n인접 리스트\n\\(O(V+E)\\)\n\\(O(V)\\)\n\n\n\n\nSee Table 1.\n\n\n\n• 최단 경로 알고리즘을 구현할 때, 어떤 자료구조가 유용할까?\n• 각각 근처의 노드와 연결되어 있는 경우가 많으므로, 간선 개수가 적어 인접 리스트가 유리하다."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-01-20_priority_queue/index.html",
    "href": "docs/blog/posts/Engineering/2023-01-20_priority_queue/index.html",
    "title": "Data Structure (9) Priority Queue",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n우선순위 큐는 우선순위에 따라서 데이터를 추출하는 자료구조다.\n컴퓨터 운영체제, 온라인 게임 매칭 등에서 활용된다.\n우선순위 큐는 일반적으로 힙(heap)을 이용해 구현한다.\n\n\n\nTable 1: a list of data structure\n\n\nNumber\nData Structure\n추출되는 데이터\n\n\n\n\n1\nstack\n가장 나중에 삽입된 데이터\n\n\n2\nqueue\n가장 먼저 삽입된 데이터\n\n\n3\npriority queue\n가장 우선 순위가 높은 데이터\n\n\n\n\nSee Table 2.\n\n\n\n\n우선순위 큐는 다양한 방법으로 구현할 수 있다.\n데이터의 개수가 N개일 때, 구현 방식에 따른 시간 복잡도는 다음과 같다.\n삭제 할때는 우선 순위가 가장 높은 것을 찾아야 하기 때문에 \\(O(N)\\) 만큼 소요될 수 있다.\n\n\n\nTable 2: a list of the priority queue building methods in Python\n\n\nNumber\n우선 순위 큐 구현 방식\n삽입 시간\n삭제 시간\n\n\n\n\n1\n리스트 자료형\n\\(O(1)\\)\n\\(O(N)\\)\n\n\n2\n힙 (Heap)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\n\n\n\nSee Table 2.\n\n일반적인 형태의 큐는 선형적인 구조를 가진다.\n반면에 우선순위 큐는 이진 트리(binary tree) 구조를 사용하는 것이 일반적이다.\n\n\n\n\n\n이진 트리(binary tree)는 최대 2개까지의 자식을 가질 수 있다.\n\n\n\n\n포화 이진 트리는 리프 노드를 제외한 모든 노드가 두 자식을 가지고 있는 트리다.\n\n\n\n\n\n완전 이진 트리는 모든 노드가 왼쪽 자식부터 차근차근 채워진 트리다.\n\n\n\n\n\n왼쪽 자식 트리와 오른쪽 자식 트리의 높이가 1 이상 차이 나지 않는 트리다.\n\n\n\n\n\n\n힙(Heap)은 원소들 중에서 최댓값 혹은 최솟값을 빠르게 찾아내는 자료구조다.\n최대 힙(Max Heap): 값이 큰 원소부터 추출한다.\n최소 힙(Min Heap): 값이 작은 원소부터 추출한다.\n힙은 원소의 삽입과 삭제를 위해 \\(O(logN)\\) 의 수행 시간을 요구한다.\n단순한 N개의 데이터를 힙에 넣었다가 모두 꺼내는 작업은 정렬과 동일하다.\n이 경우 시간 복잡도는 \\(O(NlogN)\\) 이다.\n\n\n\n\n최대 힙(max heap)은 부모 노드가 자식 노드보다 값이 큰 완전 이진 트리를 의미한다.\n최대 힙(max heap)의 루트 노드는 전체 트리에서 가장 큰 값을 가진다는 특징이 있다.\n\n\n\n\n\n힙은 완전 이진 트리 자료구조를 따른다.\n힙에서는 우선순위가 높은 노드가 루트(root)에 위치한다.\n\n\n최대 힙(max heap)\n\n\n부모 노드의 키 값이 자식 노드의 키 값보다 항상 크다.\n루트 노드가 가장 크며, 값이 큰 데이터가 우선순위를 가진다.\n\n\n최소 힙(min heap)\n\n\n부모 노드의 키 값이 자식 노드의 키 값보다 항상 작다.\n루트 노드가 가장 작으며, 값이 작은 데이터가 우선순위를 가진다.\n힙의 삽입과 삭제 연산을 수행할 때를 고려해 보자.\n직관적으로, 거슬러 갈 때마다 처리해야 하는 범위에 포함된 원소의 개수가 절반씩 줄어든다.\n따라서 삽입과 삭제에 대한 시간 복잡도는 O logN 이다.\n\n\n\n\n\n(상향식) 부모로 거슬러 올라가며, 부모보다 자신이 더 작은 경우에 위치를 교체한다.\n\n\n\n\n\n(상향식) 부모로 거슬러 올라가며, 부모보다 자신이 더 작은 경우에 위치를 교체한다.\n새로운 원소가 삽입되었을 때 O logN 의 시간 복잡도로 힙 성질을 유지하도록 할 수 있다.\n\n\n\n\n\n원소가 제거되었을 때 O logN 의 시간 복잡도로 힙 성질을 유지하도록 할 수 있다.\n원소를 제거할 때는 가장 마지막 노드가 루트 노드의 위치에 오도록 한다.\n이후에 루트 노드에서부터 하향식으로(더 작은 자식 노드로) Heapify()를 진행한다.\n\n\n\n\n\n파이썬에서는 힙(Heap) 라이브러리를 제공한다.\nheapq 라이브러리의 삽입 및 삭제에 대한 시간 복잡도는 모두 O logN 이다.\n\n\n\n\n단순히 하나의 빈 리스트를 만들면, 그것을 힙(heap) 자료구조로 사용할 수 있다.\n\n\n\nCode\nimport heapq\n\nheap = []\nprint(heap)\n\n\n[]\n\n\n\n\n\n\n원소의 삽입: heappush() 메서드\n원소의 추출: heappop() 메서드\n\n\n\nCode\nimport heapq\n\nheap = []\n\nheapq.heappush(heap, 7)\nheapq.heappush(heap, 4)\nheapq.heappush(heap, 5)\nheapq.heappush(heap, 8)\n\nwhile heap:\n  element = heapq.heappop(heap)\n  print(element, end=\" \")\n\n\n4 5 7 8 \n\n\n\n\n\n\n원소의 삽입: heappush() 메서드\n원소의 추출: heappop() 메서드\n\n\n\nCode\nimport heapq\n\nheap = []\n\nheapq.heappush(heap, 7)\nheapq.heappush(heap, 4)\nheapq.heappush(heap, 5)\nheapq.heappush(heap, 8)\n\nprint(heap[0])\n\n\n4\n\n\n\n\n\n\nheappush() 메서드를 이용해 하나씩 원소를 삽입하지 않을 수 있다.\nheapify() 메서드는 새로운 리스트를 반환하지 않고, 리스트 내부를 직접 수정한다.\n\n\n\nCode\nimport heapq\n\nheap = [9, 1, 5, 4, 3, 8, 7]\nheapq.heapify(heap)\n\nwhile heap:\n  element = heapq.heappop(heap)\n  print(element, end=\" \")\n\n\n1 3 4 5 7 8 9 \n\n\n\n\n\n\n파이썬의 heapq 라이브러리는 기본적으로 최소 힙 기능을 제공한다.\n최대 힙을 위해서는 1 삽입과 2 추출할 때 키(key)에 음수(-) 부호를 취한다.\n\n\n\nCode\nimport heapq\n\narr = [9, 1, 5, 4, 3, 8, 7]\nheap = []\n\nfor x in arr:\n  heapq.heappush(heap, -x)\n\nwhile heap:\n  x = -heapq.heappop(heap)\n  print(x, end=\" \")\n\n\n9 8 7 5 4 3 1 \n\n\n\n\n\n\n단순히 힙에 원소를 넣었다가 꺼내는 것 만으로도 정렬을 수행할 수 있다.\n\n\n\nCode\nimport heapq\n\ndef heap_sort(arr):\n  heap = []\n  for x in arr:\n    heapq.heappush(heap, x)\n\n  result = []\n  while heap:\n    x = heapq.heappop(heap)\n    result.append(x)\n\n  return result\n\nprint(heap_sort([9, 1, 5, 4, 3, 8, 7]))\n\n\n[1, 3, 4, 5, 7, 8, 9]\n\n\n\n\n\n\n최소 힙이나 최대 힙을 사용하여 n번째로 작은 값이나 n번째로 큰 값을 얻을 수 있다.\n힙(heap)을 만든 뒤에 추출(pop) 함수를 n번 호출한다.\n\n\n\nCode\nimport heapq\n\ndef n_smallest(n, arr):\n  heap = []\n  for x in arr:\n    heapq.heappush(heap, x)\n    result = None\n  for _ in range(n):\n    result = heapq.heappop(heap)\n  return result\n\narr = [9, 1, 5, 4, 3, 8, 7]\nprint(n_smallest(3, arr))\n\n\n4\n\n\n\n파이썬에서는 N번째로 작은 값을 구하는 메서드를 제공한다.\nnsmallest() 메서드는 가장 작은 n개의 값을 반환한다.\n\n\n\nCode\nimport heapq\n\narr = [9, 1, 5, 4, 3, 8, 7]\nresult = heapq.nsmallest(3, arr)\nprint(result[-1])\n\n\n4\n\n\n\n\n\n\n파이썬에서는 N번째로 큰 값을 구하는 메서드를 제공한다.\nnlargest() 메서드는 가장 큰 n개의 값을 반환한다.\n\n\n\nCode\nimport heapq\n\narr = [9, 1, 5, 4, 3, 8, 7]\nresult = heapq.nlargest(3, arr)\nprint(result[-1])\n\n\n7\n\n\n\n\nCode\nclass Heap(object):\n    def __init__(self):\n        # 첫번째 원소는 사용하지 않음\n        self.arr = [None]\n\n    # 원소 삽입(push)\n    def push(self, x):\n        # 마지막 위치에 원소를 삽입\n        self.arr.append(x)\n        # 첫 원소인 경우 종료\n        if len(self.arr) == 2:\n            return\n        # 값의 크기를 비교하며 부모를 타고 올라감\n        i = len(self.arr) - 1\n        while True:\n            parent = i // 2\n            # 작은 값을 부모 쪽으로 계속 이동\n            if 1 <= parent and self.arr[parent] > self.arr[i]:\n                self.arr[parent], self.arr[i] = self.arr[i], self.arr[parent]\n                i = parent\n            else:\n                break\n\n    # 원소 추출(pop)\n    def pop(self):\n        # 마지막 원소\n        i = len(self.arr) - 1\n        # 남은 원소가 없다면 종료\n        if i < 1:\n            return None\n        # 루트 원소와 마지막 원소를 교체하여, 마지막 원소 추출\n        self.arr[1], self.arr[i] = self.arr[i], self.arr[1]\n        result = self.arr.pop()\n        # 루트(root)에서부터 원소 정렬\n        self.heapify()\n        return result\n\n    # 루트(root)에서부터 자식 방향으로 내려가며 재정렬\n    def heapify(self):\n        # 남은 원소가 1개 이하라면 종료\n        if len(self.arr) <= 2:\n            return\n        # 루트 원소\n        i = 1\n        while True:\n            # 왼쪽 자식\n            child = 2 * i\n            # 왼쪽 자식과 오른쪽 자식 중 더 작은 것을 선택\n            if child + 1 < len(self.arr):\n                if self.arr[child] > self.arr[child + 1]:\n                    child += 1\n            # 더 이상 자식이 없거나, 적절한 위치를 찾은 경우\n            if child >= len(self.arr) or self.arr[child] > self.arr[i]:\n                break\n            # 원소를 교체하며, 자식 방향으로 내려가기\n            self.arr[i], self.arr[child] = self.arr[child], self.arr[i]\n            i = child\n\n\n\n\nCode\narr = [9, 1, 5, 4, 3, 8, 7]\nheap = Heap()\n\nfor x in arr:\n    heap.push(x)\n\nwhile True:\n    x = heap.pop()\n    if x == None:\n        break\n    print(x, end=\" \")\n\n\n1 3 4 5 7 8 9 \n\n\n\n\nCode\nimport random\nimport time\n\n\n# N개의 무작위 데이터 생성\narr = []\nn = 100000\nfor _ in range(n):\n    arr.append(random.randint(0, 1000000))\n\n# 시간 측정 시작\nstart_time = time.time()\n\n# 힙에 모든 원소 삽입\nheap = Heap()\nfor x in arr:\n    heap.push(x)\n\n# 힙에서 모든 원소 추출\nresult = []\nwhile True:\n    x = heap.pop()\n    if x == None:\n        break\n    result.append(x)\n\n# 시간 측정 종료\nprint(f\"Elapsed time: {time.time() - start_time} seconds.\")\n\n# 오름차순 정렬 여부 확인\nascending = True\nfor i in range(n - 1):\n    if result[i] > result[i + 1]:\n        ascending = False\nprint(\"Sorted:\", ascending)\n\n# 가장 작은 5개 원소와 가장 큰 5개 원소 출력\nprint(result[:5])\nprint(result[-5:])\n\n\nElapsed time: 1.280036211013794 seconds.\nSorted: True\n[19, 20, 25, 36, 42]\n[999930, 999935, 999975, 999985, 999994]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/computing_networking.html",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/computing_networking.html",
    "title": "Computing and Networking",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nAWS global infrastructure – Region / Availability zone (AZ) / Data Center\nAWS region consideration : Compliance – Latency – Pricing – Service availability\nInteracting with AWS\n\nAWS Management console\nAWS Command Line Interface (CLI) - unified tool to manage AWS services,\n\nAWS Software Development Kits (SDKs) - executing code with programming languages.\n\nSecurity in the AWS Cloud\n\nResponsibility - Customer & AWS\n\nAWS – security of the cloud, responsible for protecting and securing the infrastructure that runs all the services\nCustomer – security in the cloud, responsible for properly configuring the service and your applications, as well as ensuring your data is secure.\n\nRoot user (unristricted access)\nMulti-Factor Authentication\n\nAWS Identity and Access Management\n\nIntroduction to AWS IAM (Identity and Access Management)\nAWS account나 resource에 대한 권한을 관리하게 해주는 서비스\nAuthentication (인증)\nAuthorization (인가): resource에 대한 권한\n\nEx) 회사의 AWS account가 존재한다면, 회사 구성원들은 account 에 접근하기 위해 모두 IAM user가 되고 (authentication), account 내부의 각 AWS resource에 대한 접근 권한을 부여받아야 한다. (authorization)\n\nIAM policy: Effect (Allow/Deny), Action, Resource, Condition\nIAM Group\nIAM roles : IAM user나 AWS resource에 권한을 부여하기 위한 권한 세부 조정\nreference\n\n\n\n\n\n\n\n\nComputing\n\nit gives our application power in the form of CPU, memory, and networking capacity so that we can process our users’ requests.\nCompute Solution Examples\n\nEC2\ncontainer services: Amazon ECS(Elastic Container Service) and Amazon ECS (Elastic Container Service)\nserverless options AWS Lambda\n\ncompare them to one another.\n\nwhen and how to apply each service, to different use cases.\n\n\nNetworking\n\nneeded for being hands-on with Amazon EC2\nlaunch an EC2 instance to host the employee directory application.\nHow to Launch? → put an EC2 instance in a network, VPCs on AWS\n\n\n\n\n\n\nApps requiring computing capacity for running such as\n\nweb servers, batch jobs, databases, HR software, machine learning, or something else\non-premises computing resources are costly and have many things to deal with\nAWS already is ready to be used\n\nbuilt the infrastructure\nsecured the data centers.\nbought the servers,\nracked and stacked them\nare already online,\n\n3 computing services for different use cases.\n\nEC2 (Elastsic Compute Cloud)\ncontainer services\nserverless compute.\n\nEC2 (Flexible and scalable)\n\nEC2 instances Characteristics\n\na lot of flexibility and control in the cloud\nconfigure them to meet your needs.\nPayment\n\nat the end of the billing cycle, you only pay for what you use, either per second or per hour, depending on the type of instance.\nterminate (stop) the instance and you will stop incurring charges.\n\na range of operating systems including\n\nLinux, Mac OS, Ubuntu, Windows, and more\nTo select the OS for your server, you must choose\n\nAMI (Amazon Machine Image)\n\ncan set several configurations according to users’ use case\n\nYou can launch one or many instances from a single AMI, which would create multiple instances that all have the same configurations.\n\n\nThe instance types are grouped for use cases like\n\ncompute optimized\nmemory optimized\nstorage optimized, and more.\nread AWS documentation\nFor example,\n\nthe G instance family (graphics-intensive applications)\nthe M5 general purpose EC2 instance family (balance of resources)\nThe T3 or A1 is the instance family (the blend of the hardware capabilities)\nThen there’s the size like small, medium, large. It goes down to nano and up to many, many extra large sizes, depending on the instance type.\n\nthis type of selection: you are no longer locked into hardware decisions up front.\n\nchoose an initial EC2 instance type→ evaluate its performance for your specific use case, → change to a different type\n\nEC2 is also resizable with\n\na few clicks in the console or\ncan be done programmatically through an API call\n\n\nEC2 Instance Lifecycle\n\nyou launch an EC2 instance from an AMI\n\nit enters a pending state (booting up).\nit enters the running state (start being charged for the EC2 instance)\n\nrunning options\n\nreboot the instance\nstop your instance (stopping phase like powering down your laptop)\nstop hibernate (the stopping phase - no boot sequence required)\nthe terminate (the shutting down phase - get rid of an instance)\ntermination protection (back up in persistent storage in EC2)\n\ncharged if you are in the running state or if you are in the stopping state, preparing to hibernate.\n\n\n\n\n\nContainer Services\n\nefficiency and portability: container orchestration tools in AWS\n\nContainer orchestration\n\nprocesses to start, stop, restart and monitor containers running across not just one EC2 instance, but a number of them together that we call a cluster of EC2 instances.\nhundreds or thousands of containers - hard to manage them\nOrchestration tools : run and manage containers.\nAmazon ECS (Elastic Container Service)\n\nECS is designed to help you manage your own container orchestration software.\n\nAmazon EKS (Elastic Kubernetes Service)\n\nThe way you interact with these container services\n\nthe orchestration tool’s API\nthe orchestration tool carries out the management tasks.\nautomate scaling of your cluster\nautomate hosting your containers\nautomate the scaling of the containers themselves.\n\nsuper fast response to increasing demand when compared to virtual machines.\nhosting options : either ECS or EKS.\n\n\nServerless Compute Platform\n\nan alternative to hosting containers\n\nWhen using Amazon EC2 or Container Services running on top of EC2 as a compute platform, you are required to set up and manage your fleet of instances. This means that you are responsible for patching your instances when new software packages come out or when new security updates come out.\n\nServerless meaning\n\ncan not see or access the underlying infrastructure or instances that are hosting your solution.\nInstead, the management of the underlying environment from a provisioning, scaling, fault-tolerance and maintenance perspective is taken care of for a user.\nAll you need to do is focus on your application.\nserverless offerings are very convenient to use.\n\nAWS Fargate (the container hosting platform)\n\nserverless compute platform for ECS or EKS.\nThe scaling or fault-tolerance, OS or environments are built in\nFor user to do just\n\ndefine your container\nhow you want your container to be run\nit scales on-demand.\n\n\nAWS Lambda (the serverless compute platform)\n\npackage and upload your code to the Lambda service, creating what’s called a Lambda function.\nLambda functions run in response to triggers.\nYou configure a trigger\ncommon examples of triggers for Lambda functions\n\nan HTTP request\nan upload of a file to the storage service, Amazon S3\nevents originating from other AWS services or\neven in-app activity from mobile devices\n\nWhen the trigger is detected, the code is automatically run in a managed environment.\nLambda is currently designed to run code that has a runtime of under 15 minutes.\n\nSo, this isn’t for long running processes like deep learning or batch jobs, you wouldn’t host something like a WordPress site on AWS Lambda.\nIt’s more suited for quick processing, like a web backend for handling requests or a backend report processing service.\n\nnot billed for code that isn’t running, you only get billed for the resources that you use, down to 100 millisecond intervals.\n\n\n\n\n\n\n\n\nthe network, or VPC\n\nto launch instances, you needed to select a network. Building a custom VPC for our application that is more secure and provides more granular access to the Internet than the default option we originally chose.\nNetworking on AWS is the basis of most architectures. In this section, geared towards EC2-related services.\na Lambda function\n\nmight not need a network at all\n\n\nCreation and Concept of VPC\n\nCreation and Concept of VPC\n\nConcept\n\nIt creates a boundary where your applications and resources are isolated from any outside movement.\nnothing comes into and comes out of the VPC without your explicit permission.\n\nCreation\n\nTo create a VPC, two specific settings to declare\n\nthe region you’re selecting (In this example, the Oregon region)\nthe IP range in the form of CIDR notation. (In this example, 10.1.0.0/16)\n\nthe VPC name: app-vpc\n\nDivide VPC space into subnets\n\nput your resources such as your EC2 Instances inside of these subnets.\nThe goal of these subnets is to provide more granular controls over access to your resources.\nWith public resources, put those resources inside a subnet with internet connectivity.\nWith private resources like a database, create another subnet and have different controls to keep those resources private.\nTo create a subnet, you need three main things,\n\nthe VPC your subnet to live in which is this one,\nthe AZ (example. AZ-A = US-West-2a) your subnet to live in,\nIP range for your subnet which must be a subset of the VPC IP range\n\n\ninternet gateway for public resource\n\nenable internet connectivity\nWhen you create an internet gateway, you then need to attach it to your VPC.\n\nVGW (Virtual Private Gateway) for private resource\n\ncreate a VPN connection between a private network like an On-premise data center or internal corporate network to your VPC.\nestablish an encrypted VPN connection to your private internal AWS resources.\n\nhaving high availability: one option to make VPC better\n\nWhat that means is if this AZ goes down for whatever reason, what happens to our resources in that AZ? They go down too. So ideally, we would have resources in another easy to take on the traffic coming to our application.\nTo do this, we’d need to duplicate the resources in this AZ into the second AZ. So that means we need to create two additional subnets each within another AZ, say AZ b. All right\n\n\n\nAmazon VPC Routing\n\nThe example has two additional subnets, one public, one private in a different AZ for a total of four subnets including an EC2 instance hosting our employee directory inside of the public subnet in AZ A.\nroute tables\n\nprovide a path for the internet traffic to enter the internet gateway and find the right subnet.\nA route table contains a set of rules, called routes\n\ndetermine where network traffic is directed\nThese route tables can be applied on either the subnet level or at the VPC level.\nWhen creating a brand new VPC, AWS creates a route table called the main route table and applies it to the entire VPC.\nAWS assumes that when you create a new VPC with subnets, you want traffic to flow between those subnets.\nThe default configuration of the main route table is to allow traffic between all subnets local to the VPC.\nthe main route table of the VPC can be created in console\n\nclick on route tables on the side panel\ncreate route table.\ngive it a name such as app-route-table-public\nchoose the app-vpc and then click create.\n\n\n\n\nSecure Network with Amazone VPC Security\n\nat the base level, any new VPC is isolated from internet traffic to prevent risk.\nwhen allowing internet traffic by opening up routes, you need two options to keep your network secure, network access control lists\n\nnetwork ACLs\n\na firewall at the subnet level\ncontrol what kind of traffic is allowed to enter, and leave, your subnet\nThe default network ACL allows all traffic in and out of your subnet.\nUsing this default configuration is a good starting place but if needed, you can change the configuration of your network ACLs to further lock down your subnets.\nFor example, if you only wanted to allow HTTPS traffic into my subnet, you can do that by creating an inbound rule and outbound rule in my ACL, that allows HTTPS traffic from anywhere on port 443 and denies everything else.\n\nsecurity groups\n\nfirewalls that exist at the EC2 instance level.\nAny time you create an EC2 instance, you’ll need to place that EC2 instance inside a security group that allows the appropriate kinds of traffic to flow to your application."
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html",
    "title": "Infrastructure Security",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nAWS has implemented network isolation through a limited number of access points to the cloud, allowing for comprehensive monitoring of inbound and outbound communications and network traffic.\n\n\n\n\n\n\nTip\n\n\n\ninbound and outbound communications involve observing and analyzing the data and traffic that is entering or leaving the AWS network to ensure security and compliance.\n\n\n\nEndpoints are URLs that serve as entry points for web services.\nSome services do not support regions (like IAM), so their endpoints do not include a region. But, some services (like US-West-2) do support regions.\nAWS offers Amazon Virtual Private Cloud (VPC) as a private network within the AWS Cloud that provides isolation from other customers and from the internet\n\nVPC allows you to allocate IP address spaces and build a private infrastructure with networks isolated from the internet.\nYou can also connect your on-premises environment or other VPN infrastructures to VPC using IPSec tunnels and AWS Direct Connect.\nVPC allows resources to communicate with the internet if desired.\n\nBuilding a fort on a barren planet to protect themselves and the bees, and further isolating hives inside the fort itself is similar to the concept of isolating resources within a secure environment, such as Amazon VPC, to protect them from potential external threats.\nNetwork Isolation VPC\n\nthe concept of Virtual Private Cloud (VPC) in AWS is a way to logically separate your AWS infrastructure from other customers.\nVPC is like creating a fort around your AWS account and isolating resources into hives, using subnets or logical subdivision of IPs.\n\nex) EC2 instances are able to access the internet and be accessed from by putting them in a public subnet via Network Access Control Lists (NACLs), which are used to control inbound and outbound traffic at the subnet level.\n\nsecurity groups\n\nact as firewalls for EC2 instances by controlling both inbound and outbound traffic at the instance level.\nThis fine-grained access is defined by allow rules and looks\n\n\nhow to secure traffic between VPCs in AWS using VPC endpoints and route tables?\n\nfurther secure communication between Virtual Private Clouds (VPCs) in AWS\n\nIt compares the traditional method of sending traffic between VPCs through the internet with the use of private links, which allow for direct communication between VPCs within the AWS infrastructure, resulting in a safer path of travel for data.\nthe concept of route tables in VPCs\n\nroute tables contain rules or routes used to determine where network traffic is directed, and the option to create custom route tables for routing traffic according to specific requirements.\n\n\n\n\n\n\n\n\nDetective controls: 감사(auditing), 자동화된 분석, 경보를 가능하게 하는 사건 모니터링 및 블록 처리의 지속적인 개선\n\n잠재적인 보안 위협 또는 사건을 식별할 수 있는 능력 향상\ngovernance frameworks에 필수적\n법이나 compliance 준수 의무, 보안 작업 등의 개선을 지원\n\nDifferent types of detective controls\n\n자산 인벤토리의 작성(conducting an inventory of AWS resources)\n내부 감사(internal auditing)\n정보 시스템과 관련된 제어가 정책 및 요구사항 충족하는지 검사\n\n이상 활동 범위를 식별하고 이해하는데 도움이 됨\n\n\n\n\n\nAWS infrastructure의 보안 및 compliance를 향상시키는 AWS services (일부는 무료, 일부는 유료)\n\nAWS CloudTrail: AWS infrastructure와 interact 하는 사람 추적 가능 (잘못된 변경/데이터 유출 추적에 도움)\nAWS Config: configuration 관리 및 변경 기록, 모든 실제 config 세부 사항의 inventory 제공\nAWS Inspector: 자동 보안 평가 실행\n\ndeploy된 applications의 보안 및 compliance를 향상시키기 위해, best practies와의 차이, EC2 instances의 노출, 취약점 등을 체크\n\nTrusted Advisor\n\nAWS resources의 프로비저닝 보조 - best practices를 사용해서 리포트 제공\n\n리포트 항목: 비용 최적화, 성능, 보안, 장애 허용 정도, 서비스 제한\n조사 또는 실행을 위해, 심각한 수준(녹색,주황,적색)에 따라 권장 사항 제공\n\nSecurity section: S3 bucket의 권한, 보안 그룹, IAM 사용, root 계정의 MFA, 노출된 access keys, IAM 비밀번호 정책 등을 스캔\n\n\n\n\n\n\n\nMonitoring: Infrastructure와 관련된 데이터와 통계를 수집, 추적, 표시하는 과정\nAWS의 CloudWatch: metrics repository역할 - repository에 넣은 metrics 기반으로 통계 검색\n\n사용자가 정한 threshold를 넘었을때 경보 생성 가능\n특정 기간 동안 하나의 metric을 감시 → threshold와 비교한 metric의 상대적인 값에 따라 하나 이상의 특정 action 수행 가능\n\nCloudWatch Logs: 여러 resources의 log files을 모니터링, 저장, 접근 가능한 tool\n\napplication, 서버 OS의 로그 수집 및 저장\nCloudTrail 사용해서 API activity 수집\nAmazon Route 53(Amazon의 DNS 웹 서비스)의 DNS queries를 기록\nS3의 로그 데이터 저장\n\nCloudWatch Logs Insights: 로그 데이터를 interactive하게 검색하고 분석\n\n\n\n\n\nAmazon GuardDuty: 위협 감지 서비스\n\nAWS 계정 및 리소스에 대한 허가되지 않거나 악성인 행동들을 계속 모니터링\n머신 러닝, 이상 감지, integrated threat intelligence를 사용해서 잠재적인 위협을 식별하고 우선 순위를 정함\n여러 AWS data resources에서 수백억건의 사건 분석\n잠재적인 위협을 세 단계(low, medium, high) 심각 수준으로 나눠서 대응 우선순위 결정\nHTTPs API, CLI tools, Amazon CloudWatch events를 제공해서 보안 관련 발견에 대한 자동화된 보안 제공 지원\n\nSecurity Hub: 여러 AWS service의 보안 경고나 발견을 모으고, 정리하고, 우선 순위를 정함 → 통합 dashboards에서 시각화 요약 제공\n\nAWS best practies 및 업계 표준을 기반으로, compliance check 자동화를 통해 환경을 지속적으로 모니터링할 수 있도록 함\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Blog Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/monitoring_sharedresponsibility.html",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/monitoring_sharedresponsibility.html",
    "title": "Storage and Database",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n모니터링은 리소스의 운영 상태 및 사용에 대한 데이터를 수집하고 분석하는 기법으로 데이터를 수집, 분석 및 사용하여 IT 리소스 및 시스템에 대한 의사 결정을 내리거나 질문에 답변하는 작업이며 모니터링을 통해 수집한 데이터를 사용하여 리소스 과다 사용, 응용 프로그램 결함, 리소스 구성 오류 또는 보안 관련 이벤트와 같은 이벤트로 인해 발생하는 운영 문제를 확인 가능\n\n\n\n\n운영자가 문제에 대하여 능동적으로 대처가 가능\n\n운영 중단이 발생하였을 때 알려주는 것은 대처를 할 수 없으며 모니터링을 통해 Metric을 확인하여 사전에 운영 중단 방지를 위한 자동, 수동으로 수행하여 해결\n\n리소스의 성능과 안정성을 개선\n\n다양한 리소스를 모니터링하면 솔루션이 시스템으로 작동하는 방식을 전체적으로 파악할 수 있으며 병목 현상 또는 비효율적 아키텍처를 발견\n보안 위협 및 이벤트를 인식, 리소스에 액세스하는 비정상적인 트래픽 급증 또는 비정상적인 IP 주소와 같은 이상 징후를 발견\n\n비즈니스 의사 결정에 도움을 줌\n\n앱의 새 기능을 시작했는데 이 기능이 사용되고 있는지 확인할 때 애플리케이션 수준 Metric을 수집하고 새 기능을 사용하는 사용자 수를 체크\n\n\n\n\n\n\nAWS에서 제공하는 AWS서비스/App의 모니터링 서비스\nPublic 서비스(인터넷을 통해서 접근 또는 Interface Endpoint로 접근)\n로그, 지표, 이벤트 등의 운영데이터를 수집하여 시각화 처리\n경보 생성을 통해 자동화된 대응 가능\n\n\n\nVPC의 Private Service에 직접 접근은 불가하며 Interface EndPoint를 설정하여 접근이 가능  \n\n\n\n\n\n\n\nMetric은 CloudWatch에 게시된 시간순서 데이터 포인트 집합\nAWS 서비스/Application의 Performance를 모니터링 하기 위해 Metric을 생성\n생성 예시\n\n솔루션에 따라서 다양한 형태의 데이터 Metric을 생성\nEC2의 경우 CPU, 네트워크, 디스크 성능 등의 활용률 체크\nS3의 경우 CPU는 의미가 없어서 생성하지 않고 전체 크기 또는 버킷의 개체 수\nRDB의 경우 DB연결, 인스턴스의 CPU사용률, 디스크 공간 사용량\nCloudWatch Agent/ API를 활용하여 Custom Metric을 생성\n\nEC2의 Memory 사용량(외부 Public에서 볼 수 없는 것도 수집 가능)\nPrivate 영역은 CloudWatch 자체 서비스에서 확인 불가 #### Alarm\n\n\n수집된 Metric 값에 따라 Alarm 생성\n이에 따라, 다양한 방법으로 대응 가능(SNS로 Lambda 실행, 이메일 발송) #### Log수집 및 관리 #### 대시보드\n\n\n\n\n\n\nNamespace : CloudWatch metric의 Container이며 필수로 작성\nData Points : Metric을 구성하는 데이터 단위로 UTC를 권장\n\nPeriod : 시간을 기준으로 묶어서 보는 개념으로 60배수, 보관기간에 따라서 확인 가능한 조회기간도 상이함(1분 단위는 15일 -> 15일 이후에는 5분 단위로 확인가능)\n2주 이상 데이터 업데이트 안된 Metric은 콘솔에서 확인 불가(CLI에서만 확인가능)\n\nDimension : Tag/Category이며 Key/Value로 구성되며 Metric을 구분할 때 사용\n\n예: Dimension : (Server=prod, Domain=Seoul)\n\n\n\n\n \n\n\n\n\n\nOptimizing Solutions on AWS\n\nImprove Application Availability\n\nredundancy\n\nS3, DynamoDB는 이미 이중화로 설계되어짐.\n문제는 EC2\n\nUse a Second Availability Zone\n\n서로 다른 AZ에 배치하는 것이 중요.\n하지만 이런 문제는 인스턴스가 두개 이상이므로 다른 문제 발생. \n\nManage Replication, Redirection, and High Availability\n\nCreate a Process for Replication\n\nDNS를 통한 접속 -> 결국 ip를 변경해야하기 때문에 downtime 존재\n\nLoad balancer\n\n로드밸런서를 이용하면 수많은 요청을 분산 시킬 수 있음.\npublic ip를 여러 개 사용할 필요 없음 \n\n\n\n\nRoute Traffic with Amazon Elastic Load Balancing\n\nWhat’s a Load Balancer?\n\n작업을 분산 \n\nFeatures of ELB\n\n컨테이너, ip, aws lamda, ec2 모두 분산 가능\n\nHealth Checks \nELB Components\n\nListeners\n\nclient\n\nTarget groups\n\nEc2, lamda 등 타겟 대상\n\nRules\n\n매칭시키기 위한 룰 존재\nClient의 Source ip와 트래픽을 전송할 target groups \n\n\nALB(Application Load Balancer)\n\n특징\n\nALB routes traffic based on request data.\nSend responses directly to the client.\nALB supports TLS offloading.\nAuthenticate users.\nSecure traffic.\nALB uses the round-robin routing algorithm.\nALB uses the least outstanding request routing algorithm.\nALB has sticky sessions.\n\n\nNetwork Load Balancer\n\n특징\n\nNetwork Load Balancer supports TCP, UDP, and TLS protocols.\nNLB uses a flow hash routing algorithm.\nNLB has sticky sessions.\nNLB supports TLS offloading.\nNLB handles millions of requests per second.\nNLB supports static and elastic IP addresses.\nNLB preserves source IP address.\n\n\nELB types 그림 10\n\nGLB(Gateway Load Balancer)\n\n다른 곳의 application traffic으로 라우팅\n\n\n\n\n\nAmazon EC2 Auto Scaling 그림 11\n\nEC2의 과부화 발생하여 CloudWatch에 보고\nCloudWatch는 auto scailing을 실행\n그러면 각 인스턴스가 필요한 만큼 수평적으로 확장성을 제공.\nEC2가 다시 정상화가 되면 확장된 EC2 자동 종료.\n\n\n\n\n\n\n\n\n보안과 규정 준수는 AWS와 고객의 공동 책임입니다. 이 공유 모델은 AWS가 호스트 운영 체제 및 가상화 계층에서 서비스가 운영되는 시설의 물리적 보안에 이르기까지 구성 요소를 운영, 관리 및 제어하므로 고객의 운영 부담을 경감할 수 있습니다. 고객은 게스트 운영 체제(업데이트 및 보안 패치 포함) 및 다른 관련 애플리케이션 소프트웨어를 관리하고 AWS에서 제공한 보안 그룹 방화벽을 구성할 책임이 있습니다. 고객은 서비스를 선택할 때 신중하게 고려해야 합니다. 고객의 책임이 사용되는 서비스, IT 환경에서 이러한 서비스의 통합, 그리고 관계 법규에 다라 달라지기 때문입니다. 또한, 이러한 공동 책임의 특성은 배포를 허용하는 고객 제어권과 유연성을 제공합니다. 아래 차트에서 볼 수 있듯이 이러한 책임의 차이를 일반적으로 클라우드’의’ 보안과 클라우드’에서의’ 보안이라고 부릅니다.\n\n\nAWS는 AWS 클라우드에서 제공되는 모든 서비스를 실행하는 인프라를 보호할 책임이 있습니다. 이 인프라는 AWS 클라우드 서비스를 실행하는 하드웨어, 소프트웨어, 네트워킹 및 시설로 구성됩니다.\n\n\n고객 책임은 고객이 선택하는 AWS 클라우드 서비스에 따라 달라집니다. 이에 따라 고객이 보안 책임의 일부로 수행해야 하는 구성 작업량이 정해집니다. 예를 들어, Amazon Elastic Compute Cloud (Amazon EC2) 같은 서비스는 Iaas(Ifrastructure as a Service)로 분류되고 고객이 필요한 모든 보안 구성 및 관리 작업을 수행하도록 요구합니다. Amazon EC2 인스턴스를 배포하는 고객은 게스트 운영 체제의 관리(업데이트, 보안 패치 등), 고객이 인스턴스에 설치한 모든 애플리케이션 소프트웨어 또는 유틸리티의 관리, 인스턴스별로 AWS에서 제공한 방화벽(보안 그룹이라고 부름)의 구성 관리에 대한 책임이 있습니다. Amazon S3 및 Amazon DynamoDB와 같은 추상화 서비스의 경우, AWS는 인프라 계층, 운영 체제, 플랫폼을 운영하고 고객은 데이터를 저장하고 검색하기 위해 엔드포인트에 액세스합니다. 고객은 데이터 관리(암호화 옵션 포함), 자산 분류, 적절한 허가를 부여하는 IAM 도구 사용에 책임이 있습니다.\n\n\n\n\n\n예상되는 사용 사례, 피드백 및 수요를 기반으로 규정 준수 노력의 범위에 서비스를 포함합니다. 서비스가 현재 가장 최근 평가 범위에 포함되어 있지 않다고 해서 서비스를 사용할 수 없다는 의미는 아닙니다. 데이터의 특성을 결정하는 것은 조직의 공동 책임 의 일부입니다 . AWS에서 구축하는 항목의 특성에 따라 서비스가 고객 데이터를 처리하거나 저장할지 여부와 고객 데이터 환경의 규정 준수에 어떤 영향을 미칠지 또는 그렇지 않을지 결정해야 합니다.\nAWS 규정 준수 프로그램에 대한 자세한 내용은 https://aws.amazon.com/compliance/에서 확인할 수 있습니다.\n\n\n\nAWS Identity and Access Management(IAM)를 사용하면 AWS 서비스 및 리소스에 대한 액세스를 안전하게 관리할 수 있습니다. IAM을 사용하여 AWS 사용자 및 그룹을 생성 및 관리하고 권한을 사용하여 AWS 리소스에 대한 액세스를 허용 및 거부할 수 있습니다.\nIAM은 추가 비용 없이 제공되는 AWS 계정의 기능입니다. 사용자가 다른 AWS 서비스를 사용한 경우에만 비용이 청구됩니다.\nIAM 사용을 시작하거나 이미 AWS에 등록한 경우 AWS Management Console로 이동하여 이러한 IAM 모범 사례를 시작하십시오.\n\n\n\n\nQuiz\n질문 1\nWhat security mechanism can add an extra layer of protection to your AWS account in addition to a username password combination? ①Transport Layer Protocol or TCP ②Mult-factor Authentication or MFA ③Iris Scan Service or ISS ④Scure Bee Service or SBS\n질문 2\nIf a user wanted to read from a DynamoDB table what policy would you attach to their user profile? ①AmazonDynamoDBFullAccess ②AWSLambdaInvocation-DynamoDB ③AmazonDynamoDBReadOnlyAccess ④AWSLambdaDynamoDBExecutionRole\n질문 3\nWhat are valid MFA or Multi-factor Authentication options available to use on AWS? Select all that apply. ①Gemalto token ②Blizzard Authenticator ③yubiKey ④Google Authenticator\nAWS IoT button\n질문 4\nWhat format is an Identity and Access Management policy document in? ①XML ②HTML ③CSV ④JSON\n질문 5\nWhich are valid options for interacting with your AWS account? Select all that apply. ①Command Line Interface ②Software Development Kit ③Application Programming Interface ④AWS Console\n\n\n\nIAM user - IAM group – IAM policy\nHow to create permissions for a user\n\n\nAdd the user to IAM group\nCopy permissions from an existing user\nAttach existing policies to the user\n\nAWS Organization\n\n역할\n\nAutomate account creation and management\nCreate groups of accounts to reflect business needs\nGovern access to AWS services resources region by policies\nSet up single payment method for all AWS accounts with consolidated billing\nShare resources accross accounts\n\nAWS Organization을 만드는 연습\n\n\nPlan ahead for the structure of organization\nKeep the master account free of any operational AWS resources\nUse AWS CloudTrail – track all AWS usage\n\nApply least previlege practice to policies\nAssign policies to OU\nTest new and modified policies\nUse the APIs and AWS CloudFormation \n\n\n\n\n\nAuthentication and authorization\n\nThe user accesses an AWS account and resources\nAllowing resouces access to other resouces.\nAllow end users to access the applications\n\nServices\n\nAWS organizations : consolidated billing\nAWS IAM : users, groups and policies\nAWS Single Sign-ON (SSO)\nAmazon Cloud directory\nAmazon Congnito : Active directory\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Blog Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html",
    "href": "docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html",
    "title": "Storage and Database",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\nStorage Types \n\nBlock Storage: split into fixed size chunk of data\n\neasy to change small parts: change only block/piece of the data\nfor frequently modified data, or data with a high trasaction rate (like app or system files)\n\nObject Storage: each file = single unit of data\n\nto change even small parts: need to update the entire file\nfor WORM(write once, read many) model\n\naccessed often, but modified rarely (like photo)\n\n\nFile Storage: tree-like hierarchy (folders → subfolders)\n\nsimilar to windows file explorer or MacOS Finder\nfor files shared/managed by multiple host computers\n\nuser home directories, developmental environments\n\n\n\n\n\n\n\n\n\nblock storage\n\nboot volume for OS or data volume\n\nblock storages for EC2 instances\n\n\n\n\nblock storages for EC2 instances with EBS\n\n\n\nEC2 instance store: internal storage, ephemeral storage\n\ndirectly(physically) attached: fast, quick response\nlife cycle is tied to the instance: lose data when stop/terminate the instance\n\nAmazon Elastic Block Store(Amazon EBS): external storage, persistent storage\n\nseparate from EC2, user-configured size\none-to-one with EC2 instances (can’t be shared/attached to multiple instances): secure communication\n\ncan be detached → attached to another instance in the same AZ\nfor multiple attachements: need to use Amazon EBS Multi-Attach\n\nTypes of EBS\n\nSSD backed volumes: for random I/O\nHDD backed volumes: for sequential I/O\n\nbacking up data: (EBS)Snapshot\n\n\n\n\n\n\nfor employee photos: can’t use amazon EBS\n\ncan’t be attached to instances when the app scales\nhas size limitations\n\namazon simple storage service(Amazon S3)\n\nstandalone, don’t mount onto EC2 instances\naccess data through URL: storage for the internet\n\nsize limit for a single object: 5Tb\nflat structure: use unique identifiers(?)\ndistributed storage: store data across multiple different facilities within one AWS region\n\ndurable storage system\n\n\nS3 buckets: objects is stored in buckets\n\nneed to create buckets first\ncan make folders inside\nregion specific\nname: should be globally unique across AWS accounts, DNS compliant (no special characters, spaces, etc.)\n\nURL will be constructed using the name → should be reached with HTTP/HTTPS\n\nbucket URL → append object key to bucket URL\n\n\n\nAccesss control\n\ndefault: everything in S3 is private → can give public access\n\nto make object publically access, need to change bucket settings\n\npolicies \n\nIAM policies\nS3 bucket policies\n\nJSON format (like IAM policies)\nonly placed on buckets (can apply for another AWS accounts or anonymous users)\n\nnot for folders/objects\n\n\n\n\n\n\n\n\nRelational database(RDB): data를 table 형태로 저장, 서로 다른 table간 data는 relationship으로 연결됨\n\n\n\nRDB\n\n\n\nTable은 row와 column으로 구성\nrow는 record라고도 부르며 특정 개체에 관련된 모든 정보를 담고 있음\ncolumn은 attribute라고도 부르며 개체의 각 속성을 나타냄\n\nSchema: 각 table 별 관계 및 column에 들어갈 data type 등을 정의한 것\n\nschema는 한 번 설정하고 나면 변경하기 어려움\n예시: MySQL, PostgresQL, Oracle, SQL server, Amazon Aurora\n일반적으로 SQL query를 사용해서 data 접근 및 수정\n\n장점\n\nJoins: table을 join하여 data간 관계를 쉽게 이해 가능\nReduced redundancy: 일부 attribute만 다른 경우 table을 나누어 중복 정보가 저장되는 것을 방지할 수 있음\nFamiliarity: 오래된 기술이기 때문에 자료가 많아서 적응하기 쉬움\nAccuracy: data의 안정성 및 결과 보장 참고\n\n\n사용처\n\nSchema가 거의 변경되지 않는 application들\n작업 및 data의 안정성이 필요한 분야 전반\nEnterprise Resource Planning (ERP) applications\nCustomer Relationship Management (CRM) applications\nCommerce and financial applications\n\nRDB on AWS\n\nManaged database: EC2 or AWS database service\n\n\n\nEC2 or AWS database service\n\n\n\nAmazon RDS: AWS에서 제공하는 RDB service\n\nCommercial: Oracle, SQL Server\nOpen Source: MySQL, PostgreSQL, MariaDB\nCloud Native: Amazon Aurora Note: The cloud native option, Amazon Aurora, is a MySQL and PostgreSQL-compatible database built for the cloud. It is more durable, more available, and provides faster performance than the Amazon RDS version of MySQL and PostgreSQL\n\nDB instance type - 아래 type 내에서 size 별 선택지 존재\n\nStandard, which include general-purpose instances\nMemory Optimized, which are optimized for memory-intensive applications\nBurstable Performance, which provides a baseline performance level, with the ability to burst to full CPU usage.\n\nDB storage - the DB instance uses Amazon Elastic Block Store (EBS) volumes as its storage layer\n\n용량: 20~65536Gb\nGeneral purpose (SSD)\nProvisioned IOPS (SSD)\nMagnetic storage (not recommended)\n\nDB subnet group\n\nDB를 사용하기 위해서 VPC 및 subnet 설정 필요 => availability zone 내 subnet 지정 필요\nDB subnet은 private해야 됨 - gateway에 직접 연결 금지 for 보안\n보안의 경우 ACL 및 security group으로 통제 가능 - network section 참고\n\nIAM policy\n\nDB subnet group은 traffic을 조절\nIAM policy는 data와 table에 대한 접근 및 수정 권한을 조절\n\nBackup\n\nAutomatic\n\ndefault로 설정\nlog 및 DB instance 자체를 백업\n주기: 0~35일 0일의 경우 automatic 백업을 disable, 기존 backup도 삭제됨\n방식: point-in-time => 특정 기간 내 일어난 transaction에 대해서 recovery\n\nManual snapshot\n\n35일보다 긴 기간에 대해 backup할 때 사용\n\nBackup recovery: 새 instance를 생성\n\nRedundancy\n\nMulti-AZ를 허용할 경우, Amazon RDS가 다른 AZ에 redundant copy 생성\nPrimary copy: 평소에 사용하는 copy\nStandby copy: primary copy에 접근이 불가한 경우 사용하는 copy\n두 copy간 싱크로는 자동 유지\nDB instance 생성시 DNS를 설정하면 AWS가 이를 인식하여 자동으로 failover 수행\nRedundant copy는 subnet에 존재해야 됨\n\nAmazon DynamoDB\n\nFully managed NoSQL database service: provides fast and predictable performance with seamless scalability\nServerless\n\nRDB와 달리 size 제한 없음\n자동 scale 조절\nSSD에 자동 저장되며 replication 또한 자동 수행\nNo schema\n\n\n저장된 데이터 양과 접근 횟수에 따라 과금\n구성 요소\n\nTable: RDB와 유사하게 item의 집합으로 구성\nItem: 다른 item과 unique하게 구분가능한 data, 개수 제한 없음, attribute의 조합으로 구성됨, RDB와 달리 각 item의 attribute 개수가 다를 수 있음 RDB의 row에 대응\nAttribute: RDB와 달리 같은 attribute라도 다양한 type의 정보를 저장할 수 있음? RDB의 column에 대응\n\nAWS Database Services \n\n\n\n\n\n\n\n\n\nDatabase Type\nUse Cases\nAWS Service\n\n\n\n\nRelational\nTraditional applications, ERP, CRM, e-commerce\nAmazon RDS, Amazon Aurora, Amazon Redshift\n\n\nKey-value\nHigh-traffic web apps, e-commerce systems, gaming applications\nAmazon DynamoDB\n\n\nIn-memory\nCaching, session management, gaming leaderboards, geospatial applications\nAmazon ElastiCache for Memcached, Amazon ElastiCache for Redis\n\n\nDocument\nContent management, catalogs, user profiles\nAmazon DocumentDB (with MongoDB compatibility)\n\n\nWide column\nHigh-scale industrial apps for equipment maintenance, fleet management, and route optimization\nAmazon Keyspaces (for Apache Cassandra)\n\n\nGraph\nFraud detection, social networking, recommendation engines\nAmazon Neptune\n\n\nTime series\nIoT applications, DevOps, industrial telemetry\nAmazon Timestream\n\n\nLedger\nSystems of record, supply chain, registrations, banking transactions\nAmazon QLDB\n\n\n\n\n선택 기준\n\nRDB: 데이터 간 관계가 복잡하고 별도 관리가 필요한 경우에 사용 복잡도에 의해 overhead가 발생하기 때문\nKey-value DB: Large scale, low latency 보장, 단순 데이터 저장 및 조회 목적으로 적합 => RDB에서는 여러 table에 나누어 저장해야 되는 정보를 한 table에 저장 가능\nGraph: SNS와 같은 관계형 자료구조에 적합\nLedger: 금융과 같은 안정성, 변경 불가가 필요한 자료를 저장하는 경우에 적합\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Blog Content List"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html",
    "href": "docs/blog/posts/Engineering/guide_map/index.html",
    "title": "Content List, Engineering",
    "section": "",
    "text": "0000-00-00, Terminology"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#data-structure",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#data-structure",
    "title": "Content List, Engineering",
    "section": "2 Data Structure",
    "text": "2 Data Structure\n\n2023-01-17, Overview\n2023-01-18, Array\n2023-01-18, Linked List\n2023-01-18, Python List\n2023-01-19, Stack\n2023-01-19, Queue\n2023-01-26, Deque\n2023-01-26, Binary Search Tree\n2023-01-20, Priority Queue\n2023-01-20, Graph"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#conda",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#conda",
    "title": "Content List, Engineering",
    "section": "3 Conda",
    "text": "3 Conda"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#docker",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#docker",
    "title": "Content List, Engineering",
    "section": "4 Docker",
    "text": "4 Docker\n\n2023-01-30, Docker Install\n2023-01-31, Docker Compose\n2023-02-01, Docker Container"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#dynamic-documentation",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#dynamic-documentation",
    "title": "Content List, Engineering",
    "section": "5 Dynamic Documentation",
    "text": "5 Dynamic Documentation\n\n2023-01-19, Quarto\n2023-01-19, xaringan[R]\n2023-01-19, Bookdown[R]\n2023-01-19, DISTL\n2023-01-26, Sphinx[Python]"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#aws-cloud",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#aws-cloud",
    "title": "Content List, Engineering",
    "section": "6 AWS Cloud",
    "text": "6 AWS Cloud\nCoursera Course: AWS Fundamentals\n\n2023-03-09, Computing and Networking\n2023-03-12, Storage and Database\n2023-03-26, Monitoring and SharedResponsibility\n2023-04-05, Infrastructure Security"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#azure-cloud",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#azure-cloud",
    "title": "Content List, Engineering",
    "section": "7 Azure Cloud",
    "text": "7 Azure Cloud"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#data-modeling",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#data-modeling",
    "title": "Content List, Engineering",
    "section": "8 Data Modeling",
    "text": "8 Data Modeling"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#apache-airflow",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#apache-airflow",
    "title": "Content List, Engineering",
    "section": "9 Apache Airflow",
    "text": "9 Apache Airflow"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#apache-spark",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#apache-spark",
    "title": "Content List, Engineering",
    "section": "10 Apache Spark",
    "text": "10 Apache Spark"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#front-end",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#front-end",
    "title": "Content List, Engineering",
    "section": "11 Front End",
    "text": "11 Front End"
  },
  {
    "objectID": "docs/blog/posts/Engineering/guide_map/index.html#back-end",
    "href": "docs/blog/posts/Engineering/guide_map/index.html#back-end",
    "title": "Content List, Engineering",
    "section": "12 Back End",
    "text": "12 Back End"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html",
    "href": "docs/blog/posts/Language/guide_map/index.html",
    "title": "Content List, Language",
    "section": "",
    "text": "(Draft)"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html#r",
    "href": "docs/blog/posts/Language/guide_map/index.html#r",
    "title": "Content List, Language",
    "section": "R",
    "text": "R\n\n1111-11-11, tidyverse\n\n1111-11-11, dplyr\n1111-11-11, ggplot2\n1111-11-11, tidyr\n1111-11-11, readr\n1111-11-11, purrr\n1111-11-11, tibble\n1111-11-11, stringr\n1111-11-11, forcats\n\n1111-11-11, tidymodels\n1111-11-11, R shiny"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html#python",
    "href": "docs/blog/posts/Language/guide_map/index.html#python",
    "title": "Content List, Language",
    "section": "Python",
    "text": "Python\n\n1111-11-11, numpy\n1111-11-11, pandas\n1111-11-11, matplotlib\n1111-11-11, seaborn\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html#sql",
    "href": "docs/blog/posts/Language/guide_map/index.html#sql",
    "title": "Content List, Language",
    "section": "SQL",
    "text": "SQL\n\nSQLite\n\n\nOracle SQL\n\n\nMS-SQL\n\n\nPostgre SQL"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html#linux",
    "href": "docs/blog/posts/Language/guide_map/index.html#linux",
    "title": "Content List, Language",
    "section": "Linux",
    "text": "Linux"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html#powershell",
    "href": "docs/blog/posts/Language/guide_map/index.html#powershell",
    "title": "Content List, Language",
    "section": "Powershell",
    "text": "Powershell"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html#c",
    "href": "docs/blog/posts/Language/guide_map/index.html#c",
    "title": "Content List, Language",
    "section": "C++",
    "text": "C++"
  },
  {
    "objectID": "docs/blog/posts/Language/guide_map/index.html#javascript",
    "href": "docs/blog/posts/Language/guide_map/index.html#javascript",
    "title": "Content List, Language",
    "section": "Javascript",
    "text": "Javascript"
  },
  {
    "objectID": "docs/blog/posts/Language/pipeline.html",
    "href": "docs/blog/posts/Language/pipeline.html",
    "title": "Tensorflow - Data Input Pipeline",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nTensorflow 공식 문서 중 Dataset에 대한 기능 및 성능에 대한 비교 자료\n\n\n\n\n\n\nNote\n\n\n\nThe tf.data API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations. The tf.data API introduces a tf.data.Dataset abstraction that represents a sequence of elements, in which each element consists of one or more components. For example, in an image pipeline, an element might be a single training example, with a pair of tensor components representing the image and its label. Source: https://www.tensorflow.org/guide/data\n\n\n\n\n\n어떠한 데이터의 형태가 오더라도 Dataset object 자체가 iterative한 interface를 제공해서 for loop 등의 iteration을 이용하여 데이터의 입력 형태가 변경되어도 코드의 일관성을 유지할 수 있음\n\nTo create an input pipeline, you must start with a data source. For example, to construct a Dataset from data in memory, you can use tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices(). Alternatively, if your input data is stored in a file in the recommended TFRecord format, you can use tf.data.TFRecordDataset().\n\n\nCode\nimport tensorflow as tf\nimport pathlib\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nnp.set_printoptions(precision=4)\n\ndataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\ndataset\n\n\n<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n\n\nOnce you have a Dataset object, you can transform it into a new Dataset by chaining method calls on the tf.data.Dataset object. For example, you can apply per-element transformations such as Dataset.map, and multi-element transformations such as Dataset.batch. Refer to the documentation for tf.data.Dataset for a complete list of transformations. The Dataset object is a Python iterable. This makes it possible to consume its elements using a for loop:\n\n\nCode\nfor z in dataset:\n    print(z.numpy())\n\n\n8\n3\n0\n8\n2\n1\n\n\n\n\n\n\n\nDataset을 사용은 prefetch 기능과 interleaving 기능에 의한 연산 속도를 향상 시킬수있다.\n\n사용할 수 없는 (또는 사용하지 말아야 할) 특별한 이유가 없다면 필수로 사용해야 할 듯\n\nPipeline을 쓸 때 연산 처리 속도가 빨리지는 이유 -> cpu architecture 관련\n\nX86, ARM 프로세서\n\n실행을 위해 D램 올라감 -> cpu에서 하나의 명령이 거치는 step들 fetch(cpu에 로드), decode(해석), execution(수행) -> load (다시 메모리 이동)\n각 단계에서 수행 소요 시간 존재\n\n\n파이프라인 구조를 가지면, 한 스텝 수행 시 다음 데이터가 다른 스텝 수행 가능 -> 시간 단축\n\nTensor Flow Pipeline\n\n\n앞쪽 데이터 트레이닝 동안 다음 데이터를 미리 읽어옴\n\n\n\n어떤 작업이 끝나기 전에 dependency 없는 다른 작업 수행(async 하게)\n\n\n\n같은 데이터를 반복적으로 사용시 한번 읽은 데이터를 계속 메모리에 가지고 있음\n\n\n\n\n\n\nfloating point 로 인한 error 누적\n\n\n\nTensorflow와 Numpy로 구현한 값에 오차가 발생하는 이유는 Tensorflow를 어떻게 compile 하였느냐에 따른 차이.\n\nFloating point (IEEE-754)에서는 값의 표현에 대한 정의만 있고 실제 연산은 processor vendor마다 다르므로 약간의 오차가 있을 수 있으며, 부동소수점 연산기능을 지원하는 명령들 중 어떠한 명령을 사용하도록 compile하였느냐에 따라 계산값에 차이 발생 가능하고 대부분 무시하지만 오차가 누적이 되면 error rate에 영향이 있을 수 있음.\n부동소수점의 precision이 낮을수록 overfitting 가능성 저하되어 오히려 학습이 잘될 수도 있으며 모델을 경량화 할 수 있음 \\(\\rightarrow\\) bfloat16 type이 생기게 된 이유.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/differentiation/2023-02-04_uni_derivative.html",
    "href": "docs/blog/posts/Mathmatics/differentiation/2023-02-04_uni_derivative.html",
    "title": "Differentiation - Univariabe Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n미분은 최적화 문제를 푼다는 의미는 목적함수를 최적화한다는 말이고 그 과정에서 대부분의 경우 미분을 사용하여 상황에 맞게 목적함수의 극소값이나 극대값을 구하게 된다. 예를 들어, 목적 함수 \\(f(x;w)\\) 가 매개변수 또는 가중치 \\(w\\) 에 의해 그 모양이 결정되므로 \\(f(x;w)\\) 를 최소화하는 최적화 문제를 풀고 싶을 때 \\(f(x;w)\\) 를 cost function (=\\(f(x;w)\\) 의 함수) 또는 loss function (=\\(f(x;w)\\) 의 함수) 으로 잘 정의를 한 후 cost function 또는 loss function을 최소화하는 매개변수 \\(w\\) 를 구해야한다. 대부분의 경우, Machine Learning (Deep Learning 포함) 에서 미분은 error를 줄이기 위해 사용한다.\n목적 함수 \\(f(x;w)\\) 가 정의 됐을 때 목적 함수의 극 값을 구하기 위해서 변화량을 관찰해야 한다. 비단 목적함수를 포함한 어떤 현상을 함수로 표현할 때에도 문제를 해결하기 위해 변화량을 관찰하는 경우가 빈번하다.\n예를 들어, 선풍기 바람의 세기를 조절할 때 입력값은 바람 세기 버튼 출력값은 바람의 세기로 가정한다면 선풍기 바람 세기 입력에 따라 적절한 출력값을 갖도록 조정하고 싶을 것이다. 이때 변화량 관찰이 요구된다. 변화량을 잘 대변하는 것이 함수의 기울기이다. 함수의 기울기는 민감도 (sensitivity)로 표현되기도 한다(wiki). 기울기는 아래와 같이 정의된다.\n\n\n\n\n직접 미분\n\n이번 블로그에서 정리한 내용\n수동 미분: 식을 알고 손으로 미분하여 도함수를 얻음\nsymbolic 연산: 기호를 이용하여 미분 연산 \\(\\rightarrow\\) sympy package 이용\n함수 복잡하면 풀기 매우 어려움\n\n간접 미분\n\n수치 미분 (numerical differentiation): 도함수를 몰라도 미분계수를 구하는데 사용\n함수 복잡하면 시간 오래 걸림\n\n자동 미분 (automatic differentiation)\n\n복잡한 함수를 간단한 함수로 쪼개어 직접 미분 - Pytorch\n실무에서, 정합성 검사를 위해 자동 미분의 결과를 간접 미분의 결과와 맞추기도 한다.\n간접 미분이 정답이 된다.\n\n\n\n\n\n\nDefinition 1 The slope of line connected with the two points \\(P_1(x_1,y_1)\\) and \\(P_2(x_2,y_2)\\) is \\[\nm=\\frac{y_2-y_1}{x_2-x_1}\n\\]\n\n\nTheorem 1 The point slope equation of line through \\(P_1(x_1,y_1)\\) with slope \\(m\\) is \\[\ny-y_1=m(x-x_1)\n\\]\n\n\nTheorem 2 The slope intercept euation of line with slope m and y-intercept b is \\[\ny=mx+b\n\\]\n\n\\[\n\\begin{aligned}\n\\text{기울기}(slope)&=\\frac{\\text{출력의 변화량}}{\\text{입력의 변화량}}\\\\\n&= \\text{입력 변화량에 대한 출력 변화량} \\\\\n&=\\frac{\\Delta output}{\\Delta input}\\\\\n&= \\text{단위 입력당 출력의 변화량}\\\\\n&= \\text{민감도, 평균 변화율 (Rates Of Change), or etc}.\\\\\n\\end{aligned}\n\\]\n\nDefinition 2 \\[\n\\begin{aligned}\n\\text{평균 변화율}&=\\Delta x\\text{에 대한} \\Delta y\\text{의 비율}\\\\\n&=\\frac{\\Delta y}{\\Delta x}=\\frac{f(b)-f(a)}{b-a}\\\\\n&=\\frac{f(a+\\Delta x)-f(a)}{\\Delta x}\n\\end{aligned}\n\\]\n\n\n입력에 대한 변화율을 조절하고 싶을 때 필요한 개념\n\nex) 시약 농도 대비 신호 증폭의 변화율을 관찰\n\n하지만 관심의 대상의 관계 그래프가 직선이 아닌 곡선의 형태의 경우 평균 변화율이 대략적인 추세 정보만 제공해줄뿐 변화량의 자세한 정보를 제공해주지 못함\n\nsigmoid 형태의 경우 첫 포인트와 마지막 포인트의 평균 변화율을 보는 것 보다 구간을 짧게하여 여러 군데서 관찰하는 것이 graph의 shape를 더 잘 설명하는 것\n\n이 때, 입력값의 구간을 충분히 짧게 만들어 출력값의 변화량을 관찰하는 것이 미분이다. (limit의 개념, \\(\\epsilon-\\delta\\) method)\n\n\nDefinition 3 The tangent line to the curve \\(y=f(x)\\) at the point \\(P(a,f(a))\\) is the line through P with slope\n\\[\nm = \\lim_{x\\to a} f(x)\n\\]\nprovided that this limit exists.\n\n\nDefinition 4 When \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\) is continuous and differentiable, the derivative of a function \\(f\\) at a number \\(a \\in \\mathbb R\\), denoted by \\(f'(a)\\), is\n\\[\n\\begin{aligned}\nf'(a) &= \\lim_{h\\to 0} \\frac{f(a+h)-f(a)}{h}\\\\\n      &= \\lim_{x\\to a} \\frac{f(x)-f(a)}{x-a}\n\\end{aligned}\n\\]\nprovided that this limit exists. 이때 위의 함수의 극한값, \\(f'(a)\\) 라고도 표시하며 점 \\(a\\) 에서의 \\(f(x)\\) 의 도함수 (derivative) 라고 한다.\n\n\n\n\nJames Stewart - Calculus Early Transcedentals, 7th Eidition, P.143\n\n\n\nDefinition 5 A function \\(f\\) is differentiable at \\(a\\) if \\(f'(a)\\) exists. It is differentiable on an open interval (a,b) [or (a,\\(\\infty\\)), (-\\(\\infty\\),a) or (-\\(\\infty\\),\\(\\infty\\))] if it is differentiable at every number in the interval.\n\n\nTheorem 3 If \\(f\\) is differentiable at \\(a\\), then \\(f\\) is continuous at \\(a\\).\n\n\n순간 변화율 = 미분 계수 = 접선의 기울기\n미분 (differentiation) : 순간 변화율 구하는 행위\n도함수 (derivative) : 도함수 자체는 equation 으로, 특정 포인트에서의 순간 변화율 (값)을 출력하는 함수\n문제를 풀때 도함수를 구하는 것인지, 미분계수를 계산하는 것인지를 구별해야함\n전체 도함수를 구하는것은 보통 굉장히 어려움. 하지만 한점에서의 순간변화율 즉, 미분계수를 구하는 것은 가능\n에러를 줄이는 데에는 값으로 나오는 순간 변화율을 구하는 것이 일반적으로 실현성이 있는 문제\n\n\nDefinition 6 The natural number, \\(e\\) is the number such that \\(\\lim_{h\\to 0} \\frac{e^h-1}{h}=1\\).\n\n모든 지수 함수 \\(f(x)=a^x\\) 중에서 \\(f(x)=e^x\\) 가 점 (0.1) 에서의 접선의 기울기가 \\(f'(0)=1\\) 이 되는 수를 \\(e=2.71828...\\) 라고 정의한다.\n\n\n\\(f'(x)\\) 는 다음과 같은 기호들로도 흔히 표현된다.\n\nLagrange’s notation\n\n\\(y', f'(x)\\)\n어떤 변수로 미분하는지에 대해서 명시적으로 표현되지 않았음. 고등학교때까진 univariable function을 미분 했기떄문에 이 표기법이 많이 사용되었음.\n\nLeibniz’s notation\n\n\\(\\frac{dy}{dx}=\\frac{df}{dx}=\\frac{d}{dx}f(x)\\)\n입력 변수와 출력 변수까지 모두 명시되어있음\n\nNewton’s notation:\n\n\\(\\dot{y}, \\ddot{y}\\)\n최적화 논문과 financial engineering 에서 본적 있음\n\nEuler’s notation\n\n\\(D_xy, D_xf(x)\\)\n시계열 논문과 미분방정식 논문에서 본적 있음\n\n\n\n\n\n다음의 함수를 미분의 정의를 이용하여 도함수를 계산하시오\n\n\\(f(x)=c\\) where c is a constant\n\\(f(x)=\\log x\\)\n\\(f(x)=e^x\\)\n\\(f(x)=\\sin x\\)\n\nDerivative Formula는 모두 미분의 정의를 이용해서 구할 수 있음\n\nTheorem 4  \n\nThe Power Rule, if \\(n\\) is any real number, then the power function, \\(x^n\\) is differentiated like the following: \\[\n\\frac{d}{dx}(x^n)=nx^{n-1}\n\\]\nThe Constant Multiple Rule, if \\(c\\) is a constand and \\(f\\) is a differentiable function, then \\[\n\\frac{d}{dx}(cf'(x))=c\\frac{d}{dx}f(x)=cf'(x)\n\\]\nThe Sum Rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\n\\frac{d}{dx}[f(x)+g(x)]=\\frac{d}{dx}[f(x)]+\\frac{d}{dx}[g(x)]=f'(x) +g'(x)\n\\]\nThe Difference Rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\n\\frac{d}{dx}[f(x)-g(x)]=\\frac{d}{dx}[f(x)]-\\frac{d}{dx}[g(x)]=f'(x) -g'(x)\n\\]\nThe Product Rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\ny=f(x)g(x), y'=f'(x)g(x)+f(x)g'(x)\n\\]\nThe quotient rule, if \\(f\\) and \\(g\\) are both differentiable, then \\[\ny=\\frac{f(x)}{g(x)}, y'=\\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}\n\\]\n\n\n증명은 James Stewart의 Calculus Series 중 1개를 골라 참고하시기 바랍니다.\n\n\n\n\n\\(S(x)=\\frac{1}{1+e^{-ax}}\\) 를 \\(x\\) 에 대해 미분해보시오.\n\\(f(x)=\\alpha_1 + \\frac{\\alpha_2-\\alpha_1}{1+e^{-\\alpha_4(x-\\alpha_3)}}\\) 를 어떻게 미분할 것인지 생각해 보시오.\n\\(y=f(x)=(4x+3)^2\\) 를 \\(x\\) 에 대해 미분해보시오\n\\(y=f(x)=(4x+3)^{20}\\) 를 \\(x\\) 에 대해 어떻게 미분할 것인지 생각해보시오. (hint: composite function - Leibniz)\n\n\n\n\n\n\n앞서와 언급한대로 파이썬 sympy package를 사용할 것인데 간단한 예를 본다. 먼저 기호의 정의를 해주고 수학 연산을 진행하면 된다.\n\n\nCode\n# import packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sympy as S\nimport matplotlib as mpl\nfrom mpl_toolkits import mplot3d\nimport matplotlib.font_manager as mfm\n\n\n\n\nCode\n# 심볼 정의\nx=S.Symbol('x')\nh=S.Symbol('h')\nn=S.Symbol('n')\na=S.Symbol('a')\na1=S.Symbol('a1')\na2=S.Symbol('a2')\na3=S.Symbol('a3')\na4=S.Symbol('a4')\n\n## 인수분해\nS.factor(x**2+4*x+4)\n\n\n\\(\\displaystyle \\left(x + 2\\right)^{2}\\)\n\n\n\n인수분해와 값 넣기\n\n\n\nCode\nf=S.factor(x**2-5*x+6)\nprint(f.subs({x:3}))\nprint(f.subs({x:5}))\nf\n\n\n0\n6\n\n\n\\(\\displaystyle \\left(x - 3\\right) \\left(x - 2\\right)\\)\n\n\n\n\\(2^x\\) 미분과 값 넣기\n\n\n\nCode\ndf=S.limit((2**(x+h)-2**x)/h,h,0)\nprint(df.subs({x:3}))\nprint(df.subs({x:5}))\n\nprint(df.subs({x:3}).evalf()) # 정확한 값을 원할 경우\ndf\n\n\n8*log(2)\n32*log(2)\n5.54517744447956\n\n\n\\(\\displaystyle 2^{x} \\log{\\left(2 \\right)}\\)\n\n\n\n\\(x^n\\) 미분\n\n\n\nCode\ndf=S.limit(((x+h)**n-x**n)/h,h,0)\nprint(df.subs({x:3}))\nprint(df.subs({x:5}))\ndf\n\n\n3**(n - 1)*n\n5**(n - 1)*n\n\n\n\\(\\displaystyle n x^{n - 1}\\)\n\n\n\n\n\n\n\\(f(x)=c\\) 의 도함수 where c is a constant. c=2로 고정\n\n\n\nCode\nf=2\ndf=S.limit(((2)-2)/h,h,0)\ndf\n\n\n\\(\\displaystyle 0\\)\n\n\n\n\\(f(x)=\\log x\\) 의 도함수\n\n\n\nCode\nf=S.log(x)\ndf=S.limit((S.log(x+h)-S.log(x))/h,h,0)\ndf\n\n\n\\(\\displaystyle \\frac{1}{x}\\)\n\n\n\n\\(f(x)=e^x\\) 의 도함수\n\n\n\nCode\nf=S.exp(x)\ndf=S.limit((S.exp(x+h)-S.exp(x))/h,h,0)\ndf\n\n\n\\(\\displaystyle e^{x}\\)\n\n\n\n\\(f(x)=\\sin x\\) 의 도함수\n\n\n\nCode\nf=S.sin(x)\ndf=S.limit((S.sin(x+h)-S.sin(x))/h,h,0)\ndf\n\n\n\\(\\displaystyle \\cos{\\left(x \\right)}\\)\n\n\n\n\\(S(x)=\\frac{1}{1+e^{-ax}}\\) 를 \\(x\\) 에 대해 미분해보시오.\n\n\n\nCode\nf=1/(1+S.exp(-a*x))\ndf=S.limit((1/(1+S.exp(-a*(x+h)))-1/(1+S.exp(-a*x)))/h,h,0)\ndf\n\n\n\\(\\displaystyle \\frac{a e^{a x}}{\\left(e^{a x} + 1\\right)^{2}}\\)\n\n\n이번 문제에서 \\(\\frac{ae^x}{(e^{-ax}+1)^2}\\) 라는 sigmoid function의 도함수를 얻었다. 이 도함수를 간단한 수학적 조작으로 다른 표현으로 유도해보면 다음과 같다. \\[\n\\begin{aligned}\n\\frac{d}{dx}S(x)&=\\frac{ae^{-ax}}{(e^{ax}+1)^2}\\\\\n                &=a\\frac{1}{(e^{ax}+1)}\\frac{e^{-ax}}{(e^{ax}+1)}\\\\\n                &=a\\frac{1}{(e^{ax}+1)}\\frac{1+e^{-ax}-1}{(e^{ax}+1)}\\\\\n                &=a\\frac{1}{(e^{ax}+1)}(1-\\frac{1}{(e^{ax}+1)})\\\\\n                &=aS(x)(1-S(x))\\\\\n\\end{aligned}\n\\]\n위와 같이 \\(S(x)\\) 의 도함수는 \\(aS(x)(1-S(x))\\) 로 표현될 수 있다. sigmoid function은 neural network에서 activation function으로 사용되는데 forward propagation에서 이미 한 번 계산이 된다. backward propagation에서 activation function인 \\(S(x)\\) 의 도함수를 다시 연산을 해야하는데 도함수가 \\(aS(x)(1-S(x))\\) 것을 알면 복잡한 고차원 행렬곱 연산을 다시 수행하지 않아도 된다. 그래서 \\(S(x)\\) 의 도함수를 \\(\\frac{ae^x}{(e^{-ax}+1)^2}\\) 라고 코딩하는 것 보다는 \\(S(x)\\) 의 행렬을 재활용하여 \\(aS(x)(1-S(x))\\) 로 코딩해놓으면 연산 과정에서의 시간 복잡도를 줄일 수 있다. 이처럼 machine learning에서 수학적 통계적 지식을 잘 활용하면 좀 더 효율적인 모델링을 구현 할 수 있다.\n\n\nCode\nx1 = np.linspace(-6, 6, 100)\nsx = 1/(1+np.exp(-x1))\nd_sx = sx*(1-sx)\n\nplt.plot(x1,sx,color='black',label='S(x)')\nplt.plot(x1,d_sx,color='red',label='dS(x)')\n\nplt.xlabel('X')\nplt.xlabel('S(x)')\nplt.title('Simgoid Curve with its Derivative')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\\(f(x;\\mathbf\\alpha)=\\alpha_1 + \\frac{\\alpha_2-\\alpha_1}{1+e^{-\\alpha_4(x-\\alpha_3)}}\\) 를 어떻게 미분할 것인지 생각해 보시오.\n\n위의 식은 logistic fucntion의 genral formular 형태인데 sigmoid function이 특수한 예이다. 함수의 shpae는 parameter에 의해 결정되는데 위의 경우 \\(\\alpha_1\\) 은 함수의 최솟값 , \\(\\alpha_2\\) 은 함수의 최댓값, \\(\\alpha_3\\) 은 함수의 변곡점 및 \\(\\alpha_4\\) logistic curve가 변곡점을 지나면서 증가하는 변화율을 묘사한다. sigmoid 형태의 data를 fitting하기 위해 위의 함수를 이용한다면 error를 최소화하는 parameter를 구해야하는데 이 또한 최적화 문제로 4개의 변수에 대한 미분이 필요하다. 2개 이상의 변수에 대해서 미분은 partial derivative (편미분)라고 하는데 다음 블로그에서 다룰 것이다.\n\n\\(y=f(x)=(4x+3)^2\\) 를 \\(x\\) 에 대해 미분해보시오.\n\n\n\nCode\nS.limit(((4*(x+h)+3)**2-(4*x+3)**2)/h,h,0)\n\n\n\\(\\displaystyle 32 x + 24\\)\n\n\n위의 도함수는 미분 공식 중 곱의 법칙을 사용하면 구할 수 있다.\n\n\\(y=f(x)=(4x+3)^{20}\\) 를 \\(x\\) 에 대해 어떻게 미분할 것인지 생각해보시오. (hint: composite function - Leibniz)\n\n위의 문제처럼 곱의 법칙을 사용하면 20개의 인수에 대해서 차례대로 미분을 해야하므로 계산량이 엄청나게 많아진다. 이때 composite function (합성 함수)의 derivative를 구하는 chain rule을 이용하면 간단한 연산으로 도함수를 구할 수 있게 된다. deep learning 모델의 기초인 neural network는 layer nodes이 복잡하게 합성이 되는 합성 함수를 만들면서 forward propagation을 진행하고 backward propabation에서 이 복잡한 합성 함수의 미분을 수행하게 된다. 그러므로 합성 함수의 미분이 어떻게 수행되는지 아는 것은 deep learning을 수리적으로 이해하고 싶은 사람에게 있어서 중요할 수 있다. 합성 함수의 미분은 다른 블로그에서 다루도록 하겠다.\n\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/differentiation/2023-02-10_composite_partial_derivative.html",
    "href": "docs/blog/posts/Mathmatics/differentiation/2023-02-10_composite_partial_derivative.html",
    "title": "Differentiation - Chain Rule & Partial Derivative",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nNeural Network (NN)는 여러 노드(node)가 여러 층(layer)을 갖고 층 사이에 함수 관계가 있는 구조이다. NN의 결과값을 계산할 때 순방향(forward)으로 전 층의 노드가 그 다음 층의 입력값으로 들어가 복잡한 합성 함수가 만들어져 결과값을 출력하고, 학습을 할 때 역방향(backword)으로 그 합성 함수의 미분을 수행하게 된다 (Back Propagation). NN은 deep learning의 기본 구조가 되므로 합성 함수가 무엇이고 어떤 원리로 미분이 되는지 알 필요가 있다.\n\n\n\n\nTheorem 1 If the fucntions, \\(f(x)\\), \\(g(x)\\), are differentiable, then the composite function of \\(f(x)\\) and \\(g(x)\\), \\(u=f\\circ g\\) defined by \\(u(x)=g(f(x))\\) is differentiable at \\(x\\) and \\(u\\) is given by the product, the chain rule is \\[\n\\begin{aligned}\nu'(x)&=g'(f(x))f'(x)\n\\end{aligned}\n\\]\nIn Leibniz notation, if \\(y=f(x)\\) and \\(u=g(y)\\) are both differentiable functions, then \\[\n\\frac{du}{dx} = \\frac{du}{dy}\\frac{dy}{dx}\n\\]\n\nChain rule은 합성 함수의 미분으로 겉에 있는 함수를 미분하고 안에 있는 함수를 미분을 연달아 하는 방식이다.\n\n\n다음 식들의 도함수를 chain rule에 따라 구해본다.\n\n\\(u(x)=(4x^2+7x)^{50}\\)\n\n\\[\n\\begin{aligned}\n&\\text{when the two functions are }f(x)=4x^2+7x, \\space g(x)=x^{50}, \\\\\n&u(x)=(4x^2+7x)^{50}=g(f(x))\\\\\n&f'(x)=8x+7\\\\\n&g'(x)=50x^{49}\\\\\n&u'(x)=g'(f(x))f'(x)\\\\\n&u'(x)=50f(x)^{49}(8x+7)=50(4x^2+7x)^{49}(8x+7)\n\\end{aligned}\n\\]\n\n\\(f(x)=x+1\\), \\(g(x)=\\sqrt{x+1}\\), \\(u(x)=g(f(x))\\)\n\n\\[\n\\begin{aligned}\n&f(x)=x+1, \\space g(x)=\\sqrt{x+1} \\\\\n&u(x)=g(f(x))=\\sqrt{(x+1)+1}\\\\\n&f'(x)=1\\\\\n&g'(x)=\\frac{1}{2\\sqrt{x+1}}\\\\\n&u'(x)=g'(f(x))f'(x)\\\\\n&u'(x)=\\frac{1}{2\\sqrt{f(x)+1}}f'(x)=\\frac{1}{2\\sqrt{(x+1)+1}}\n\\end{aligned}\n\\]\n$$\n\\[\\begin{aligned}\n  \\lim_{\\Delta x \\to0}\\frac{\\Delta u}{\\Delta x}&=\n  \\lim_{\\Delta y \\to0}\\frac{\\Delta u}{\\Delta y}\\lim_{\\Delta x \\to0}\\frac{\\Delta y}{\\Delta x}\\\\&=\\lim_{\\Delta y \\to0}\\frac{\\Delta u}{\\Delta y}\\frac{dy}{dx}\\\\\n  \\frac{dz}{dx}&=\\frac{dz}{dy}\\frac{dy}{dx} &(\\because \\Delta x \\to 0 \\Longrightarrow \\Delta y \\to 0 )\n\n\\end{aligned}\\]\n$$\n\\(\\Delta x\\rightarrow \\Delta y \\rightarrow \\Delta z\\)\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/differentiation/2023-03-18_higher_order_derivative.html",
    "href": "docs/blog/posts/Mathmatics/differentiation/2023-03-18_higher_order_derivative.html",
    "title": "Differentiation - Higher Order Derivative",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 미분 가능한 함수 \\(f(x)\\) 의 \\(f'(x)\\) 가 존재할 때, \\(f'(x)\\) 의 도함수 \\((f'(x))'\\) 를 \\[\nf^{''}(x), \\frac{d^2f(x)}{dx^2}, \\text{ or } \\frac{d^2y}{dx^2}\n\\] 라 표기하고 \\(f(x)\\) 의 2차 도함수 or second derivative라고 한다.\n같은 방법으로, \\(n\\) 차 도함수 \\(f^{(n)}(x)\\) or \\(\\frac{d^nf(x)}{dx^n}\\) 가 정의 된다.\n\n\n\n\n다음의 \\(n\\) 차 도함수\n\n\\(f(x)=x^{\\alpha} \\text{ } (x>0, \\alpha \\ne -1)\\)\n\n\\[\n\\begin{aligned}\n    f'(x)&=\\alpha x^{\\alpha-1}\\\\\n    f^{''}(x)&=\\alpha(\\alpha-1) x^{\\alpha-2}\\\\\n    &\\vdots\\\\\n    f^{n}(x)&=\\alpha(\\alpha-1)\\cdots(\\alpha-(n-1)) x^{\\alpha-n}\n\\end{aligned}\n\\]\n\n\\(f(x)=ln(1+x)\\)\n\n\\[\n\\begin{aligned}\n    f'(x)&=\\frac{1}{1+x}\\\\\n    f^{''}(x)&=-\\frac{1}{(1+x)^2}\\\\\n    f^{3}(x)&=(-1)^2\\frac{1 \\cdot 2 }{(1+x)^3}\\\\\n    &\\vdots\\\\\n    f^{n}(x)&=(-1)^{n-1}\\frac{(n-1)!}{(1+x)^n}\\\\\n\\end{aligned}\n\\]\n\n\\(f(x)=\\sin(x)\\)\n\n\\[\n\\begin{aligned}\n    f'(x)&=\\cos(x)=\\sin(x+1\\cdot\\frac{\\pi}{2})\\\\\n    f^{2}(x)&=(-1)\\sin(x)=\\sin(x+2\\cdot\\frac{\\pi}{2})\\\\\n    f^{3}(x)&=-\\cos(x)=\\sin(x+3\\cdot\\frac{\\pi}{2})\\\\\n    f^{4}(x)&=(-1)^2\\sin(x)=\\sin(x+4\\cdot\\frac{\\pi}{2})\\\\\n    &\\vdots\\\\\n    f^{n}(x)&=\\sin(x+n\\cdot\\frac{\\pi}{2})\n\\end{aligned}\n\\]\n\n\n\n\nTheorem 1 If A function \\(f(x)\\) is differentiable on \\(\\[a,b\\]\\), then there exists \\(c \\in (a,b)\\) such that \\[\n\\begin{aligned}\n    \\frac{f(b)-f(a)}{b-a} = f'(c), (a<c<b)\n\\end{aligned}\n\\]\n\n\n\n\nBy 4C - 자작, based on PNG version, CC BY-SA 3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/epsilon_delta/index.html",
    "href": "docs/blog/posts/Mathmatics/epsilon_delta/index.html",
    "title": "\\(\\epsilon - \\delta\\) Method",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 Let \\(f\\) be a function defined on some open interval that contains the number \\(a\\), exccept possibly at \\(a\\) itself. Then, \\(f\\) is said to converge to the real number \\(L\\) provided that for every number \\(\\epsilon>0\\), there is a number \\(\\delta>0\\) such that \\[\n\\text{if } 0<|x-a|<\\delta \\text{ then } |f(x)-L|<\\epsilon\n\\]\nIf \\(f(x)\\) converges to \\(L\\), then \\(L\\) is called the limit of \\(f(x)\\) as \\(x\\) approaches \\(a\\) , and we write \\[\n\\lim_{x \\to a} f(x) =L\n\\]\nIf a function does not coverge to a real number, it is said to diverge.\n\n먼저, 위의 정의를 하나씩 곱 씹어보면,\n\n\\(\\text{if } 0<|x-a|<\\delta \\text{ then } |f(x)-L|<\\epsilon\\) 에서 \\(|x-a|\\) 와 \\(|f(x)-L|\\) 가 절대값으로 표기가 되어 있기 때문에 거리(distance)로 해석이 된다.\n또한, \\(|x-a|\\) 는 임의의 충분히 작은 수 \\(\\delta\\) 와 대응이 되고 \\(|f(x)-L|\\) 는 임의의 충분히 작은 수 \\(\\epsilon\\) 와 대응이 되는 것을 숙지해야한다.\n그리고 \\(\\epsilon - \\delta\\) Method 라는 표현에서도 이해에 대한 실마리를 얻을 수 있는데 \\(\\epsilon\\) 이 먼저 정해지면 \\(\\delta\\) 를 그 후에 결정할 수 있다. 좀 더 자세히 말하면, 궁극적으로 \\(\\delta\\) 를 \\(\\epsilon\\) 의 함수 \\(\\delta(\\epsilon)\\) 로 표현하여 함수의 수렴성을 증명하게 된다.\n\n그럼 본격적으로 limit의 notation, \\(\\lim_{x \\to a} f(x) =L\\) 을 해석하면,\n\n고등학교 때 우리는 이 표현을 \\(x\\) 가 \\(a\\) 로 한없이 가까워질 때 \\(f(x)\\) 의 limit은 \\(L\\) 이다 라고 배웠다.\n이를 조금 더 정밀하게 해석하면, \\(x\\) 와 \\(a\\) 사이의 거리가 만족할 만큼 충분히 작아질 때, (하지만 \\(x\\ne a\\)), \\(f(x)\\) 와 \\(L\\) 사이의 거리가 임의대로 작아 질수 있다는 것을 의미한다.\n\n하지만 두 번째 표현 역시 수학적이지 않다. 왜냐하면 위의 문장은 만족할 만큼 충분히 작아질 때 라는 표현 때문에 명제(statement)가 될 수 없기 때문이다. 사람의 주관마다 어떤 사람은 거리가 1 일 때 충분히 작다고 말 할 수 있고 좀 더 정밀한 사람의 경우 0.0001 이 충분히 작다고 말할 수 있기 때문이다. 또 어떤 사람은 100이 충분히 작은 거리라고 표현할 수 있는 모호성이 존재한다.\n여기서, \\(\\lim_{x \\to a} f(x) =L\\) 의 해석을 분석해보고 수식화 시켜보자.\n‘\\(x\\) 와 \\(a\\) 사이의 거리가 만족할 만큼 충분히 작아질 때, (하지만 \\(x\\ne a\\)), \\(f(x)\\) 와 \\(L\\) 사이의 거리가 임의대로 작아 질수 있다.’ 는\n\n조건절: ’\\(x\\) 와 \\(a\\) 사이의 거리가 만족할 만큼 충분히 작아질 때, (하지만 \\(x\\ne a\\)),\n결과절: \\(f(x)\\) 와 \\(L\\) 사이의 거리가 임의대로 작아 질수 있다.’\n\n와 같이 조건절과 결과절로 나눌 수 있다.\n이를 수식으로 표현하면, 거리의 의미는 수학에서 절대값으로 표현될 수 있기 때문에\n\n조건절: \\(|x-a|\\) 가 만족할 만큼 충분히 작아질 때, (하지만 \\(x\\ne a\\)),\n결과절: \\(|f(x)-L|\\) 가 임의대로 작아 질수 있다.’\n\n와 같이 표현 될 수 있다.\n위의 정의 Definition 1 의 \\(\\text{if } 0<|x-a|<\\delta \\text{ then } |f(x)-L|<\\epsilon\\) 를 유심히 보면 애매한 표현을 수학적으로 표현하기 위해 누구나 만족할만한 충분히 작은 수 를 임의의 양수 (every number \\(\\epsilon>0\\) or any number \\(\\epsilon>0\\))라고 표현하여 명제화 시키는 것을 볼 수 있다.\n부등식으로 표현된 명제, \\(\\text{if } 0<|x-a|<\\delta \\text{ then } |f(x)-L|<\\epsilon\\) 를 대수적으로 변형시켜 분석해보자.\n\n조건절: \\(\\text{if } 0<|x-a|<\\delta\\)\n\n\\[\n\\begin{aligned}\n    |&x-a|<\\delta \\\\\n    -\\delta<&x-a<\\delta \\text{  }(\\because \\delta>0)\\\\\n    a-\\delta<&x<a+\\delta\n\\end{aligned}\n\\tag{1}\\]\n\\[\n\\begin{aligned}\n    0<|&x-a|\\text{  }(\\because x \\ne a)\n\\end{aligned}\n\\tag{2}\\]\n\n결과절: \\(\\text{ then } |f(x)-L|<\\epsilon\\) \\[\n\\begin{aligned}\n  |&f(x)-L|<\\epsilon \\\\\n  -\\epsilon<&f(x)-L<\\epsilon \\text{  }(\\because \\epsilon>0)\\\\\n  L-\\epsilon<&f(x)<L+\\epsilon\\\\\n\\end{aligned}\n\\tag{3}\\]\n\n위의 간단한 부등식 조작으로 3가지 사실을 재정리했다.\n\n\\(x\\) 의 범위 (see Equation 1) : \\(a-\\delta<x<a+\\delta\\)\n\\(|x-a|\\) 의 범위 (see Equation 2): \\(0<|x-a|\\)\n\\(f(x)\\) 의 범위 (see Equation 3): \\(L-\\epsilon<f(x)<L+\\epsilon\\\\\\)\n\n이 3가지 사실을 기반으로 limit의 정의 (Definition 1)를 다시 해석해보면,\n\\(lim_{x\\to a}f(x)=L\\) 은 모든 임의의 양수 \\(\\epsilon>0\\) 에 대해서,\\(x \\in (a-\\delta,a-\\delta)\\) (i.e. \\(x\\) 가 \\((a-\\delta,a-\\delta)\\) 범위안에 있다) 이고 \\(x\\ne a\\) 라면 \\(f(x) \\in (L-\\epsilon,L+\\epsilon)\\) 을 만족시키는 임의의 양수 \\(\\delta>0\\) 가 존재한다\n라고하는 좀 더 쉬운 해석이 가능해진다. 다른 방식으로 표현하면,\n\\[\n\\begin{aligned}\n\\text{If } f(x) \\in (L-\\epsilon,L+\\epsilon), &\\text{then } \\exists \\text{ } x \\in (a-\\delta,a+\\delta)  \\ni \\\\\nf:(a-\\delta,a+\\delta) &\\rightarrow (L-\\epsilon,L+\\epsilon)\n\\end{aligned}\n\\]\n여기서, \\(f(x)\\in (L-\\epsilon,L+\\epsilon)\\) 은 \\(f(x)\\) 를 \\(L\\) 의 근방 \\((\\text{i.e. } L-\\text{neighborhood})\\) 으로 한정시켰다고 표현한다. 같은 방식으로, \\(x \\in (a-\\delta,a+\\delta)\\) 은 \\(x\\) 를 \\(a\\) 의 근방 \\((\\text{i.e. } a-\\text{neighborhood})\\) 한정시켰다고 표현한다. 여기서, neighborhood는 \\(\\epsilon\\) 과 \\(\\delta\\) 가 정해져야 결정될 수 있는 것을 볼 수 있다. 그리고 if 조건문에 의해 \\(\\epsilon\\) 에 의해 \\(\\delta\\) 가 정해진다는 것을 미루어 짐작할 수 있다.\n이를 또 다르게 해석할 수 있는데,\n\\[\n\\begin{aligned}\n\\text{If } \\lim_{x \\to a}f(x)=L, &\\text{ then } \\exists \\delta > 0 \\ni\\\\\n\\text{if } x \\in (a-\\delta,a+\\delta), &\\text{ then } f(x) \\in (L-\\epsilon,L+\\epsilon)\n\\end{aligned}\n\\]\n위의 표현을 해석해보면, \\(x\\) 가 \\(a\\) 로 한없이 다가가서 \\(L\\) 에 수렴한다면, \\(x\\) 를 \\(a\\) 근방에 한정시켜 \\(f(x)\\) 가 \\(L\\) 근방에 한정되는 임의의 양수 \\(\\delta\\) 가 존재한다 라고 해석할 수 있다.\n\n\n\nfind \\(\\delta\\) corresponding to \\(\\epsilon=0.5\\) in the definition of a limit for \\(f(x)=x+5\\) with \\(a=1\\) and \\(L=6\\). \\[\n\\text{if } |x-1|<\\delta \\text{ then} |(x+5)-6|<0.5\n\\]\n\nSolution)\n\\[\n\\begin{aligned}\n    -0.5<&(x+5)-6<0.5 \\\\\n    5.5 <&x+5<6.5 \\\\\n    0.5 <&x<1.5 \\\\\n\\text{If } 0.5 <x<1.5, &\\text{ then } 5.5 <x+5<6.5 \\\\\n\\therefore \\text{If }|x-1|<0.5, &\\text{ then } |(x+5)-6|<0.5 (\\because (0.5.1.5) \\text{ is symmetric about }x=1)\n\\end{aligned}\n\\]\nIf the interval of \\(x\\) is not symmetric about x=a, the smaller number is chosed as \\(\\delta\\). 만약 \\(x\\) 의 구간이 \\(a\\) 를 기준으로 대칭이 아니라면 더 짧은 근방을 \\(\\delta\\) 로 설정한다.\n\\(\\text{If }|x-1|<0.5, \\text{ then } |(x+5)-6|<0.5\\) 을 해석하면\n\\(a=1\\) 을 중심으로 \\(0.5(=\\delta)\\) 근방의 \\(x\\) 를 설정하면, \\(L=6\\) 을 중심으로 한 \\(0.5 (=\\epsilon)\\) 근방의 \\(f(x)\\) 를 얻을 수 있다.\n\nProve that \\(\\lim_{x \\to 3} (4x-5)=7\\)\n\nProof)\nThe 1st step is to find \\(\\delta\\) : \\[\n\\begin{aligned}\n    \\text{If } |x-3|<\\delta, &\\text{ then } |4x-5|-7<\\epsilon \\\\\n    |4x-5|-7&=|4x-12|=4|x-3| \\\\\n    4|x-3|<\\epsilon \\\\\n    \\text{If } |x-3|<\\delta, &\\text{ then } 4|x-3|<\\epsilon \\\\\n    \\text{If } |x-3|<\\delta, &\\text{ then } |x-3|<\\frac{\\epsilon}{4} \\\\\n    \\therefore \\delta = \\frac{\\epsilon}{4}\n\\end{aligned}\n\\]\nThe 2nd step is to prove that the \\(\\delta\\) works:\nGiven \\(\\epsilon>0\\), choose \\(\\delta=\\frac{\\epsilon}{4}\\) \\[\n\\begin{aligned}\n    \\text{If } 0<|x-3|<\\delta, &\\text{ then } 4|x-3|<4\\delta=4\\frac{\\epsilon}{4}=\\epsilon \\\\\n    \\text{Thus, } \\text{If } 0<|x-3|<\\delta, &\\text{ then } |(4x-5)-7|<\\epsilon \\\\\n    \\therefore \\lim_{x \\to 3}(4x-5)=7\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nreference : james steward, Calculus"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/function/composite_function.html",
    "href": "docs/blog/posts/Mathmatics/function/composite_function.html",
    "title": "Composite Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nDefinition 1 Given two functions f and g, the composite function \\(f\\circ g\\) (also called the composition of \\(f\\) and \\(g\\)) is defined by\n\\[\n(f\\circ g)(x)=f(g(x))\n\\tag{1}\\]\n\nThe domain of \\(f\\circ g\\) is the set of all \\(x\\) in the domain of \\(g\\) such that \\(g(x)\\) is in the domain of \\(f\\). In other words, \\((f\\circ g)(x)\\) is defined whenever both \\(g(x)\\) and \\(f(g(x))\\) are defined.\n\n\nIf \\(f(x)=x^2\\) and \\(g(x)=e^x\\), find \\(f\\circ g\\) and \\(g\\circ f\\).\n\\[\n\\begin{aligned}\n(f\\circ g)(x)&=f(g(x))=f(e^x)=(e^x)^2=e^{2x}\\\\\n(g\\circ f)(x)&=g(f(x))=g(x^2)=e^{x^2}=e^{x^2}\\\\\n\\end{aligned}\n\\]\n위의 예제처럼, \\(f\\circ g \\ne f\\circ g\\).\n\n\n\n\n\n\n\nDefinition 2 Given two functions f and g, the composite function \\(f\\circ g\\) (also called the composition of \\(f\\) and \\(g\\)) is defined by\n\\[\n(f\\circ g)(x)=f(g(x))\n\\tag{2}\\]\n\nThe domain of \\(f\\circ g\\) is the set of all \\(x\\) in the domain of \\(g\\) such that \\(g(x)\\) is in the domain of \\(f\\). In other words, \\((f\\circ g)(x)\\) is defined whenever both \\(g(x)\\) and \\(f(g(x))\\) are defined.\n\n\nIf \\(f(x)=x^2\\) and \\(g(x)=e^x\\), find \\(f\\circ g\\) and \\(g\\circ f\\).\n\\[\n\\begin{aligned}\n(f\\circ g)(x)&=f(g(x))=f(e^x)=(e^x)^2=e^{2x}\\\\\n(g\\circ f)(x)&=g(f(x))=g(x^2)=e^{x^2}=e^{x^2}\\\\\n\\end{aligned}\n\\]\nLike the example above, \\(f\\circ g \\ne f\\circ g\\).\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/function/index.html",
    "href": "docs/blog/posts/Mathmatics/function/index.html",
    "title": "Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\\[\ny=f(x;\\alpha)\n\\]\n\n독립변수 (independent variable): \\(x\\), 함수의 출력값을 결정하는 변수로서, feature, decision variable 등으로도 표현\n매개변수 (parameter): \\(\\alpha\\), 함수의 모양을 결정하기 위한 변수\n종속 변수 (dependent variable): \\(y\\) or \\(f(x)\\) 독립변수와 매개 변수에 의해 값이 결정되는 변수\n\n\n\n\n\n\n\nConfusing Terms about Rectangular Data\n\n\n\n독립 변수와 종속 변수는 각 분야에서 쓰이는 이름이 다르다. 아래 표 참고 (정확하진 않음. 아직도 업데이트 중) See Table 1.\n(The below table may be incorrect still under research)\n\n\nTable 1: Confusing Terms in Data Science\n\n\n\n\n\n\n\n\n\nTerms\nMathmatics\nStatistics\nComputer Science\nData Engineering\n\n\n\n\ngraph\nvisulaized plot\nvisulaized plot\nconnections among entities\nconnections among entities, data structure\n\n\nvariable\nindependent variable\npredictor, experimental variable, explanatory variable\nfeature, input\nattribute, column\n\n\noutcome\ndependent variable\nresponse variable, outcome\ntarget, output\nattribute, column\n\n\nRecords\npattern, example\ncase, sample, observation\ninstance, record, row\ninstance, record, row\n\n\n\n\n\n\n\n\n\n\n함수, 분수함수, 지수함수, 로그함수, 삼각함수 등\n\n\n\n\n\n2차 함수\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef f(x,a=1,b=2,c=3): #x: independent variable, f(x): dependent variable\n    return a*x**2+b*x+c # a,b,c : parameters\nx=np.linspace(-10,10,1000)\nfig=plt.figure()\nax=plt.axes()\nax.plot(x,f(x))\nax.plot(x,f(x,2,-3,2))\nplt.show()\n\n\n\n\n\n\n\\(f(x)=2x^2-3x+2\\)\n\n함수: \\(f(x;a,b,c)\\)\n종속 변수: \\(f(x)\\)\n독립 변수: \\(x\\)\n매개 변수 (parameter): \\(a=2, \\space b=-3, \\space c=2\\)\n\n예시2\n\nlinear regression\n\n매개변를 수학적으로 최적화하여 데이터에 맞는 직선의 방정식을 찾는 알고리즘\n함수 : \\(f(x;\\mathbf \\beta)=\\beta_1x+\\beta_0\\)\n종속 변수: \\(f(x)=y\\)\n독립 변수: \\(x\\)\n매개 변수 (parameter): \\(\\mathbf \\beta = (\\beta_0, \\space \\beta_1)\\)\n\n\n\n\\[\n\\begin{aligned}\nL(oss)&=\\frac{1}{2}\\sum_{n=1}^{N}(y_n-f(x_n,\\mathbf \\beta))^2 \\\\\n&=\\frac{1}{2}\\sum_{n=1}^{N}(y_n-\\beta_1x_n-\\beta_0)^2\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/function/multivariable_scalar_function.html",
    "href": "docs/blog/posts/Mathmatics/function/multivariable_scalar_function.html",
    "title": "Function - Multivariable Scalar Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom mpl_toolkits import mplot3d\nimport matplotlib.font_manager as mfm\n\n\n\n\nA multivariable scalar function is a mathematical function that takes multiple input variables and outputs a single scalar value. More formally, let \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) be a function that maps \\(n\\) real-valued input variables \\(\\mathbf{x}=(x_1,x_2,\\ldots,x_n)\\) to a single real-valued output \\(y=f(\\mathbf{x})\\). \\[\ny=f(\\mathbf x), \\space f:\\mathbb R^n \\rightarrow \\mathbb R\n\\]\n\n\nLet \\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\) be a function that maps a pair of real-valued input variables \\((x,y)\\) to a single real-valued output \\(z=f(x,y)\\).\nFor example,\n\n\\(f(\\mathbf x)=f(x_1,x_2)=x_1^2+x_2^2\\), which can also be represented as \\(f(\\mathbf x)=||x||^2=x_1^2+x_2^2\\). When \\(g(\\mathbf x)=g(x_1,x_2)=\\sqrt{x_1^2+x_2^2}\\), it is the trace of points equidistant from the origin on a plane.\nBivariable function with a quadratic form\n\n\\[\nf(x_1,x_2)=2x_1^2+3x_1x_2+4x_2^2\n\\]\nIn this example, the function \\(f\\) takes two input variables \\(x_1\\) and \\(x_2\\), and outputs a single scalar value. The function has a quadratic form, meaning that it has terms that involve products of the input variables, with the highest order of these terms being 2. The function can also be written in matrix form as: \\[\nf(\\mathbf x) = \\begin{bmatrix}x_1 & x_2\\end{bmatrix}\\begin{bmatrix}2 & \\frac{3}{2}\\\\\\frac{3}{2} & 4\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}\n\\]\nHere, \\(\\mathbf{x}=(x_1,x_2)\\) is a column vector of the input variables, and the matrix \\(\\begin{bmatrix} 2 & \\frac{3}{2} \\\\ \\frac{3}{2} & 4 \\end{bmatrix}\\) encodes the quadratic form of the function.\nIf the quadratic form is constrained by \\(y=mx\\),\n\\[\nf(x,mx)=2x^2+3mx^2+4m^2x^2=(2+3m+4m^2)x^2\n\\]\nIn this example, the function \\(f\\) takes two input variables \\(x\\) and \\(mx\\), constrained by the relationship \\(y=mx\\), and outputs a single scalar value. The function has a quadratic form, and is constrained to lie on a line with slope \\(m\\). Note that we can rewrite the function as:\n\\[\nf(x,mx)=(2+3m+4m^2)x^2=a_mx^2\n\\]\nwhere \\(a_m = 2+3m+4m^2\\) is a quadratic coefficient that depends on the slope \\(m\\). This shows that the function \\(f\\) has a family of quadratic forms, parameterized by the slope \\(m\\) of the line it is constrained to lie on.\n\n\nCode\n# Define the function\ndef f(x, m):\n    a = 2 + 3*m + 4*m**2\n    return a * x**2\n\n# Define the domain\nx = np.linspace(-5, 5, 100)\n\n# Define the slopes to visualize\nslopes = [0.5, 1, 2]\n\n# Plot the functions for each slope\nfor m in slopes:\n    y = m*x\n    z = f(x, m)\n    plt.plot(x, y, label=f\"y={m}x\")\n    plt.plot(x, z, label=f\"f(x,y)={2+3*m+4*m**2:.2f}x^2\")\n\n# Set the plot properties\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\\(y=f(x_1,x_2)=50(x_2 - x_1^2)^2 + (2-x_1)^2, \\space f:\\mathbb R^2\\rightarrow \\mathbb R\\). The \\(y\\) value takes the shape of a curved surface.\n\n\n\nCode\nfig = plt.figure(figsize=(7, 7))\nax = plt.axes(projection='3d')\nx1=np.linspace(0,1,100)\nx2=np.linspace(0,1,100)\nx1,x2 = np.meshgrid(x1,x2) # multivariate scalar function 만들 때 사용\n\n#print(x2)  #x 축으로 값이 불변하고 y축으로는 값이 변함\n\ny = 50*(x2 - x1**2)**2 + (2-x1)**2\n#print(z)\n\nax.scatter3D(x1, x2, y, marker='.', color='gray')\nplt.show()\n\n\n\n\n\n\n\n\nA trivariable scalar function is a mathematical function that takes three input variables and outputs a single scalar value.\n\\[\nf: \\mathbb{R}^3 \\to \\mathbb{R}\n\\] It is a function that maps a triple of real-valued input variables \\((x,y,z)\\) to a single real-valued output \\(w=f(x,y,z)\\).\nFor example,\n\nthe function \\(f(x,y,z) = x^2 + y^2 + z^2\\) is a trivariable scalar function that takes three input variables \\(x\\), \\(y\\), and \\(z\\), and outputs a single scalar value, which is the sum of their squares.\n\n\n\nCode\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define the function\ndef f(x, y, z):\n    return x**2 + y**2 + z**2\n\n# Define the domain\nx = np.linspace(-5, 5, 50)\ny = np.linspace(-5, 5, 50)\nX, Y = np.meshgrid(x, y)\nZ = f(X, Y, 0)\n\n# Plot the level sets\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.set_title(r'Level sets of $f(x,y,z) = x^2 + y^2 + z^2$')\nfor level in np.arange(0, 26, 5):\n    ax.contour(X, Y, Z, [level], zdir='z', offset=level, cmap='Accent')\nplt.show()\n\n\n\n\n\n\nThe function \\(f(x,y,z)=2x^2+3y^2+4z^2+5xy+6xz+7yz+8x+9y+10z+11\\), which is a trivariable function with a quadratic form that we’ve been discussing can be written in matrix form as:\n\n\\[\nf(\\mathbf x) = \\begin{bmatrix}x & y & z\\end{bmatrix}\\begin{bmatrix}2 &2.5& 3\\\\2.5&3&3.5\\\\3&3.5&4\\end{bmatrix}\\begin{bmatrix}x\\\\ y \\\\ z\\end{bmatrix} + \\begin{bmatrix}4 & 4.5 & 5\\end{bmatrix}\\begin{bmatrix}x\\\\ y \\\\ z\\end{bmatrix}+11\n\\]\nHere, \\(\\mathbf{x} = [x, y, z]^T\\) is a column vector of the input variables, and the matrix \\(A\\) is the symmetric matrix of quadratic coefficients:\n\\[\n\\mathbf A = \\begin{bmatrix}2 &2.5& 3\\\\2.5&3&3.5\\\\3&3.5&4\\end{bmatrix}\n\\]\nThe linear coefficients are represented by the column vector \\(\\mathbf{b} = [4, 4.5, 5]^T\\), and the constant term is \\(11\\).\n\n\n\n\n\nContour(등고선) Graph: 한쪽 변이 상수로 고정\n\\(f(x,y)=c\\) (\\(x\\) 와 \\(y\\)의 관계가 가려져 있어서 음함수)\nbivariable scalar function \\(f(x_1,x_2)=c\\)\n\n\n\nCode\nfig = plt.figure(figsize=(7, 7))\nax = fig.add_subplot(1, 1, 1)\nax.xaxis.set_tick_params(labelsize=18)\nax.yaxis.set_tick_params(labelsize=18)\n\nx1 = np.linspace(-2, 2, 51)\nx2 = np.linspace(-1, 3, 51)\nX1, X2 = np.meshgrid(x1, x2)\nZ = 50*(X2 - X1**2)**2 + (2-X1)**2\n\ncontours = ax.contour(X1, X2, Z, levels=[30, 200, 600],  colors='k', \n            linestyles=['solid','dashed', 'dotted'])\n\nax.clabel(contours, contours.levels, fmt=\"%d\", inline=True, fontsize=20)\n\nax.set_xlabel(r'$x_1$', fontsize=25)\nax.set_ylabel(r'$x_2$', fontsize=25)\n\n#if file_print == True :\n#    fig.savefig(\"imgs/chap3/fig3-6.png\", dpi=300, bbox_inches='tight')\n#    fig.savefig(\"imgs/chap3/fig3-6.pdf\", format='pdf', bbox_inches='tight')\n#    \nplt.show()\n\n\n\n\n\n\n\nCode\npaths200=contours.collections[0].get_paths()\npaths600=contours.collections[1].get_paths()\n\n\nfig = plt.figure(figsize=(7, 7))\n\nx1 = np.linspace(-2, 2, 51)\nx2 = np.linspace(-1, 3, 51)\nX1, X2 = np.meshgrid(x1, x2)\nZ = 50*(X2 - X1**2)**2 + (2-X1)**2\n\nax = plt.axes(projection='3d')\nax.xaxis.set_tick_params(labelsize=15)\nax.yaxis.set_tick_params(labelsize=15)\nax.zaxis.set_tick_params(labelsize=15)\n\nax.plot_surface(X1, X2, Z, cmap=plt.cm.binary, edgecolor=\"k\")\n#ax.plot_wireframe(X1, X2, Z, cmap=plt.cm.OrRd, edgecolor=\"k\")\n\nax.plot3D(paths200[0].vertices[:,0], paths200[0].vertices[:,1], [200]*paths200[0].vertices.shape[0], \n          lw=2, color='k', linestyle='--')\nax.plot3D(paths200[1].vertices[:,0], paths200[1].vertices[:,1], [200]*paths200[1].vertices.shape[0], \n          lw=2, color='k', linestyle='--')\nax.plot3D(paths200[2].vertices[:,0], paths200[2].vertices[:,1], [200]*paths200[2].vertices.shape[0], \n          lw=2, color='k', linestyle='--')\n\nax.plot3D(paths600[0].vertices[:,0], paths600[0].vertices[:,1], [600]*paths600[0].vertices.shape[0], \n          lw=2, color='w', linestyle='-')\nax.plot3D(paths600[1].vertices[:,0], paths600[1].vertices[:,1], [600]*paths600[1].vertices.shape[0], \n          lw=2, color='w', linestyle='-')\n\nax.text(1.5, -1,  400, r\"$f(x_1,x_2)=200$\", color='k', fontsize=18)\nax.text(1., -1, 800, r\"$f(x_1,x_2)=600$\", color='k', fontsize=18)\n\nax.set_xlabel(r'$x_1$', fontsize=20)\nax.set_ylabel(r'$x_2$', fontsize=20)\nax.set_zlabel(r'$z$', fontsize=20)\n\nax.view_init(50, 80)\n\n#if file_print == True :\n#    fig.savefig(\"imgs/chap3/fig3-5.png\", dpi=300, bbox_inches='tight')\n#    fig.savefig(\"imgs/chap3/fig3-5.pdf\", format='pdf', bbox_inches='tight')\n#    \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/function/mutivariable_vector_function.html",
    "href": "docs/blog/posts/Mathmatics/function/mutivariable_vector_function.html",
    "title": "Function - Multivariable Vector Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbf{y} & =\\mathbf F(\\mathbf{x}),\\space f:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{n}\\\\\n\\mathbf{F}(\\mathbf{x}) & =(f_{1}(\\mathbf{x}),f_{2}(\\mathbf{x}),...,f_{n}(\\mathbf{x}))^{T},\\mathbf{x}\\in\\mathbb{R}^{m}\n\\end{align*}\n\\]\n\n입력: vector\n출력: vector\n예시\n\n\\(\\mathbf s(u,v)=(u,v,1+u^2+\\frac{v}{1+v^2})^T, 0\\leq u,v\\leq 1\\) \\[\\begin{aligned}\n\\mathbb R^2 &\\rightarrow \\mathbb R^3 \\\\\n(u,v) &\\rightarrow (x,y,z)\n\\end{aligned}\n\\] where \\(x=u, y=1+u^2, z=1+u^2+\\frac{v}{1+v^2}\\)\n\ngraph 표현\n\n\n\n\n\n\n계산그래프\n\n\n\n\n\n\n인공신경망\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/function/univariable_scalar_function.html",
    "href": "docs/blog/posts/Mathmatics/function/univariable_scalar_function.html",
    "title": "Function - Univariable Scalar Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\\[\ny=f(x), \\space f: \\mathbb R \\rightarrow \\mathbb R\n\\]\n\nOne to One\n\\(f:\\mathbb{R} \\rightarrow \\mathbb{R}\\) or \\(y=f(x)\\) 으로 표현\n\n\\(f:\\mathbb{R} \\rightarrow \\mathbb{R}\\) : 입력과 출력의 개수를 직관적으로 알 수 있음\n예시: \\(f(x)=x^2, f(x)=2^x, f(x)=log_3x\\)\n\n\n\n\n\nPolynomials\n\nmonomial: one term, 상수 또는 변수 항 하나로 이루어진 식\n\nex) \\(2,x,x^2,\\frac{x}{3}\\)\n\nbinomial: two terms, 단항식이 덧셈과 뻴셈으로 연결된 식\n\nex) \\(3x^2+2\\), coefficient=3. \\(x\\)= 변수, degree = 2, constant = 2\n\ntrinomial: three terms\nquadrinomial: four terms\nquintinomial: five terms\nmultinomial: more than one terms, a combination of nomials with +/-.\n\nex) \\(x^2+2x+3\\)\n\n\nMultinomial Function\n\n다항식으로 구성된 함수\nex) \\(f(x)=x^2+2x+4\\)\n연속이고 미분 가능\n\nExponential Function\n\n정의 : \\(y=a^x\\) where \\(z>0, \\space a\\not=1\\)\n특징\n\n\\(a>1\\): 양의 방향으로 증가\n\\(0<a<1\\): 양의 방향으로 갈 수록 감소\n지수법칙\n입력되는 x 값에 비해 출력되는 y값이 급격히 변화\n미분 가능 & 연속\nExponent Rule\n\n\nLog Function\n\n정의 : \\(log_ax=c \\leftrightarrow a^c=x\\)\n특징\n\n미분 가능 & 연속\n지수함수와 역함수 관계\n입력되는 x의 변화량에 비해 출력되는 y의 변화량이 작음\n\n\\(y=log_{10}x\\) : x가 10만큼 증가해야 y가 1 증가함\n\\(y=log_{e}x\\) : x가 e(약 2.718)만큼 증가해야 y가 1 증가함\n\n\n\nLogistic Sigmoid Function\n\n정의: \\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\)\n특징\n\n미분 가능 & 연속\n신경망에서 뉴런의 활성을 결정하는 활성함수로 사용\n모든 실수\\(z\\)를 \\(0~1\\) 사이로 변환시킴\n입력값을 확률 값으로 출력하기 위해 사용됨\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/function/univariable_vector_function.html",
    "href": "docs/blog/posts/Mathmatics/function/univariable_vector_function.html",
    "title": "Function - Univariable Vector Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\\[\nr(t)=(f_1(t),f_2(t),...,f_n(t))^T, f:\\mathbb R\\rightarrow \\mathbb R^n\n\\]\n\none to many\n평면 또는 공간에 존재하는 곡선: 시간에 따른 물체의 이동 경로\n입력: 스칼라\n출력: vector\n예시: \\(t\\rightarrow(x,y,z)\\)\n\n\\[\nr(t)=f(x(t),y(y),z(t))=(\\cos(-10t),\\frac{3}{4}t,\\frac{t^2}{6})^T\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom mpl_toolkits import mplot3d\nimport matplotlib.font_manager as mfm\n\n# numpy 출력 형식 지정\nnp.set_printoptions(precision=4, linewidth=150)\n\n# matplotlib 스타일 지정\nmpl.style.use('bmh')\nmpl.style.use('seaborn-whitegrid')\nstyle = plt.style.library['bmh']\n# 스타일 컬러를 쉽게 쓸 수 있도록 리스트 저장\nstyle_colors = [ c['color'] for c in style['axes.prop_cycle'] ]\n\n# 그림을 로컬 폴더에 저장하고 싶으면 True로 수정 \nfile_print = False\n\n\n\n\nCode\nfig = plt.figure(figsize=(7, 7))\nax = plt.axes(projection='3d')\nax.xaxis.set_tick_params(labelsize=15)\nax.yaxis.set_tick_params(labelsize=15)\nax.zaxis.set_tick_params(labelsize=15)\nax.set_xlabel('$x$', fontsize=20)\nax.set_ylabel('$y$', fontsize=20)\nax.set_zlabel('$z$', fontsize=20)\n\nt = np.linspace(0, 2, 101)\nx = np.sin(6*t)\ny = 1/4 * t\nz = t**2 / 2\n\nax.plot3D(x, y, z, c='k')\nax.plot([x[0]],  [y[0]],  [z[0]],  'o', markersize=10, color='k',\n        label=\"t = {:.2f}\".format(t[0]))\nax.plot([x[50]], [y[50]], [z[50]], '^', markersize=10, color='k',\n        label=\"t = {:.2f}\".format(t[50]))\nax.plot([x[-1]], [y[-1]], [z[-1]], '*', markersize=10, color='k',\n        label=\"t = {:.2f}\".format(t[-1])) \n\nax.legend(fontsize=15, loc=\"upper left\")\n    \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/guide_map/index.html",
    "href": "docs/blog/posts/Mathmatics/guide_map/index.html",
    "title": "Content List, Mathematics",
    "section": "",
    "text": "2023-03-24, Variable types\n1111-11-11, Function\n\n2023-01-31, Function (1) - Univariable Scalar Function (One to One)\n2023-01-31, Function (2) - Multi-variable Scalar Function (Many to One)\n2023-01-31, Function (3) - Univariable Vector Function (One to Many)\n2023-01-31, Function (4) - Multi-variable Vector Function (Many to Many)\n2023-02-18, Function (5) - Composite Function\n\n2023-02-18, Transformations of Functions\n1111-11-11, Vector & Matrix\n2023-03-15, Limit, \\(\\epsilon-\\delta\\) Method\nDifferentiation\n\n2023-02-04, Derivative (1) - Univariable Scalar Funtion\n1111-11-11, Derivative (2) - Chain Rule & Partial Derivative\n1111-11-11, Derivative (3) - Higher Order Derivative\n1111-11-11, Derivative (4) - Mean Value Theorem\n1111-11-11, Derivative (5) - Gradient\n\n2023-03-15, Talyer’s Series\n1111-11-11, Gradient Direction\n1111-11-11, Random Variable\n1111-11-11, Probability Distribution\n1111-11-11, Information Theory - Entropy\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n\n\n\n\n\n2023-03-30, Basics (1) - Vector Operations\n2023-03-30, Basics (2) - Matrix Operations\n2023-03-30, Basics (3) - Special Matrix\n2023-04-14, Lineqr Equations\n2023-04-14, Vector Space and Subspaces\n2023-04-14, [Orthogonality]\n1111-11-11, [Determinants]\n1111-11-11, [Eigen Value & Eigen Vector]\n1111-11-11, [Linear Transformations]\n1111-11-11, Basis, Dimension, & Rank\n1111-11-11,\n1111-11-11, Eigen Decomposition\n1111-11-11, Singular Value Decomposition (SVD)\n1111-11-11, Gram-Schmidt\n1111-11-11, Group\n1111-11-11, Rotation & Group\n2023-04-02, Matrix Transformation (5) - Quadratic Form\n2023-04-02, Matrix Calculus (1) - Quadratic Form\n1111-11-11,\n1111-11-11,\n1111-11-11,\n\n\n\n\n\n2023-03-23, Minimizer & Minimum\n1111-11-11, Convex Set\n1111-11-11, Convex Function\n1111-11-11, Unconstrained Optimization\n1111-11-11, Non-linear Least Square\n1111-11-11, Largrange Multiplier Method\n\n1111-11-11, Largrange Primal Function\n1111-11-11, Largrange Dual Function\n1111-11-11, KKT conditions\n\n1111-11-11, Gradient Descent Optimizers\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n\n\n\n\n\nStatistics\n\nGeorge Casella & Rogeer L. Berger - Statistcal Inference, 2nd Edition\n슬기로운 통계생활 - https://www.youtube.com/@statisticsplaybook\n슬기로운 통계생활 - https://github.com/statisticsplaybook\n다수의 Youtube, and Documents from Googling\n\nMathematics\n\nJames Stewart - Calculus Early Transcedentals, 7th Eidition\nany James Stewart series\n임장환 - 머신러닝, 인공지능, 컴퓨터 비전 전공자를 위한 최적화 이론\n다수의 Youtube, and Documents from Googling\n\nDeep Learning\n\n조준우 - 머신러닝·딥러닝에 필요한 기초 수학 with 파이썬\n조준우 - https://github.com/metamath1/noviceml\n동빈나 - https://www.youtube.com/c/dongbinna\n혁펜하임 - https://www.youtube.com/channel/UCcbPAIfCa4q0x7x8yFXmBag\n다수의 Youtube, and Documents from Googling"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/01.basic_vector.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/01.basic_vector.html",
    "title": "Basics (1) - Vector Operations",
    "section": "",
    "text": "Deep learning is a pile of neural networks that are made up of layers of interconnected nodes or neurons, and the weights of the connections between them are learned through a process called backpropagation.\nLinear algebra is fundamental to deep learning because many of the computations involved in training neural networks can be expressed as linear algebra operations. For example, matrix multiplication is used to compute the output of each layer in a neural network, and the gradients of the loss function with respect to the weights are computed using the chain rule of calculus, which involves matrix multiplication and vector operations.\nIn addition to matrix multiplication, other linear algebra concepts such as eigenvectors, eigenvalues, and singular value decomposition (SVD) are also important in deep learning. For example, SVD can be used to reduce the dimensionality of a dataset or to compute principal components, which are useful for data visualization and feature extraction.\nLinear algebra libraries such as Numpy, Scipy, and PyTorch provide efficient implementations of these operations, which are essential for training large-scale neural networks on GPUs. Without these libraries, implementing deep learning algorithms would be much more difficult and time-consuming.\nReference: Motivation to Learn Linear Algebra\n\n\nA scalar is a single mathematical quantity, usually a real number, which can be represented by a single value. Scalars are typically denoted by lowercase letters, such as \\(a, b, c,\\) and so on.\n\n\n\nA vector \\(\\textbf{v}\\) is a mathematical object that represents a quantity with both a magnitude and a direction. In \\(n\\)-dimensional Euclidean space \\(\\mathbb{R}^n\\), a vector \\(\\textbf{v}\\) is typically represented as an ordered list of \\(n\\) real numbers:\n\nthe magnitude of \\(\\mathbf{v} =\\begin{bmatrix} x\\\\ y \\end{bmatrix}\\) is \\(||\\mathbf{v}|| = \\sqrt{x^{2} + y^{2}}\\)\nthe direction of it is the angle with the x axis, \\(\\theta=\\tan^{-1}(\\frac{y}{x})\\)\nIf magnitude and vector are equal, then they are equal vectors\n\n\\[\n\\textbf{v}=\n\\begin{bmatrix}\n  v_1 \\\\\n  v_2 \\\\\n  \\vdots \\\\\n  v_n\n\\end{bmatrix}\n\\]\nwhere \\(v_1, v_2, \\ldots, v_n\\) are the components of the vector \\(\\textbf{v}\\).\n\n\nExample\nMap \\(\\begin{bmatrix} 3\\\\ 2 \\end{bmatrix}\\) into \\(x=3\\), \\(y=2\\) on the Coordinate Plane\n\n\n\n\n\nReference: Read This Article with Interactive Visualization - Points and Vectors\n\n\n\n\n\n\nThe addition of two vectors is the process of adding their corresponding components. If \\(\\textbf{a}\\) and \\(\\textbf{b}\\) are two vectors of the same dimension, then their sum \\(\\textbf{c} = \\textbf{a} + \\textbf{b}\\) is a vector whose \\(i\\)-th component is the sum of the \\(i\\)-th components of \\(\\textbf{a}\\) and \\(\\textbf{b}\\).\n\\[\n\\begin{align*}\n  \\textbf{c}&=\\textbf{a}+\\textbf{b}\\\\\n  c_i &= a_i + b_i\n\\end{align*}\n\\]\nFor example, if \\(\\textbf{a} = [3, 2]\\) and \\(\\textbf{b} = [-2,1]\\), then their sum \\(\\textbf{c} = [1,3]\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe subtraction of two vectors is the process of subtracting their corresponding components. If \\(\\textbf{a}\\) and \\(\\textbf{b}\\) are two vectors of the same dimension, then their difference \\(\\textbf{c} = \\textbf{a} - \\textbf{b}\\) is a vector whose \\(i\\)-th component is the difference between the \\(i\\)-th components of \\(\\textbf{a}\\) and \\(\\textbf{b}\\). The formal definition is:\n\\[\n\\begin{align*}\n  \\textbf{c}&=\\textbf{a} - \\textbf{b}\\\\\n  c_i &= a_i - b_i\n\\end{align*}\n\\]\nFor example, if \\(\\textbf{a} = [1, 2, 3]\\) and \\(\\textbf{b} = [4, 5, 6]\\), then their difference \\(\\textbf{c} = [-3, -3, -3]\\).\n\n\n\nThe scalar multiplication of a vector is the process of multiplying each component of the vector by a scalar. If \\(\\textbf{a}\\) is a vector and \\(k\\) is a scalar, then the scalar multiple \\(\\textbf{c} = k\\textbf{a}\\) is a vector whose \\(i\\)-th component is \\(k\\) times the \\(i\\)-th component of \\(\\textbf{a}\\). The formal definition is:\n\\[\n\\begin{align*}\n  \\textbf{c}&=k\\textbf{a}\\\\\n  c_i &= ka_i\n\\end{align*}\n\\]\nFor example, if \\(\\textbf{a} = [1, 2, 3]\\) and \\(k = 2\\), then their scalar multiple \\(\\textbf{c} = [2, 4, 6]\\).\nReference: Read This Article with Interactive Visualization - Properties of Vector Arithmetic\n\n\n\nThe dot product of two vectors is the sum of the products of their corresponding components (a.k.a dot product & scalar product). If \\(\\textbf{a}\\) and \\(\\textbf{b}\\) are two vectors of the same dimension, then their dot product \\(c = \\textbf{a} \\cdot \\textbf{b}\\) is a scalar given by the formula:\n\\[\n\\begin{align*}\n  c&=\\textbf{a}\\cdot \\textbf{b}\\\\\n  &= \\sum_{i=1}^{n}a_ib_i\n\\end{align*}\n\\]\n\nDot product can be used to measure the similarity between two vectors.\nFor the two vectors, \\(\\mathbf{a} = [a_1, a_2, \\cdots a_n]\\) , \\(\\mathbf{b} = [b_1, b_2, \\cdots b_n]\\), dot product can be defined as \\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{a}^{T} \\mathbf{b} = ||\\mathbf{a}||\\text{ } ||\\mathbf{b}|| \\cos \\theta\n\\]\nWhen two vectors are orthogonal, \\(\\cos 90^{\\circ} = 0\\), the similarity of the two vectors is 0.\nIn the Euclidean space, dot product is often called inner product (inner product is a generalization of dot product)\n\n\n\n\n\n\n\nInner Product vs Dot Product\n\n\n\nIn general, an inner product is a mathematical operation that takes two vectors and produces a scalar. It satisfies certain properties, such as being linear in the first argument, conjugate linear in the second argument, and positive-definite. In other words, an inner product is a bilinear form that satisfies the following properties for all vectors \\(\\mathbf{x}\\), \\(\\mathbf{y}\\), and \\(\\mathbf{z}\\), and all scalars \\(a\\) and \\(b\\):\n\n“Linear in the first argument” means that for any fixed vector \\(\\mathbf u\\), the function \\(f\\) defined by \\(f(\\mathbf v) = \\langle\\mathbf u, \\mathbf v\\rangle\\) is a linear function of \\(\\mathbf v\\), i.e., \\(f(a\\mathbf x + b\\mathbf y) = af(\\mathbf x) + bf(\\mathbf y)\\) for any scalars \\(a\\), \\(b\\), and vectors \\(\\mathbf{x}\\), \\(\\mathbf{y}\\).\n\n\\(\\langle a\\mathbf{x} + b\\mathbf{y}, \\mathbf{z}\\rangle = a\\langle\\mathbf{x}, \\mathbf{z}\\rangle + b\\langle\\mathbf{y}, \\mathbf{z}\\rangle\\), the inner product is linear with respect to the first argument. If we multiply a vector by a scalar and add it to another vector, the resulting inner product is the same as if we had calculated the inner product of each vector separately and then added them.\n\n“Conjugate linear in the second argument” means that for any fixed vector \\(\\mathbf v\\), the function \\(g\\) defined by \\(g(\\mathbf u) = \\langle\\mathbf u, \\mathbf v\\rangle\\) is a conjugate linear function of \\(\\mathbf u\\), i.e., \\(g(a \\mathbf x + b \\mathbf y) = \\bar{a} g(\\mathbf x) + \\bar{b} * g(\\mathbf y)\\) for any scalars \\(a\\), \\(b\\), and vectors \\(\\mathbf x\\), \\(\\mathbf y\\), where \\(\\bar{a}\\) denotes the complex conjugate of \\(a\\).\n\n\\(\\langle \\mathbf{x}, a\\mathbf{y}, b\\mathbf{z}\\rangle = a\\langle\\mathbf{x}, \\mathbf{y}\\rangle + b\\langle\\mathbf{x}, \\mathbf{z}\\rangle\\). this property says that the inner product is linear with respect to the second argument, but with complex conjugation. If we multiply a vector by a scalar and add it to another vector, the resulting inner product is the same as if we had calculated the inner product of each vector separately, complex-conjugated the second vector, and then added them.\n\n“Symmetry” means \\(\\langle \\mathbf{x},\\mathbf{y}\\rangle= \\langle \\mathbf{y},\\mathbf{x}\\rangle\\)\n\nthe order of the vectors doesn’t matter when calculating the inner product.\n\n\n“Positive-definite” means that for any nonzero vector v, the inner product \\(\\langle\\mathbf u, \\mathbf v\\rangle\\) is a positive real number. In other words, the inner product of a vector with itself is always positive, except when the vector is the zero vector.\n\n\\(\\langle \\mathbf{x},\\mathbf{x}\\rangle\\ge 0, \\langle \\mathbf{x},\\mathbf{x}\\rangle=0\\) only if \\(\\mathbf{x}=0\\)\n\n\n\n\n\n\n\n\nLinear Transformation\n\n\n\nA function is said to be linear if it satisfies two properties: additivity and homogeneity.\n\nAdditivity means that for any two inputs, the output of the function applied to their sum is equal to the sum of the outputs applied to each input separately. In other words, if we have a function \\(f\\) and vectors \\(\\mathbf x\\) and \\(\\mathbf y\\), then\n\n\\[\nf(\\mathbf x + \\mathbf y) = f(\\mathbf x) + f(\\mathbf y)\n\\]\n\nHomogeneity means that for any input and scalar \\(c\\), the output of the function applied to the input scaled by \\(c\\) is equal to the output applied to the unscaled input multiplied by \\(c\\). In other words,\n\n\\[\nf(c\\mathbf x) = c f(\\mathbf x)\n\\] These two properties together are what we mean when we say a function is linear.\nTry to compare \\(y=2x\\) for liniearity vs \\(y=2x^2\\) for non-linearity and which one satisfies the linear properties?\n\n\nLet’s consider the standard inner product of two vectors in \\(\\mathbb R^2\\), given by \\(\\langle \\mathbf x, \\mathbf y\\rangle\\) = \\(x_1y_1 + x_2y_2\\), where \\(\\mathbf x = [x_1, x_2]^T\\) and \\(\\mathbf y = [y_1, y_2]^T\\).\n\nLinearity in the first argument:\n\n\\[\n\\langle 2 \\mathbf x + 3\\mathbf y, \\mathbf z\\rangle = (2x_1 + 3y_1)z_1 + (2x_2 + 3y_2)z_2 = 2\\langle \\mathbf x, \\mathbf z \\rangle + 3\\langle \\mathbf y, \\mathbf z \\rangle\n\\]\n\nConjugate linearity in the second argument:\n\n\\[\n\\langle \\mathbf x, 2\\mathbf y+3\\mathbf z\\rangle = x_1(2y_1 + 3z_1) + x_2(2y_2 + 3z_2) = 2\\langle \\mathbf x, \\mathbf y \\rangle + 3\\langle \\mathbf x, \\mathbf z \\rangle\n\\]\n\nSymmetry:\n\n\\[\n\\langle \\mathbf x,\\mathbf y\\rangle = x_1y_1 + x_2y_2 = y_1x_1 + y_2x_2 = \\langle \\mathbf y, \\mathbf x \\rangle\n\\]\n\nPositive-definite:\n\n\\[\n\\langle \\mathbf x,\\mathbf x\\rangle =  x_1^2 + x_2^2 \\ge 0 = \\langle \\mathbf x, \\mathbf x \\rangle \\text{ only if } \\mathbf x = [0, 0]^T\n\\]\nLet’s see another example of two complex vectors for 2. Conjugate linearity in the second argument, \\(\\mathbf{u}=\\begin{bmatrix} 1+i \\\\ 2 \\end{bmatrix}\\) and \\(\\mathbf{v}=\\begin{bmatrix} 3-2i \\\\ 1 \\end{bmatrix}\\).\nTheir inner product would be: \\[\n\\begin{aligned}\n\\langle \\mathbf{u}, \\mathbf{v} \\rangle &= \\begin{bmatrix} 1+i \\\\ 2 \\end{bmatrix}^H \\begin{bmatrix} 3-2i \\\\ 1 \\end{bmatrix} \\\\\n&= \\begin{bmatrix} 1-i & 2 \\end{bmatrix} \\begin{bmatrix} 3-2i \\\\ 1 \\end{bmatrix} \\\\\n&= (1-i)(3-2i) + 2(1) \\\\\n&= 1 + i + 6 - 4i + 2 \\\\\n&= 9 - 3i.\n\\end{aligned}\n\\]\nwhere \\(H\\) is the Hermitian transpose, also known as the conjugate transpose, which is similar to the transpose operation, but also involves taking the complex conjugate of each element. For a matrix \\(\\mathbf A\\), the Hermitian transpose is denoted by \\(\\mathbf A^H\\) or \\(A^\\dagger\\) and is defined as the transpose of the complex conjugate of \\(\\mathbf A\\). Mathematically, for a matrix \\(\\mathbf A\\) with elements \\(a_{i,j}\\), the Hermitian transpose \\(\\mathbf A^H\\) is defined as:\n\\[\n(\\mathbf A^H)_{i,j} = \\overline{a_{j,i}}\n\\]\nwhere \\(\\overline{a_{j,i}}\\) denotes the complex conjugate of \\(a_{j,i}\\).\nIn the case of a real-valued matrix, the Hermitian transpose reduces to the ordinary transpose, denoted by \\(\\mathbf A^T\\).\nNow let’s see the conjugate linearity property in the second argument:\n\\[\n\\begin{aligned}\n\\langle \\mathbf{u}, c \\mathbf{v} \\rangle &= \\begin{bmatrix} 1+i \\\\2 \\end{bmatrix}^H \\left(c \\begin{bmatrix} 3-2i \\\\ 1 \\end{bmatrix}\\right) \\\\\n&= \\begin{bmatrix} 1-i & 2 \\end{bmatrix} \\begin{bmatrix} 3c-2ci \\\\ c \\end{bmatrix} \\\\\n&= (1-i)(3c-2ci) + 2(c) \\\\\n&= 3c - 2ci + 2c - 2ci \\\\\n&= (3+2)c - 4ci \\\\\n&= c(3+2i) - 4i\\overline{c}.\n\\end{aligned}\n\\]\nWe can see that the second component of the result is \\(-4i\\overline{c}\\), which is the conjugate of \\(4ic\\). Therefore, we can say that the inner product is conjugate linear in the second argument.\nA dot product is a specific type of inner product that is defined for Euclidean spaces, which are spaces with a notion of distance or length. The dot product of two vectors is defined as the sum of the products of their corresponding components. In other words, if \\(\\mathbf a = [a_1, a_2, ..., a_n]\\) and \\(\\mathbf b = [b_1, b_2, ..., b_n]\\) are two vectors in \\(\\mathbb R^n\\), then their dot product is given by:\n\\[\n\\mathbf a \\cdot \\mathbf b = a_1b_1 + a_2b_2 + ... + a_nb_n\n\\]\nThe dot product satisfies some of the properties of an inner product, such as being linear in the first argument and symmetric. However, it is not conjugate linear in the second argument, and it is not positive-definite in general.\nSo, while a dot product is a specific type of inner product, not all inner products are dot products.\n\n\nFor example, if \\(\\textbf{a} = [1, 2, 3]\\) and \\(\\textbf{b} = [4, 5, 6]\\), then their dot product \\(c = 1\\cdot 4 + 2\\cdot 5 + 3\\cdot 6 = 32\\).\n\n\n\n\n\n\nNorm\n\n\n\nThe norm of a vector \\(\\mathbf{x}\\) is a non-negative scalar value that represents the size or length of the vector. The norm is denoted by \\(||\\mathbf{x}||\\) and satisfies the following properties:\n\nNon-negativity: \\(||\\mathbf{x}||\\geq 0\\), with equality if and only if \\(\\mathbf{x}=\\mathbf{0}\\).\nHomogeneity: \\(||\\alpha\\mathbf{x}||=|\\alpha| \\text{ }||\\mathbf{x}||\\) for any scalar \\(\\alpha\\).\nTriangle Inequality: \\(||\\mathbf{x}+\\mathbf{y}||\\leq ||\\mathbf{x}||+||\\mathbf{y}||\\).\n\nHere is an example of finding the Euclidean norm of a vector: Suppose we have a vector \\(\\mathbf{x}=\\begin{bmatrix}1 \\\\ -2 \\\\ 2\\end{bmatrix}\\). We can find its norm as follows:\n\\[\n||\\mathbf x||=\\sqrt{1^2+(-2)^2+2^2}=\\sqrt{9}=3\n\\]\nTherefore, the norm of \\(\\mathbf{x}\\) is 3.\nThere are several types of norms:\n\nManhattan Norm or Absolute Norm or \\(l_1\\)-norm \\[\n\\begin{equation*}\n||\\mathbf{x}||_{l_1} = \\sum_{i=1}^{n} |x_i|\n\\end{equation*}\n\\] where \\(\\mathbf{x}\\) is a vector of length \\(n\\). Example: For \\(\\mathbf{x} = [1, -2, 3]\\), \\(||\\mathbf{x}||_{l_1} = |1| + |-2| + |3| = 6\\).\nEuclidean Norm or \\(l_2\\)-norm \\[\n\\begin{equation*}\n||\\mathbf{x}||_{l_2} = \\sqrt{\\sum_{i=1}^{n} x_i^2}\n\\end{equation*}\n\\] where \\(\\mathbf{x}\\) is a vector of length \\(n\\). Example: For \\(\\mathbf{x} = [1, 2, 3]\\), \\(||\\mathbf{x}||_{l_2} = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14}\\).\n\n\n\n\n\\(l_2\\)-norm\n\n\n\np-norm(\\(l_2\\)-norm)\n\nFor \\(p \\geq 1\\), \\[\n\\begin{equation*}\n||\\mathbf{x}||_p = (\\sum_{i=1}^n |x_i|^p)^{\\frac{1}{p}}\n\\end{equation*}\n\\] where \\(\\mathbf{x}\\) is a vector of length \\(n\\). Example: For \\(\\mathbf{x} = [1, 2, 3]\\), \\(||\\mathbf{x}||_{l_p} = \\sqrt{1^p + 2^p + 3^p}\\).\n\nMaximum Norm \\[\n\\begin{equation*}\n||\\mathbf{x}||_{\\infty} = \\max_{1 \\leq i \\leq n} |x_i|\n\\end{equation*}\n\\] where \\(\\mathbf{x}\\) is a vector of length \\(n\\). Example: For \\(\\mathbf{x} = [1, -2, 3]\\), \\(||\\mathbf{x}||_{\\infty} = \\max{(1, |-2|, 3)} = 3\\).\n\n\n\n\n\\(l_1\\)-norm vs \\(l_2\\)-norm vs \\(\\max\\)-norm\n\n\n\nFrobenius Norm: \\[\n\\begin{equation*}\n||\\mathbf{a}||_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^2}\n\\end{equation*}\n\\] where \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix. Example: For \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\), \\(||\\mathbf{A}||_{F} = \\sqrt{1^2 + 2^2 + 3^2 + 4^2} = \\sqrt{30}\\).\n\n\n\n\n\n\n\n\n\nProjection\n\n\n\nLet \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be two vectors. The projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{v}\\) is defined as the vector:\nThis vector is the closest vector to \\(\\mathbf{u}\\) that lies on the line spanned by \\(\\mathbf{v}\\). \\[\n\\mathbf w=\\text{proj}_{\\mathbf v}\\mathbf u =\\frac{\\mathbf u \\mathbf v}{||\\mathbf v||^2} \\mathbf v\n\\] \n\n\\(\\mathbf{w} = ||\\mathbf{w}||\\mathbf{v} = ||\\mathbf{u}|| \\cos \\theta \\mathbf{v}\\)\n\\(\\mathbf{u}^T \\mathbf{u} = ||\\mathbf{u}|| ||\\mathbf{u}|| = ||\\mathbf{u}||^2\\)\nthe magnitude of \\(\\mathbf{u}\\) = \\(||\\mathbf{u}|| = \\sqrt{\\mathbf{u}^T \\mathbf{u}}\\)\nunit vector: a normalized vector by dividing it by its magnitude, so the magnitude of a unit vector is 1 \\[\n\\hat{\\mathbf{u}} = \\frac{\\mathbf{u}}{||\\mathbf{u}||} = \\frac{\\mathbf{u}}{\\sqrt{\\mathbf{u}^T \\mathbf{u}}}\n\\]\nProjected vector, \\(\\mathbf{w}\\)\n\nthe product of \\(\\frac{\\mathbf{u}^T \\mathbf{v}}{||\\mathbf{v}||}\\) and a unit vector of \\(\\mathbf{v}\\) \\[\n\\frac{\\mathbf{u}^T \\mathbf{v}}{||\\mathbf{v}||} \\frac{\\mathbf{v}}{||\\mathbf{v}||} = \\frac{\\mathbf{u}^T \\mathbf{v}}{||\\mathbf{v}||^2}\\mathbf{v}\n\\]\n\n\nFor example, let \\(\\mathbf{u} = \\begin{bmatrix}2 \\\\ 3\\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}\\). Then, the projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{v}\\) is:\n\\[\n\\text{proj}_{\\mathbf v}\\mathbf u =\\frac{\\mathbf u^{T} \\mathbf v}{||\\mathbf v||^2} \\mathbf v =\\frac{\\begin{bmatrix}2 \\\\ 3\\end{bmatrix}\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}}{\\bigg{|}\\bigg{|}\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}\\bigg{|}\\bigg{|}^2}=\\frac{5}{2}\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}=\\begin{bmatrix}5 \\\\ 2\\end{bmatrix}\n\\]\nThis vector is the closest vector to \\(\\mathbf{u}\\) that lies on the line spanned by \\(\\mathbf{v} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}\\).\nReference: Read This Article with Interactive Visualization - Projection\n\n\n\n\n\n\n\n\nCauchy-Schwarz Inequality\n\n\n\na fundamental result in mathematics that relates to inner products and norms. It states that for any vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in an inner product space, the following inequality holds: \\[\n  |\\langle \\mathbf u,\\mathbf v\\rangle|\\le ||\\mathbf u|| ||\\mathbf v ||\n\\]\nwhere \\(\\langle \\mathbf{u}, \\mathbf{v}\\rangle\\) denotes the inner product of vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), and \\(|\\mathbf{u}|\\) and \\(|\\mathbf{v}|\\) denote their respective norms. In terms of the cosine formula, the Schwarz inequality can be written as: \\[\n\\cos \\theta \\le 1\n\\]\nwhere \\(\\theta\\) is the angle between vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), and \\(\\cos{\\theta} = \\frac{\\langle \\mathbf{u}, \\mathbf{v}\\rangle}{|\\mathbf{u}| |\\mathbf{v}|}\\).\nGeometrically, the Schwarz inequality states that the magnitude of the projection of one vector onto the other cannot exceed the length of the vector being projected. In other words, it bounds the correlation between two vectors and ensures that their inner product is always less than or equal to the product of their norms.\n\n\n\n\n\n\n\n\nTriangle Inequality\n\n\n\nThe triangle inequality states that for any two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), the length of the sum of the vectors is less than or equal to the sum of the lengths of the vectors themselves. In terms of the cosine formula, this can be expressed as: \\[\n||\\mathbf u + \\mathbf v||^2 \\le ||\\mathbf u||^2 + 2||\\mathbf u ||||\\mathbf v || + ||\\mathbf v ||^2\n\\]\nequivalently,\n\\[\n||\\mathbf u + \\mathbf v|| \\le ||\\mathbf u|| + ||\\mathbf v ||\n\\]\nthis inequality means that the distance between two points in a space, represented by vectors, is always shorter than or equal to the sum of the distances between the two vectors. In other words, it is impossible to make a straight line from one point to another that is shorter than the distance represented by the two vectors.\n\n\n\n\n\n\n\n\n\n\nA unit vector is a vector that has a magnitude of 1. A unit vector can be obtained by dividing a non-zero vector \\(\\mathbf{v}\\) by its magnitude \\(||\\mathbf{v}||\\),\n\\[\n\\begin{equation*}\n  \\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{||\\mathbf{v}||}\n\\end{equation*}\n\\]\nwhere \\(\\mathbf{\\hat{v}}\\) is the unit vector in the direction of \\(\\mathbf{v}\\).\nFor example, let \\(\\mathbf{v} = \\begin{bmatrix} 1 \\ 2 \\end{bmatrix}\\) be a non-zero vector in \\(\\mathbb{R}^2\\). The magnitude of \\(\\mathbf{v}\\) is \\(||\\mathbf{v}|| = \\sqrt{1^2 + 2^2} = \\sqrt{5}\\). Therefore, a unit vector in the direction of \\(\\mathbf{v}\\) is:\n\\[\n\\begin{equation*}\n\\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{||\\mathbf{v}||} = \\frac{1}{\\sqrt{5}}\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{bmatrix}\n\\end{equation*}\n\\]\nThus, \\(\\begin{bmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{bmatrix}\\) is a unit vector in the direction of \\(\\mathbf{v}\\).\n\n\n\nThe cross product of two vectors is a vector that is perpendicular to both of them. If \\(\\textbf{a}\\) and \\(\\textbf{b}\\) are two vectors in \\(\\mathbb{R}^3\\), then their cross product \\(\\textbf{c} = \\textbf{a} \\times \\textbf{b}\\) is a vector given by the formula\n\\[\n\\textbf{c} = \\textbf{a} \\times \\textbf{b} \\\\\n          = ||\\textbf{a}|| ||\\textbf{b}||\\sin(\\theta) \\mathbf n           \n\\]\nwhere:\n\n\\(\\theta\\) is the angle between \\(\\textbf{a}\\) and \\(\\textbf{b}\\) in the plane containing them (hence, it \\(0 \\le \\theta \\le \\pi\\))\n\\(||\\textbf{a}||\\) and \\(||\\textbf{b}||\\) are the magnitudes of vectors \\(||\\textbf{a}||\\) and \\(||\\textbf{b}||\\)\nand \\(||\\textbf{n}||\\) is a unit vector perpendicular to the plane containing \\(||\\textbf{a}||\\) and \\(||\\textbf{a}||\\), with direction such that the ordered set (\\(||\\textbf{a}||\\), \\(||\\textbf{b}||\\), \\(||\\textbf{n}||\\)) is positively-oriented.\n\nIf the vectors \\(\\textbf{a}\\) and \\(\\textbf{b}\\) are parallel (that is, \\(\\theta\\) between them is either \\(0\\) or \\(\\pi\\)), by the above formula, the cross product of \\(\\textbf{a}\\) and \\(\\textbf{b}\\) is the zero vector 0.\nReference: read the explanations in wiki\n\n\n\n \n\n\n\nFor example, \\[\n\\textbf{c} = \\textbf{a} \\times \\textbf{b} = [a_2b_3 - a_3b_2, a_3b_1 - a_1b_3, a_1b_2 - a_2b_1]           \n\\]\nIf \\(\\textbf{a} = [1, 2, 3]\\) and \\(\\textbf{b} = [4, 5, 6]\\), then their cross product \\(\\textbf{c} = [-3, 6, -3]\\).\n\n\n\nA column vector \\(\\mathbf{u}\\) with \\(n\\) elements is an \\(m \\times 1\\) matrix, which can be represented as: \\[\n\\mathbf{u} =\n\\begin{bmatrix}\nu_{1} \\\\\nu_{2} \\\\\n\\vdots \\\\\nu_{m}\n\\end{bmatrix}\n\\]\nIn an \\(m \\times n\\) matrix, the column vectors can be represented as:\n\\[\n\\mathbf U  = \\begin{bmatrix}  \\mathbf u_{1} &\\mathbf u_{2} & \\dots &\\mathbf u_{n} \\end{bmatrix} \\\\\n=\n\\begin{bmatrix}\n  u_{11} & u_{12} & \\cdots & u_{1n} \\\\\n  u_{21} & u_{22} & \\cdots & u_{2n} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  u_{m1} & u_{m2} & \\cdots & u_{mn}\n\\end{bmatrix}\n\\]\nwhere \\(u_i\\) is the \\(i\\)-th element of the column vector \\(\\mathbf{u}\\), \\(n\\) is the number of columns, and \\(m\\) is the number of rows in the matrices.\nA row vector \\(\\mathbf{u}\\) with \\(m\\) elements is a \\(1 \\times n\\) matrix, which can be represented as: \\[\n\\mathbf{u} =\n\\begin{bmatrix}\nu_{1} & u_{2} & \\cdots & u_{m}\n\\end{bmatrix}\n\\]\nIn an \\(m \\times n\\) matrix, the row vectors can be represented as:\n\\[\n\\mathbf U  = \\begin{bmatrix}  \\mathbf u_{1} \\\\\\mathbf u_{2} \\\\ \\vdots \\\\\\mathbf u_{m} \\end{bmatrix} \\\\\n=\n\\begin{bmatrix}\n  u_{11} & u_{12} & \\cdots & u_{1n} \\\\\n  u_{21} & u_{22} & \\cdots & u_{2n} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  u_{m1} & u_{m2} & \\cdots & u_{mn}\n\\end{bmatrix}\n\\]\nwhere \\(u_i\\) is the \\(i\\)-th element of the row vector \\(\\mathbf{u}\\) and \\(n\\) is the number of columns in the matrix.\n\n\n\nA linear combination of vectors \\(\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_n\\) in a vector space \\(V\\) over a field \\(\\mathbb{F}\\) is a vector of the form: \\[\na_1\\mathbf{v_1}+a_2\\mathbf{v_2}+\\dots+a_n\\mathbf{v_n}\n\\]\nwhere \\(a_1,a_2,\\dots,a_n\\in\\mathbb{F}\\).\nFor example, suppose we have two vectors \\(\\mathbf{v}_1=\\begin{bmatrix} 1 \\ 2 \\ 3 \\end{bmatrix}\\) and \\(\\mathbf{v}_2=\\begin{bmatrix} 4 \\ 5 \\ 6 \\end{bmatrix}\\) in \\(\\mathbb{R}^3\\). Then, a linear combination of \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) is of the form:\n\\[\na_1\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}+a_2\\begin{bmatrix}4\\\\5\\\\6\\end{bmatrix}=\\begin{bmatrix}a_1+4a_2\\\\2a_1+5a_2\\\\3a_1+6a_2\\end{bmatrix}\n\\]\nHere, \\(a_1\\) and \\(a_2\\) are scalar coefficients that determine the resulting linear combination vector.\n\n\n\nThe outer product of two vectors \\(\\mathbf{u} = [u_1, u_2, \\dots, u_m]^T\\) and \\(\\mathbf{v} = [v_1, v_2, \\dots, v_n]^T\\) is a matrix \\(\\mathbf{u} \\mathbf{v}^T\\) of size \\(m \\times n\\), defined by:\n$$\n\\[\\begin{aligned}\n\\mathbf{u} \\otimes \\mathbf{v} &=\n\\begin{bmatrix}\nu_1v_1 &u_1v_2& \\dots & u_1v_n \\\\\nu_2v_1 &u_2v_2& \\dots & u_2v_n \\\\\n\\vdots &\\vdots& \\ddots & u_1v_n \\\\\nu_mv_1 &u_mv_2& \\dots & u_mv_n \\\\\n\n\\end{bmatrix}\n\\end{aligned}\\]\n$$\n\\[\n(\\mathbf{u} \\otimes \\mathbf{v})_{i,j} = u_i v_j\n\\]\nwhere \\(\\mathbf{u} = [u_1, u_2, \\dots, u_m]\\) and \\(\\mathbf{v} = [v_1, v_2, \\dots, v_n]\\).\nThe outer product is also called the tensor product, and it is a type of binary operation between two vectors that results in a matrix. It is important in linear algebra and other fields such as physics and engineering.\nHere is an example: Let \\(\\mathbf{u} = [2, 4, 6]^T\\) and \\(\\mathbf{v} = [1, 3]^T\\). The outer product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is:\nSo the outer product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is a \\(3 \\times 2\\) matrix.\nWhat is a matrix? Go to the Next Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/02.basic_matrix.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/02.basic_matrix.html",
    "title": "Basics (2) - Matrix Operations",
    "section": "",
    "text": "A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, it can be represented as: \\[\n\\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\] where \\(a_{ij}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column of the matrix \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/02.basic_matrix.html#basic-matrix-operations",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/02.basic_matrix.html#basic-matrix-operations",
    "title": "Basics (2) - Matrix Operations",
    "section": "2 Basic Matrix Operations",
    "text": "2 Basic Matrix Operations\n\n2.1 Matrix addition\nThe sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.\nGiven two \\(m \\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is defined by:\n\\[\nc_{i,j}=a_{i,j}+b_{i,j}​\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\n\\[\n\\begin{bmatrix}\n  1 & 2 \\\\\n  3 & 4 \\\\\n  5 & 6\n\\end{bmatrix} +\n\\begin{bmatrix}\n  -1 & 0 \\\\\n  2 & -3 \\\\\n  -5 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n  0 & 2 \\\\\n  5 & 1 \\\\\n  0 & 10\n\\end{bmatrix}\n\\]\n\n\n2.2 Scalar multiplication\nThe product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.\nGiven a scalar \\(k\\) and an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), their product \\(k\\mathbf{A}\\) is defined by: \\[\n(k\\mathbf{A})_{i,j} = k(a_{i,j})\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\nExample: \\[\n2\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 & 4 \\\\\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\]\n\n\n2.3 Matrix multiplication\nThe product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is a matrix obtained by multiplying the rows of \\(\\mathbf{A}\\) by the columns of \\(\\mathbf{B}\\).\nGiven two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with dimensions \\(m \\times n\\) and \\(n \\times p\\), respectively, their product \\(\\mathbf{C} = \\mathbf{AB}\\) is an \\(m \\times p\\) matrix defined by: \\[\nc_{i,j} = \\sum_{k=1}^n a_{i,k}b_{k,j}\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq p\\).\nExample: \\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} \\begin{bmatrix}\n-1 & 0 & 2 \\\\\n2 & -3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 & -6 & 4 \\\\\n5 & -12 & 10 \\\\\n7 & -18 & 16\n\\end{bmatrix}\n\\]\n\n\n2.4 Matrix with Combinations of Vectors\nA matrix can be written as combinations of vectors.\nLet’s see and apply the concept of a matrix with combinations of vectors to a linear combination of vectors. We can represent a linear combination of vectors as a matrix form with combinations of vectors.\nGiven vectors \\(\\mathbf{a}_1, \\mathbf{a}_2, \\dots, \\mathbf{a}_n \\in \\mathbb{R}^m\\) and the vector \\(\\mathbf x\\) whose entries are scalars \\(x_1, x_2, \\dots, x_n \\in \\mathbb{R}\\), a linear combination of the vectors and scalars is written as:\n\\[\nx_1\\mathbf a_1 +x_2\\mathbf a_2 +\\dots+x_n \\mathbf a_n\n\\]\nThe combinations of the \\(\\mathbf{a}\\) vectors is represented as a matrix that can be written as combinations of column vectors with the \\(\\mathbf{a}\\) vectors\n\\[\n\\begin{aligned}\n\\mathbf{A}_{m\\times n} &=\\begin{bmatrix} \\mathbf{a_1}&\\mathbf{a_2}& \\dots &\\mathbf{a_n} \\end{bmatrix} \\quad \\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\n\\end{aligned}\n\\]\nThus, the linear combination is simply written as:\n\\[\n\\begin{aligned}\n&x_1\\mathbf a_1 +x_2\\mathbf a_2 +\\dots+x_n \\mathbf a_n \\\\\n&=\\begin{bmatrix} \\mathbf{a_1}&\\mathbf{a_2}& \\dots &\\mathbf{a_n} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\\\\n&=\\begin{bmatrix} \\mathbf{a_1}&\\mathbf{a_2}& \\dots &\\mathbf{a_n} \\end{bmatrix} \\mathbf x \\\\\n&=\\mathbf{Ax}\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{x} = \\begin{bmatrix} x_1 & x_2 & \\dots & x_n \\end{bmatrix}^T\\) is a column vector of scalars.\n\n\n\n\n\n\nTip\n\n\n\nIt is a different example of a matrix with combinations of the product of vectors from that of matrix combination of vectors. A matrix can be represented as the outer product of column vectors and standard basis vectors, and their sum like the following example:\n\\[\n\\mathbf A =\n\\begin{bmatrix}\n1&4&7\\\\\n2&5&8\\\\\n3&6&9\\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n1\\\\\n2\\\\\n3\\\\\n\\end{bmatrix}\\mathbf{e}_1^\\text{T}+\n\\begin{bmatrix}\n4\\\\\n5\\\\\n6\\\\\n\\end{bmatrix}\\mathbf{e}_2^\\text{T}+\n\\begin{bmatrix}\n7\\\\\n8\\\\\n9\\\\\n\\end{bmatrix}\\mathbf{e}_3^\\text{T}\n\\] where \\(\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3\\) are the standard basis vectors of \\(\\mathbb{R}^3\\).\n\n\n\n\n2.5 Matrix Multiplication with a Vector\nLet \\(\\mathbf{A}\\) be an \\(m \\times n\\) matrix and \\(\\mathbf{x}\\) be a \\(n \\times 1\\) column vector. The matrix-vector product \\(\\mathbf{Ax}\\) is defined as:\n\\[\n\\begin{aligned}\n\\mathbf{Ax}&=\\mathbf{b}\\\\\n\\mathbf{Ax}&=\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\na_{11}x_1 + a_{12}x_2 + \\dots + a_{1n}x_n \\\\\na_{21}x_1 + a_{22}x_2 + \\dots + a_{2n}x_n \\\\\n\\vdots  \\\\\na_{m1}x_1 + a_{m2}x_2 + \\dots + a_{mn}x_n\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\nb_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n\n\\end{bmatrix}\\\\\n&=\\mathbf{b}\n\\end{aligned}\n\\]\nIn other words, each entry of the resulting column vector is the dot product of the corresponding row of the matrix \\(\\mathbf{A}\\) and the column vector \\(\\mathbf{x}\\).\nThe matrix \\(\\mathbf{A}\\) acts on the vector \\(\\mathbf{x}\\). The result \\(\\mathbf{Ax}\\) is a combination \\(\\mathbf{b}\\) of the columns of \\(\\mathbf{A}\\). The input is \\(\\mathbf{x}\\) and the output is \\(\\mathbf{b}=\\mathbf{Ax}\\)\nFor example, let\n\\[\n\\mathbf A =\n\\begin{bmatrix}\n2 & 1 \\\\\n3 & 4 \\\\\n1 & 2\n\\end{bmatrix} \\quad\n\\mathbf x=\n\\begin{bmatrix}\n  x_1\\\\x_2\n\\end{bmatrix} \\quad\n\\mathbf b=\n\\begin{bmatrix}\n  1\\\\-5\\\\-1\n\\end{bmatrix}\n\\]\nThen we have\n\\[\n\\begin{aligned}\n\\mathbf{Ax} &= \\mathbf{b} \\\\\n\\begin{bmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{bmatrix}\n\\begin{bmatrix}\n  x_1\\\\x_2\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n  1\\\\-1\n\\end{bmatrix} \\\\\n\\begin{bmatrix}\n2x_1 + x_2 \\\\\nx_1 + 2x_2\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n  1\\\\-1\n\\end{bmatrix}\n\\end{aligned}\n\\]\nThe solution to the above system of equation is \\(x_1=1, x_2=-1\\).\n\n\n2.6 Linear Equations of a Matrix\nBefore the introduction of a matrix to the solution to \\(x_1\\mathbf a_1 +x_2\\mathbf a_2 +\\dots+x_n \\mathbf a_n=\\mathbf b\\), we used the concept of a linear combination of \\(\\mathbf a\\) vectors to find \\(\\mathbf{x}\\) for \\(\\mathbf{b}\\).\nBut after the introduction of a matrix, the viewpoint can changes:\n\nAs-Is: Compute the linear combination \\(x_1\\mathbf a_1 +x_2\\mathbf a_2 +\\dots+x_n \\mathbf a_n\\) to find \\(\\mathbf b\\).\nTo-Be: Which combination of \\(\\mathbf a\\) vectors produces a particular vector \\(\\mathbf b\\)?\n\nAnwering the two questions means looking for \\(\\mathbf{x}\\) for \\(\\mathbf{b}\\). To do so, we have two ways:\n\nto solve a system of linear equations and\nto find an inverse of \\(\\mathbf{A}\\)\n\n\n2.6.1 Solving a System of Linear Equations\nGiven an \\(m \\times n\\) matrix \\(\\mathbf A\\) and an \\(n \\times 1\\) vector \\(\\mathbf{x}\\), the matrix-vector product \\(\\mathbf A\\mathbf{x}\\) is a linear combination of the columns of \\(\\mathbf A\\) with coefficients given by the entries of \\(\\mathbf{x}\\). The system of linear equations represented by \\(\\mathbf A\\mathbf{x}=\\mathbf{b}\\) has a unique solution if and only if the columns of \\(\\mathbf A\\) are linearly independent.\nA system of linear equations can be written in matrix form as follows:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}=\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_m\n\\end{bmatrix}\n\\end{aligned}\n\\]\nwhere \\(a_{ij}\\) are the coefficients of the system, \\(x_i\\) are the variables, and \\(b_j\\) are the constants.\nWe call the above \\(\\mathbf{A}\\) matrix is a coefficient matrix from the point of view of a system of lineqr equations and the above \\(\\mathbf{Ax}=\\mathbf{b}\\) a matrix equation.\nHere’s an example of a system of linear equations represented by a matrix: \\[\n\\begin{align*}\n2x_1 + 3x_2 &= 8 \\\\\n4x_1 + 5x_2 &= 13\n\\end{align*}\n\\]\nThis can be written as the matrix equation \\(A\\mathbf{x}=\\mathbf{b}\\), where\n\\[\n\\begin{equation}\n  \\mathbf{A} =\n    \\begin{bmatrix}\n    2 & 3\\\\\n    4 & 5\n    \\end{bmatrix} \\quad\n  \\mathbf{x} =\n    \\begin{bmatrix}\n    x_1\\\\\n    x_2\n    \\end{bmatrix}\\quad\n  \\mathbf{b} =\n    \\begin{bmatrix}\n    8\\\\\n    13\n    \\end{bmatrix}\n\\end{equation}\n\\]\nThis can be written in matrix form as: \\[\n\\begin{equation}\n\\begin{bmatrix}\n2 & 3 \\\\\n4 & -1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}=\n\\begin{bmatrix}\n5 \\\\\n2\n\\end{bmatrix}\n\\end{equation}\n\\]\nConsider the following system of equations:\n\\[\n\\begin{aligned}\n2x_1 + 3x_2 &= 5 \\\\\n4x_1 - x_2 &= 2\n\\end{aligned}\n\\]\n\n\n2.6.2 Finding \\(\\mathbf{A}^{-1}\\)\nThe solution to this system can be found by computing the inverse of \\(\\mathbf A\\) (if it exists) and multiplying both sides of the equation by it:\n\\[\n\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}\n\\]\nIf \\(\\mathbf A\\) does not have an inverse, then the system of equations may have either no solutions or infinitely many solutions.\n\n\n\n2.7 Determinant\nLet \\(\\mathbf{A}\\) be an \\(n \\times n\\) square matrix. The determinant of \\(\\mathbf{A}\\), denoted by \\(|\\mathbf{A}|\\) or \\(\\det(\\mathbf{A})\\), is a scalar value calculated as the sum of the products of the elements in any row or column of \\(\\mathbf{A}\\) with their corresponding cofactors, that is,\n\\[\n|\\mathbf{A}|=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n\\]\nwhere \\(a_{ij}\\) is the element of \\(\\mathbf{A}\\) in the \\(i\\)-th row and \\(j\\)-th column, and \\(C_{ij}\\) is the cofactor of \\(a_{ij}\\). The cofactor of \\(a_{ij}\\), denoted by \\(C_{ij}\\), is given by \\((-1)^{i+j}\\) times the determinant of the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)-th row and \\(j\\)-th column of \\(\\mathbf{A}\\).\nThe determinant of an \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is a scalar value denoted as \\(|\\mathbf{A}|\\). It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a \\(3 \\times 3\\) matrix \\(\\mathbf{A}\\), \\[\n\\mathbf{A}=\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\]\nis given by:\n\\[\n|\\mathbf{A}| = a_{11}\n\\begin{vmatrix}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{vmatrix}\n- a_{12}\n\\begin{vmatrix}\na_{21} & a_{23} \\\\\na_{31} & a_{33}\n\\end{vmatrix}\n+ a_{13}\n\\begin{vmatrix}\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{vmatrix}\n\\]\nFor example, consider the \\(3 \\times 3\\) matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\). We can calculate the determinant of \\(\\mathbf{A}\\) using any row or column. Let’s use the first column:\n\\[\n|\\mathbf{A}| = 1\n\\begin{vmatrix}\n5 & 6 \\\\\n8 & 9\n\\end{vmatrix}\n- 4\n\\begin{vmatrix}\n2 & 3\\\\\n8 & 9\n\\end{vmatrix}\n+ 7\n\\begin{vmatrix}\n2 & 5 \\\\\n3 & 6\n\\end{vmatrix} = 0\n\\]\nTherefore, the determinant of \\(\\mathbf{A}\\) is zero.\n\n\n2.8 Inverse\nThe inverse of a square matrix \\(A\\) of size \\(n\\) is a matrix \\(A^{-1}\\) such that the product of \\(A\\) and \\(A^{-1}\\) is the identity matrix \\(I_n\\), i.e. \\(A \\times A^{-1} = I_n\\). If such a matrix exists, then \\(A\\) is said to be invertible or non-singular.\nThe inverse of a square matrix \\(\\mathbf{A}\\) is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the unique matrix that satisfies the following equation: \\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\nFor example, consider the \\(2\\times 2\\) matrix \\[\n\\mathbf{A} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}.\n\\] The inverse of \\(\\mathbf{A}\\) is given by:\n\\[\n\\mathbf{A}^{-1} =\n\\frac{1}{-2}\n\\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\]\nWe can verify that \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\) by computing:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} =\n\\begin{bmatrix} -2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\nLet me give another example and A be a \\(3\\times 3\\) square matrix:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, the inverse of \\(\\mathbf{A}\\), denoted as \\(\\mathbf{A}^{-1}\\), is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\na_{22}a_{33}-a_{23}a_{32} & a_{13}a_{32}-a_{12}a_{33} & a_{12}a_{23}-a_{13}a_{22} \\\\\na_{23}a_{31}-a_{21}a_{33} & a_{11}a_{33}-a_{13}a_{31} & a_{13}a_{21}-a_{11}a_{23} \\\\\na_{21}a_{32}-a_{22}a_{31} & a_{12}a_{31}-a_{11}a_{32} & a_{11}a_{22}-a_{12}a_{21} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nwhere \\(\\operatorname{det}(\\mathbf{A})\\) is the determinant of \\(\\mathbf{A}\\).\nFor example, let:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, \\(\\operatorname{det}(A)\\) = -57, and the inverse of \\(\\mathbf{A}\\) is:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{-57}\\begin{bmatrix}\n-24 & 18 & 5 \\\\\n20 & -15 & -4 \\\\\n-3 & 2 & 1 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThere are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.\n\n\n2.9 Linear Equations\n\n2.9.1 Unique Solution\nA linear system of equations has a unique solution if and only if the coefficient matrix is non-singular (i.e., its determinant is nonzero).\n$$\n\\[\\begin{aligned}\n2x_1 + 3x_2 &= 10 \\\\\n4x_1 + 5x_2 &= 20 \\\\\n\n\\mathbf{A} \\mathbf{x} &= \\mathbf{b} \\\\\n\\begin{bmatrix}\n2 & 3 \\\\\n4 & 5 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n10 \\\\\n20 \\\\\n\\end{bmatrix} \\\\\n\\text{det}(\\mathbf{A}) &= (a_{11}a_{22}) - (a_{12}a_{21}) \\text{ where } a_{ij} \\text{ is the element of } \\mathbf{A}\\\\\n\\text{det}(\\mathbf{A}) &= (2 \\cdot 5) - (3 \\cdot 4) = -2\\\\\n\\mathbf{A}^{-1} &= \\frac{1}{\\text{det}(\\mathbf{A})} \\begin{bmatrix}\na_{22} & -a_{12} \\\\\n-a_{21} & a_{11}\n\\end{bmatrix} \\\\\n\n\\mathbf{A}^{-1} &=\n\n\\begin{bmatrix}\n-5/2 & 3/2 \\\\\n2 & -1 \\\\\n\\end{bmatrix} \\\\\n\\mathbf{A}^{-1}\\mathbf{A} \\mathbf{x} &= \\mathbf{A}^{-1}\\mathbf{b}\\\\\n\\mathbf{x} &= \\mathbf{A}^{-1}\\mathbf{b}\\\\\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}&=\n\\begin{bmatrix}\n-5/2 & 3/2 \\\\\n2 & -1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n10 \\\\\n20 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n5 \\\\\n0 \\\\\n\\end{bmatrix}\n\\end{aligned}\\]\n$$\nThe unique solution is \\((x_1,x_2) = (5,0)\\).\n\n\n2.9.2 Infinitely Many Solutions\nA linear system of equations has infinitely many solutions if and only if the system has at least one solution and the coefficient matrix is singular (i.e., its determinant is zero), and the system has more unknowns variables than linearly independent equations.\n$$ \\[\\begin{align*}\n\n2x_1 + 3x_2 &= 10 \\\\\n4x_1 + 6x_2 &= 20 \\\\\n\n\\mathbf{A} \\mathbf{x} &= \\mathbf{b} \\\\\n\\begin{bmatrix}\n2 & 3 \\\\\n4 & 6 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}\n&=\\begin{bmatrix}\n10 \\\\\n20 \\\\\n\\end{bmatrix} \\\\\n\\text{det}(\\mathbf{A}) &= (2 \\cdot 6) - (3 \\cdot 4) = 0\n\n\\end{align*}\\] $$\nSince the determinant of \\(\\mathbf{A}\\) is \\(0\\), the matrix \\(\\mathbf{A}\\) is singular and does not have an inverse. This implies that the system of linear equations has infinitely many solutions, as the determinant of the coefficient matrix is \\(0\\).\n\n\n2.9.3 No Solution\nA linear system of equations has no solution if the coefficient matrix is singular (i.e., its determinant is zero) and it has more linearly independent equations than unknowns variables.\n$$ \\[\\begin{align*}\n\n3x_1 + 4x_2 &= 10 \\\\\n6x_1 + 8x_2 &= 20 \\\\\n\n\\mathbf{A} \\mathbf{x} &= \\mathbf{b} \\\\\n\\begin{bmatrix}\n3 & 4 \\\\\n6 & 8 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n10 \\\\\n20 \\\\\n\\end{bmatrix} \\\\\n\\text{det}(\\mathbf{A}) &= (a_{11}a_{22}) - (a_{12}a_{21}) \\text{ where } a_{ij} \\text{ is the element of } \\mathbf{A}\\\\\n\\text{det}(\\mathbf{A}) &= (3 \\cdot 8) - (4 \\cdot 6) = 0\\\\\n\\end{align*}\\] $$\nthe determinant of \\(\\mathbf A\\) is \\(0\\), which means that \\(\\mathbf A\\) is a singular matrix and does not have an inverse.\n\n\n\n\n\n\n2.10 Independence and Dependence\n\n2.10.1 Independence\na set of vectors is said to be linearly independent if none of the vectors in the set can be written as a linear combination of the other vectors in the set. . In other words, a set of vectors \\({\\mathbf{v_1},\\mathbf{v_2},\\dots,\\mathbf{v_n}}\\) is linearly independent if the only solution to the linear equation \\(c_1\\mathbf{v_1}+c_2\\mathbf{v_2}+\\dots+c_n\\mathbf{v_n}=0\\) is \\(c_1=c_2=\\dots=c_n=0\\), where \\(c_i\\) is a scalar.\n\n\n2.10.2 Dependence\nA set of vectors is said to be linearly dependent if at least one vector in the set can be written as a linear combination of the other vectors in the set. In other words, a set of vectors \\(\\{v_1, v_2, \\ldots, v_n\\}\\) is linearly dependent if there exist scalars \\(c_1, c_2, \\ldots, c_n\\) (not all zero) such that \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\ldots + c_n\\mathbf{v}_n = 0\\)\n\n\n\n\n\n\nImportant\n\n\n\n\nIndependent columns: \\(\\mathbf{Ax} = \\mathbf{0}\\) has one solution. \\(\\mathbf{A}\\) is an invertible matrix.\nDependent columns: \\(\\mathbf{Ax} = \\mathbf{0}\\) has many solutions. \\(\\mathbf{A}\\) is a singular matrix.\n\n\n\n\n\n2.10.3 Example\nLet \\(\\mathbf{v}_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}, \\quad \\mathbf{v}_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}\\). These two vectors are linearly independent because no linear combination of and \\(\\mathbf{v}_1,\\quad \\mathbf{v}_2\\) can yield the zero vector, except when all the coefficients are zero. In other words, the only solution to the linear equation \\[\nc_1\\mathbf v_1​ + c_2\\mathbf v_2 = 0\n\\]\nis \\(c_1=c_2=0\\).\nThe more generalized case like the matrix \\(\\mathbf{A}_{m \\times n}\\) is going to be dealt with in another blog for understanding \\(\\mathbf{Ax} = \\mathbf{b}\\).\n\n\n\n2.11 Transpose\nThe transpose of an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), denoted by \\(\\mathbf{A}^T\\), is the \\(n \\times m\\) matrix obtained by interchanging the rows and columns of \\(\\mathbf{A}\\). Formally, if \\(\\mathbf{A} = [a_{ij}]\\) is an \\(m \\times n\\) matrix, then its transpose \\(\\mathbf{A}^T = [b_{ij}]\\) is an \\(n x m\\) matrix where \\(b_{ij}\\) = \\(a_{ji}\\) for all \\(i\\) and \\(j\\). In other words, the element in the \\(i\\) th row and \\(j\\) th column of \\(A^T\\) is equal to the element in the \\(j\\) th row and ith column of \\(\\mathbf{A}\\).\nGiven an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix defined by: \\[\n(\\mathbf{A^T})_{i,j} = \\mathbf{A}_{j,i}\n\\]\n\nWhen \\(\\mathbf{A}\\) is transposed, diagnoal entries(\\(a_{ii}\\)) do not change but off-diagnoal elements(\\(a_{ij} \\; i \\neq j\\)) change.\nA column vector is tranposed into a row vector, and vice versa.\nsymmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)\n\nExample:\nLet \\(\\mathbf A\\) be the matrix \\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\] The transpose of \\(\\mathbf A\\), denoted by \\(\\mathbf A^T\\), is the matrix \\[\n\\mathbf{A}^T = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}\n\\]\n\n2.11.1 Properties\nLet \\(\\mathbf A\\) be an \\(m \\times n\\) matrix and \\(\\mathbf B\\) be an \\(n \\times p\\) matrix, and let \\(c\\) be a scalar. Then:\n\n\\((\\mathbf{A}^T)^T = \\mathbf{A}\\)\n\\((\\mathbf{A} + \\mathbf{B})^T = \\mathbf{A}^T + \\mathbf{B}^T\\)\n\\((c\\mathbf{A})^T = c\\mathbf{A}^T\\)\n\\((\\mathbf{AB})^T = \\mathbf{B}^T \\mathbf{A}^T\\)\n\\((\\mathbf{Ax})^T = \\mathbf{x}^T \\mathbf{A}^T\\)\n\n\\((\\mathbf{Ax})\\) combies the columns of \\(\\mathbf A\\) while \\(\\mathbf{x}^T \\mathbf{A}^T\\) combines the rows of \\(\\mathbf{A}^T\\)\n\n\\((\\mathbf{ABC})^T = \\mathbf{C}^T\\mathbf{B}^T \\mathbf{A}^T\\) , (cyclic properties)\n\nIf \\(\\mathbf A=(\\mathbf{LDU})^T\\) then,\\(\\mathbf{A}^T=\\mathbf{U}^T\\mathbf{D}^T \\mathbf{L}^T\\) where \\(\\mathbf{D} = \\mathbf{D}^T\\)\n\n\\((\\mathbf{A}^{-1})^T = (\\mathbf{A}^{T})^{-1}\\)\n\n\\(\\mathbf{A}^{T}(\\mathbf{A}^{-1})^T = \\mathbf{A}^{T}(\\mathbf{A}^{T})^{-1}=\\mathbf{I}\\)\n\n\n\n\n\n2.12 Rank\nThe rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by \\(\\text{rank}(\\mathbf{A})\\).\nFor example, consider the following matrix:\n\\[\n\\begin{equation*}\n  \\mathbf{A} = \\begin{bmatrix}\n    1 & 2 & 3\\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\\\\\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe columns of \\(\\mathbf{A}\\) are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of \\(\\mathbf{A}\\) is 2.\n\n\n2.13 Trace\nThe trace of a square matrix \\(\\mathbf{A}\\), denoted by \\(\\mathrm{tr}(\\mathbf{A})\\), is defined as the sum of the diagonal elements of \\(\\mathbf{A}\\). In other words, if \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix, then its trace is given by:\n\\[\n\\mathrm{tr}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}\n\\]\nwhere \\(a_{ii}\\) denotes the \\(i\\) th diagonal element of \\(\\mathbf{A}\\).\nFor example, let \\[\n\\begin{bmatrix}\n  2 & 3 & 1 \\\\\n  0 & 5 & 2 \\\\\n  1 & 1 & 4\n\\end{bmatrix}\n\\]\nThen, the trace of \\(\\mathbf{A}\\) is \\(\\mathrm{tr}(\\mathbf{A}) = 2 + 5 + 4 = 11\\)\n\n\n2.14 Eigenvalues and Eigenvectors\nLet A be an \\(n × n\\) square matrix. A scalar \\(\\lambda\\) is called an eigenvalue of \\(\\mathbf A\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that \\[\n\\mathbf{Av}=\\lambda\\mathbf{v}\n\\]\nSuch a vector \\(\\mathbf{v}\\) is called an eigenvector corresponding to \\(\\lambda\\).\nExample:\nLet \\(\\mathbf A\\) be the matrix\nTo find the eigenvalues of \\(\\mathbf A\\), we solve the characteristic equation \\(\\text{det}(\\mathbf A - \\lambda \\mathbf I ) = 0\\), where I is the n × n identity matrix.\n\\[\n\\begin{align*}\n  \\text{det}(\\mathbf A - \\lambda \\mathbf I )\n  &=\n    \\begin{vmatrix}\n    3 - \\lambda & 1 \\\\\n    1 & 3 - \\lambda\n    \\end{vmatrix} \\\\\n  &=\n  (3 - \\lambda)(3 - \\lambda) - 1 \\\\\n  &= \\lambda^2 - 6\\lambda + 8 = 0\n\\end{align*}\n\\]\nSolving this quadratic equation gives us the eigenvalues of \\(\\mathbf A\\): \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 4\\).\nTo find the eigenvectors corresponding to \\(\\lambda_1 = 2\\), we solve the equation \\((\\mathbf A - 2 \\mathbf I)\\mathbf{v} = \\mathbf{0}\\), where \\(\\mathbf I\\) is the \\(2 \\times 2\\) identity matrix.\n\\[\n\\begin{align*}\n  (\\mathbf A - 2 \\mathbf I)\\mathbf{v} =\n    \\begin{bmatrix}\n      1 & 1 \\\\\n      1 & 1\n    \\end{bmatrix}\n  \\begin{bmatrix}\n    x \\\\\n    y\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    0 \\\\\n    0\n  \\end{bmatrix}\n\\end{align*}\n\\]\nSolving this system of equations gives us the eigenvectors corresponding to \\(\\lambda_1 = 2\\): \\(\\mathbf{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\)\nSimilarly, for \\(\\lambda_2 = 4\\), we solve the equation \\((\\mathbf A - 4\\mathbf I)\\mathbf{v}\\) = \\(\\mathbf{0}\\) to get the eigenvectors corresponding to \\(\\lambda_2 = 4\\): \\(\\mathbf{v_2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/03.basic_special_matrix.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/03.basic_special_matrix.html",
    "title": "Basics (3) - Special Matrices",
    "section": "",
    "text": "A square matrix \\(\\mathbf{A}\\) is a matrix with the same number of rows and columns, i.e., \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix.\nFor example, the following is a \\(3 \\times 3\\) square matrix:\n\\[\n\\mathbf A= \\begin{bmatrix}\n1 & 4 & 7\\\\\n2 & 5 & 8\\\\\n3 & 6 & 9\n\\end{bmatrix}\n\\]\n\n\nLet \\(\\mathbf{A}\\) be an \\(n\\times n\\) square matrix. Then, the following properties hold:\n\n\\(\\mathbf{A}\\) is invertible if and only if \\(\\text{det}(\\mathbf{A}) \\neq 0\\).\nThe trace of \\(\\mathbf{A}\\) is defined as \\(\\text{tr}(\\mathbf{A}) = \\sum_{i=1}^n a_{ii}\\), where \\(a_{ii}\\) is the \\(i\\)th diagonal element of \\(\\mathbf{A}\\).\nIf \\(\\mathbf{A}\\) is symmetric, then it has \\(n\\) real eigenvalues and an orthonormal set of eigenvectors.\nIf \\(\\mathbf{A}\\) is diagonalizable, then \\(\\mathbf{A}\\) can be written as \\(\\mathbf{A} = \\mathbf{PDP}^{-1}\\), where \\(\\mathbf{P}\\) is the matrix whose columns are the eigenvectors of \\(\\mathbf{A}\\), and \\(\\mathbf{D}\\) is the diagonal matrix whose diagonal elements are the corresponding eigenvalues.\nThe transpose of \\(\\mathbf{A}\\), denoted \\(\\mathbf{A}^\\top\\), is obtained by reflecting \\(\\mathbf{A}\\) across its main diagonal. That is, \\((\\mathbf{A}^\\top)_{ij} = a_{ji}\\).\n\nHere’s an example of a \\(3 \\times 3\\) matrix:\n\\[\n\\mathbf A= \\begin{bmatrix}\n1 & 4 & 7\\\\\n2 & 5 & 8\\\\\n3 & 6 & 9\n\\end{bmatrix}\n\\]\nWith this matrix, we can see that \\(\\text{det}(\\mathbf{A}) = 0\\), so \\(\\mathbf{A}\\) is not invertible. The trace of \\(\\mathbf{A}\\) is \\(\\text{tr}(\\mathbf{A}) = 1 + 5 + 9 = 15\\). Since \\(\\mathbf{A}\\) is not symmetric, we cannot say that it has real eigenvalues and an orthonormal set of eigenvectors. However, we can check that \\(\\mathbf{A}\\) is diagonalizable, and we can find that \\(\\mathbf{A} = \\mathbf{PDP}^{-1}\\) with\n\\[\n\\begin{equation*}\nP=\\begin{pmatrix}\n-0.8252 & -0.2886 & 0.4848\\\\\n-0.3779 & -0.7551 & -0.5375\\\\\n0.2185 & -0.5800 & 0.7830\n\\end{pmatrix}, \\text{ } D=\\begin{pmatrix}\n16.1168 & 0 & 0\\\\\n0 & -1.1168 & 0\\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\end{equation*}\n\\]\n\n\n\n\nA diagonal matrix is a square matrix in which all the off-diagonal elements are zero. The diagonal elements can be any scalar value.\n\\[\n\\begin{equation*}\n\\mathbf{D} = \\begin{pmatrix}\nd_{1} & 0 & 0 & \\cdots & 0 \\\\\n0 & d_{2} & 0 & \\cdots & 0 \\\\\n0 & 0 & d_{3} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & d_{n}\n\\end{pmatrix}\n\\end{equation*}\n\\]\nHere, \\(\\mathbf{D}\\) is an \\(n \\times n\\) diagonal matrix with diagonal elements \\(d_1, d_2, \\ldots, d_n\\). An example of a \\(3 \\times 3\\) diagonal matrix is:\n\\[\n\\begin{equation*}\n\\mathbf{D} = \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & 4\n\\end{pmatrix}\n\\end{equation*}\n\\]\n\n\nA diagonal matrix is a square matrix in which all the off-diagonal elements are zero, i.e., \\(a_{ij} = 0\\) for \\(i \\neq j\\). Some properties of a diagonal matrix are:\n\nFor two diagnoal matrices \\(\\mathbf D\\) and \\(\\mathbf E\\), \\[\n\\begin{equation*}\n\\mathbf{DE}=\n\\begin{pmatrix}\nd_{1}e_{1} & 0 & 0 & \\cdots & 0 \\\\\n0 & d_{2}e_{2} & 0 & \\cdots & 0 \\\\\n0 & 0 & d_{3}e_{3} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & d_{n}e_{n}\n\\end{pmatrix}\n\\end{equation*}\n\\]\nThe determinant of a diagonal matrix is the product of its diagonal entries.\nThe trace of a diagonal matrix is the sum of its diagonal entries.\nThe inverse of a non-singular diagonal matrix is a diagonal matrix with the reciprocal of its diagonal entries as its diagonal entries.\n\n\n\n\n\nAn identity matrix is a square matrix in which all the diagonal elements are equal to \\(1\\) and all the off-diagonal elements are equal to 0. The notation for an identity matrix of size \\(n\\) is \\(\\mathbf{I}_n\\).\n\n\nExample of a \\(3\\times 3\\) identity matrix, \\(\\mathbf{I}_3\\): \\[\n\\begin{equation}\n\\mathbf{I}_3 =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\end{equation}\n\\]\n\n\n\nSome properties of an identity matrix include:\n\nMultiplying any matrix by an identity matrix results in the same matrix: \\(\\mathbf{A} \\mathbf{I} = \\mathbf{I} \\mathbf{A} = \\mathbf{A}\\).\nThe product of any matrix and its corresponding inverse is an identity matrix: \\(\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}\\).\nThe determinant of an identity matrix is 1: \\(\\det(\\mathbf{I}) = 1\\).\nAn identity matrix is symmetric: \\(\\mathbf{I} = \\mathbf{I}^T\\).\n\n\n\n\n\nA symmetric matrix is a square matrix that is equal to its own transpose, i.e., \\(\\mathbf{A} = \\mathbf{A}^T\\). Let \\(\\mathbf{A}\\) be an \\(n \\times n\\) matrix, then \\(\\mathbf{A}\\) is symmetric if and only if \\(a_{ij} = a_{ji}\\) for all \\(i\\) and \\(j\\) such that \\(1 \\le i\\), \\(j \\le n\\).\nHere’s an example of a symmetric matrix: \\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 5 \\\\\n3 & 5 & 6\n\\end{bmatrix}\n\\]\n\n\nA symmetric matrix is a square matrix that is equal to its own transpose. Some properties of symmetric matrices include:\n\nThe diagonal entries are real numbers.\nThe matrix is diagonalizable, meaning it can be expressed as a product of diagonal and orthogonal matrices.\nThe eigenvalues of a symmetric matrix are real numbers.\nThe eigenvectors corresponding to different eigenvalues are orthogonal.\nThe sum and difference of two symmetric matrices is also symmetric.\n\n\n\n\n\nAn idempotent matrix is a square matrix that when multiplied by itself yields itself. In other words, an idempotent matrix \\(\\mathbf{P}\\) satisfies \\(\\mathbf{P}^2 = \\mathbf{P}\\).\nAn example of an idempotent matrix is:\n\\[\n\\begin{equation}\n\\mathbf{P} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\end{equation}\n\\]\n\n\n\n\\(\\mathbf{A}^2 = \\mathbf A\\).\nIf \\(\\mathbf{A}\\) is an idempotent matrix, \\(\\mathbf{I_n}-\\mathbf{A}\\) is also an idempotent matrix. \\[\n\\begin{aligned}\n(\\mathbf{I_n}-\\mathbf{A})^2&=(\\mathbf{I_n}-\\mathbf{A})(\\mathbf{I_n}-\\mathbf{A})\\\\\n&=\\mathbf{I_n}\\mathbf{I_n}-\\mathbf{I_n}\\mathbf{A}-\\mathbf{A}\\mathbf{I_n}+\\mathbf{A}\\mathbf{A}\\\\\n&=\\mathbf{I_n}\\mathbf{I_n}-\\mathbf{A}-\\mathbf{A}+\\mathbf{A} \\quad \\because \\text{A is idempotent}\\\\\n&=\\mathbf{I_n}\\mathbf{I_n}-\\mathbf{A}\\\\\n&=\\mathbf{I_n}-\\mathbf{A}\n\\end{aligned}\n\\]\nThe determinant of \\(\\mathbf{A}\\) is either 0 or 1.\nIf \\(\\mathbf{A}\\) is symmetric, \\(\\mathbf{A}\\) is idempotent if only if the eigenvalue of \\(\\mathbf{A}\\) is either \\(0\\) or \\(1\\).\nThe rank of \\(\\mathbf A\\) is equal to the trace of \\(\\mathbf A\\), which is the sum of the diagonal elements of \\(\\mathbf A\\).\n\n\n\n\n\nThe ones matrix, denoted as \\(\\mathbf{J}\\), is a matrix in which every entry is equal to 1.\n\n\n\n\n\n\nWarning\n\n\n\nThe letter \\(\\mathbf{J}\\) is not related to the jacobian matrix at all. The letter \\(\\mathbf{J}\\) of the ones matrix is used for convention.\n\n\nHere is an example of a \\(3\\times 3\\) ones matrix:\n\\[\n\\mathbf{J} =\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}\n\\]\n\n\n\n\\(\\mathbf J\\) is a square matrix\n\\(\\mathbf J\\) is symmetric\n\\(\\mathbf J\\) has rank 1\nTrace: The trace of the \\(\\mathbf J\\) matrix is equal to the dimension of the matrix. For the \\(\\mathbf J\\) matrix, it is equal to the number of rows or columns in the matrix.\nMatrix multiplication: When multiplied by any matrix \\(\\mathbf{A}\\), the \\(\\mathbf{J}\\) matrix results in a matrix where each row (or column, depending on the multiplication order) is the sum of the elements of the corresponding row (or column) of \\(\\mathbf{A}\\).\n\nIn other words, \\(\\mathbf{J}\\) multiplied by a matrix \\(\\mathbf{A}\\) performs a row (or column) summation operation on \\(\\mathbf{A}\\).\nIf A is any \\(n \\times n\\) matrix, then \\(\\mathbf{AJ} = \\mathbf{JA} = \\operatorname{trace}(\\mathbf{A})\\mathbf{J}\\), where \\(\\operatorname{trace}(\\mathbf{A})\\) is the sum of the diagonal elements of \\(\\mathbf{A}\\).\n\n\\(\\mathbf J_{m\\times n}\\) can be represented as the product of two vectors, \\(\\mathbf 1_{m}\\), \\(\\mathbf 1_{n}\\), i.e., \\(\\mathbf J_{m\\times n} = \\mathbf 1_{m} \\mathbf 1_{n}^T\\) where \\(m=n\\)\nEigenvalues and eigenvectors: The \\(\\mathbf J\\) matrix has one eigenvalue equal to the dimension of the matrix, with the corresponding eigenvector being a vector of all 1’s.\nInverse: The \\(\\mathbf J\\) matrix is a special case where it does not have an inverse, as all its rows (or columns) are linearly dependent.\n\n\n\n\n\ncalculate a sum using a \\(\\mathbf 1_n\\) vector for \\(\\mathbf x_n\\): \\[\n\\mathbf 1^T\\mathbf x =\\sum_{i=1}^{n}1\\times x_i=x_1+x_2+\\dots+x_n\n\\]\ncalculate a mean using a \\(\\mathbf 1_n\\) vector for \\(\\mathbf x_n\\):\n\n\\[\n\\bar{x}=\\frac{1}{n}\\mathbf 1^T\\mathbf x =\\frac{1}{n}\\sum_{i=1}^{n}1\\times x_i=\\frac{1}{n}(x_1+x_2+\\dots+x_n)\n\\]\n\ncalculate \\(n\\) column sums of a dataset using a \\(\\mathbf 1_m\\) vector for \\(\\mathbf X_{m\\times n}\\): \\[\n\\bar{\\mathbf x}=\\frac{1}{m}\\mathbf X^T\\mathbf 1_m\n\\] \\[\n\\begin{aligned}\n\\bar{\\mathbf x}&=\\frac{1}{m}\\mathbf X^T\\mathbf 1 \\\\\n&=\\frac{1}{m}\n\\begin{bmatrix}\nx_{11} & x_{21} & \\cdots & x_{m1} \\\\\nx_{12} & x_{22} & \\cdots & x_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{1n} & x_{2n} & \\cdots & x_{mn}\n\\end{bmatrix}\n\\begin{bmatrix}\n1_1 \\\\ 1_2 \\\\ \\vdots \\\\ 1_m\n\\end{bmatrix}\\\\\n&=\\frac{1}{m}\n\\begin{bmatrix}\nx_{11} + x_{21} + \\cdots + x_{m1} \\\\\nx_{12} + x_{22} + \\cdots + x_{m2} \\\\\n\\vdots  \\\\\nx_{1n} + x_{2n} + \\cdots + x_{mn}\n\\end{bmatrix} \\\\\n&=\\frac{1}{m}\n\\begin{bmatrix}\n\\sum_{i=1}^{m }x_{i1} \\\\\n\\sum_{i=1}^{m }x_{i2} \\\\\n\\vdots  \\\\\n\\sum_{i=1}^{m }x_{jn}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n\\bar{x}_{1} \\\\\n\\bar{x}_{2} \\\\\n\\vdots  \\\\\n\\bar{x}_{n}\n\\end{bmatrix} \\\\\n&=\\bar{\\mathbf x}\n\\end{aligned}\n\\]\n\n\n\n\n\nA centering matrix is a square matrix that is used in multivariate statistical analysis to center data by subtracting the mean of each variable from each observation. The resulting matrix is called the centered data matrix with the mean equal to \\(0\\). The centering matrix is defined as:\n\\[\n\\begin{equation}\n  \\mathbf C = \\mathbf I - \\frac{1}{m}\\mathbf J\n\\end{equation}\n\\]\nwhere \\(\\mathbf I\\) is the identity matrix, \\(\\mathbf J\\) is a matrix of ones, and \\(m\\) is the number of observations.\n\n\nHere is an example of a centering matrix of size \\(3 \\times 3\\):\n\\[\n\\begin{equation*}\n\\mathbf C = \\frac{1}{3} \\begin{bmatrix}\n2 & -1 & -1 \\\\\n-1 & 2 & -1 \\\\\n-1 & -1 & 2\n\\end{bmatrix}\n\\end{equation*}\n\\]\n\n\n\nThe centering matrix is often used in multivariate statistical analysis, such as principal component analysis, to transform the data into a new coordinate system where the variance of each variable is equal to its eigenvalue.\n\nA centering matrix is a square matrix.\nA centering matrix is a symmetric matrix.\nA centering matrix is a idempotent matrix.\nThe diagonal elements of a centering matrix are all equal and are given by \\(\\frac{1}{m}\\), where \\(n\\) is the size of the matrix.\nThe off-diagonal elements of a centering matrix are all equal and are given by \\(-\\frac{1}{m}\\).\nMultiplying a matrix \\(\\mathbf A\\) on the left by a centering matrix \\(\\mathbf C\\), \\(\\mathbf{CA}\\) is equivalent to subtracting the mean of the columns of \\(\\mathbf A\\) from each column of \\(\\mathbf A\\).\nMultiplying a matrix \\(\\mathbf A\\) on the right by a centering matrix \\(\\mathbf C\\), \\(\\mathbf{AC}\\) is equivalent to subtracting the mean of the rows of \\(\\mathbf A\\) from each row of \\(\\mathbf A\\).\n\n\n\n\n\nfind a centered matrix, \\(\\tilde{\\mathbf X}\\) using a \\(\\mathbf 1_m\\) vector for \\(\\mathbf X_{m\\times n}\\):\n\n\\[\n\\mathbf C = \\mathbf I - \\frac{1}{m}\\mathbf J\n\\] \\[\n\\begin{aligned}\n\\mathbf X&=\n\\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1n} \\\\\nx_{21} & x_{22} & \\cdots & x_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{m1} & x_{m2} & \\cdots & x_{mn}\n\\end{bmatrix} \\text{ } \\\\\n\\mathbf {1_m\\bar{x}^T}&=\n\\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\vdots \\\\\n1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bar{x}_{1} & \\bar{x}_{2} & \\dots & \\bar{x}_{n}\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\n\\bar{x}_{1} & \\bar{x}_{2} & \\dots & \\bar{x}_{n}\\\\\n\\bar{x}_{1} & \\bar{x}_{2} & \\dots & \\bar{x}_{n}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\bar{x}_{1} & \\bar{x}_{2} & \\dots & \\bar{x}_{n}\n\\end{bmatrix}_{m\\times n}\\\\\n\\\\\n\\tilde{\\mathbf X}&=\\mathbf X -{\\mathbf{1}}_{m}\\bar{\\mathbf x}^T\\\\\n&=\\mathbf X -\\mathbf{1}_{m}(\\frac{1}{m}\\mathbf X^T \\mathbf{1}_m)^T\\\\\n&=\\mathbf X -\\mathbf{1}_{m}\\mathbf{1}_m^T\\frac{1}{m}\\mathbf X\\\\\n&=(\\mathbf I -\\frac{1}{m}\\mathbf J)\\mathbf X\\\\\n&=\\mathbf C \\mathbf X\\\\\n\\end{aligned}\n\\]\n\n\n\n\nThe covariance matrix of a dataset matrix \\(\\mathbf{X}\\) with \\(m\\) observations and \\(n\\) variables is a symmetric \\(n \\times n\\) matrix given by:\n\n\nFor two random variables, \\(X_1,X_2\\), if there are \\(m\\) of the observed samples or realized values, we can represent the sample data as:\n\\[\n(x_{11},x_{12}),(x_{21},x_{22}),\\dots,(x_{m1},x_{m2})\n\\]\nThen, the sample variance between the two random variables are represented as:\n\\[\ns_{x_1x_2} = \\frac{\\sum_{i=1}^{m}(x_{i1}-\\bar{x_1})(x_{i2}-\\bar{x_2})}{m-1}\n\\]\n\n\n\n\n\n\nSample Variance\n\n\n\nFor each variate, \\(X_j\\), \\(j=1,\\dots, n\\)\n\\[\ns^2_{x}=s_{xx} = \\frac{\\sum_{i=1}^{m}(x_{ij}-\\bar{x})^2}{m-1}\n\\]\nFor two random variables, \\(X_1\\), \\(X_2\\), \\[\ns_{x_1x_2} = \\frac{\\sum_{i=1}^{m}(x_{i1}-\\bar{x_1})(x_{i2}-\\bar{x_2})}{m-1}\n\\]\nOften for the notation of sample variance, the squared power is added for the sample variance with one random variable, but it is not for the covariance between two random variables.\n\n\nFor two random variables ,\\(X_1,X_2\\),\n\\[\n\\begin{aligned}\n  \\mathbf{S}\n  &=\\begin{bmatrix}\n    s_{11}&s_{12}\\\\\n    s_{21}&s_{22}\n  \\end{bmatrix}\\\\\n  &=\\begin{bmatrix}\n    s_{x_1x_1}&s_{x_1x_2}\\\\\n    s_{x_2x_1}&s_{x_2x_2}\n  \\end{bmatrix}\\\\\n  &=\\frac{1}{n-1}\n  \\begin{bmatrix}\n    \\sum_{i=1}^{m}(x_{i1}-\\bar{x}_1)(x_{i1}-\\bar{x}_1)&\\sum_{i=1}^{m}(x_{i1}-\\bar{x}_1)(x_{i2}-\\bar{x}_2)\\\\\n    \\sum_{i=1}^{m}(x_{i2}-\\bar{x}_2)(x_{i1}-\\bar{x}_1)&\\sum_{i=1}^{m}(x_{i2}-\\bar{x}_2)(x_{i2}-\\bar{x}_2)\n  \\end{bmatrix}\n\\end{aligned}\n\\]\nFor \\(n\\) of random variables, \\(X_1,\\dots,X_n\\), if we observed the sample vectors with \\(m\\) observations, the sample covariance matrix can be represented as:\n\\[\n\\mathbf{S}=\\begin{bmatrix}\ns_{11}&s_{12}&\\dots&s_{1n}\\\\\ns_{21}&s_{22}&\\dots&s_{2n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\ns_{n1}&s_{n2}&\\dots&s_{nn}\n\\end{bmatrix}\n\\]\nwhere \\(s_{jk} = \\frac{\\sum_{i=1}^{m}(x_{ij}-\\bar{x_j})(x_{ik}-\\bar{x_k})}{m-1}\\). The diagonal entries are the sample variances of each random variable, and the off-diagonal entires are the sample covariance between two different random variables.\n\n\n\nThe complicating algebraic notation can be represented as the simpler form in linear algebra. To do so, let a sample vector \\(\\mathbf x =\\begin{bmatrix}x_{1}\\\\x_{2}\\\\ \\vdots\\\\x_{n}\\end{bmatrix}\\). In othe words, a record, a row, or the observations across \\(n\\) variables in a dataset:\n\\[\n\\mathbf{X}=\n\\begin{bmatrix}\n\\mathbf{x}_{1}^T\\\\\n\\mathbf{x}_{2}^T\\\\\n\\vdots\\\\\n\\mathbf{x}_{m}^T\n\\end{bmatrix}\n=\\begin{bmatrix}\nx_{11}&x_{12}&\\dots&x_{1n}\\\\\nx_{21}&x_{22}&\\dots&x_{2n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nx_{m1}&x_{m2}&\\dots&x_{mn}\n\\end{bmatrix}\n\\]\n\\[\n\\begin{aligned}\n\\mathbf S\n&= \\operatorname{Cov}(\\mathbf{X}) \\\\\n&= \\frac{1}{m-1}\\sum_{i=1}^{m}(\\mathbf x_i - \\bar{\\mathbf x}_i)(\\mathbf x_i - \\bar{\\mathbf x}_i)^T\\\\\n&= \\frac{1}{m-1}\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\\\\n&= \\frac{1}{m-1}(\\mathbf{CX})^T(\\mathbf{CX})\\\\\n&= \\frac{1}{m-1}\\mathbf{X}^T\\mathbf{C}^T\\mathbf{CX}\\\\\n&= \\frac{1}{m-1}\\mathbf{X}^T\\mathbf{C}\\mathbf{C}\\mathbf{X}\\quad (\\because \\mathbf{C}\\text{ is symmetric})\\\\\n&= \\frac{1}{m-1}\\mathbf{X}^T\\mathbf{C}\\mathbf{X}\\quad (\\because \\mathbf{C}\\text{ is an idempotent matrix})\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{X}\\) is the dataset matrix, \\(\\mathbf{x_i}=\\begin{bmatrix}x_{i1}\\\\x_{i2}\\\\ \\vdots\\\\x_{in}\\end{bmatrix}\\) is the observations of each variable along the columns, \\(\\bar{\\mathbf{x}}_i=\\begin{bmatrix}\\bar{x}_{1}\\\\\\bar{x}_{2}\\\\ \\vdots\\\\\\bar{x}_{n}\\end{bmatrix}\\) and \\((\\mathbf{x}_i - \\bar{\\mathbf{x}}_i)^T=\\tilde{\\mathbf{X}}\\) is the transpose of the centered dataset matrix.\nThe size of \\(\\mathbf S\\) is \\((1 \\times n)^T(1 \\times n)=(n \\times 1)(1 \\times n)=(n \\times n)\\)\n\n\n\n\n\n\nTip\n\n\n\nFor the \\(i\\) th observation of \\(m\\) observations and the \\(j\\) th variable of \\(n\\) variables in a dataset, \\(\\mathbf{X}\\), the outer product of \\((\\mathbf x_i - \\bar{\\mathbf x}_i)(\\mathbf x_i - \\bar{\\mathbf x}_i)^T\\) generates the following \\(n \\times n\\) matrix.\n\\[\n\\begin{aligned}\n(\\mathbf x_i - \\bar{\\mathbf x}_i)(\\mathbf x_i - \\bar{\\mathbf x}_i)^T\n&= \\begin{bmatrix}\nx_{i1} -\\bar{x}_{i1}\\\\ x_{i2} -\\bar{x}_{i2}\\\\ \\vdots \\\\ x_{in} -\\bar{x}_{in}\n\\end{bmatrix}\\begin{bmatrix}\nx_{i1} -\\bar{x}_{i1}&x_{i2} -\\bar{x}_{i2}&\\dots&x_{in} -\\bar{x}_{in}\n\\end{bmatrix} \\\\\n&=\\begin{bmatrix}\n(x_{i1}-\\bar{x}_{i1})(x_{i1}-\\bar{x}_{i1}) & (x_{i1}-\\bar{x}_{i1})(x_{i2}-\\bar{x}_{i2}) & \\dots & (x_{i1}-\\bar{x}_{i1})(x_{in}-\\bar{x}_{in})\\\\\n(x_{i2}-\\bar{x}_{i2})(x_{i1}-\\bar{x}_{i1}) & (x_{i2}-\\bar{x}_{i2})(x_{i2}-\\bar{x}_{i2}) & \\dots & (x_{i2}-\\bar{x}_{i2})(x_{in}-\\bar{x}_{in})\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n(x_{in}-\\bar{x}_{in})(x_{i1}-\\bar{x}_{i1})&(x_{in}-\\bar{x}_{in})(x_{i2}-\\bar{x}_{i2})&\\dots&(x_{in}-\\bar{x}_{in})(x_{in}-\\bar{x}_{in})\n\\end{bmatrix}_{n\\times n}\n\\end{aligned}\n\\]\nThe \\(n \\times n\\) matrix right above is gnerated by \\(m\\) times for the \\(m\\) observations of \\(\\mathbf{X}_{m \\times n}\\), the \\(\\operatorname{Cov}(\\mathbf X)\\) is the sum of the \\(m\\) of \\(n \\times n\\) matrices divided by \\(\\frac{1}{m-1}\\). In other words,\n\\[\n\\frac{1}{m-1}\\sum_{i=1}^{m}(\\mathbf x_i - \\bar{\\mathbf x}_i)(\\mathbf x_i - \\bar{\\mathbf x}_i)^T\n\\]\n\n\n\n\n\nExample 1\nLet’s say we have a dataset with three variables, \\(\\mathbf{x}_1, \\mathbf{x}_2, \\text{ and } \\mathbf{x}_3,\\) with \\(m\\) observations. The dataset can be represented as a matrix \\(\\mathbf{X}\\) with dimensions \\(m \\times 3\\), where each row represents an observation and each column represents a variable. The covariance matrix, \\(\\operatorname{Cov}(\\mathbf{X})\\), of the dataset can be computed as follows:\n\\[\n\\begin{aligned}\n\\operatorname{Cov}(\\mathbf{X}) &= \\frac{1}{m-1}\\sum_{i=1}^{m}(\\mathbf x_i - \\bar{\\mathbf x}_i)^T(\\mathbf x_i - \\bar{\\mathbf x}_i)\\\\\n&=\\frac{\\tilde{\\mathbf X}^T\\tilde{\\mathbf X}}{m-1}\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{X}\\) is the dataset matrix, \\(\\bar{x}\\) is the mean of each variable computed along the rows, and \\((\\mathbf{x_i} - \\bar{\\mathbf{x_i}})^T\\) is the transpose of the centered dataset matrix. The covariance matrix \\(\\operatorname{Cov}(\\mathbf{X})\\) will be a \\(3 \\times 3\\) matrix, with the \\((i, j)\\) th entry representing the covariance between the \\(i\\) th and \\(j\\) th variables in the dataset.\nExample 2\nThe second example is to calculate \\(\\operatorname{Cov}(\\mathbf{X})\\) where \\(\\mathbf{X}\\) is the mtcars dataset.\n\n\nCode\nX<-as.matrix(mtcars)\nx_bar_vec<-colMeans(mtcars)\n\nas.numeric(mtcars[1,1:5]-x_bar_vec)%*%t(as.numeric(mtcars[1,1:5]-x_bar_vec)) # for 1th row\n\n\n            [,1]        [,2]       [,3]        [,4]         [,5]\n[1,]   0.8269629 -0.17050781  -64.31271  -33.362695   0.27593848\n[2,]  -0.1705078  0.03515625   13.26035    6.878906  -0.05689453\n[3,] -64.3127051 13.26035156 5001.58360 2594.608789 -21.45966895\n[4,] -33.3626953  6.87890625 2594.60879 1345.972656 -11.13236328\n[5,]   0.2759385 -0.05689453  -21.45967  -11.132363   0.09207432\n\n\nCode\nas.numeric(mtcars[2,1:5]-x_bar_vec)%*%t(as.numeric(mtcars[2,1:5]-x_bar_vec)) # for 2nd row\n\n\n            [,1]        [,2]       [,3]        [,4]         [,5]\n[1,]   0.8269629 -0.17050781  -64.31271  -33.362695   0.27593848\n[2,]  -0.1705078  0.03515625   13.26035    6.878906  -0.05689453\n[3,] -64.3127051 13.26035156 5001.58360 2594.608789 -21.45966895\n[4,] -33.3626953  6.87890625 2594.60879 1345.972656 -11.13236328\n[5,]   0.2759385 -0.05689453  -21.45967  -11.132363   0.09207432\n\n\nCode\nX_sweeped<-sweep(X,MARGIN=2,STAT=x_bar_vec,FUN='-')\nresult<-matrix(0,ncol=ncol(mtcars),nrow=ncol(mtcars))\nfor (i in 1:nrow(mtcars)){\n  result<-result+X_sweeped[i,]%*%t(X_sweeped[i,])\n}\nresult/(nrow(mtcars)-1)\n\n\n              mpg         cyl        disp          hp         drat          wt\n [1,]   36.324103  -9.1723790  -633.09721 -320.732056   2.19506351  -5.1166847\n [2,]   -9.172379   3.1895161   199.66028  101.931452  -0.66836694   1.3673710\n [3,] -633.097208 199.6602823 15360.79983 6721.158669 -47.06401915 107.6842040\n [4,] -320.732056 101.9314516  6721.15867 4700.866935 -16.45110887  44.1926613\n [5,]    2.195064  -0.6683669   -47.06402  -16.451109   0.28588135  -0.3727207\n [6,]   -5.116685   1.3673710   107.68420   44.192661  -0.37272073   0.9573790\n [7,]    4.509149  -1.8868548   -96.05168  -86.770081   0.08714073  -0.3054816\n [8,]    2.017137  -0.7298387   -44.37762  -24.987903   0.11864919  -0.2736613\n [9,]    1.803931  -0.4657258   -36.56401   -8.320565   0.19015121  -0.3381048\n[10,]    2.135685  -0.6491935   -50.80262   -6.358871   0.27598790  -0.4210806\n[11,]   -5.363105   1.5201613    79.06875   83.036290  -0.07840726   0.6757903\n              qsec           vs           am        gear        carb\n [1,]   4.50914919   2.01713710   1.80393145   2.1356855 -5.36310484\n [2,]  -1.88685484  -0.72983871  -0.46572581  -0.6491935  1.52016129\n [3,] -96.05168145 -44.37762097 -36.56401210 -50.8026210 79.06875000\n [4,] -86.77008065 -24.98790323  -8.32056452  -6.3588710 83.03629032\n [5,]   0.08714073   0.11864919   0.19015121   0.2759879 -0.07840726\n [6,]  -0.30548161  -0.27366129  -0.33810484  -0.4210806  0.67579032\n [7,]   3.19316613   0.67056452  -0.20495968  -0.2804032 -1.89411290\n [8,]   0.67056452   0.25403226   0.04233871   0.0766129 -0.46370968\n [9,]  -0.20495968   0.04233871   0.24899194   0.2923387  0.04637097\n[10,]  -0.28040323   0.07661290   0.29233871   0.5443548  0.32661290\n[11,]  -1.89411290  -0.46370968   0.04637097   0.3266129  2.60887097\n\n\nCode\n# J matrix\nm<-nrow(mtcars)\nJ<-(1/m)*matrix(1,ncol=m,nrow=m)\n# Centering matrix\nC<-diag(m)-J\n# Covariance\n(1/(m-1))*t(X)%*%C%*%X\n\n\n             mpg         cyl        disp          hp         drat          wt\nmpg    36.324103  -9.1723790  -633.09721 -320.732056   2.19506351  -5.1166847\ncyl    -9.172379   3.1895161   199.66028  101.931452  -0.66836694   1.3673710\ndisp -633.097208 199.6602823 15360.79983 6721.158669 -47.06401915 107.6842040\nhp   -320.732056 101.9314516  6721.15867 4700.866935 -16.45110887  44.1926613\ndrat    2.195064  -0.6683669   -47.06402  -16.451109   0.28588135  -0.3727207\nwt     -5.116685   1.3673710   107.68420   44.192661  -0.37272073   0.9573790\nqsec    4.509149  -1.8868548   -96.05168  -86.770081   0.08714073  -0.3054816\nvs      2.017137  -0.7298387   -44.37762  -24.987903   0.11864919  -0.2736613\nam      1.803931  -0.4657258   -36.56401   -8.320565   0.19015121  -0.3381048\ngear    2.135685  -0.6491935   -50.80262   -6.358871   0.27598790  -0.4210806\ncarb   -5.363105   1.5201613    79.06875   83.036290  -0.07840726   0.6757903\n             qsec           vs           am        gear        carb\nmpg    4.50914919   2.01713710   1.80393145   2.1356855 -5.36310484\ncyl   -1.88685484  -0.72983871  -0.46572581  -0.6491935  1.52016129\ndisp -96.05168145 -44.37762097 -36.56401210 -50.8026210 79.06875000\nhp   -86.77008065 -24.98790323  -8.32056452  -6.3588710 83.03629032\ndrat   0.08714073   0.11864919   0.19015121   0.2759879 -0.07840726\nwt    -0.30548161  -0.27366129  -0.33810484  -0.4210806  0.67579032\nqsec   3.19316613   0.67056452  -0.20495968  -0.2804032 -1.89411290\nvs     0.67056452   0.25403226   0.04233871   0.0766129 -0.46370968\nam    -0.20495968   0.04233871   0.24899194   0.2923387  0.04637097\ngear  -0.28040323   0.07661290   0.29233871   0.5443548  0.32661290\ncarb  -1.89411290  -0.46370968   0.04637097   0.3266129  2.60887097\n\n\nCode\nvar(X) #=cov(X)\n\n\n             mpg         cyl        disp          hp         drat          wt\nmpg    36.324103  -9.1723790  -633.09721 -320.732056   2.19506351  -5.1166847\ncyl    -9.172379   3.1895161   199.66028  101.931452  -0.66836694   1.3673710\ndisp -633.097208 199.6602823 15360.79983 6721.158669 -47.06401915 107.6842040\nhp   -320.732056 101.9314516  6721.15867 4700.866935 -16.45110887  44.1926613\ndrat    2.195064  -0.6683669   -47.06402  -16.451109   0.28588135  -0.3727207\nwt     -5.116685   1.3673710   107.68420   44.192661  -0.37272073   0.9573790\nqsec    4.509149  -1.8868548   -96.05168  -86.770081   0.08714073  -0.3054816\nvs      2.017137  -0.7298387   -44.37762  -24.987903   0.11864919  -0.2736613\nam      1.803931  -0.4657258   -36.56401   -8.320565   0.19015121  -0.3381048\ngear    2.135685  -0.6491935   -50.80262   -6.358871   0.27598790  -0.4210806\ncarb   -5.363105   1.5201613    79.06875   83.036290  -0.07840726   0.6757903\n             qsec           vs           am        gear        carb\nmpg    4.50914919   2.01713710   1.80393145   2.1356855 -5.36310484\ncyl   -1.88685484  -0.72983871  -0.46572581  -0.6491935  1.52016129\ndisp -96.05168145 -44.37762097 -36.56401210 -50.8026210 79.06875000\nhp   -86.77008065 -24.98790323  -8.32056452  -6.3588710 83.03629032\ndrat   0.08714073   0.11864919   0.19015121   0.2759879 -0.07840726\nwt    -0.30548161  -0.27366129  -0.33810484  -0.4210806  0.67579032\nqsec   3.19316613   0.67056452  -0.20495968  -0.2804032 -1.89411290\nvs     0.67056452   0.25403226   0.04233871   0.0766129 -0.46370968\nam    -0.20495968   0.04233871   0.24899194   0.2923387  0.04637097\ngear  -0.28040323   0.07661290   0.29233871   0.5443548  0.32661290\ncarb  -1.89411290  -0.46370968   0.04637097   0.3266129  2.60887097\n\n\n\n\n\n\nSymmetry\nDiagonal Entries: The diagonal entries of the covariance matrix represent the variances of the individual variables.\nLinearity: The covariance matrix exhibits linearity in the sense that the covariance of a linear combination of variables can be expressed as a linear combination of their covariances. Mathematically, if \\(a\\) and \\(b\\) are constants and \\(X\\), \\(Y\\), and \\(Z\\) are random variables, then \\(\\operatorname{Cov}(aX + bY, Z) = a\\operatorname{Cov}(X, Z) + b\\operatorname{Cov}(Y, Z)\\).\nScale Invariance: The covariance between variables is invariant to changes in scale or units of measurement. For example, if variables are measured in different units or are on different scales, the covariance matrix will still be valid and informative for measuring the linear relationship between the variables.\nIndependence: If two variables \\(X_i\\) and \\(X_j\\) are independent, their covariance \\(\\operatorname{Cov}(X_j, X_k)\\) is zero.\nPositive Semidefiniteness: The covariance matrix is positive semidefinite, which means that all of its eigenvalues are non-negative. This property ensures that the covariance matrix is a valid variance-covariance matrix.\n\n\n\n\n\nA symmetric matrix \\(\\mathbf{A}\\) is positive definite if and only if the quadratic form \\(f(\\mathbf{x})=\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) is positive for all nonzero vectors \\(\\mathbf{x}\\).\nTo see why this is true, consider the eigenvalue decomposition of \\(\\mathbf{A}\\), which can be written as \\(\\mathbf{A} = \\mathbf{Q} \\Lambda \\mathbf{Q}^T\\), where \\(\\mathbf{Q}\\) is an orthogonal matrix and \\(\\Lambda\\) is a diagonal matrix containing the eigenvalues of \\(\\mathbf{A}\\). Then, for any nonzero vector \\(\\mathbf{x}\\),\n\\[\n\\begin{aligned}\n\\mathbf x^T \\mathbf A \\mathbf x&=\\mathbf x^T \\mathbf Q \\mathbf \\Lambda \\mathbf Q^T \\mathbf x\\\\\n&=(\\mathbf x^T \\mathbf Q)\\mathbf \\Lambda ( \\mathbf Q^T\\mathbf x)\\\\\n&=\\sum_{i=1}^{n} \\lambda_iy_i^2\n\\end{aligned}\n\\]\nwhere \\(y_i = (\\mathbf{x}^T \\mathbf{Q})_i\\) is the \\(i\\) th coordinate (i.e., a scalar value that represents the position of a point or a vector relative to a chosen basis) of \\(\\mathbf{x}^T \\mathbf{Q}\\) and \\(n\\) is the dimension of \\(\\mathbf{x}\\) and \\(\\mathbf{A}\\). Note that since \\(\\mathbf{Q}\\) is orthogonal, we have \\(\\mathbf{Q}^T \\mathbf{Q} = I\\), so \\(y_i = \\mathbf{q}_i^T \\mathbf{x}\\), where \\(\\mathbf{q}_i\\) is the \\(i\\) th column of \\(Q\\). Therefore, the quadratic form \\(f(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}\\) can be written in terms of the eigenvalues of \\(\\mathbf{A}\\) and the coordinates of \\(\\mathbf{x}\\) with respect to the eigenvectors of \\(\\mathbf{A}\\).\nSince \\(\\mathbf{A}\\) is positive definite, we have \\(\\lambda_i > 0\\) for all \\(i\\), and so \\(\\sum_{i=1}^n \\lambda_i y_i^2 > 0\\) for all nonzero vectors \\(\\mathbf{x}\\). Therefore, the quadratic form \\(f(\\mathbf{x})=\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) is positive for all nonzero vectors \\(\\mathbf{x}\\), which implies that \\(\\mathbf{A}\\) is positive definite.\nIn other words, the positive definiteness of a symmetric matrix \\(\\mathbf{A}\\) is equivalent to the positivity of the associated quadratic form \\(f(\\mathbf{x})=\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) for all nonzero vectors \\(\\mathbf{x}\\).\nTherefore, a symmetric matrix, \\(\\mathbf A\\) is said to be positive definite if all of its eigenvalues are positive or equivalently, a symmetric matrix, \\(\\mathbf A\\) is positive definite if left-multiplying and right-multiplying it by the same vector, \\(\\mathbf x\\) always gives a positive number if \\(\\mathbf x^T \\mathbf A \\mathbf x\\)\n\n\nProperties of a positive definite matrix:\n\nAll eigenvalues are positive.\nThe matrix is symmetric.\nAll principal submatrices have determinants that are positive.\n\nExample of a positive definite matrix:\n\\[\n\\begin{equation}\n\\mathbf{A} = \\begin{bmatrix}\n4 & -1 & 0 \\\\\n-1 & 5 & -1 \\\\\n0 & -1 & 2\n\\end{bmatrix}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/04.basic_tensor copy.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/04.basic_tensor copy.html",
    "title": "Basics (4) - Tensor",
    "section": "",
    "text": "A tensor is a mathematical object that generalizes vectors and matrices to higher dimensions. A tensor of order \\(n\\) is an object that can be represented by a multidimensional array of \\(n\\) indices. Each index can take on a range of values, which determine the dimensionality of the tensor along that axis.\nFor example, a rank-2 tensor (i.e., a matrix) can be represented as:\n\\[\n\\mathbf A_{ij} \\text{ , } i=1,\\dots m \\text{, } j=1,\\dots,n\n\\] where \\(\\mathbf A\\) is the tensor, \\(i\\) and \\(j\\) are the indices, and \\(m\\) and \\(n\\) are the dimensions of the tensor along each axis. The entries of the tensor are given by \\(a_{ij}\\).\nA rank-3 tensor can be represented as: \\[\n\\mathbf A_{ijk} \\text{ , } i=1,\\dots m \\text{, } j=1,\\dots,n\\text{, } k=1,\\dots,p\n\\] where \\(\\mathbf A\\) is the tensor, \\(i\\), \\(j\\), and \\(k\\) are the indices, and \\(m\\), \\(n\\), and \\(p\\) are the dimensions of the tensor along each axis. The entries of the tensor are given by \\(A_{ijk}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/04.basic_tensor copy.html#basic-tensor-operations",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/04.basic_tensor copy.html#basic-tensor-operations",
    "title": "Basics (4) - Tensor",
    "section": "2 Basic Tensor Operations",
    "text": "2 Basic Tensor Operations\n\n2.1"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/04.lineqr_equations.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/04.lineqr_equations.html",
    "title": "Linear Equations",
    "section": "",
    "text": "In the previous blog, we looked at some properties of a matrix such as scalar multiplication, vector addition, linear combination, coefficient matrix, matrix equation, dot products with rows combination of columns, see the previous blog: basic matrix operations.\nThis blog is going to focus on solving \\(n\\) equations in \\(n\\) unknowns variables or columns (for any \\(n\\)).\n\n\n\n\n\\[\n\\begin{align*}\nx + 2y - z &= 1 \\\\\n2x - y + 3z &= -2 \\\\\nx + 3y + z &= 3 \\\\\n\\end{align*}\n\\]\ncan be written in matrix form as: \\[\n\\begin{align*}\n\\begin{bmatrix}\n3 & 2 & -1 \\\\\n2 & -1 & 3 \\\\\n1 & 3 & 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\nz \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 \\\\\n-2 \\\\\n3 \\\\\n\\end{bmatrix}\n\\end{align*}\n\\]\nThe matrix form can be represented as :\n\\[\n\\begin{align*}\nx\n\\begin{bmatrix}\n3 \\\\\n2 \\\\\n1 \\\\\n\\end{bmatrix} +\ny\n\\begin{bmatrix}\n2 \\\\\n-1 \\\\\n3 \\\\\n\\end{bmatrix} +\nz\n\\begin{bmatrix}\n-1 \\\\\n3 \\\\\n1 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 \\\\\n-2 \\\\\n3 \\\\\n\\end{bmatrix}\n\\end{align*}\n\\]\n\n\nCode\n# Coefficient matrix of the system of equations\nA = np.array([[3, 2, -1],\n              [2, -1, 3],\n              [1, 3, 1]])\n\n# Right-hand side vector of the system of equations\nb = np.array([1, -2, 3])\n\n# Solve the system of equations\nx = np.linalg.solve(A, b)\n\n# Extract the solutions for x, y, and z\nx_val, y_val, z_val = x\n\n# Print the solution\nprint(\"Intersection point:\")\nprint(\"x =\", x_val)\nprint(\"y =\", y_val)\nprint(\"z =\", z_val)\n\n# Generate points on the planes\nx_plane1, y_plane1 = np.meshgrid(np.linspace(-2, 2, 10), np.linspace(-2, 2, 10))\nz_plane1 = (1 - 3 * x_plane1 - 2 * y_plane1) / -1\n\nx_plane2, z_plane2 = np.meshgrid(np.linspace(-2, 2, 10), np.linspace(-2, 2, 10))\ny_plane2 = (-2 - 2 * x_plane2 + 3 * z_plane2) / 1\n\ny_plane3, z_plane3 = np.meshgrid(np.linspace(-2, 2, 10), np.linspace(-2, 2, 10))\nx_plane3 = (1 - 3 * y_plane3 - z_plane3) / 2\n\n# Create 3D plot\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Plot the planes\nax.plot_surface(x_plane1, y_plane1, z_plane1, alpha=0.5)\nax.plot_surface(x_plane2, y_plane2, z_plane2, alpha=0.5)\nax.plot_surface(x_plane3, y_plane3, z_plane3, alpha=0.5)\n\n# Plot the intersection point\nax.scatter(x_val, y_val, z_val, color='red', label='Intersection Point')\n\n# Set labels and title\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.set_title('Intersection of Three Planes')\n\n# Show legend\nax.legend()\n\n# Show the plot\nplt.show()\n\n\nIntersection point:\nx = -0.42857142857142855\ny = 1.1428571428571428\nz = -8.881784197001253e-17\n\n\n\n\n\n\n\nCode\n# Define the vectors\nv1 = np.array([-3, 2, 1])\nv2 = np.array([-2, -1, 3])\nv3 = np.array([1, 3, 1])\n\n# Define the right-hand side vector\nrhs = np.array([1, -2, 3])\n\n# Create 3D plot\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Plot the vectors\nax.quiver(0, 0, 0, v1[0], v1[1], v1[2], color='red', label='v1')\nax.quiver(0, 0, 0, v2[0], v2[1], v2[2], color='green', label='v2')\nax.quiver(0, 0, 0, v3[0], v3[1], v3[2], color='blue', label='v3')\n\n# Plot the right-hand side vector\nax.quiver(0, 0, 0, rhs[0], rhs[1], rhs[2], color='black', label='b')\n\n# Set equal axes scales\nax.set_xlim([-3, 3])\nax.set_ylim([-3, 3])\nax.set_zlim([-3, 3])\n\n# Set labels and title\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.set_title('Vectors in 3D Space')\n\n# Show legend\nax.legend()\n\n# Set viewing angle\nax.view_init(elev=30, azim=120)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nThe elimination method, also known as Gaussian elimination, is a systematic way to solve systems of linear equations by transforming the augmented matrix of the system into row-echelon form or reduced row-echelon form using a sequence of elementary row operations.\n\nDefinition 1 The elimination method (Gaussian elimination) is a systematic way to solve a system of linear equations by performing a sequence of elementary row operations on the augmented matrix of the system, with the goal of transforming the augmented matrix into row-echelon form or reduced row-echelon form.\n\n\nDefinition 2 An augmented matrix is a matrix formed by appending the column vector \\(\\mathbf{b}\\) to the right of matrix \\(\\mathbf{A}\\) in the system of linear equations \\(\\mathbf{Ax}=\\mathbf{b}\\). More formally, the augmented matrix is defined as: \\[\n\\begin{align*}\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} & b_1 \\\\\na_{21} & a_{22} & \\cdots & a_{2n} & b_2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} & b_m \\\\\n\\end{bmatrix}\n\\end{align*}\n\\]\nwhere \\(\\mathbf{A}\\) is an \\(m\\times n\\) matrix, \\(\\mathbf{b}\\) is an \\(m\\times 1\\) column vector, and the augmented matrix is an \\(m\\times (n+1)\\) matrix.\n\n\n\n\\[\n\\begin{align*}\nx_1 + 2x_2 - 3x_3 &= 4 \\\\\n2x_1 - x_2 + 4x_3 &= 7 \\\\\nx_1 + 3x_2 + 2x_3 &= 5\n\\end{align*}\n\\]\nThe corresponding augmented matrix is:\n\\[\n\\begin{align*}\n\\left[\\begin{array}{ccc|c}\n1 & 2 & -3 & 4 \\\\\n2 & -1 & 4 & 7 \\\\\n1 & 3 & 2 & 5\n\\end{array}\\right]\n\\end{align*}\n\\]\nThen the augmented matrix \\([\\mathbf{A}|\\mathbf{b}]\\) is\n\\[\n\\begin{align*}\n[\\mathbf{A}|\\mathbf{b}] = \\left[\\begin{array}{cc|c} 1 & 2 & 3 \\\\ 2 & 3 & 4 \\end{array}\\right].\n\\end{align*}\n\\]\nSolving the system of linear equations \\(\\mathbf{Ax=b}\\) is equivalent to finding the row echelon form (REF) or reduced row echelon form (RREF) of the augmented matrix \\([\\mathbf{A}|\\mathbf{b}]\\).\n\nDefinition 3 Elementary row operations are specific operations that can be performed on the rows of a matrix to transform it into a different matrix with the same row space. There are three types of elementary row operations:\n\nRow scaling: Multiply a row of the matrix by a nonzero scalar.\nRow addition: Add a multiple of one row to another row.\nRow interchange: Swap the positions of two rows.\n\n\nElementary row operations are specific operations that can be performed on the rows of a matrix to transform it into a different matrix with the same row space.\n\nDefinition 4 A matrix is said to be in row-echelon form if it satisfies the following conditions:\n\nAll zero rows are at the bottom of the matrix.\nThe leading coefficient (the first non-zero entry) in each row is 1.\nAll other entries in the same column as a leading coefficient are zero.\nThe leading coefficient of a row occurs to the right of the leading coefficient of the row above it.\n\n\n\\[\n\\begin{align*}\n[\\mathbf{A}|\\mathbf{b}]=\n\\left[\\begin{array}{ccc|c}\n3 & 0 & 5 & -3 \\\\\n0 & -9 & 3 & 5 \\\\\n0 & 0 & -1 & -4\n\\end{array}\\right]\n\\end{align*}\n\\]\n\nDefinition 5 A matrix is said to be in the row-reduced echelon form (or RREF) if it satisfies the following conditions:\n\nAll rows that contain a nonzero element are above any rows that contain only zeros (i.e., rows of all zeros, if any, are at the bottom of the matrix).\nThe leading coefficient (the first nonzero entry) in each nonzero row is 1.\nThe leading coefficient of any nonzero row is strictly to the right of the leading coefficient of the row above it.\nAll other entries in the same column as a leading coefficient are zero.\n\n\n$$ \\[\\begin{align*}\n[\\mathbf{A}|\\mathbf{b}] =\n\n\\left[\\begin{array}{ccc|c}\n3 & 0 & 5 & -3 \\\\\n0 & -9 & 3 & 5 \\\\\n0 & 0 & -1 & -4\n\\end{array}\\right]_{\\text{row echolon form}} \\rightarrow\n[\\mathbf{A}|\\mathbf{b}] =\n\\left[\\begin{array}{ccc|c}\n1 & 0 & 0 & -\\frac{47}{27} \\\\\n0 & 1 & 0 & \\frac{23}{27} \\\\\n0 & 0 & 1 & 4\n\\end{array}\\right]_{\\text{row reduced echolon form}}\n\\end{align*}\\] $$\nThe row-echelon form (or row-reduced echelon form or RREF) is a systematic way of representing a matrix such that it has certain properties, making it easier to solve linear equations using methods like Gaussian elimination. In row-echelon form, the leading coefficient (the first non-zero entry) in each row is 1, and all other entries in the same column are zero.\nFrom the row-echolon form, it produces an upper triangular system and the reduced one or RREF is the final goal to solve the system.\n\n\n\n\n\n$$ \\[\\begin{align*}\n& 2x + y - z = 3 \\\\\n& 3x - 2y + 2z = 1 \\\\\n& x + 3y - z = 4 \\\\\n\n& \\text{the augmented matrix:} \\\\\n& \\begin{bmatrix}\n2 & 1 & -1 & | & 3 \\\\\n3 & -2 & 2 & | & 1 \\\\\n1 & 3 & -1 & | & 4 \\\\\n\\end{bmatrix} \\\\\n\\\\\n& \\text{Perform elementary row operations:} \\\\\n\\\\\n& \\text{Row2 = Row2 - 3/2 * Row1} \\\\\n& \\text{Row3 = Row3 - 1/2 * Row1} \\\\\n\\\\\n& \\begin{bmatrix}\n2 & 1 & -1 &| & 3 \\\\\n0 & -5/2 & 7/2&| & -7/2 \\\\\n0 & 5/2 & -1/2&| & 5/2 \\\\\n\\end{bmatrix} \\\\\n\\\\\n& \\text{Row3 = Row3 + Row2} \\\\\n\\\\\n& \\text{Row Echelon Form:} \\\\\n& \\begin{bmatrix}\n2 & 1 & -1 &| & 3 \\\\\n0 & -5/2 & 7/2 &| & -7/2 \\\\\n0 & 0 & 3 &| & -1 \\\\\n\\end{bmatrix} \\\\\n\\\\\n& \\text{Reduced Row Echelon Form:} \\\\\n&\\begin{bmatrix}\n1 & 0 & 0 &| & 1 \\\\\n0 & 1 & -1 &|& 1 \\\\\n0 & 0 & 1 &|& -\\frac{1}{3} \\\\\n\\end{bmatrix}\n\\\\\n\\\\\n& \\text{Breakdown: Perform back substitution to obtain the solution:} \\\\\n& z = -1/3 \\\\\n& y = -2/3 \\\\\n& x = 4/3\n\\end{align*}\\] $$\n\n\n\n$$ \\[\\begin{align*}\n& 3x + 2y - z = 4 \\\\\n& 6x + 4y - 2z = 8 \\\\\n& 9x + 6y - 3z = 12 \\\\\n& \\text{the augmented matrix:} \\\\\n& \\begin{bmatrix}\n3 & 2 & -1 & | & 4 \\\\\n6 & 4 & -2 & | & 8 \\\\\n9 & 6 & -3 & | & 12 \\\\\n\\end{bmatrix} \\\\\n\n& \\text{Perform elementary row operations:} \\\\\n\\\\\n& \\text{Row2 = Row2 - 2 * Row1} \\\\\n& \\text{Row3 = Row3 - 3 * Row1} \\\\\n\\\\\n& \\begin{bmatrix}\n3 & 2 & -1 &| & 4 \\\\\n0 & 0 & 0 &| & 0 \\\\\n0 & 0 & 0 &| & 0 \\\\\n\\end{bmatrix} \\\\\n\\\\\n& \\text{Row Echelon Form:} \\\\\n& \\begin{bmatrix}\n3 & 2 & -1 &| & 4 \\\\\n0 & 0 & 0 &| & 0 \\\\\n0 & 0 & 0 &| & 0 \\\\\n\\end{bmatrix} \\\\\n\\\\\n& \\text{Reduced Row Echelon Form:} \\\\\n& \\begin{bmatrix}\n1 & \\frac{2}{3} & -\\frac{1}{3} &| & \\frac{4}{3} \\\\\n0 & 0 & 0 &| & 0 \\\\\n0 & 0 & 0 &| & 0 \\\\\n\\end{bmatrix} \\\\\n\\\\\n& \\text{Breakdown:} \\\\\n& \\text{Perform back substitution to obtain the solution:} \\\\\n& z = t \\quad \\text{(where t is a parameter)} \\\\\n& y = s \\quad \\text{(where s is a parameter)} \\\\\n& x = \\frac{4}{3} - \\frac{2}{3}y + \\frac{1}{3}z \\quad \\text{(in terms of t and s)}\n\\end{align*}\\] $$\nIn this example, the system of linear equations has infinitely many solutions because after performing row operations, the rows become all zeros in the second and third rows, indicating that there are infinitely many values of \\(x\\), \\(y\\), and \\(z\\) that satisfy the system of equations. The parameters t and s can take any real values, and the values of \\(x\\), \\(y\\), and \\(z\\) can be expressed in terms of \\(t\\) and \\(s\\).\n\n\n\n$$ \\[\\begin{align*}\n& 2x + 3y - z = 7 \\\\\n& 4x + 6y - 2z = 12 \\\\\n& 3x + 4y - z = 8 \\\\\n\n& \\text{the augmented matrix:} \\\\\n& \\begin{bmatrix}\n2 & 3 & -1 & | & 7 \\\\\n4 & 6 & -2 & | & 12 \\\\\n3 & 4 & -1 & | & 8 \\\\\n\\end{bmatrix} \\\\\n\n& \\text{Perform elementary row operations:} \\\\\n\\\\\n& \\text{Row2 = Row2 - 2 * Row1} \\\\\n& \\text{Row3 = Row3 - 3/2 * Row1} \\\\\n\\\\\n& \\begin{bmatrix}\n2 & 3 & -1 &| & 7 \\\\\n0 & 0 & 0&| & -2 \\\\\n0 & 1/2 & 1/2&| & 1 \\\\\n\\end{bmatrix} \\\\\n\\\\\n& \\text{Row3 = Row3 - 1/2 * Row2} \\\\\n\\\\\n& \\text{Row Echelon Form:} \\\\\n& \\begin{bmatrix}\n2 & 3 & -1 &| & 7 \\\\\n0 & 1/2 & 1/2 &| & 1 \\\\\n0 & 0 & 0 &| & -2 \\\\\n\\end{bmatrix} \\\\\n\\\\\n& \\text{Reduced Row Echelon Form:} \\\\\n&\\begin{bmatrix}\n1 & 3/2 & -1/2 &| & 7/2 \\\\\n0 & 1 & 1 &|& 2 \\\\\n0 & 0 & 0 &|& -2\n\\end{bmatrix}\n\\end{align*}\\] $$\nThe last row of the reduced row echelon form has all zeros except for the right-hand side (RHS) part, which is \\(-2\\). This implies that \\(0 = -2\\), which is not possible. Therefore, there is no solution to this system of linear equations.\n\n\n\n\n\nYou reference some rules of matrix operations in the other blog (See the basic matrix operation).\n\n\nMatrix addition is commutative, which means that changing the order of the matrices being added does not affect the result.\n\\[\n\\begin{align*}\n\\mathbf A + \\mathbf B = \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} + \\begin{bmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{bmatrix} = \\begin{bmatrix}\n1 + 5 & 2 + 6 \\\\\n3 + 7 & 4 + 8\n\\end{bmatrix} = \\begin{bmatrix}\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\\\\n\\mathbf B + \\mathbf A = \\begin{bmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{bmatrix} + \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} = \\begin{bmatrix}\n5 + 1 & 6 + 2 \\\\\n7 + 3 & 8 + 4\n\\end{bmatrix} = \\begin{bmatrix}\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\end{align*}\n\\]\n\n\n\nMatrix addition distributes over matrix multiplication, which means that multiplying a matrix by the sum of two matrices is the same as multiplying the matrix by each individual matrix and then adding the results.\n\\[\n\\begin{align*}\n\\mathbf A (\\mathbf B + \\mathbf C )\n&= \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} \\cdot \\left( \\begin{bmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{bmatrix} + \\begin{bmatrix}\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix} \\right) = \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n14 & 16 \\\\\n18 & 20\n\\end{bmatrix} = \\begin{bmatrix}\n70 & 76 \\\\\n158 & 172\n\\end{bmatrix}\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\mathbf A \\mathbf B + \\mathbf A \\mathbf C\n&= \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{bmatrix} + \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix} = \\begin{bmatrix}\n19 & 22 \\\\\n43 & 50\n\\end{bmatrix} + \\begin{bmatrix}\n23 & 26 \\\\\n31 & 34\n\\end{bmatrix} = \\begin{bmatrix}\n70 & 76 \\\\\n158 & 172\n\\end{bmatrix}\n\\end{align*}\n\\]\n\n\n\nMatrix addition is associative, which means that changing the grouping of the matrices being added does not affect the result.\n\\[\n\\begin{align*}\n(\\mathbf A + \\mathbf B )+ \\mathbf C\n&= \\left( \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} + \\begin{bmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{bmatrix} \\right) + \\begin{bmatrix}\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix} = \\begin{bmatrix}\n1 + 5 & 2 + 6 \\\\\n3 + 7 & 4 + 8\n\\end{bmatrix} + \\begin{bmatrix}\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix} = \\begin{bmatrix}\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix} + \\begin{bmatrix}\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix} = \\begin{bmatrix}\n15 & 18 \\\\\n21 & 24\n\\end{bmatrix}\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\mathbf A + (\\mathbf B + \\mathbf C)\n&= \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} + \\left( \\begin{bmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{bmatrix} + \\begin{bmatrix}\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix} \\right) = \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} + \\begin{bmatrix}\n14 & 16 \\\\\n18 & 20\n\\end{bmatrix} = \\begin{bmatrix}\n15 & 18 \\\\\n21 & 24\n\\end{bmatrix}\n\\end{align*}\n\\]\n\n\n\n\nA block matrix is a matrix that is partitioned into smaller matrices, or blocks, arranged in a rectangular grid. The blocks can be of any size, and the resulting matrix is used to represent a system of linear equations with multiple variables or equations.\n\n\nLet \\(\\mathbf A\\) be a block matrix with four blocks, \\(\\mathbf A_{11}\\), \\(\\mathbf A_{12}\\), \\(\\mathbf A_{21}\\), and \\(\\mathbf A_{22}\\), as shown below:\n\\[\n\\mathbf A = \\begin{bmatrix}\n\\mathbf A_{11} & \\mathbf A_{12} \\\\\n\\mathbf A_{21} & \\mathbf A_{22}\n\\end{bmatrix}\n\\]\nwhere \\(\\mathbf A_{11}, \\mathbf A_{12}, \\mathbf A_{21},\\text{ and }\\mathbf A_{22}\\) are individual matrices. This block matrix can be used to represent a system of linear equations with four variables or equations, where the blocks \\(\\mathbf A_{11}, \\mathbf A_{12}, \\mathbf A_{21},\\text{ and }\\mathbf A_{22}\\) represent the coefficients of the variables in the linear equations.\n\n\n\nBlock multiplication is a matrix operation used with block matrices, where a matrix is partitioned into smaller matrices, or blocks, and the blocks are multiplied according to certain rules.\n\n\n\\[\n\\mathbf A = \\begin{bmatrix}\n\\mathbf A_{11} & \\mathbf A_{12} \\\\\n\\mathbf A_{21} & \\mathbf A_{22}\n\\end{bmatrix}\n\\quad\n\\mathbf B = \\begin{bmatrix}\n\\mathbf B_{11} & \\mathbf B_{12} \\\\\n\\mathbf B_{21} & \\mathbf B_{22}\n\\end{bmatrix}\n\\]\nThe block multiplication of \\(\\mathbf A\\) and \\(\\mathbf B\\), denoted as \\(\\mathbf{AB}\\), can be computed as:\n\\[\n\\mathbf{AB} = \\begin{bmatrix}\n\\mathbf A_{11} & \\mathbf A_{12} \\\\\n\\mathbf A_{21} & \\mathbf A_{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf B_{11} & \\mathbf B_{12} \\\\\n\\mathbf B_{21} & \\mathbf B_{22}\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\mathbf A_{11}\\mathbf B_{11} + \\mathbf A_{12}\\mathbf B_{21} & \\mathbf A_{11}\\mathbf B_{12} + \\mathbf A_{12}\\mathbf B_{22} \\\\\n\\mathbf A_{21}\\mathbf B_{11} + \\mathbf A_{22}\\mathbf B_{21} & \\mathbf A_{21}\\mathbf B_{12} + \\mathbf A_{22}\\mathbf B_{22}\n\\end{bmatrix}\n\\]\nwhere \\(\\mathbf{A}_{11}\\mathbf{B}_{11}, \\mathbf{A}_{12}\\mathbf{B}_{21}, \\mathbf{A}_{11}\\mathbf{B}_{12}, \\mathbf{A}_{12}\\mathbf{B}_{22}, \\mathbf{A}_{21}\\mathbf{B}_{11}, \\mathbf{A}_{22}\\mathbf{B}_{21}, \\mathbf{A}_{21}\\mathbf{B}_{12},\\text{ and }\\mathbf{A}_{22}\\mathbf{B}_{22}\\) are block multiplications of the corresponding blocks.\nWhen matrices split into blocks, it is often simpler to see how they act.\nThis block unit can be reduced to the vector:\n\\[\n\\mathbf{AB} = \\begin{bmatrix}\n\\mathbf a_{1} & \\dots & \\mathbf a_{n}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf b_{1} \\\\\n\\vdots\\\\\n\\mathbf b_{n}\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\mathbf a_{1}\\mathbf b_{1} + \\dots + \\mathbf a_{n}\\mathbf b_{n}\n\\end{bmatrix}\n\\]\n\n\n\n\nBlock elimination is a technique used to solve systems of linear equations by reducing the system to a smaller set of equations. The process involves breaking the system down into smaller sub-systems or blocks, then eliminating one set of variables by expressing them in terms of the remaining variables.\n\n\nThe Schur complement is a matrix obtained by block elimination, where a large matrix \\(\\mathbf{A}\\) is partitioned into blocks:\n\\[\n\\begin{align*}\n\\mathbf{A} =\n\\begin{bmatrix}\n\\mathbf{A}_{11} & \\mathbf{A}_{12} \\\\\n\\mathbf{A}_{21} & \\mathbf{A}_{22}\n\\end{bmatrix}\n\\end{align*}\n\\]\nwhere \\(\\mathbf{A}_{11}\\) is a square sub-matrix of \\(\\mathbf{A}\\). The Schur complement of \\(\\mathbf{A}_{22}\\) with respect to \\(\\mathbf{A}_{11}\\) is defined as:\n\\[\n\\begin{align*}\n\\mathbf{S} = \\mathbf{A}_{22} - \\mathbf{A}_{21} \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12}\n\\end{align*}\n\\]\nThe Schur complement is useful in many areas of mathematics and engineering, including control theory, optimization, and signal processing.\nAs an example, consider the following system of linear equations:\n\\[\n\\begin{align*}\n\\begin{bmatrix}\n\\mathbf{A}_{11} & \\mathbf{A}_{12} \\\\\n\\mathbf{A}_{21} & \\mathbf{A}_{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{x}_1 \\\\\n\\mathbf{x}_2\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{b}_1 \\\\\n\\mathbf{b}_2\n\\end{bmatrix}\n\\end{align*}\n\\]\nWe can eliminate the variables \\(\\mathbf{x}_2\\) by solving for them in terms of \\(\\mathbf{x}_1\\):\n\\[\n\\begin{align*}\n\\mathbf{A}_{22} \\mathbf{x}_2 = \\mathbf{b}_2 - \\mathbf{A}_{21} \\mathbf{x}_1 \\\\\n\\mathbf{x}_2 = \\mathbf{A}_{22}^{-1} (\\mathbf{b}_2 - \\mathbf{A}_{21} \\mathbf{x}_1)\n\\end{align*}\n\\]\nSubstituting this into the first equation, we obtain:\n\\[\n\\begin{align*}\n\\mathbf{A}_{11} \\mathbf{x}_1 + \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} (\\mathbf{b}_2 - \\mathbf{A}_{21} \\mathbf{x}_1) = \\mathbf{b}_1\n\\end{align*}\n\\]\nRearranging terms, we obtain an equation in the form \\(\\mathbf{B} \\mathbf{x}_1 = \\mathbf{c}\\), where:\n\\[\n\\begin{align*}\n\\mathbf{B} &= \\mathbf{A}_{11} - \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\mathbf{A}_{21} \\\\\n\\mathbf{c} &= \\mathbf{b}_1 - \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\mathbf{b}_2\n\\end{align*}\n\\]\nThus, we have obtained a smaller system.\n\n\n\n\n\n\nDefinition 6 The inverse of a square matrix \\(A\\) of size \\(n\\) is a matrix \\(A^{-1}\\) such that the product of \\(A\\) and \\(A^{-1}\\) is the identity matrix \\(I_n\\), i.e. \\(A \\times A^{-1} = I_n\\). If such a matrix exists, then \\(A\\) is said to be invertible or non-singular.\nThe inverse of a square matrix \\(\\mathbf{A}\\) is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the unique matrix that satisfies the following equation: \\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\n\n\n\n\nExistence: The inverse of the matrix \\(\\mathbf A\\) exists if and only if elimination produces \\(n\\) pivots, where \\(n\\) is the number of rows (or columns) of \\(\\mathbf A\\). Elimination solves \\(\\mathbf{Ax}=\\mathbf{b}\\) without explicitly using the matrix\n\n\nPivots are the non-zero elements that are selected during the elimination process and used as the basis for row operations. If \\(n\\) pivots are obtained, then the matrix \\(\\mathbf A\\) is said to be full rank, and its inverse exists. If fewer than \\(n\\) pivots are obtained, then the matrix \\(\\mathbf A\\) is singular, and its inverse does not exist\n\n\nUnique Inverse: the matrix \\(\\mathbf A\\) cannot have two different inverses\nIf \\(\\mathbf A\\) is invertible, the one and only solution to \\(\\mathbf{Ax}=\\mathbf{b}\\) is \\(\\mathbf{x}=\\mathbf{A^{-1}b}\\)\n(Important) Suppose there is a nonzero vector \\(\\mathbf A\\) such that \\(\\mathbf{Ax}=\\mathbf{0}\\). Then \\(\\mathbf A\\) cannot have an inverse. No matrix can bring \\(\\mathbf 0\\) back to \\(\\mathbf x\\).\n\n\nIf \\(\\mathbf A\\) is invertible, then \\(\\mathbf{Ax}=\\mathbf{0}\\) can only have the zero solution \\(\\mathbf{x}=\\mathbf{A^{-1}0=0}\\).\n\n\nA 2 by 2 matrix is invertible if and only if \\(ad - bc\\) is not zero: \\[\nA = \\begin{bmatrix}\na & b \\\\\nc & d \\\\\n\\end{bmatrix}\n\\quad\nA^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix}\nd & -b \\\\\n-c & a \\\\\n\\end{bmatrix}\n\\]\nA diagonal matrix has an inverse provided no diagonal entries are zero\nInverse of Inverse: \\((\\mathbf A^{-1})^{-1} = \\mathbf A\\)\nInverse of Product: If \\(\\mathbf{AB = I}\\), where \\(\\mathbf I\\) is the identity matrix, then \\(\\mathbf{B = A^{-1}}\\).\nScalar Multiple: If \\(c\\) is a scalar, then \\((c\\mathbf A)^{-1} = \\frac{1}{c}\\mathbf A^{-1}\\) (if \\(c \\neq 0\\)).\nProduct of Inverses: If \\(\\mathbf A^{-1}\\) and \\(\\mathbf B^{-1}\\) both exist, then \\((\\mathbf{AB})^{-1} = \\mathbf B^{-1}\\mathbf A^{-1}\\) (if \\(\\mathbf{AB}\\) is invertible).\nReverse Order: \\((\\mathbf{ABC})^{-1}\\)=\\(\\mathbf C^{-1}\\) \\(\\mathbf B^{-1}\\) \\(\\mathbf A^{-1}\\)\n\n\n\n\nGiven a square matrix \\(\\mathbf A\\), to find its inverse \\(\\mathbf A^{-1}\\):\nStep 1: Augment the matrix \\(\\mathbf A\\) with an identity matrix \\(\\mathbf I\\) of the same size: \\([\\mathbf A | \\mathbf I]\\)\nStep 2: Perform elementary row operations to transform the left half \\(\\mathbf A\\) into the identity matrix \\(\\mathbf I\\): - Interchange rows - Multiply a row by a scalar - Add a multiple of one row to another row\nStep 3: Apply the same row operations to the right half \\(\\mathbf I\\) to obtain \\(\\mathbf A^{-1}\\).\nStep 4: If \\(\\mathbf A\\) is not invertible, the augmented matrix \\([\\mathbf A | \\mathbf I]\\) will not result in an identity matrix on the left half.\n\n\n\\[\n\\mathbf A= \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \\\\\n\\end{bmatrix}\n\\]\nStep 1: Augment the matrix \\(\\mathbf A\\) with an identity matrix \\(\\mathbf I\\) of the same size: \\[\n[\\mathbf A | \\mathbf I] = \\begin{bmatrix}\n1 & 2 & 3 & | & 1 & 0 & 0 \\\\\n4 & 5 & 6 & | & 0 & 1 & 0 \\\\\n7 & 8 & 9 & | & 0 & 0 & 1\n\\end{bmatrix}\n\\]\nStep 2: Perform elementary row operations to transform the left half \\(\\mathbf A\\) into the identity matrix \\(\\mathbf I\\): - Interchange rows - Multiply a row by a scalar - Add a multiple of one row to another row\nStep 3: Apply the same row operations to the right half \\(\\mathbf I\\) to obtain \\(\\mathbf A^{-1}\\).\nStep 4: If \\(\\mathbf A\\) is not invertible, the augmented matrix \\([\\mathbf A | \\mathbf I]\\) will not result in an identity matrix on the left half. In this case, \\(\\mathbf A\\) does not have an inverse because \\(\\text{det}(\\mathbf A) = 1(5 \\cdot 9 - 6 \\cdot 8) - 2(4 \\cdot 9 - 6 \\cdot 7) + 3(4 \\cdot 8 - 5 \\cdot 7) = 0\\)\n\nDefinition 7 An elementary matrix is a square matrix obtained by performing a single elementary row operation on the identity matrix \\(\\mathbf{I}\\).\nThere are three types of elementary row operations:\n\nSwapping two rows: The elementary matrix obtained by swapping two rows of the identity matrix is denoted by \\(\\mathbf{E}_i\\), where \\(i\\) indicates the row numbers to be swapped. \\(\\mathbf{E}_1 = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}\\)\nScaling a row by a nonzero scalar: The elementary matrix obtained by scaling a row of the identity matrix by a nonzero scalar \\(c\\) is denoted by \\(\\mathbf{E}_i(c)\\), where \\(i\\) indicates the row number to be scaled and \\(c\\) is the scalar. \\(\\mathbf{E}_2(2) = \\begin{bmatrix} 1 & 0 \\\\ 0 & 2 \\end{bmatrix}\\)\nAdding a multiple of one row to another row: The elementary matrix obtained by adding a multiple of one row of the identity matrix to another row is denoted by \\(\\mathbf{E}_{ij}(c)\\), where \\(i\\) indicates the row number from which a multiple is added, \\(j\\) indicates the row number to which the multiple is added, and \\(c\\) is the scalar multiple. \\(\\mathbf{E}_{12}(3) = \\begin{bmatrix} 1 & 3 \\\\ 0 & 1 \\end{bmatrix}\\)\n\n\n\nTheorem 1 Elementary Matrix Theorem\nLet \\(\\mathbf{A}\\) be an invertible \\(n \\times n\\) matrix. Then \\(\\mathbf{A}\\) can be represented as the product of elementary matrices \\(\\mathbf{E}_1, \\mathbf{E}_2, \\ldots, \\mathbf{E}_k\\), where each \\(\\mathbf{E}_i\\) is an elementary matrix corresponding to a single elementary row operation.\n\nAny invertible matrix can be obtained by performing a sequence of elementary row operations on the identity matrix.\nLet \\(\\mathbf{A} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 5 \\end{bmatrix}\\) be an invertible matrix. We can represent \\(\\mathbf{A}\\) as the product of elementary matrices \\(\\mathbf{E}_1\\) and \\(\\mathbf{E}_2\\) as follows:\n\\[\n\\begin{align*}\n\\mathbf{E}_1 = \\begin{bmatrix} 1 & 0 \\\\ -2 & 1 \\end{bmatrix}, \\quad\n\\mathbf{E}_2 = \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\end{align*}\n\\]\nsuch that \\(\\mathbf{A} = \\mathbf{E}_2 \\mathbf{E}_1 \\mathbf{I}\\).\nLet \\(\\mathbf{B} = \\begin{bmatrix} 3 & 2 & 1 \\\\ 1 & 1 & 1 \\\\ 2 & 3 & 4 \\end{bmatrix}\\) be an invertible matrix. We can represent \\(\\mathbf{B}\\) as the product of elementary matrices \\(\\mathbf{E}_1\\), \\(\\mathbf{E}_2\\), and \\(\\mathbf{E}_3\\) as follows:\n\\[\n\\begin{align*}\n\\mathbf{E}_1 = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad\n\\mathbf{E}_2 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}, \\quad\n\\mathbf{E}_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & \\frac{1}{2} \\end{bmatrix}\n\\end{align*}\n\\]\nsuch that \\(\\mathbf{B} = \\mathbf{E}_3 \\mathbf{E}_2 \\mathbf{E}_1 \\mathbf{I}\\).\n\n\n\n\n\n\nDefinition 8 The LU factorization, also known as the LU decomposition, is a matrix factorization method that expresses a given matrix \\(\\mathbf{A}\\) as the product of two matrices: a lower triangular matrix \\(\\mathbf{L}\\) and an upper triangular matrix \\(\\mathbf{U}\\): \\[\n\\mathbf{A} = \\mathbf{LU}\n\\]\nwhere\n\n\\(\\mathbf{A}\\) is the given matrix,\n\\(\\mathbf{L}\\) is the lower triangular matrix with ones on the diagonal, and\n\\(\\mathbf{U}\\) is the upper triangular matrix.\n\n\nIt decomposes a given square matrix into the product of two matrices, a lower triangular matrix (\\(\\mathbf{L}\\)) and an upper triangular matrix (\\(\\mathbf{U}\\)).\n\nEfficient Solution of Linear Systems: Once a matrix is factorized into its LU form, it can be used to efficiently solve systems of linear equations. This is because solving a system of equations involving triangular matrices (such as L and U) is computationally more efficient compared to directly solving the original system of equations involving a general matrix.\nMatrix Inversion: LU decomposition can also be used to efficiently calculate the inverse of a matrix. Once a matrix is factorized into its LU form, the inverse can be obtained by solving two triangular systems of equations, which is computationally more efficient compared to direct methods for matrix inversion.\nNumerical Stability: LU decomposition can be used as a more numerically stable method for solving linear systems compared to direct methods, such as Gaussian elimination, because it avoids the issues of division by small or zero pivots.\n\n\n\n\nThe LU decomposition of a matrix is not unique. There can be multiple factorizations of the same matrix into different combinations of L and U matrices.\nIf the original matrix has a determinant of zero, it is singular and does not have a unique LU decomposition.\nThe LU decomposition can be used for square as well as rectangular matrices, although in the case of rectangular matrices, it may not be unique and may involve additional techniques such as pivoting.\nThe LU decomposition can be calculated using various algorithms, such as Gaussian elimination, Crout’s method, and Doolittle’s method, among others, with different advantages and disadvantages in terms of computational complexity and numerical stability.\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbf{A} &= \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nn} \\\\\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\nl_{21} & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nl_{n1} & l_{n2} & \\cdots & 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nu_{11} & u_{12} & \\cdots & u_{1n} \\\\\n0 & u_{22} & \\cdots & u_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & u_{nn} \\\\\n\\end{bmatrix}\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\mathbf{A} &= \\begin{bmatrix}\n2 & 3 & 1 \\\\\n4 & 9 & 5 \\\\\n6 & 15 & 9\n\\end{bmatrix} \\\\\n\\mathbf{A} &= \\mathbf{LU} \\\\\n\\mathbf{L} &= \\begin{bmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n3 & 5 & 1\n\\end{bmatrix}, \\quad\n\\mathbf{U} = \\begin{bmatrix}\n2 & 3 & 1 \\\\\n0 & 3 & 3 \\\\\n0 & 0 & 2\n\\end{bmatrix}\n\\end{align*}\n\\]\n\n\nCode\nimport scipy\nimport numpy as np\n\n#def lu_factorization(A):\n#    n = A.shape[0]\n#    L = np.zeros((n, n))\n#    U = np.copy(A)\n#\n#    for k in range(n-1):\n#        if U[k][k] == 0:\n#            print(\"LU factorization is not possible as U[{}][{}] is zero\".format(k, k))\n#            return None, None\n#        \n#        for i in range(k+1, n):\n#            L[i][k] = U[i][k] / U[k][k]\n#            U[i][k+1:n] -= L[i][k] * U[k][k+1:n]\n#            U[i][k] = 0\n#\n#    np.fill_diagonal(L, 1)\n#    return L, U\n#\n# Example\n\nA = np.array([[2, 3, 1],\n              [0, 4, -2],\n              [0, 0, 3]])\n\n#L, U = lu_factorization(A)\n#print(\"L:\")\n#print(L)\n#print(\"U:\")\n#print(U)\n\n\n\n\n\n# Define the right-hand side vector b\nb = np.array([7, 4, 6])\n\n# Solve the system of equations Ax = b using LU decomposition\n# First, perform LU decomposition of A\nP, L, U = scipy.linalg.lu(A)\n\n# Then, solve the lower triangular system Lc = Pb for c\nc = np.linalg.solve(L, np.dot(P, b))\n\n# Finally, solve the upper triangular system Ux = c for x\nx = np.linalg.solve(U, c)\n\n# Print the solution vector x\nprint(\"Solution vector x:\", x)\n\n\nSolution vector x: [-0.5  2.   2. ]\n\n\n\n\n\n\n\n\nRead the previous blog: the basic matrix operations\n\n\n\nA permutation is a reordering of a finite sequence of elements. In the context of solving \\(\\mathbf{Ax=b}\\), a permutation can be used to reorder the rows of the augmented matrix \\(\\begin{bmatrix} \\mathbf{A} & \\mathbf{b} \\end{bmatrix}\\) to simplify the process of finding the row echelon form.\n\nDefinition 9 A permutation of a set of size \\(n\\) is a bijective function \\(\\sigma: {1, 2, \\ldots, n} \\to {1, 2, \\ldots, n}\\), which means that every element in the set is mapped to a unique element in the set and vice versa. A common way to represent a permutation is by using a permutation matrix, which is a square matrix with a single 1 in each row and each column and 0s elsewhere. The location of the 1 in each row represents the new position of that row after the permutation.\n\n\n\nConsider the permutation \\(\\sigma\\) of the set \\({1, 2, 3}\\) defined by \\(\\sigma(1) = 2\\), \\(\\sigma(2) = 3\\), and \\(\\sigma(3) = 1\\). The corresponding permutation matrix is:\n\\[\n\\begin{align*}\n\\mathbf{P} = \\begin{bmatrix}\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{bmatrix}\n\\end{align*}\n\\]\nTo apply this permutation to the matrix \\(\\mathbf{A}\\), we multiply \\(\\mathbf{A}\\) on the left by \\(\\mathbf{P}\\), i.e., \\(P\\mathbf{A}\\). Similarly, to apply the permutation to the vector \\(\\mathbf{b}\\), we multiply \\(\\mathbf{b}\\) on the left by \\(\\mathbf{P}\\), i.e., \\(P\\mathbf{b}\\). This gives us the reordered augmented matrix:\n\\[\n\\begin{align*}\n\\mathbf{P}\\begin{bmatrix} \\mathbf{A} & \\mathbf{b} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{P}\\mathbf{A} & \\mathbf{P}\\mathbf{b} \\end{bmatrix}\n\\end{align*}\n\\]\nBy permuting the rows of the augmented matrix, we can obtain a row echelon form that is easier to work with and may lead to simpler solutions for \\(\\mathbf{x}\\).\n\n\n\nPermutations are used in solving linear systems of equations using Gaussian elimination, which is a common method for finding solutions to \\(\\mathbf{Ax=b}\\). Here are some formal properties of permutations:\n\nA permutation matrix \\(\\mathbf{P}\\) is a square matrix obtained by permuting the rows of the identity matrix \\(\\mathbf{I}\\).\nThe product of two permutation matrices is also a permutation matrix.\nThe inverse of a permutation matrix is its transpose.\nPermutation matrices can be used to interchange rows of a matrix.\n\n\\[\n\\begin{align*}\nx_1 + 2x_2 + 3x_3 &= 7 \\\\\n4x_1 + 5x_2 + 6x_3 &= 8 \\\\\n7x_1 + 8x_2 + 9x_3 &= 10\n\\end{align*}\n\\]\nThe augmented matrix for this system is:\n\\[\n\\begin{align*}\n\\left[\\begin{array}{ccc|c}\n1 & 2 & 3 & 7 \\\\\n4 & 5 & 6 & 8 \\\\\n7 & 8 & 9 & 10\n\\end{array}\\right]\n\\end{align*}\n\\]\nTo perform Gaussian elimination, we might want to interchange the first and second rows of the matrix. We can do this by multiplying the matrix by the permutation matrix:\n\\[\n\\begin{align*}\n\\mathbf{P}_1 = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\end{align*}\n\\]\nWe can verify that \\(\\mathbf{P}_1\\) is a permutation matrix, and we can apply it to the augmented matrix:\n\\[\n\\begin{align*}\n\\mathbf{P}_1\\left[\\begin{array}{ccc|c}\n1 & 2 & 3 & 7 \\\\\n4 & 5 & 6 & 8 \\\\\n7 & 8 & 9 & 10\n\\end{array}\\right] &= \\left[\\begin{array}{ccc|c}\n4 & 5 & 6 & 8 \\\\\n1 & 2 & 3 & 7 \\\\\n7 & 8 & 9 & 10\n\\end{array}\\right]\n\\end{align*}\n\\]\nWe can continue with the Gaussian elimination process on the new augmented matrix. If we want to interchange the second and third rows, we can use the permutation matrix:\n\\[\n\\begin{align*}\n\\mathbf{P}_2 = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix}\n\\end{align*}\n\\]\nWe can verify that \\(\\mathbf{P}_2\\) is a permutation matrix, and we can apply it to the augmented matrix:\n\\[\n\\begin{align*}\n\\mathbf{P}_2\\left[\\begin{array}{ccc|c}\n4 & 5 & 6 & 8 \\\\\n1 & 2 & 3 & 7 \\\\\n7 & 8 & 9 & 10\n\\end{array}\\right] &= \\left[\\begin{array}{ccc|c}\n4 & 5 & 6 & 8 \\\\\n7 & 8 & 9 & 10 \\\\\n1 & 2 & 3 & 7\n\\end{array}\\right]\n\\end{align*}\n\\]\nWe can now continue with Gaussian elimination on this new augmented matrix, which is in row echelon form."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html",
    "title": "Vector Space",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#definition",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#definition",
    "title": "Vector Space",
    "section": "1.1 Definition",
    "text": "1.1 Definition\n\nDefinition 1 The space \\(\\mathbb{R}^{n}\\) consists of all column vectors \\(\\mathbf v\\) with \\(n\\) components. The space \\(\\mathbb{C}^{n}\\) consists of all column vectors \\(\\mathbf v\\) with \\(n\\) components. where \\(n=1,2,3,\\ldots\\), \\(\\mathbb{R}\\) stands for a set of real numbers, and \\(\\mathbb{C}\\) stands for a set of complex numbers.\n\nFor example, \\(\\mathbb{R}^{5}\\) contains all column vectors \\(\\mathbf v\\) with \\(5\\) components, which is called ‘5-dimensional space’\n\\[\n\\begin{bmatrix}2 \\\\ 4 \\end{bmatrix} \\in \\mathbb{R}^{2} \\quad \\begin{bmatrix}2 & 2 & 43 & 56 & 4 \\end{bmatrix} \\in \\mathbb{R}^{5} \\quad \\begin{bmatrix}2+4i \\\\ 4-21i \\end{bmatrix} \\in \\mathbb{C}^{2}\n\\]\nA vector space is a set \\(\\mathbf{V}\\) equipped with two operations:\n\nvector addition and\nscalar multiplication\n\n,which satisfy the following properties for all vectors \\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w}\\) in \\(V\\) and all scalars \\(c, d\\):\n\nClosure under vector addition: \\(\\mathbf{u} + \\mathbf{v} \\in V\\)\nAssociativity of vector addition: \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\)\nIdentity element of vector addition: There exists a vector \\(\\mathbf{0}\\) in \\(V\\) such that \\(\\mathbf{u} + \\mathbf{0} = \\mathbf{u}\\) for all \\(\\mathbf{u}\\) in \\(V\\)\nExistence of additive inverse: For each \\(\\mathbf{u}\\) in \\(V\\), there exists a vector \\(-\\mathbf{u}\\) in \\(\\mathbf{V}\\) such that \\(\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}\\)\nClosure under scalar multiplication: \\(c\\mathbf{u} \\in V\\)\nDistributive law of scalar multiplication with respect to scalar addition: \\((c + d)\\mathbf{u} = c\\mathbf{u} + d\\mathbf{u}\\)\nDistributive law of scalar multiplication with respect to vector addition: \\(c(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}\\)\nAssociativity of scalar multiplication: \\((cd)\\mathbf{u} = c(d\\mathbf{u})\\)\nIdentity element of scalar multiplication: \\(1\\mathbf{u} = \\mathbf{u}\\)\n\nThe closure or inside the vector space mean that the result of the properties stays in the space.\n\n1.1.1 Example\nLet \\(V = \\mathbb{R}^3\\), the set of all 3-dimensional real vectors. The vector space \\(V\\) is equipped with vector addition and scalar multiplication defined as usual component-wise. Vector addition and scalar multiplication satisfy all the properties listed in the definition of a vector space.\n\n\n1.1.2 Three Special Vector Spaces\n\n\\(\\mathbb{M}\\) the vector space of all real 3 by 2 matrices\n\nthe vectors \\(\\in \\mathbb{M}\\) are really matrices.\n\n\\(\\mathbb{F}\\) the vector space of all real functions f(x)\n\nthe vectors \\(\\in \\mathbb{F}\\) are really functions.\n\n\\(\\mathbb{Z}\\) the vector space that consists only of a zero vectors.\n\nthe vectors \\(\\in \\mathbb{Z}\\) are used for the addition \\(\\mathbf{0}+\\mathbf{0}=\\mathbf{0}\\)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#subspaces",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#subspaces",
    "title": "Vector Space",
    "section": "1.2 Subspaces",
    "text": "1.2 Subspaces\n\n1.2.1 Definition\n\nDefinition 2 A subset \\(W\\) of a vector space \\(V\\) over a field \\(F\\) is called a subspace of \\(V\\) if \\(W\\) is also a vector space over \\(F\\) under the same vector addition and scalar multiplication operations defined on \\(V\\). A subspace of a vector space is a set of vectors (including \\(\\mathbf 0\\)) that satisifies two requirements: if \\(\\mathbf u\\) and \\(\\mathbf v\\) in the subspace and \\(c\\) is any scalar, then\n\n\\(\\mathbf v + \\mathbf w\\) is in the subspace\n\\(c \\mathbf v\\) is in the subspace.\n\n\n\n\n\n\n\n\nField\n\n\n\nA field is a set \\(F\\) along with two operations, addition (\\(+\\)) and multiplication (\\(\\cdot\\)), satisfying the following properties:\n\nClosure: For all \\(a, b \\in F\\), both \\(a + b\\) and \\(a \\cdot b\\) are in \\(F\\).\nAssociativity: For all \\(a, b, c \\in F\\), \\((a + b) + c = a + (b + c)\\) and \\((a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)\\).\nCommutativity: For all \\(a, b \\in F\\), \\(a + b = b + a\\) and \\(a \\cdot b = b \\cdot a\\).\nIdentity: There exists an element \\(0 \\in F\\) such that for all \\(a \\in F\\), \\(a + 0 = a\\). There exists an element \\(1 \\in F\\) such that for all \\(a \\in F\\), \\(a \\cdot 1 = a\\).\nInverse: For every \\(a \\in F\\) with \\(a \\neq 0\\), there exists an element \\(b \\in F\\) such that \\(a + b = 0\\). For every \\(a \\in F\\) with \\(a \\neq 0\\), there exists an element \\(c \\in F\\) such that \\(a \\cdot c = 1\\).\n\n\n1.2.2 Example\n\nThe set of real numbers \\(\\mathbb{R}\\) with addition and multiplication.\nThe set of complex numbers \\(\\mathbb{C}\\) with addition and multiplication.\nThe set of rational numbers \\(\\mathbb{Q}\\) with addition and multiplication.\nThe set of integers modulo a prime number \\(p\\), denoted as \\(\\mathbb{Z}_p\\), with addition and multiplication modulo \\(p\\).\n\n\n\n\n\n\n1.2.3 Properties\nA subset \\(W\\) of a vector space \\(V\\) is called a subspace of \\(V\\) if it satisfies the following properties:\n\nClosure under Addition: For all \\(\\mathbf{u}, \\mathbf{v} \\in W\\), \\(\\mathbf{u} + \\mathbf{v} \\in W\\).\nClosure under Scalar Multiplication: For all \\(\\mathbf{u} \\in W\\) and \\(c \\in \\mathbb{R}\\) (or any field), \\(c\\mathbf{u} \\in W\\).\nContains the Zero Vector: The zero vector \\(\\mathbf{0}\\) of \\(V\\) is in \\(W\\).\nEvery subspace contains the zero vector\nLines through the origin are also subspaces. (If we try to keep only part of a plane or line, the requirements for a subspace don’t hold.)\nA subspace containing \\(\\mathbf u\\) and \\(\\mathbf v\\) must contain all linear combinations \\(c\\mathbf v +d\\mathbf w\\)\n\n\n\n1.2.4 Example\n\nThe set of all real-valued \\(n\\)-dimensional column vectors, denoted as \\(\\mathbb{R}^n\\), is a subspace of the vector space of all \\(n\\)-dimensional column vectors, denoted as \\(\\mathbb{R}^{n \\times 1}\\), with closure under addition and scalar multiplication.\n\n\nSubset \\(V_3 = {(x, y, z) \\in \\mathbb{R}^3 : x + y + z = 0}\\) of vector space \\(\\mathbb{R}^3\\): \\(V_3 = \\{(x, y, z) \\in \\mathbb{R}^3 : x + y + z = 0\\}\\)\n\n\nThe set of all \\(2 \\times 2\\) symmetric matrices, denoted as \\(\\mathcal{S}^2\\), is a subspace of the vector space of all \\(2 \\times 2\\) matrices, denoted as \\(\\mathbb{R}^{2 \\times 2}\\), with closure under addition and scalar multiplication.\n\n\nSubset \\(V_1 = {(x, y) \\in \\mathbb{R}^2 : x + y = 0}\\) of vector space \\(\\mathbb{R}^2\\): \\(V_1 = \\{(x, y) \\in \\mathbb{R}^2 : x + y = 0\\}\\)\n\n\nThe set of all polynomials of degree at most \\(n\\), denoted as \\(P_n\\), is a subspace of the vector space of all polynomials, denoted as \\(P\\), with closure under addition and scalar multiplication.\n\n\nSubset \\(V_2 = {p(x) \\in \\mathbb{R}[x] : \\text{deg}(p(x)) \\leq 3}\\) of vector space \\(\\mathbb{R}[x]\\): \\(V_2 = \\{p(x) \\in \\mathbb{R}[x] : \\text{deg}(p(x)) \\leq 3\\}\\)\n“deg” stands for “degree” and refers to the degree of a polynomial. In the example, \\(V_2 = {p(x) \\in \\mathbb{R}[x] : \\text{deg}(p(x)) \\leq 3}\\), \\(p(x)\\) represents a polynomial, and \\(\\text{deg}(p(x))\\) represents the degree of the polynomial \\(p(x)\\). The condition \\(\\text{deg}(p(x)) \\leq 3\\) specifies that the subset \\(V_2\\) includes all polynomials of degree 3 or lower in the vector space \\(\\mathbb{R}[x]\\) of polynomials with real coefficients.\n\n\nAll upper triangular matrices \\(\\begin{bmatrix}a&b\\\\0&d\\end{bmatrix} \\in \\mathbb M\\) are a subspace\nAll diagonal matrices \\(\\begin{bmatrix}a&0\\\\0&d\\end{bmatrix} \\in \\mathbb M\\) are a subspace"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#the-column-space-of-mathbf-a",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#the-column-space-of-mathbf-a",
    "title": "Vector Space",
    "section": "1.3 The Column Space of \\(\\mathbf A\\)",
    "text": "1.3 The Column Space of \\(\\mathbf A\\)\n\n1.3.1 Definition\n\nDefinition 3 Given a matrix \\(\\mathbf A\\) with columns \\(\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\), the column space of \\(\\mathbf A\\), denoted as \\(\\text{col}(\\mathbf A)\\), is the set of all possible linear combinations of the columns of \\(\\mathbf A\\). The combinations are all possible vectors \\(\\mathbf{Ax}\\). In other words, \\[\n\\begin{align*}\n\\text{col}(\\mathbf A) &= \\text{span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\} \\\\\nS &= \\text{ set of vectors in } V (\\text{ probably not a subspace }) \\\\\nSS &= \\text{ all combinations of vectors in } S \\\\\nSS &= \\text{ all } c_1\\mathbf{v}_1+\\ldots+c_n\\mathbf{v}_n \\\\\n  &= \\text{ the subspace of } V \\text{ spanned by } S\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nImportant\n\n\n\n\nTo solve \\(\\mathbf Ax=\\mathbf b\\) is to express \\(\\mathbf b\\) as a combination of the columns \\(\\mathbf a\\). \\(\\mathbf b\\) has to be in \\(\\text{col}(\\mathbf A)\\) or no solution.\n\nThe system \\(\\mathbf Ax=\\mathbf b\\) is solvable if and only if \\(\\mathbf b\\) is in \\(\\text{col}(\\mathbf A)\\).\n\nThe column space of \\(\\mathbf A\\) or \\(\\text{col}(\\mathbf A)\\) is a subspace of \\(\\mathbb R^{m}\\) not \\(\\mathbb R^{n}\\).\n\n\n\n\nGilbert Strang - Introduction to Linear Algebra\n\n\n\n\n\n\n1.3.2 Example\n\nConsider the matrix \\(\\mathbf A = \\begin{bmatrix}1 & 2 & 3 \\\\4 & 5 & 6 \\\\7 & 8 & 9 \\end{bmatrix}\\)\n\nThe column space of \\(\\mathbf A\\) is the span of its columns: \\(\\text{col}(\\mathbf A) = \\text{span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\mathbf{a}_3\\}\\)\n\nDescribe the column spaces for the following matrices\n\n\n\\(\\mathbf I = \\begin{bmatrix}1 & 0 \\\\0 & 1 \\end{bmatrix}\\)\n\nThe column space of the identity matrix \\(\\mathbf{I}\\), denoted as \\(\\text{Col}(\\mathbf{I})\\), spans the entire space \\(\\mathbb{R}^2\\), as it includes all possible linear combinations of the standard unit vectors \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\). Thus, \\(\\text{col}(\\mathbf A)\\) is \\(\\mathbb R^2\\)\n\n\\(\\mathbf A = \\begin{bmatrix}1 & 2 \\\\2 & 4 \\end{bmatrix}\\)\n\n\\(\\text{Col}(\\mathbf{A})\\), is the subspace spanned by the columns of \\(\\mathbf{A}\\). Since the second column of \\(\\mathbf{A}\\) is a scalar multiple (twice) of the first column, the column space of \\(\\mathbf{A}\\) is a one-dimensional subspace, which is the line in \\(\\mathbb{R}^2\\) that lies along the direction of the first column of \\(\\mathbf{A}\\), \\(\\text{col}(\\mathbf A) = \\text{span}\\left\\{\\begin{bmatrix}1 \\\\ 2\\end{bmatrix}\\right\\}\\)\nThe equation \\(\\mathbf{Ax = b}\\) is only solvable when \\(\\mathbf{b}\\) is on the line.\n\n\\(\\mathbf B = \\begin{bmatrix}1 & 2 & 4\\\\0 & 0 & 4 \\end{bmatrix}\\)\n\nThese column vectors span the column space of \\(\\mathbf{B}\\). Since column 2 is a scalar multiple of column 1, and column 3 is a linear combination of columns 1 and 2, the column space of \\(\\mathbf{B}\\) is the span of the first two column vectors, which is a two-dimensional subspace of \\(\\mathbb{R}^2\\), \\(\\text{col}(\\mathbf B) = \\text{span}\\{\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3\\}\\) is \\(\\mathbb R^2\\).\n\n\n\n\n1.3.3 Properties\nThe column space of a matrix \\(\\mathbf{A}\\), denoted as \\(\\text{col}(\\mathbf{A})\\) or \\(\\text{span}{\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n}\\), where \\(\\mathbf{a}_i\\) represents the columns of \\(\\mathbf{A}\\), has several important properties:\nConsists of all possible linear combinations: The column space consists of all possible linear combinations of the columns of \\(\\mathbf{A}\\). In other words, any vector in \\(\\text{col}(\\mathbf{A})\\) can be expressed as a linear combination of the columns of \\(\\mathbf{A}\\).\n\n1.3.3.1 Is a subspace\nThe column space is a subspace of the vector space \\(\\mathbb{R}^m\\), meaning it is closed under vector addition and scalar multiplication. This property allows for the column space to be used in linear combinations and as a solution space for non-homogeneous linear equations.\n\n\n1.3.3.2 Is spanned by the columns of \\(\\mathbf{A}\\)\nThe column space is spanned by the columns of \\(\\mathbf{A}\\), meaning any vector in \\(\\text{col}(\\mathbf{A})\\) can be expressed as a linear combination of the columns of \\(\\mathbf{A}\\).\n\n\n1.3.3.3 Has the same dimensionality as the row space\nThe dimensionality of the column space, denoted as \\(\\text{dim}(\\text{col}(\\mathbf{A}))\\), is equal to the dimensionality of the row space of \\(\\mathbf{A}\\), denoted as \\(\\text{dim}(\\text{row}(\\mathbf{A}))\\). This property is known as the rank-nullity theorem.\n\n\n1.3.3.4 Has the same dimensionality as the row space\nThe dimensionality of the column space, denoted as \\(\\text{dim}(\\text{col}(\\mathbf{A}))\\), is equal to the dimensionality of the row space of \\(\\mathbf{A}\\), denoted as \\(\\text{dim}(\\text{row}(\\mathbf{A}))\\). This property is known as the rank-nullity theorem. Is orthogonal to the nullspace: The column space is orthogonal to the nullspace of \\(\\mathbf{A}\\), meaning any vector in the column space is orthogonal to any vector in the nullspace of \\(\\mathbf{A}\\). This property is known as the fundamental theorem of linear algebra."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#nullspace-of-solving",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#nullspace-of-solving",
    "title": "Vector Space",
    "section": "1.4 Nullspace of : Solving ",
    "text": "1.4 Nullspace of : Solving \n\n1.4.1 Definition\n\nDefinition 4 The nullspace of a matrix refers to the set of all vectors that, when multiplied by the matrix, result in the zero vector \\(\\mathbf{0}\\). For a matrix \\(\\mathbf{A}\\), the nullspace is denoted as \\(\\text{null}(\\mathbf{A})\\) and defined as follows: \\[\n\\text{null}(\\mathbf{A}) = \\{\\mathbf{x} \\in \\mathbb{R}^n \\mid \\mathbf{Ax} = \\mathbf{0}\\}\n\\]\nThe nullspace of \\(\\mathbf{A}\\) is the set of all solutions \\(\\mathbf{x}\\) to the homogeneous equation \\(\\mathbf{Ax} = \\mathbf{0}\\).\n\nAll the solutions \\(\\mathbf{x} \\in \\mathbb R^n\\), so the \\(\\text{null}(\\mathbf{A})\\) is a subspace of \\(\\mathbb R^n\\), and \\(\\operatorname{Col}(\\mathbf A)\\) is a subspace of \\(\\mathbb R^m\\) (from Gilbere Strang) If the right side \\(\\mathbf{b}\\) is not \\(\\mathbf{0}\\), the solutions of \\(\\mathbf{Ax=b}\\) do not form a subspace. The vector \\(\\mathbf{x=0}\\) is only a solution if \\(\\mathbf{b=0}\\). When the set of solutions does not include \\(\\mathbf{x=0}\\), it cannot be a subspace. Section 3.4 will show how the solutions to \\(\\mathbf{Ax=b}\\) (if there are any solutions) are shifted away from the origin by one particular solution.\n\n\n1.4.2 Example\n\n1.4.2.1 Example 1\n\\[\n\\begin{align*}\n\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix} \\quad \\mathbf{Ax=0} \\\\\n\\text{null}(\\mathbf{A}) = \\{\\mathbf{x} \\in \\mathbb{R}^n \\mid \\mathbf{Ax} = \\mathbf{0}\\} \\text{ is a subspace of }\\mathbb R^3\n\\end{align*}\n\\]\n\n\nCode\n# Define the coefficients of the planes\na1, b1, c1, d1 = 1, 2, 3, 0\na2, b2, c2, d2 = 1, 2, 3, 6\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\n# Compute the corresponding z values for each x, y pair for the first plane\nZ1 = (-a1*X - b1*Y - d1) / c1\n\n# Compute the corresponding z values for each x, y pair for the second plane\nZ2 = (-a2*X - b2*Y - d2) / c2\n\n# Check if the origin is in the blue plane\norigin_in_blue_plane = a1*0 + b1*0 + c1*0 + d1 == 0\n\n# Check if the origin is in the red plane\norigin_in_red_plane = a2*0 + b2*0 + c2*0 + d2 == 0\n\n# Create a 3D plot\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Plot the first plane\nax.plot_surface(X, Y, Z1, color='blue', alpha=0.5)\n\n# Plot the second plane\nax.plot_surface(X, Y, Z2, color='red', alpha=0.5)\n\n# Plot the origin as a black point\nax.scatter(0, 0, 0, color='black', s=50)\n\n\n# Highlight the origin in the blue plane\nif origin_in_blue_plane:\n    ax.scatter(0, 0, 0, color='blue', s=50, label=r'Origin in the $x+2y+3z=0$ Plane')\n\n# Highlight the origin in the red plane\nif origin_in_red_plane:\n    ax.scatter(0, 0, 0, color='red', s=50, label=r'Origin in the $x+2y+3z=6$ Plane')\n\n# Set labels and title\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title(r'Example of Nullspace: $x+2y+3z=0$ vs $x+2y+3z=6$')\n\n# Add a legend\nax.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n1.4.2.2 Example 2\n\\[\n\\begin{align*}\n\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\\\ 2 & 4 \\end{bmatrix} \\quad\n\\text{null}(\\mathbf{A}) = \\left\\{ \\begin{bmatrix} -2t \\\\ t \\end{bmatrix} \\mid t \\in \\mathbb{R} \\right\\}\n\\end{align*}\n\\]\nThe null space of a matrix \\(\\mathbf{A}\\), denoted as \\(\\text{null}(\\mathbf{A})\\), is the set of all vectors \\(\\mathbf{x}\\) that satisfy the equation \\(\\mathbf{Ax} = \\mathbf{0}\\), where \\(\\mathbf{0}\\) is the zero vector.\nFor the given matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\\\ 2 & 4 \\end{bmatrix}\\), we can find the null space by solving the equation \\(\\mathbf{Ax} = \\mathbf{0}\\).\nLet’s set up the augmented matrix \\([\\mathbf{A} | \\mathbf{0}]\\) and perform Gaussian elimination to find the row-reduced echelon form (RREF):\n\\[\n\\left[ \\begin{array}{ccc|c}\n1 & 2 & 0 & 0 \\\\\n3 & 6 & 0 & 0 \\\\\n2 & 4 & 0 & 0 \\\\\n\\end{array} \\right]\n\\]\nRow 2 is equal to Row 1 multiplied by 3, and Row 3 is equal to Row 1 multiplied by 2. Thus, Row 3 is redundant and can be eliminated.\n\\[\n\\begin{bmatrix}{ccc|c}\n1 & 2 & 0 & 0 \\\\\n3 & 6 & 0 & 0 \\\\\n2 & 4 & 0 & 0 \\\\\n\\end{bmatrix}\n\\]\n\\[\n\\begin{align*}\n\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\\\ 2 & 4 \\end{bmatrix} \\quad\n\\text{null}(\\mathbf{A}) = \\left\\{ \\begin{bmatrix} -2t \\\\ t \\end{bmatrix} \\mid t \\in \\mathbb{R} \\right\\}\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\mathbf{B} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\quad\n\\text{null}(\\mathbf{B}) = \\left\\{ \\begin{bmatrix} 0 \\\\ t \\\\ 0 \\end{bmatrix} \\mid t \\in \\mathbb{R} \\right\\}\n\\end{align*}\n\\]\n\n\n\n1.4.3 Properties\n\n1.4.3.1 Nullspace Contains the Zero Vector\nThe nullspace always contains the zero vector \\(\\mathbf{0}\\), since \\(\\mathbf{A}\\mathbf{0} = \\mathbf{0}\\).\n\n\n1.4.3.2 Nullspace Is a Subspace\nThe nullspace is a subspace of the vector space \\(\\mathbb{R}^n\\), meaning it is closed under vector addition and scalar multiplication. This property allows for the nullspace to be used in linear combinations and as a solution space for homogeneous linear equations.\n\n\n1.4.3.3 Nullspace Is Orthogonal to Row Space\nThe nullspace is orthogonal to the row space of \\(\\mathbf{A}\\), meaning any vector in the nullspace is orthogonal to all vectors in the row space of \\(\\mathbf{A}\\).\n\n\n1.4.3.4 Nullspace Can Be Non-mpty\nThe nullspace can be non-empty, meaning there can be non-trivial solutions to the homogeneous equation \\(\\mathbf{Ax} = \\mathbf{0}\\).\n\n\n1.4.3.5 Dimensionality\nThe dimensionality of the nullspace, denoted as \\(\\text{dim}(\\text{null}(\\mathbf{A}))\\), is also known as the nullity of \\(\\mathbf{A}\\). The nullity of \\(\\mathbf{A}\\) is equal to the number of linearly independent columns or the number of free variables in the row-reduced echelon form (RREF) of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#rank-and-row-reduced-form",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#rank-and-row-reduced-form",
    "title": "Vector Space",
    "section": "1.5 Rank and Row Reduced Form",
    "text": "1.5 Rank and Row Reduced Form"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#complete-solution",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#complete-solution",
    "title": "Vector Space",
    "section": "1.6 Complete Solution",
    "text": "1.6 Complete Solution"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#independence-basis-and-dimension",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#independence-basis-and-dimension",
    "title": "Vector Space",
    "section": "1.7 Independence, Basis, and Dimension",
    "text": "1.7 Independence, Basis, and Dimension"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#dimensions-of-four-subspaces",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#dimensions-of-four-subspaces",
    "title": "Vector Space",
    "section": "1.8 Dimensions of Four Subspaces",
    "text": "1.8 Dimensions of Four Subspaces"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#basis-vector",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#basis-vector",
    "title": "Vector Space",
    "section": "1.9 Basis Vector",
    "text": "1.9 Basis Vector\nA basis for a vector space \\(V\\) is a set of vectors \\({v_1, v_2, \\ldots, v_n}\\) that spans \\(V\\) and is linearly independent. In other words, every vector in \\(V\\) can be expressed as a linear combination of the basis vectors, and the basis vectors are linearly independent, meaning that no basis vector can be written as a linear combination of the other basis vectors.\nMathematically, a set of vectors \\({v_1, v_2, \\ldots, v_n}\\) is a basis for a vector space \\(V\\) if it satisfies the following conditions:\nThe vectors \\({v_1, v_2, \\ldots, v_n}\\) span \\(V\\), which means that for any vector \\(v\\) in \\(V\\), there exist scalars \\(c_1, c_2, \\ldots, c_n\\) such that \\(v = c_1 v_1 + c_2 v_2 + \\ldots + c_n v_n\\).\nThe vectors \\({v_1, v_2, \\ldots, v_n}\\) are linearly independent, which means that the only solution to the equation \\(c_1 v_1 + c_2 v_2 + \\ldots + c_n v_n = 0\\) is \\(c_1 = c_2 = \\ldots = c_n = 0\\).\n\n1.9.1 Example\nConsider the vector space \\(V = \\mathbb{R}^3\\), the set of all real-valued vectors with three components. A basis for \\(V\\) can be given by the following set of vectors:\n\\[\nv_1 = \\begin{bmatrix}\n1 \\\\ 0 \\\\ 0\n\\end{bmatrix}, \\quad\nv_2 = \\begin{bmatrix}\n0 \\\\ 1 \\\\ 0\n\\end{bmatrix}, \\quad\nv_3 = \\begin{bmatrix}\n0 \\\\ 0 \\\\ 1\n\\end{bmatrix}\n\\]\nThese three vectors form a basis for \\(\\mathbb{R}^3\\), as they span \\(\\mathbb{R}^3\\) (any vector in \\(\\mathbb{R}^3\\) can be expressed as a linear combination of these vectors) and they are linearly independent (the only solution to the equation \\(c_1 v_1 + c_2 v_2 + c_3 v_3 = 0\\) is \\(c_1 = c_2 = c_3 = 0\\)).\nBasis vectors are fundamental in defining and understanding vector spaces because they provide a way to express any vector in the vector space as a linear combination of basis vectors. The properties of basis vectors, such as linear independence, spanning set, minimal set, and unique representation, are essential in establishing the foundational concepts of vector spaces and their properties. By using basis vectors, we can represent vectors in a vector space in a concise and systematic way, and they provide a basis for studying and analyzing vector spaces in various mathematical and practical applications.\n\n\n1.9.2 Properties\n\nLinear independence: A set of basis vectors is linearly independent, meaning that no vector in the set can be expressed as a linear combination of the others. In other words, the coefficients of the basis vectors in any linear combination that equals the zero vector are all zero.\nSpanning set: The set of basis vectors spans the entire vector space, meaning that any vector in the vector space can be expressed as a linear combination of the basis vectors. In other words, the basis vectors “span” the vector space by forming a basis for it.\nMinimal set: The set of basis vectors is minimal, meaning that no vector can be removed from the set without changing the span of the vector space. In other words, the basis vectors form the smallest possible set that can generate the entire vector space.\nUnique representation: Any vector in the vector space can be uniquely represented as a linear combination of the basis vectors. This means that there is only one way to express a vector as a linear combination of the basis vectors, ensuring that the representation is unique."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#subspace",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#subspace",
    "title": "Vector Space",
    "section": "1.10 Subspace",
    "text": "1.10 Subspace\n\n1.10.1 Definition of a subspace\nA subset \\(W\\) of \\(V\\) is called a subspace of \\(V\\) if it satisfies the following three conditions: 1. \\(W\\) contains the zero vector \\(\\mathbf{0}\\) of \\(V\\). 2. \\(W\\) is closed under vector addition, i.e., for any vectors \\(\\mathbf{u}, \\mathbf{v} \\in W\\), their sum \\(\\mathbf{u} + \\mathbf{v}\\) is also in \\(W\\). 3. \\(W\\) is closed under scalar multiplication, i.e., for any vector \\(\\mathbf{u} \\in W\\) and any scalar \\(c\\), their product \\(c\\mathbf{u}\\) is also in \\(W\\).\n\n\n1.10.2 Example\nConsider the vector space \\(V = \\mathbb{R}^3\\) with standard vector addition and scalar multiplication. Let \\(W\\) be the subset of \\(V\\) consisting of all vectors of the form \\(\\begin{bmatrix} x \\\\ y \\\\ 0 \\end{bmatrix}\\), where \\(x, y\\) are real numbers. We can express \\(W\\) as: \\(W = \\left\\{ \\begin{bmatrix} x \\\\ y \\\\ 0 \\end{bmatrix} \\, \\middle| \\, x, y \\in \\mathbb{R} \\right\\}\\) Then \\(W\\) is a subspace of \\(V\\) because it satisfies the three conditions mentioned above.\n\n\n1.10.3 Properties\nA subspace is a subset of a vector space that retains the structure of a vector space. Here are some properties of a subspace in relation to a vector space:\n\nContains the zero vector: A subspace must contain the zero vector (denoted as \\(\\mathbf{0}\\)) of the vector space it is a subset of. This is because the zero vector is required for closure under vector addition and scalar multiplication.\nClosed under vector addition: If \\(\\mathbf{u}, \\mathbf{v} \\in W\\), then \\(\\mathbf{u} + \\mathbf{v} \\in W\\).\nClosed under scalar multiplication: If \\(\\mathbf{u} \\in W\\) and \\(c\\) is a scalar, then \\(c\\mathbf{u} \\in W\\).\n\nThese properties provide a way to describe subsets of vector spaces that inherit certain properties from the parent vector space. Subspaces are useful for understanding the structure, properties, and behavior of vector spaces in a more focused and simplified manner. Some reasons why we need subspaces are:\n\nSimplification and abstraction: Subspaces allow us to simplify the study of vector spaces by focusing on smaller, more manageable subsets that share similar properties. This abstraction can help us understand the fundamental structure and behavior of vector spaces without getting bogged down by the complexity of the entire vector space.\nStudy of special cases: Subspaces can represent special cases or special structures within a vector space that are of particular interest.\n\nFor example, subspaces can represent sub-spaces of solutions to linear systems of equations, eigenspaces associated with eigenvalues of matrices, or orthogonal subspaces related to orthogonality and projections.\n\nApplications in various fields: Subspaces have numerous applications in various fields, such as physics, computer graphics, data analysis, signal processing, and optimization, among others. Subspaces provide a mathematical framework for modeling, analyzing, and solving problems in these fields.\nComputational efficiency: Subspaces can be used in algorithms and techniques for solving problems involving vector spaces in a computationally efficient manner. Techniques such as subspace methods, subspace approximation, and subspace projection can be employed to reduce the computational complexity of certain problems by working within lower-dimensional subspaces.\n\n\n\n1.10.4 Span\n\n\n1.10.5 Definition\nThe span of a set of vectors \\({v_1, v_2, \\ldots, v_n}\\) in a vector space \\(V\\), denoted by \\(\\text{span}{v_1, v_2, \\ldots, v_n}\\), is the set of all possible linear combinations of these vectors. Mathematically, it is defined as:\n\\[\n\\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\} = \\left\\{ c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\ldots + c_n\\mathbf{v}_n | c_1, c_2, \\ldots, c_n \\in \\mathbb{R} \\right\\}\n\\]\nwhere \\(c_1, c_2, \\ldots, c_n\\) are scalar coefficients and \\(\\mathbb{R}\\) represents the set of real numbers.\nExample: Let’s consider the set of vectors \\(v_1\\), \\(v_2\\), and \\(v_3\\) defined as:\n\\(\\mathbf{v}_1 = \\begin{bmatrix} 1 \\ 0 \\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\ 1 \\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_3 = \\begin{bmatrix} 0 \\ 0 \\ 1 \\end{bmatrix}\\)\nThe span of these vectors, denoted by \\(\\text{span}{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3}\\), is the set of all possible linear combinations of these vectors, which in this case is the entire three-dimensional vector space \\(\\mathbb{R}^3\\), since any vector in \\(\\mathbb{R}^3\\) can be expressed as a linear combination of \\(\\mathbf{v}_1\\), \\(\\mathbf{v}_2\\), and \\(\\mathbf{v}_3\\).\n\n\n1.10.6 Proporties\n\nSpan is a subspace: The span of a set of vectors is always a subspace of the vector space in which the vectors belong. This means that it satisfies all the properties of a vector space, including closure under vector addition and scalar multiplication.\nSpan is the smallest subspace: The span of a set of vectors is the smallest subspace that contains all the vectors in the set. It is the intersection of all subspaces that contain the vectors, and thus it forms the smallest subspace that spans the set of vectors.\nSpan is closed under linear combinations: The span of a set of vectors is closed under linear combinations of the vectors. This means that any linear combination of the vectors in the set will also be in the span.\nSpan generates the entire vector space: The span of a set of vectors is capable of generating the entire vector space to which the vectors belong. This means that any vector in the vector space can be expressed as a linear combination of the vectors in the span.\nSpan is unique: The span of a set of vectors is unique, meaning that it is uniquely determined by the set of vectors being spanned. However, the span may be different for different sets of vectors.\n\nThese properties make the concept of span important in linear algebra, as it allows us to understand the space spanned by a set of vectors and the relationships between vectors within a vector space.\nThe concept of span is important in linear algebra because of:\n\nUnderstanding vector space: The span of a set of vectors helps us understand the space that can be generated by those vectors within a vector space. It provides insight into the range of possible values and combinations that can be obtained using the given set of vectors.\nSolving systems of linear equations: Span is closely related to solving systems of linear equations. The solutions to a system of linear equations can be expressed as linear combinations of the vectors in the span of the coefficient matrix. By finding the span of a set of vectors, we can determine the possible solutions to a system of linear equations and understand the relationship between different solutions.\nBasis for vector spaces: The span of a set of vectors can form a basis for a vector space. A basis is a set of linearly independent vectors that span the entire vector space. By finding the span of a set of vectors, we can determine if they form a basis for a vector space, and if so, use them as a foundation for understanding and manipulating vectors within that space.\nVector space operations: Span is closed under vector space operations, such as vector addition and scalar multiplication. This property allows us to perform operations on vectors within the span and obtain new vectors that are still within the span. It also allows us to express any vector in the vector space as a linear combination of vectors in the span.\nDimensionality and rank: The span of a set of vectors is closely related to the dimensionality and rank of a vector space. The dimension of a vector space is the number of linearly independent vectors in a basis for that space, and the rank of a matrix is the dimension of the span of its column vectors. Understanding the span of vectors can help us determine the dimensionality and rank of a vector space, which has applications in areas such as data analysis, machine learning, and signal processing.\n\nSpan provides insights into the space spanned by a set of vectors, the relationships between vectors within a vector space, and the possible solutions to systems of linear equations. It also serves as a basis for vector spaces and facilitates vector space operations, and is closely related to the dimensionality and rank of a vector space."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#dimensionality-and-rank",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#dimensionality-and-rank",
    "title": "Vector Space",
    "section": "1.11 Dimensionality and Rank",
    "text": "1.11 Dimensionality and Rank\n\n1.11.1 Definition\n\nThe dimensionality of a vector space is defined as the number of linearly independent vectors in its basis.\nThe rank of a matrix is defined as the maximum number of linearly independent rows or columns in the matrix.\n\n\n\n1.11.2 Example\nLet \\(V\\) be a vector space with a basis \\({ \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n }\\). The dimensionality of \\(V\\), denoted as \\(\\text{dim}(V)\\), is the number of linearly independent vectors in its basis, which is equal to \\(n\\). Let \\(\\mathbf A\\) be an \\(m \\times n\\) matrix. The rank of \\(\\mathbf A\\), denoted as \\(\\text{rank}(\\mathbf A)\\), is the maximum number of linearly independent rows or columns in \\(\\mathbf A\\). It can also be defined as the dimensionality of the column space or row space of \\(\\mathbf A\\).\n\n\n1.11.3 Properties\n\n1.11.3.1 Dimensionality\n\nDimensionality refers to the number of elements or components in a vector or the size of a vector space.\nThe dimensionality of a vector space is always a positive integer.\nDimensionality is additive, meaning that the dimensionality of the direct sum of vector spaces is equal to the sum of their individual dimensionality.\nA set of vectors is linearly independent if and only if the dimensionality of the vector space they span is equal to the number of vectors in the set.\n\n\n\n1.11.3.2 Rank\n\nThe rank of a matrix is the maximum number of linearly independent rows or columns in the matrix.\nThe rank of a matrix is always a non-negative integer.\nThe rank of a matrix is equal to the dimensionality of its column space and row space.\nThe rank of a matrix is invariant under elementary row and column operations.\nThe rank of a matrix is less than or equal to the minimum of the number of rows and columns in the matrix.\n\n\n\n\n\n\n\nNote\n\n\n\nIn some contexts, the term “dimensionality” may also refer to the dimensionality of the column space or row space of a matrix, which is equivalent to the rank of the matrix."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#column-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#column-space",
    "title": "Vector Space",
    "section": "1.12 Column Space",
    "text": "1.12 Column Space\n\n1.12.1 Definition\nThe column space of a matrix is the subspace spanned by its column vectors. It is denoted by \\(\\text{Col}(\\mathbf A)\\) or \\(\\text{span}{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n}\\), where \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\) are the column vectors of the matrix \\(\\mathbf A\\).\n\n\n1.12.2 Example\n\\[\n\\mathbf A=\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\n\\]\nThe column space of matrix \\(\\mathbf A\\), denoted by \\(\\text{Col}(\\mathbf A)\\) or \\(\\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}\\), is the subspace spanned by its column vectors \\(\\mathbf{v}_1 = [1, 4, 7]^T\\), \\(\\mathbf{v}_2 = [2, 5, 8]^T\\), and \\(\\mathbf{v}_3 = [3, 6, 9]^T\\).\n\\[\n\\text{Col}(\\mathbf{A}) = \\text{span}\\left\\{\n\\begin{bmatrix}\n1 \\\\ 4 \\\\ 7 \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n2 \\\\ 5 \\\\ 8 \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n3 \\\\ 6 \\\\ 9 \\\\\n\\end{bmatrix}\n\\right\\}\n\\]\n\n\n1.12.3 Properties\n\nIt is a subspace: The column space of a matrix is a subspace of the vector space in which the matrix’s columns reside. This means it is closed under vector addition and scalar multiplication, and contains the zero vector.\nIt is spanned by the columns of the matrix: The column space of a matrix is the span of its column vectors. In other words, it is the smallest subspace that contains all the column vectors of the matrix.\nIt is the range of the corresponding linear transformation: The column space of a matrix is the range of the linear transformation associated with that matrix. This means it contains all possible output vectors that can be obtained by applying the linear transformation to input vectors.\nIt has the same dimension as the rank of the matrix: The dimension of the column space of a matrix is equal to the rank of the matrix. This is known as the column space’s dimensionality property.\nIt provides a basis for the row space and null space: The column space of a matrix provides a basis for both the row space and the null space of the matrix. The row space is the orthogonal complement of the null space, and the column space is the orthogonal complement of the left null space.\nIt determines the solvability of linear systems: The column space of a coefficient matrix in a system of linear equations determines whether the system has a unique solution, infinitely many solutions, or no solution at all. If the column space spans the entire vector space, the system has a unique solution. If the column space does not span the entire vector space, the system has either infinitely many solutions (if the rank of the matrix is less than the number of variables) or no solution (if the rank of the matrix is less than the number of equations)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#row-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#row-space",
    "title": "Vector Space",
    "section": "1.13 Row Space",
    "text": "1.13 Row Space\n\n1.13.1 Definition\nThe row space of a matrix \\(\\mathbf{A}\\), denoted as \\(\\text{Row}(\\mathbf{A})\\), is the subspace spanned by the rows of \\(\\mathbf{A}\\).\n\\[\n\\text{Row}(\\mathbf{A}) = \\text{span}\\{\\mathbf{r}_1, \\mathbf{r}_2, \\ldots, \\mathbf{r}_m\\}\n\\]\nwhere \\(\\mathbf{r}_1, \\mathbf{r}_2, \\ldots, \\mathbf{r}_m\\) are the rows of \\(\\mathbf{A}\\).\n\n\n1.13.2 Example\nThe row space of \\(\\mathbf{A}\\) is the subspace spanned by the rows of \\(\\mathbf{A}\\), which can be expressed as:\n\\[\n\\text{Row}(\\mathbf{A}) = \\text{span}\\left\\{\n\\begin{bmatrix}\n1 \\\\ 2 \\\\ 3 \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n4 \\\\ 5 \\\\ 6 \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n7 \\\\ 8 \\\\ 9 \\\\\n\\end{bmatrix}\n\\right\\}\n\\]\n\n\n1.13.3 Properties\n\nIt is a subspace: The row space of a matrix is a subspace of the vector space in which the matrix’s rows reside. This means it is closed under vector addition and scalar multiplication, and contains the zero vector.\nIt is spanned by the rows of the matrix: The row space of a matrix is the span of its row vectors. In other words, it is the smallest subspace that contains all the row vectors of the matrix.\nIt is the range of the corresponding linear transformation: The row space of a matrix is the range of the linear transformation associated with the transpose of that matrix. This means it contains all possible output vectors that can be obtained by applying the transpose of the linear transformation to input vectors.\nIt has the same dimension as the rank of the matrix: The dimension of the row space of a matrix is equal to the rank of the matrix. This is known as the row space’s dimensionality property.\nIt provides a basis for the null space and left null space: The row space of a matrix provides a basis for both the null space (kernel) and the left null space (cokernel) of the matrix. The null space is the orthogonal complement of the row space, and the left null space is the orthogonal complement of the column space.\nIt determines the row-rank and column-rank equality: The row space of a matrix determines the row-rank and column-rank equality property, which states that the number of linearly independent rows (the row-rank) is equal to the number of linearly independent columns (the column-rank) of the matrix.\nIt determines the solvability of linear systems: The row space of a coefficient matrix in a system of linear equations determines whether the system has a unique solution, infinitely many solutions, or no solution at all. If the row space spans the entire vector space, the system has a unique solution. If the row space does not span the entire vector space, the system has either infinitely many solutions (if the rank of the matrix is less than the number of variables) or no solution (if the rank of the matrix is less than the number of equations)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#null-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#null-space",
    "title": "Vector Space",
    "section": "1.14 Null Space",
    "text": "1.14 Null Space\n\\[\n\\text{Null}(\\mathbf{A}) = \\left\\{ \\mathbf{x} \\in \\mathbb{R}^n \\, \\middle| \\, \\mathbf{A}\\mathbf{x} = \\mathbf{0} \\right\\}\n\\]\nwhere:\n\n\\(\\text{Null}\\) represents the null space of a matrix,\n\\(\\mathbf{A}\\) represents the given matrix,\n\\(\\mathbf{x}\\) represents a vector in the null space of ,\n\\(\\mathbb{R}^n\\) represents the n-dimensional real vector space, and\n\\(\\mathbf{0}\\) represents the zero vector.\n\n\n1.14.1 Example\n\\[\n\\mathbf A=\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\n\\]\nThe null space of \\(\\mathbf{A}\\) is the set of all vectors \\(\\mathbf{x} \\in \\mathbb{R}^3\\) that satisfy \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\).\n\n\n1.14.2 Properties\nThe null space of a matrix, also known as the kernel space, is the set of all vectors that are mapped to the zero vector by the linear transformation associated with the matrix.\n\nContains the zero vector: The null space always contains the zero vector, as any matrix multiplied by the zero vector results in the zero vector.\nSubspace property: The null space is a subspace of the vector space from which the vectors are drawn. This means that it is closed under vector addition and scalar multiplication. In other words, if two vectors are in the null space, their sum and any scalar multiple of them will also be in the null space.\nDimensionality: The dimension of the null space is equal to the number of linearly independent solutions to the homogeneous linear system associated with the matrix. This is known as the nullity of the matrix.\nBasis: The null space has a basis consisting of linearly independent vectors that span the entire null space. This basis is used to represent all vectors in the null space as linear combinations of the basis vectors.\nRelationship to solvability: The null space is directly related to the solvability of a system of linear equations. If the null space contains only the zero vector, the system has a unique solution. If the null space contains non-zero vectors, the system has infinitely many solutions, and the null space vectors represent the general solutions.\nOrthogonal complement of row space: The null space is orthogonal to the row space of the matrix. This means that any vector in the null space is orthogonal to every vector in the row space, and vice versa.\nRelationship to matrix rank: The dimension of the null space, also known as the nullity, is related to the rank of the matrix through the rank-nullity theorem. The rank of the matrix plus the nullity of the matrix is equal to the number of columns in the matrix.\nComputation: The null space can be computed by finding the solutions to the homogeneous linear system \\(\\mathbf{Ax} = \\mathbf{0}\\), where \\(\\mathbf{A}\\) is the coefficient matrix of the system of linear equations."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#column-space-vs-row-space-vs-null-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/05.vector_space.html#column-space-vs-row-space-vs-null-space",
    "title": "Vector Space",
    "section": "1.15 Column Space vs Row Space vs Null Space",
    "text": "1.15 Column Space vs Row Space vs Null Space\nWhile both column space and row space are subspaces associated with a matrix, they serve different roles in linear algebra.\n\n1.15.1 Column Space vs Row Space\n\nDimensionality: The dimensionality of the column space and row space can differ. The dimension of the column space is determined by the number of linearly independent columns, which is also known as the rank of the matrix. On the other hand, the dimension of the row space is determined by the number of linearly independent rows of the matrix, which may not necessarily be the same as the rank of the matrix.\nBasis and Span: The column space is typically used to find a basis for the range (output space) of a linear transformation associated with the matrix, while the row space is used to find a basis for the null space (kernel) and left null space (cokernel) of the matrix. The column space spans the range of the linear transformation, while the row space spans the orthogonal complement of the null space and left null space.\nSolvability of linear systems: The row space of the coefficient matrix in a system of linear equations determines the solvability of the system, while the column space is not directly related to the solvability. Specifically, if the row space spans the entire vector space, the system has a unique solution. If the row space does not span the entire vector space, the system may have infinitely many solutions or no solution. The column space, on the other hand, does not directly determine the solvability of the system.\nTranspose relationship: The row space of a matrix is related to the range of the linear transformation associated with the transpose of the matrix, while the column space is directly related to the range of the original matrix. This means that the row space and column space are related through the transpose operation, but they are not identical.\n\n\n\n1.15.2 Null Space\nNull space: The null space of a matrix is the set of all vectors that are mapped to the zero vector by the linear transformation associated with the matrix. It represents the subspace of the vector space that consists of solutions to the homogeneous linear system \\(\\mathbf{Ax} = \\mathbf{0}\\), where \\(\\mathbf{A}\\) is the coefficient matrix of the system of linear equations.\n\n\n1.15.3 The relationships between these spaces\n\nThe column space and row space are related, as they are orthogonal complements of each other. This means that any vector in the row space is orthogonal to any vector in the null space, and vice versa. This relationship is known as the row-space-null-space decomposition.\nThe dimension of the column space is equal to the rank of the matrix, which is the maximum number of linearly independent columns or rows. Similarly, the dimension of the row space is also equal to the rank of the matrix.\nThe null space is orthogonal to both the column space and the row space. This means that any vector in the null space is orthogonal to any vector in the column space and the row space.\nThe null space is useful in determining the solvability of a system of linear equations. If the null space contains only the zero vector, the system has a unique solution. If the null space contains non-zero vectors, the system has infinitely many solutions."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html",
    "title": "Basics (3) - Special Matrices",
    "section": "",
    "text": "A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, it can be represented as: \\[\n\\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\] where \\(a_{ij}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column of the matrix \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#basic-matrix-operations",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#basic-matrix-operations",
    "title": "Basics (3) - Special Matrices",
    "section": "2 Basic Matrix Operations",
    "text": "2 Basic Matrix Operations\n\n2.1 Matrix addition\nThe sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.\nGiven two \\(m \\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is defined by:\n\\[\nc_{i,j}=a_{i,j}+b_{i,j}​\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\n\\[\n\\begin{bmatrix}\n  1 & 2 \\\\\n  3 & 4 \\\\\n  5 & 6\n\\end{bmatrix} +\n\\begin{bmatrix}\n  -1 & 0 \\\\\n  2 & -3 \\\\\n  -5 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n  0 & 2 \\\\\n  5 & 1 \\\\\n  0 & 10\n\\end{bmatrix}\n\\]\n\n\n2.2 Scalar multiplication\nThe product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.\nGiven a scalar \\(k\\) and an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), their product \\(k\\mathbf{A}\\) is defined by: \\[\n(k\\mathbf{A})_{i,j} = k(a_{i,j})\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\nExample: \\[\n2\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 & 4 \\\\\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\]\n\n\n2.3 Matrix multiplication\nThe product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is a matrix obtained by multiplying the rows of \\(\\mathbf{A}\\) by the columns of \\(\\mathbf{B}\\).\nGiven two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with dimensions \\(m \\times n\\) and \\(n \\times p\\), respectively, their product \\(\\mathbf{C} = \\mathbf{AB}\\) is an \\(m \\times p\\) matrix defined by: \\[\nc_{i,j} = \\sum_{k=1}^n a_{i,k}b_{k,j}\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq p\\).\nExample: \\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} \\begin{bmatrix}\n-1 & 0 & 2 \\\\\n2 & -3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 & -6 & 4 \\\\\n5 & -12 & 10 \\\\\n7 & -18 & 16\n\\end{bmatrix}\n\\]\n\n\n2.4 Transpose\nThe transpose of an \\(m x n\\) matrix A, denoted by \\(A^T\\), is the \\(n x m\\) matrix obtained by interchanging the rows and columns of A. Formally, if \\(\\mathbf{A} = [a_{ij}]\\) is an m x n matrix, then its transpose \\(\\mathbf{A}^T = [b_{ij}]\\) is an \\(n x m\\) matrix where \\(b_{ij}\\) = \\(a_{ji}\\) for all \\(i\\) and \\(j\\). In other words, the element in the \\(i\\) th row and \\(j\\) th column of \\(A^T\\) is equal to the element in the \\(j\\) th row and ith column of \\(\\mathbf{A}\\).\nGiven an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix defined by: $$ {i,j}^T = {j,i}\n$$\n\nWhen \\(\\mathbf{A}\\) is transposed, diagnoal entries(\\(a_{ii}\\)) do not change but off-diagnoal elements(\\(a_{ij} \\; i \\neq j\\)) change.\nA column vector is tranposed into a row vector, and vice versa.\nsymmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)\n\nExample:\nLet A be the matrix \\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\] The transpose of A, denoted by A^T, is the matrix \\[\n\\mathbf{A}^T = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}\n\\]\n\n\n2.5 Determinant\nLet \\(\\mathbf{A}\\) be an \\(n \\times n\\) square matrix. The determinant of \\(\\mathbf{A}\\), denoted by \\(|\\mathbf{A}|\\) or \\(\\det(\\mathbf{A})\\), is a scalar value calculated as the sum of the products of the elements in any row or column of \\(\\mathbf{A}\\) with their corresponding cofactors, that is,\n\\[\n|\\mathbf{A}|=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n\\]\nwhere \\(a_{ij}\\) is the element of \\(\\mathbf{A}\\) in the \\(i\\)-th row and \\(j\\)-th column, and \\(C_{ij}\\) is the cofactor of \\(a_{ij}\\). The cofactor of \\(a_{ij}\\), denoted by \\(C_{ij}\\), is given by \\((-1)^{i+j}\\) times the determinant of the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)-th row and \\(j\\)-th column of \\(\\mathbf{A}\\).\nThe determinant of an \\(n x n\\) matrix \\(\\mathbf{A}\\) is a scalar value denoted as \\(|\\mathbf{A}|\\). It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a \\(3 x 3\\) matrix \\(\\mathbf{A} =\\) \\[\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\]\nis given by:\n\\[\n|\\mathbf{A}| = a_{11}\n\\begin{vmatrix}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{vmatrix}\n- a_{12}\n\\begin{vmatrix}\na_{21} & a_{23} \\\\\na_{31} & a_{33}\n\\end{vmatrix}\n+ a_{13}\n\\begin{vmatrix}\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{vmatrix}\n\\]\nFor example, consider the \\(3 \\times 3\\) matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\). We can calculate the determinant of \\(\\mathbf{A}\\) using any row or column. Let’s use the first column:\n\\[\n|\\mathbf{A}| = 1\n\\begin{vmatrix}\n5 & 6 \\\\\n8 & 9\n\\end{vmatrix}\n- 4\n\\begin{vmatrix}\n2 & 3\\\\\n8 & 9\n\\end{vmatrix}\n+ 7\n\\begin{vmatrix}\n2 & 5 \\\\\n3 & 6\n\\end{vmatrix} = 0\n\\]\nTherefore, the determinant of \\(\\mathbf{A}\\) is zero.\n\n\n2.6 Inverse\nThe inverse of a square matrix \\(A\\) of size \\(n\\) is a matrix \\(A^{-1}\\) such that the product of \\(A\\) and \\(A^{-1}\\) is the identity matrix \\(I_n\\), i.e. \\(A \\times A^{-1} = I_n\\). If such a matrix exists, then \\(A\\) is said to be invertible or non-singular.\nThe inverse of a square matrix \\(\\mathbf{A}\\) is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the unique matrix that satisfies the following equation: \\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\nFor example, consider the \\(2\\times 2\\) matrix \\[\n\\mathbf{A} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}.\n\\] The inverse of \\(\\mathbf{A}\\) is given by:\n\\[\n\\mathbf{A}^{-1} =\n\\frac{1}{-2}\n\\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\]\nWe can verify that \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\) by computing:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} =\n\\begin{bmatrix} -2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\nLet me give another example and A be a \\(3x3\\) square matrix:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, the inverse of \\(\\mathbf{A}\\), denoted as \\(\\mathbf{A}^{-1}\\), is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\na_{22}a_{33}-a_{23}a_{32} & a_{13}a_{32}-a_{12}a_{33} & a_{12}a_{23}-a_{13}a_{22} \\\\\na_{23}a_{31}-a_{21}a_{33} & a_{11}a_{33}-a_{13}a_{31} & a_{13}a_{21}-a_{11}a_{23} \\\\\na_{21}a_{32}-a_{22}a_{31} & a_{12}a_{31}-a_{11}a_{32} & a_{11}a_{22}-a_{12}a_{21} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nwhere \\(det(\\mathbf{A})\\) is the determinant of \\(\\mathbf{A}\\).\nFor example, let:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, \\(det(A)\\) = -57, and the inverse of \\(\\mathbf{A}\\) is:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{-57}\\begin{bmatrix}\n-24 & 18 & 5 \\\\\n20 & -15 & -4 \\\\\n-3 & 2 & 1 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThere are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.\n\n\n2.7 Rank\nThe rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by \\(\\text{rank}(\\mathbf{A})\\).\nFor example, consider the following matrix:\n\\[\n\\begin{equation*}\n  \\mathbf{A} = \\begin{bmatrix}\n    1 & 2 & 3\\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\\\\\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe columns of \\(\\mathbf{A}\\) are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of \\(\\mathbf{A}\\) is 2.\n\n\n2.8 Trace\nThe trace of a square matrix \\(\\mathbf{A}\\), denoted by \\(\\mathrm{tr}(\\mathbf{A})\\), is defined as the sum of the diagonal elements of \\(\\mathbf{A}\\). In other words, if \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix, then its trace is given by:\n\\[\n\\mathrm{tr}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}\n\\]\nwhere \\(a_{ii}\\) denotes the \\(i\\) th diagonal element of \\(\\mathbf{A}\\).\nFor example, let \\[\n\\begin{bmatrix}\n  2 & 3 & 1 \\\\\n  0 & 5 & 2 \\\\\n  1 & 1 & 4\n\\end{bmatrix}\n\\]\nThen, the trace of \\(\\mathbf{A}\\) is \\(\\mathrm{tr}(\\mathbf{A}) = 2 + 5 + 4 = 11\\)\n\n\n2.9 Eigenvalues and Eigenvectors\nLet A be an \\(n × n\\) square matrix. A scalar \\(\\lambda\\) is called an eigenvalue of \\(\\mathbf A\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that \\[\n\\mathbf{Av}=\\lambda\\mathbf{v}\n\\]\nSuch a vector \\(\\mathbf{v}\\) is called an eigenvector corresponding to \\(\\lambda\\).\nExample:\nLet \\(\\mathbf A\\) be the matrix\nTo find the eigenvalues of \\(\\mathbf A\\), we solve the characteristic equation \\(\\text{det}(\\mathbf A - \\lambda \\mathbf I ) = 0\\), where I is the n × n identity matrix.\n\\[\n\\begin{align*}\n  \\text{det}(\\mathbf A - \\lambda \\mathbf I )\n  &=\n    \\begin{vmatrix}\n    3 - \\lambda & 1 \\\\\n    1 & 3 - \\lambda\n    \\end{vmatrix} \\\\\n  &=\n  (3 - \\lambda)(3 - \\lambda) - 1 \\\\\n  &= \\lambda^2 - 6\\lambda + 8 = 0\n\\end{align*}\n\\]\nSolving this quadratic equation gives us the eigenvalues of \\(\\mathbf A\\): \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 4\\).\nTo find the eigenvectors corresponding to \\(\\lambda_1 = 2\\), we solve the equation \\((\\mathbf A - 2 \\mathbf I)\\mathbf{v} = \\mathbf{0}\\), where \\(\\mathbf I\\) is the \\(2 \\times 2\\) identity matrix.\n\\[\n\\begin{align*}\n  (\\mathbf A - 2 \\mathbf I)\\mathbf{v} =\n    \\begin{bmatrix}\n      1 & 1 \\\\\n      1 & 1\n    \\end{bmatrix}\n  \\begin{bmatrix}\n    x \\\\\n    y\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    0 \\\\\n    0\n  \\end{bmatrix}\n\\end{align*}\n\\]\nSolving this system of equations gives us the eigenvectors corresponding to \\(\\lambda_1 = 2\\): \\(\\mathbf{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\)\nSimilarly, for \\(\\lambda_2 = 4\\), we solve the equation \\((\\mathbf A - 4\\mathbf I)\\mathbf{v}\\) = \\(\\mathbf{0}\\) to get the eigenvectors corresponding to \\(\\lambda_2 = 4\\): \\(\\mathbf{v_2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)\n\n\n2.10 Singular value and Singluar Vectors\nThe singular value decomposition (SVD) of a matrix \\(\\mathbf A\\) is a factorization of \\(\\mathbf A\\) into the product of three matrices as follows:\n\\[\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n\\] where \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) rectangular diagonal matrix with non-negative real numbers on the diagonal, and \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix.\nThe diagonal entries of \\(\\mathbf{\\Sigma}\\) are called the singular values of \\(\\mathbf{A}\\), denoted as \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_r\\) (where \\(r\\) is the rank of \\(\\mathbf{A}\\)), and are arranged in descending order. The columns of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are called the left and right singular vectors of \\(\\mathbf{A}\\), respectively, and are orthonormal vectors.\nFor example, let \\(\\mathbf{A}\\) be a 3 by 2 matrix given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} =\n  \\begin{bmatrix}\n    1 & 2\\\\\n    3 & 4\\\\\n    5 & 6\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe SVD of \\(\\mathbf{A}\\) is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T =\n\\begin{bmatrix}\n  -0.23 & -0.53 & -0.81\\\\\n  -0.53 & -0.72 & 0.45\\\\\n  -0.81 & 0.45 & -0.38\n\\end{bmatrix}\n\\begin{bmatrix}\n  9.53 & 0\\\\\n  0 & 0.90\\\\\n  0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  -0.62 & -0.78\\\\\n  -0.78 & 0.62\n\\end{bmatrix}^T\n\\end{equation*}\n\\]\nwhere the left singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{U}\\), the right singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{V}\\), and the singular values of \\(\\mathbf{A}\\) are the diagonal entries of \\(\\boldsymbol{\\Sigma}\\).\n\n연립 방정식을 행렬의 곱으로 나타내보기 \\[\\begin{matrix}x_1+2y_1=4\\\\2x_1+5y_1=9\\end{matrix} \\quad \\quad \\quad \\begin{matrix}x_2+2y_2=3\\\\2x_2+5y_2=7\\end{matrix}\\] \\[ \\begin{bmatrix} 1 & 2 \\\\ 2 & 5 \\end{bmatrix} \\begin{bmatrix} x_1 & x_2 \\\\ y_1 & y_2 \\end{bmatrix} = \\begin{bmatrix} 4 & 9 \\\\ 3 & 7 \\end{bmatrix}\\]\n중요한 사실(….당연한 사실?)\n\n곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함\n\n\\(A_{m \\times n} \\times B_{o \\times p}\\) 에서 \\(n = o\\) 여야 곱셈 성립\n\n곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 \\(\\times\\) 곱셈의 오른쪽 행렬의 열 수\n\n\\(A_{m \\times n} \\times B_{o \\times p} = C_{m \\times p}\\)\n\n교환법칙(Commutative property)이 성립하지 않음\n\n\\(AB \\neq BA\\)\n\n\n행렬 곱셈의 여러가지 관점\n\n내적으로 바라보기 \\[ A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_1^T b_1} & \\mathbf{a_1^T b_2} & \\cdots & \\mathbf{a_1^T b_m} \\\\ \\mathbf{a_2^T b_1} & \\mathbf{a_2^T b_2} & \\cdots & \\mathbf{a_2^T b_m} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ \\mathbf{a_m^T b_1} & \\mathbf{a_m^T b_2} & \\cdots & \\mathbf{a_m^T b_m} \\end{bmatrix}\\]\nrank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) \\[AB = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1^T} \\\\ \\mathbf{b_2^T} \\\\ \\vdots \\\\ \\mathbf{b_m^T} \\end{bmatrix} = \\mathbf{a_1 b_1^T} + \\mathbf{a_2 b_2^T} + \\cdots + \\mathbf{a_m b_m^T}\\]\ncolumn space로 바라보기 \\[A\\mathbf{x} = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} = \\mathbf{a_1} x_1 + \\mathbf{a_2} x_2 + \\cdots + \\mathbf{a_m} x_m \\] (스칼라배의 합)\n\n\\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) 는 2차원 좌표평면의 모든 점을, \\(A=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)은 3차원 좌표평면의 모든 점 표현 가능\n\\(AB = A \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} A \\mathbf{b_1} & A \\mathbf{b_2} & \\cdots & A \\mathbf{b_m} \\end{bmatrix}\\)\ncolumn space: A의 column vector로 만들 수 있는 부분 공간\n\nrow space로 바라보기 \\[\\mathbf{x^T}A = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_m \\end{bmatrix} \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} = x_1 \\mathbf{a_1^T} + x_2 \\mathbf{a_2^T} + \\cdots + x_m \\mathbf{a_m^T} \\]"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#열공간column-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#열공간column-space",
    "title": "Basics (3) - Special Matrices",
    "section": "3 열공간(Column Space)",
    "text": "3 열공간(Column Space)\n\n\ncolumn space: column vector 들이 span 하는 space\n\n\\(A\\)의 column space = \\(C(A)\\) 또는 \\(range(A)\\)\n\nspan: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합\n\nvector에 따라, 점일수도 선일수도 평면일 수도 있음\nvector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space\n\nvector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것\n\n\\(\\mathbf{v_1}\\) 과 \\(\\mathbf{v_2}\\)의 linear combination으로 2차원 좌표평면 나타내기"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#선형-독립linear-independent",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#선형-독립linear-independent",
    "title": "Basics (3) - Special Matrices",
    "section": "4 선형 독립(Linear Independent)",
    "text": "4 선형 독립(Linear Independent)\n\n…and also see\n\n\n선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌\northogonal 하면 independent\n\nbut independent해도 항상 orthogonal하지는 않음 (Independent > Orthogonal)\n\ndefinition: \\(a_1 \\mathbf{v_1} + a_2 \\mathbf{v_2} + a_3 \\mathbf{v_3} \\cdots a_n \\mathbf{v_n} = \\mathbf{0}\\) 를 만족하는 \\(a_1, a_2, a_3, \\cdots a_n\\) 이 \\(a_1 = a_2 = a_3 = \\cdots = a_n = 0\\) 밖에 없을때\n\n\\(\\mathbf{0}\\)는 모든 elements가 \\(0\\)인 벡터\n예: \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\\) 는 \\(-2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) 이 되므로, linearly independent 하지 않음\nindependent한 vector 들의 수 = 표현할 수 있는 차원의 dimension"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#기저basis",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#기저basis",
    "title": "Basics (3) - Special Matrices",
    "section": "5 기저(basis)",
    "text": "5 기저(basis)\n\n주어진 vector space를 span하는 linearly independent한 vectors\n어떤 공간을 이루는 필수적인 구성요소\northogonal 하면 orthogonal basis\n예: 2차원 좌표평면에 대해\n\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) : orthogonal basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) : orthogonal 하지 않은 basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}\\) : linearly independent 하지 않으므로 basis 아님"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "title": "Basics (3) - Special Matrices",
    "section": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)",
    "text": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)\n\n\n6.1 Identity matrix(항등행렬)\n\n항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)\n\n실수에서 곱셈의 항등원은 1\n\n행렬의 항등원: 항등행렬(\\(I\\)) \\[I = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\\]\n\n\\(A_{m \\times n} \\times I_{n \\times n = n} = A_{m \\times n}\\)\n\\(I_{m \\times m = m} \\times A_{m \\times n} = A_{m \\times n}\\)\n\n\n\n\n6.2 Inverse matrix(역행렬)\n\n역원: 연산 결과 항등원이 나오게 하는 연소\n\n실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): \\(a \\times a^{-1} = 1\\)\n\n행렬의 역원: 역행렬(\\(A^{-1}\\)) \\[A \\times A^{-1} = I , A^{-1} \\times A = I\\]\n\n존재하지 않는 경우도 있음\n존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림\n\n존재하지 않으면 singular, degenerate라고 불림\n\nsquare matrix(정사각행렬, \\(m = n\\))은 특수한 경우를 제외하면 역행렬이 항상 존재\n\n역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우\n\n\\(m \\neq n\\)인 행렬의 경우에는 역행렬이 존재하지 않음\n\n다만, 경우에 따라 \\(A \\times A^{-1} = I\\) 를 만족하거나(right inverse), \\(A^{-1} \\times A = I\\)를 만족하는(left inverse)는 \\(A^{-1}\\)이 존재함\n\n연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 \\[A\\mathbf{x} = \\mathbf{b} \\Rightarrow A^{-1}A\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow I\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow \\mathbf{x} = A^{-1}\\mathbf{b}\\]\n\n\n\n\n6.3 Diagonal Matrix(대각행렬)\n\ndiagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix \\[ D = Diag(\\mathbf{a}) = \\begin{bmatrix} a_{1,1} & 0 & \\cdots & 0 \\\\ 0 & a_{2,2} & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & a_{n,n} \\end{bmatrix}\\]\n\nidentity matrix는 diagonal matrix\ndiagnomal matrix는 symmetric matrix 이기도 함\n보통은 square matrix에서 주로 사용됨\n\nsquare matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix\n\n\n\n\n\n6.4 Orthogonal matrix(직교행렬, orthonomal matrix)\n\n행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) \\[A A^T = A^T A = I\\]\nidentity matrix는 orthogonal matrix\nsquare matrix에서만 정의됨\nOrthogonal matrix인 \\(A\\)이면 \\(A^{-1} = A^{T}\\)\n\n각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임\n\ncomplex matrix(복소수 행렬)에서는 unitary matrix라고 부름"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#계수rank",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#계수rank",
    "title": "Basics (3) - Special Matrices",
    "section": "7 계수(Rank)",
    "text": "7 계수(Rank)\n\n\nrank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension\nindependent한 column의 수 = independent한 행의 수: \\(rank(A) = rank(A^T)\\)\n\nproof: Wikipedia\n\n예: \\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix} \\Rightarrow rank=1\\] \\[\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\Rightarrow rank=2\\]\n\\(A_{m \\times n}\\) 의 최대 랭크는 \\(min\\{m,n\\}\\)\n\n\\(rank(A) < min\\{m,n\\}\\) 면 rank-deficient, \\(rank(A) = min\\{m,n\\}\\)면 full (row/column) rank"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#영공간null-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#영공간null-space",
    "title": "Basics (3) - Special Matrices",
    "section": "8 영공간(Null space)",
    "text": "8 영공간(Null space)\n\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) 을 만족하는 \\(\\mathbf{x}\\)의 집합\n\ncolumn space 관점에서 보기: \\(A\\mathbf{x} = x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{0}\\)\nnull space에 항상 들어가는 \\(\\mathbf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\) : trivial solution\n\n모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 \\(\\mathbf{x}=\\mathbf{0}\\)하나 밖에 없음\n\n\n\\(\\mathbf{x}=\\mathbf{0}\\) 가 아닌 vector가 null space에 있으면, 스칼라배(constant \\(c\\)에 대해 \\(c \\mathbf{x}\\)) 역시 null space에 포함됨\n혼동 주의! null space는 column space의 일부가 아님\n\nrow vector의 차원이 null space가 존재하는 공간\n\nrank와 null space의 dimension의 합은 항상 matrix의 column의 수\n\n\\(A_{m \\times n}\\)에 대해, \\(dim(N(A)) = n - r\\)\n\n모든 columns이 다 lienarly independent 하면 null space는 0차원(점)\n\nnull space는 row space와 수직한 space\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0\nrank는 row space의 dimension → row space의 dimension(\\(dim(R(A))\\))과 null space의 dimension(\\(dim(N(A))\\))의 합이 \\(n\\)\n\\(\\mathbb{R^n}\\) 공간에 표현: \n\n겹친 점: 영벡터\n\n\n\nleft null space: \\(\\mathbf{x^T} A = \\mathbf{0^T}\\) 인 \\(\\mathbf{x}\\)\n\n위의 성질을 row에 대해 적용\n\nm 차원에 놓인 벡터\ndimension: \\(dim(N_L(A)) = m - r\\)\ncolumn space와 수직: \\(dim(N_L(A)) +dim(C(A)) = m\\)\n\n\n\\(R(A)\\)에 있는 vector \\(\\mathbf{x_r}\\) 와 \\(N(A)\\)에 있는 vector \\(\\mathbf{x_n}\\)에 대해:\n\n\\(\\mathbf{x_r}\\)에 \\(A\\)를 곱하면 column space로 감\n\\(\\mathbf{x_n}\\)에 \\(A\\)를 곱하면 $\n\\(A(\\mathbf{x_r} +\\mathbf{x_n}) = A\\mathbf{x_r} + A\\mathbf{x_n} = A\\mathbf{x_r} = \\mathbf{b}\\)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#ax-b의-해의-수",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.linear_transformation.html#ax-b의-해의-수",
    "title": "Basics (3) - Special Matrices",
    "section": "9 Ax = b의 해의 수",
    "text": "9 Ax = b의 해의 수\n\n\nfull column rank 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 해가 하나\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음 \n\nfull row rank 일때\n\n\\(\\mathbf{b}\\)는 항상 column space 안에 있음: 무한의 해를 가짐\n임의의 특정한 해(particular solution) \\(\\mathbf{x_p}\\)와 null space의 vector \\(\\mathbf{x_n}\\)에 대해, \\(A(\\mathbf{x_p} +\\mathbf{x_n})=\\mathbf{b}\\)\n\n즉, \\(\\mathbf{x_p} +\\mathbf{x_n}\\) 도 해가 됨: complete solution\n\nnull space는 무한하므로, 해도 무한함\n\n\n\nfull rank 일때(square matrix): 해가 하나 존재 (\\(\\mathbf{x} = A^{-1}\\)$)\nrank-deficient 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 무한한 해를 가짐\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html",
    "title": "Basics (3) - Special Matrices",
    "section": "",
    "text": "A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, it can be represented as: \\[\n\\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\] where \\(a_{ij}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column of the matrix \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#basic-matrix-operations",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#basic-matrix-operations",
    "title": "Basics (3) - Special Matrices",
    "section": "2 Basic Matrix Operations",
    "text": "2 Basic Matrix Operations\n\n2.1 Matrix addition\nThe sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.\nGiven two \\(m \\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is defined by:\n\\[\nc_{i,j}=a_{i,j}+b_{i,j}​\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\n\\[\n\\begin{bmatrix}\n  1 & 2 \\\\\n  3 & 4 \\\\\n  5 & 6\n\\end{bmatrix} +\n\\begin{bmatrix}\n  -1 & 0 \\\\\n  2 & -3 \\\\\n  -5 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n  0 & 2 \\\\\n  5 & 1 \\\\\n  0 & 10\n\\end{bmatrix}\n\\]\n\n\n2.2 Scalar multiplication\nThe product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.\nGiven a scalar \\(k\\) and an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), their product \\(k\\mathbf{A}\\) is defined by: \\[\n(k\\mathbf{A})_{i,j} = k(a_{i,j})\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\nExample: \\[\n2\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 & 4 \\\\\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\]\n\n\n2.3 Matrix multiplication\nThe product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is a matrix obtained by multiplying the rows of \\(\\mathbf{A}\\) by the columns of \\(\\mathbf{B}\\).\nGiven two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with dimensions \\(m \\times n\\) and \\(n \\times p\\), respectively, their product \\(\\mathbf{C} = \\mathbf{AB}\\) is an \\(m \\times p\\) matrix defined by: \\[\nc_{i,j} = \\sum_{k=1}^n a_{i,k}b_{k,j}\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq p\\).\nExample: \\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} \\begin{bmatrix}\n-1 & 0 & 2 \\\\\n2 & -3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 & -6 & 4 \\\\\n5 & -12 & 10 \\\\\n7 & -18 & 16\n\\end{bmatrix}\n\\]\n\n\n2.4 Transpose\nThe transpose of an \\(m x n\\) matrix A, denoted by \\(A^T\\), is the \\(n x m\\) matrix obtained by interchanging the rows and columns of A. Formally, if \\(\\mathbf{A} = [a_{ij}]\\) is an m x n matrix, then its transpose \\(\\mathbf{A}^T = [b_{ij}]\\) is an \\(n x m\\) matrix where \\(b_{ij}\\) = \\(a_{ji}\\) for all \\(i\\) and \\(j\\). In other words, the element in the \\(i\\) th row and \\(j\\) th column of \\(A^T\\) is equal to the element in the \\(j\\) th row and ith column of \\(\\mathbf{A}\\).\nGiven an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix defined by: $$ {i,j}^T = {j,i}\n$$\n\nWhen \\(\\mathbf{A}\\) is transposed, diagnoal entries(\\(a_{ii}\\)) do not change but off-diagnoal elements(\\(a_{ij} \\; i \\neq j\\)) change.\nA column vector is tranposed into a row vector, and vice versa.\nsymmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)\n\nExample:\nLet A be the matrix \\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\] The transpose of A, denoted by A^T, is the matrix \\[\n\\mathbf{A}^T = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}\n\\]\n\n\n2.5 Determinant\nLet \\(\\mathbf{A}\\) be an \\(n \\times n\\) square matrix. The determinant of \\(\\mathbf{A}\\), denoted by \\(|\\mathbf{A}|\\) or \\(\\det(\\mathbf{A})\\), is a scalar value calculated as the sum of the products of the elements in any row or column of \\(\\mathbf{A}\\) with their corresponding cofactors, that is,\n\\[\n|\\mathbf{A}|=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n\\]\nwhere \\(a_{ij}\\) is the element of \\(\\mathbf{A}\\) in the \\(i\\)-th row and \\(j\\)-th column, and \\(C_{ij}\\) is the cofactor of \\(a_{ij}\\). The cofactor of \\(a_{ij}\\), denoted by \\(C_{ij}\\), is given by \\((-1)^{i+j}\\) times the determinant of the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)-th row and \\(j\\)-th column of \\(\\mathbf{A}\\).\nThe determinant of an \\(n x n\\) matrix \\(\\mathbf{A}\\) is a scalar value denoted as \\(|\\mathbf{A}|\\). It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a \\(3 x 3\\) matrix \\(\\mathbf{A} =\\) \\[\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\]\nis given by:\n\\[\n|\\mathbf{A}| = a_{11}\n\\begin{vmatrix}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{vmatrix}\n- a_{12}\n\\begin{vmatrix}\na_{21} & a_{23} \\\\\na_{31} & a_{33}\n\\end{vmatrix}\n+ a_{13}\n\\begin{vmatrix}\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{vmatrix}\n\\]\nFor example, consider the \\(3 \\times 3\\) matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\). We can calculate the determinant of \\(\\mathbf{A}\\) using any row or column. Let’s use the first column:\n\\[\n|\\mathbf{A}| = 1\n\\begin{vmatrix}\n5 & 6 \\\\\n8 & 9\n\\end{vmatrix}\n- 4\n\\begin{vmatrix}\n2 & 3\\\\\n8 & 9\n\\end{vmatrix}\n+ 7\n\\begin{vmatrix}\n2 & 5 \\\\\n3 & 6\n\\end{vmatrix} = 0\n\\]\nTherefore, the determinant of \\(\\mathbf{A}\\) is zero.\n\n\n2.6 Inverse\nThe inverse of a square matrix \\(A\\) of size \\(n\\) is a matrix \\(A^{-1}\\) such that the product of \\(A\\) and \\(A^{-1}\\) is the identity matrix \\(I_n\\), i.e. \\(A \\times A^{-1} = I_n\\). If such a matrix exists, then \\(A\\) is said to be invertible or non-singular.\nThe inverse of a square matrix \\(\\mathbf{A}\\) is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the unique matrix that satisfies the following equation: \\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\nFor example, consider the \\(2\\times 2\\) matrix \\[\n\\mathbf{A} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}.\n\\] The inverse of \\(\\mathbf{A}\\) is given by:\n\\[\n\\mathbf{A}^{-1} =\n\\frac{1}{-2}\n\\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\]\nWe can verify that \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\) by computing:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} =\n\\begin{bmatrix} -2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\nLet me give another example and A be a \\(3x3\\) square matrix:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, the inverse of \\(\\mathbf{A}\\), denoted as \\(\\mathbf{A}^{-1}\\), is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\na_{22}a_{33}-a_{23}a_{32} & a_{13}a_{32}-a_{12}a_{33} & a_{12}a_{23}-a_{13}a_{22} \\\\\na_{23}a_{31}-a_{21}a_{33} & a_{11}a_{33}-a_{13}a_{31} & a_{13}a_{21}-a_{11}a_{23} \\\\\na_{21}a_{32}-a_{22}a_{31} & a_{12}a_{31}-a_{11}a_{32} & a_{11}a_{22}-a_{12}a_{21} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nwhere \\(det(\\mathbf{A})\\) is the determinant of \\(\\mathbf{A}\\).\nFor example, let:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, \\(det(A)\\) = -57, and the inverse of \\(\\mathbf{A}\\) is:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{-57}\\begin{bmatrix}\n-24 & 18 & 5 \\\\\n20 & -15 & -4 \\\\\n-3 & 2 & 1 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThere are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.\n\n\n2.7 Rank\nThe rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by \\(\\text{rank}(\\mathbf{A})\\).\nFor example, consider the following matrix:\n\\[\n\\begin{equation*}\n  \\mathbf{A} = \\begin{bmatrix}\n    1 & 2 & 3\\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\\\\\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe columns of \\(\\mathbf{A}\\) are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of \\(\\mathbf{A}\\) is 2.\n\n\n2.8 Trace\nThe trace of a square matrix \\(\\mathbf{A}\\), denoted by \\(\\mathrm{tr}(\\mathbf{A})\\), is defined as the sum of the diagonal elements of \\(\\mathbf{A}\\). In other words, if \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix, then its trace is given by:\n\\[\n\\mathrm{tr}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}\n\\]\nwhere \\(a_{ii}\\) denotes the \\(i\\) th diagonal element of \\(\\mathbf{A}\\).\nFor example, let \\[\n\\begin{bmatrix}\n  2 & 3 & 1 \\\\\n  0 & 5 & 2 \\\\\n  1 & 1 & 4\n\\end{bmatrix}\n\\]\nThen, the trace of \\(\\mathbf{A}\\) is \\(\\mathrm{tr}(\\mathbf{A}) = 2 + 5 + 4 = 11\\)\n\n\n2.9 Eigenvalues and Eigenvectors\nLet A be an \\(n × n\\) square matrix. A scalar \\(\\lambda\\) is called an eigenvalue of \\(\\mathbf A\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that \\[\n\\mathbf{Av}=\\lambda\\mathbf{v}\n\\]\nSuch a vector \\(\\mathbf{v}\\) is called an eigenvector corresponding to \\(\\lambda\\).\nExample:\nLet \\(\\mathbf A\\) be the matrix\nTo find the eigenvalues of \\(\\mathbf A\\), we solve the characteristic equation \\(\\text{det}(\\mathbf A - \\lambda \\mathbf I ) = 0\\), where I is the n × n identity matrix.\n\\[\n\\begin{align*}\n  \\text{det}(\\mathbf A - \\lambda \\mathbf I )\n  &=\n    \\begin{vmatrix}\n    3 - \\lambda & 1 \\\\\n    1 & 3 - \\lambda\n    \\end{vmatrix} \\\\\n  &=\n  (3 - \\lambda)(3 - \\lambda) - 1 \\\\\n  &= \\lambda^2 - 6\\lambda + 8 = 0\n\\end{align*}\n\\]\nSolving this quadratic equation gives us the eigenvalues of \\(\\mathbf A\\): \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 4\\).\nTo find the eigenvectors corresponding to \\(\\lambda_1 = 2\\), we solve the equation \\((\\mathbf A - 2 \\mathbf I)\\mathbf{v} = \\mathbf{0}\\), where \\(\\mathbf I\\) is the \\(2 \\times 2\\) identity matrix.\n\\[\n\\begin{align*}\n  (\\mathbf A - 2 \\mathbf I)\\mathbf{v} =\n    \\begin{bmatrix}\n      1 & 1 \\\\\n      1 & 1\n    \\end{bmatrix}\n  \\begin{bmatrix}\n    x \\\\\n    y\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    0 \\\\\n    0\n  \\end{bmatrix}\n\\end{align*}\n\\]\nSolving this system of equations gives us the eigenvectors corresponding to \\(\\lambda_1 = 2\\): \\(\\mathbf{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\)\nSimilarly, for \\(\\lambda_2 = 4\\), we solve the equation \\((\\mathbf A - 4\\mathbf I)\\mathbf{v}\\) = \\(\\mathbf{0}\\) to get the eigenvectors corresponding to \\(\\lambda_2 = 4\\): \\(\\mathbf{v_2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)\n\n\n2.10 Singular value and Singluar Vectors\nThe singular value decomposition (SVD) of a matrix \\(\\mathbf A\\) is a factorization of \\(\\mathbf A\\) into the product of three matrices as follows:\n\\[\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n\\] where \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) rectangular diagonal matrix with non-negative real numbers on the diagonal, and \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix.\nThe diagonal entries of \\(\\mathbf{\\Sigma}\\) are called the singular values of \\(\\mathbf{A}\\), denoted as \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_r\\) (where \\(r\\) is the rank of \\(\\mathbf{A}\\)), and are arranged in descending order. The columns of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are called the left and right singular vectors of \\(\\mathbf{A}\\), respectively, and are orthonormal vectors.\nFor example, let \\(\\mathbf{A}\\) be a 3 by 2 matrix given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} =\n  \\begin{bmatrix}\n    1 & 2\\\\\n    3 & 4\\\\\n    5 & 6\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe SVD of \\(\\mathbf{A}\\) is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T =\n\\begin{bmatrix}\n  -0.23 & -0.53 & -0.81\\\\\n  -0.53 & -0.72 & 0.45\\\\\n  -0.81 & 0.45 & -0.38\n\\end{bmatrix}\n\\begin{bmatrix}\n  9.53 & 0\\\\\n  0 & 0.90\\\\\n  0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  -0.62 & -0.78\\\\\n  -0.78 & 0.62\n\\end{bmatrix}^T\n\\end{equation*}\n\\]\nwhere the left singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{U}\\), the right singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{V}\\), and the singular values of \\(\\mathbf{A}\\) are the diagonal entries of \\(\\boldsymbol{\\Sigma}\\).\n\n연립 방정식을 행렬의 곱으로 나타내보기 \\[\\begin{matrix}x_1+2y_1=4\\\\2x_1+5y_1=9\\end{matrix} \\quad \\quad \\quad \\begin{matrix}x_2+2y_2=3\\\\2x_2+5y_2=7\\end{matrix}\\] \\[ \\begin{bmatrix} 1 & 2 \\\\ 2 & 5 \\end{bmatrix} \\begin{bmatrix} x_1 & x_2 \\\\ y_1 & y_2 \\end{bmatrix} = \\begin{bmatrix} 4 & 9 \\\\ 3 & 7 \\end{bmatrix}\\]\n중요한 사실(….당연한 사실?)\n\n곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함\n\n\\(A_{m \\times n} \\times B_{o \\times p}\\) 에서 \\(n = o\\) 여야 곱셈 성립\n\n곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 \\(\\times\\) 곱셈의 오른쪽 행렬의 열 수\n\n\\(A_{m \\times n} \\times B_{o \\times p} = C_{m \\times p}\\)\n\n교환법칙(Commutative property)이 성립하지 않음\n\n\\(AB \\neq BA\\)\n\n\n행렬 곱셈의 여러가지 관점\n\n내적으로 바라보기 \\[ A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_1^T b_1} & \\mathbf{a_1^T b_2} & \\cdots & \\mathbf{a_1^T b_m} \\\\ \\mathbf{a_2^T b_1} & \\mathbf{a_2^T b_2} & \\cdots & \\mathbf{a_2^T b_m} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ \\mathbf{a_m^T b_1} & \\mathbf{a_m^T b_2} & \\cdots & \\mathbf{a_m^T b_m} \\end{bmatrix}\\]\nrank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) \\[AB = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1^T} \\\\ \\mathbf{b_2^T} \\\\ \\vdots \\\\ \\mathbf{b_m^T} \\end{bmatrix} = \\mathbf{a_1 b_1^T} + \\mathbf{a_2 b_2^T} + \\cdots + \\mathbf{a_m b_m^T}\\]\ncolumn space로 바라보기 \\[A\\mathbf{x} = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} = \\mathbf{a_1} x_1 + \\mathbf{a_2} x_2 + \\cdots + \\mathbf{a_m} x_m \\] (스칼라배의 합)\n\n\\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) 는 2차원 좌표평면의 모든 점을, \\(A=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)은 3차원 좌표평면의 모든 점 표현 가능\n\\(AB = A \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} A \\mathbf{b_1} & A \\mathbf{b_2} & \\cdots & A \\mathbf{b_m} \\end{bmatrix}\\)\ncolumn space: A의 column vector로 만들 수 있는 부분 공간\n\nrow space로 바라보기 \\[\\mathbf{x^T}A = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_m \\end{bmatrix} \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} = x_1 \\mathbf{a_1^T} + x_2 \\mathbf{a_2^T} + \\cdots + x_m \\mathbf{a_m^T} \\]"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#열공간column-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#열공간column-space",
    "title": "Basics (3) - Special Matrices",
    "section": "3 열공간(Column Space)",
    "text": "3 열공간(Column Space)\n\n\ncolumn space: column vector 들이 span 하는 space\n\n\\(A\\)의 column space = \\(C(A)\\) 또는 \\(range(A)\\)\n\nspan: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합\n\nvector에 따라, 점일수도 선일수도 평면일 수도 있음\nvector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space\n\nvector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것\n\n\\(\\mathbf{v_1}\\) 과 \\(\\mathbf{v_2}\\)의 linear combination으로 2차원 좌표평면 나타내기"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#선형-독립linear-independent",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#선형-독립linear-independent",
    "title": "Basics (3) - Special Matrices",
    "section": "4 선형 독립(Linear Independent)",
    "text": "4 선형 독립(Linear Independent)\n\n…and also see\n\n\n선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌\northogonal 하면 independent\n\nbut independent해도 항상 orthogonal하지는 않음 (Independent > Orthogonal)\n\ndefinition: \\(a_1 \\mathbf{v_1} + a_2 \\mathbf{v_2} + a_3 \\mathbf{v_3} \\cdots a_n \\mathbf{v_n} = \\mathbf{0}\\) 를 만족하는 \\(a_1, a_2, a_3, \\cdots a_n\\) 이 \\(a_1 = a_2 = a_3 = \\cdots = a_n = 0\\) 밖에 없을때\n\n\\(\\mathbf{0}\\)는 모든 elements가 \\(0\\)인 벡터\n예: \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\\) 는 \\(-2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) 이 되므로, linearly independent 하지 않음\nindependent한 vector 들의 수 = 표현할 수 있는 차원의 dimension"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#기저basis",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#기저basis",
    "title": "Basics (3) - Special Matrices",
    "section": "5 기저(basis)",
    "text": "5 기저(basis)\n\n주어진 vector space를 span하는 linearly independent한 vectors\n어떤 공간을 이루는 필수적인 구성요소\northogonal 하면 orthogonal basis\n예: 2차원 좌표평면에 대해\n\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) : orthogonal basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) : orthogonal 하지 않은 basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}\\) : linearly independent 하지 않으므로 basis 아님"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "title": "Basics (3) - Special Matrices",
    "section": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)",
    "text": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)\n\n\n6.1 Identity matrix(항등행렬)\n\n항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)\n\n실수에서 곱셈의 항등원은 1\n\n행렬의 항등원: 항등행렬(\\(I\\)) \\[I = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\\]\n\n\\(A_{m \\times n} \\times I_{n \\times n = n} = A_{m \\times n}\\)\n\\(I_{m \\times m = m} \\times A_{m \\times n} = A_{m \\times n}\\)\n\n\n\n\n6.2 Inverse matrix(역행렬)\n\n역원: 연산 결과 항등원이 나오게 하는 연소\n\n실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): \\(a \\times a^{-1} = 1\\)\n\n행렬의 역원: 역행렬(\\(A^{-1}\\)) \\[A \\times A^{-1} = I , A^{-1} \\times A = I\\]\n\n존재하지 않는 경우도 있음\n존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림\n\n존재하지 않으면 singular, degenerate라고 불림\n\nsquare matrix(정사각행렬, \\(m = n\\))은 특수한 경우를 제외하면 역행렬이 항상 존재\n\n역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우\n\n\\(m \\neq n\\)인 행렬의 경우에는 역행렬이 존재하지 않음\n\n다만, 경우에 따라 \\(A \\times A^{-1} = I\\) 를 만족하거나(right inverse), \\(A^{-1} \\times A = I\\)를 만족하는(left inverse)는 \\(A^{-1}\\)이 존재함\n\n연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 \\[A\\mathbf{x} = \\mathbf{b} \\Rightarrow A^{-1}A\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow I\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow \\mathbf{x} = A^{-1}\\mathbf{b}\\]\n\n\n\n\n6.3 Diagonal Matrix(대각행렬)\n\ndiagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix \\[ D = Diag(\\mathbf{a}) = \\begin{bmatrix} a_{1,1} & 0 & \\cdots & 0 \\\\ 0 & a_{2,2} & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & a_{n,n} \\end{bmatrix}\\]\n\nidentity matrix는 diagonal matrix\ndiagnomal matrix는 symmetric matrix 이기도 함\n보통은 square matrix에서 주로 사용됨\n\nsquare matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix\n\n\n\n\n\n6.4 Orthogonal matrix(직교행렬, orthonomal matrix)\n\n행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) \\[A A^T = A^T A = I\\]\nidentity matrix는 orthogonal matrix\nsquare matrix에서만 정의됨\nOrthogonal matrix인 \\(A\\)이면 \\(A^{-1} = A^{T}\\)\n\n각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임\n\ncomplex matrix(복소수 행렬)에서는 unitary matrix라고 부름"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#계수rank",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#계수rank",
    "title": "Basics (3) - Special Matrices",
    "section": "7 계수(Rank)",
    "text": "7 계수(Rank)\n\n\nrank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension\nindependent한 column의 수 = independent한 행의 수: \\(rank(A) = rank(A^T)\\)\n\nproof: Wikipedia\n\n예: \\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix} \\Rightarrow rank=1\\] \\[\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\Rightarrow rank=2\\]\n\\(A_{m \\times n}\\) 의 최대 랭크는 \\(min\\{m,n\\}\\)\n\n\\(rank(A) < min\\{m,n\\}\\) 면 rank-deficient, \\(rank(A) = min\\{m,n\\}\\)면 full (row/column) rank"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#영공간null-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#영공간null-space",
    "title": "Basics (3) - Special Matrices",
    "section": "8 영공간(Null space)",
    "text": "8 영공간(Null space)\n\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) 을 만족하는 \\(\\mathbf{x}\\)의 집합\n\ncolumn space 관점에서 보기: \\(A\\mathbf{x} = x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{0}\\)\nnull space에 항상 들어가는 \\(\\mathbf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\) : trivial solution\n\n모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 \\(\\mathbf{x}=\\mathbf{0}\\)하나 밖에 없음\n\n\n\\(\\mathbf{x}=\\mathbf{0}\\) 가 아닌 vector가 null space에 있으면, 스칼라배(constant \\(c\\)에 대해 \\(c \\mathbf{x}\\)) 역시 null space에 포함됨\n혼동 주의! null space는 column space의 일부가 아님\n\nrow vector의 차원이 null space가 존재하는 공간\n\nrank와 null space의 dimension의 합은 항상 matrix의 column의 수\n\n\\(A_{m \\times n}\\)에 대해, \\(dim(N(A)) = n - r\\)\n\n모든 columns이 다 lienarly independent 하면 null space는 0차원(점)\n\nnull space는 row space와 수직한 space\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0\nrank는 row space의 dimension → row space의 dimension(\\(dim(R(A))\\))과 null space의 dimension(\\(dim(N(A))\\))의 합이 \\(n\\)\n\\(\\mathbb{R^n}\\) 공간에 표현: \n\n겹친 점: 영벡터\n\n\n\nleft null space: \\(\\mathbf{x^T} A = \\mathbf{0^T}\\) 인 \\(\\mathbf{x}\\)\n\n위의 성질을 row에 대해 적용\n\nm 차원에 놓인 벡터\ndimension: \\(dim(N_L(A)) = m - r\\)\ncolumn space와 수직: \\(dim(N_L(A)) +dim(C(A)) = m\\)\n\n\n\\(R(A)\\)에 있는 vector \\(\\mathbf{x_r}\\) 와 \\(N(A)\\)에 있는 vector \\(\\mathbf{x_n}\\)에 대해:\n\n\\(\\mathbf{x_r}\\)에 \\(A\\)를 곱하면 column space로 감\n\\(\\mathbf{x_n}\\)에 \\(A\\)를 곱하면 $\n\\(A(\\mathbf{x_r} +\\mathbf{x_n}) = A\\mathbf{x_r} + A\\mathbf{x_n} = A\\mathbf{x_r} = \\mathbf{b}\\)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#ax-b의-해의-수",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/10.svd.html#ax-b의-해의-수",
    "title": "Basics (3) - Special Matrices",
    "section": "9 Ax = b의 해의 수",
    "text": "9 Ax = b의 해의 수\n\n\nfull column rank 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 해가 하나\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음 \n\nfull row rank 일때\n\n\\(\\mathbf{b}\\)는 항상 column space 안에 있음: 무한의 해를 가짐\n임의의 특정한 해(particular solution) \\(\\mathbf{x_p}\\)와 null space의 vector \\(\\mathbf{x_n}\\)에 대해, \\(A(\\mathbf{x_p} +\\mathbf{x_n})=\\mathbf{b}\\)\n\n즉, \\(\\mathbf{x_p} +\\mathbf{x_n}\\) 도 해가 됨: complete solution\n\nnull space는 무한하므로, 해도 무한함\n\n\n\nfull rank 일때(square matrix): 해가 하나 존재 (\\(\\mathbf{x} = A^{-1}\\)$)\nrank-deficient 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 무한한 해를 가짐\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/11.bilinear_form.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/11.bilinear_form.html",
    "title": "Matrix Transformation (4) - Biinear Form",
    "section": "",
    "text": "A bilinear form of a matrix is a function that extends the linear form and takes two vectors as inputs and produces a scalar as output. It is linear in both of its arguments, meaning that it satisfies the following properties:\n\\[\n\\begin{aligned}\nB(\\mathbf u+\\mathbf v)&=B(\\mathbf u+\\mathbf w)+B(\\mathbf v+\\mathbf w)\\\\\nB(\\mathbf u,\\alpha \\mathbf v)&=\\alpha B(\\mathbf u,\\mathbf v)\\\\\nB(\\alpha\\mathbf u,\\mathbf v)&=\\alpha B(\\mathbf u,\\mathbf v)\n\\end{aligned}\n\\]\nfor all vectors \\(u\\), \\(v\\), \\(w\\) and scalars \\(\\alpha\\).\nA bilinear form can be represented by a matrix \\(B\\) such that \\(B_{i,j}\\) is the coefficient of the product \\(u_i v_j\\) in the expansion of \\(B(u,v)\\). The bilinear form can then be written as:\n\\[\nB(\\mathbf u,\\mathbf v)=\\mathbf u^T B \\mathbf v\n\\]\nwhere \\(\\mathbf u\\) and \\(\\mathbf v\\) are column vectors and \\(B\\) is a matrix.\nFor example, consider the bilinear form \\(B(\\mathbf u,\\mathbf v) = u_1 v_1 + u_2 v_2\\). This bilinear form can be represented by the matrix:\n\\[\nB=\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\n\\]\nand written as:\n\\[\nB(\\mathbf u,\\mathbf v)=\\begin{bmatrix}u_1& u_2\\end{bmatrix}\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\end{bmatrix}=u_1v_1+u_2v_2\n\\]\nThis bilinear form computes the dot product of \\(u\\) and \\(v\\), which measures the similarity between the two vectors. Bilinear forms are commonly used in applications such as optimization, geometry, and physics, where they capture the interaction between two quantities or variables.\nThe covariance matrix can be represented as a bilinear form using matrix multiplication. Let’s say we have a random vector \\(\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^T\\) with mean vector \\(\\mathbf{\\mu} = [\\mu_1, \\mu_2, \\ldots, \\mu_n]^T\\) and covariance matrix \\(\\mathbf{\\Sigma}\\). Then, we can represent the covariance matrix as a bilinear form in the following way:\n\\[\n\\begin{aligned}\n\\Sigma&=\\operatorname{E}[(\\mathbf x-\\mathbf \\mu)(\\mathbf x-\\mathbf \\mu)^T]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})(x_i-\\bar{x})^T\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{E}\\) denotes the expectation operator. We can expand this expression as:\n\\[\n\\begin{aligned}\n\\Sigma&=\\operatorname{E}[\\mathbf x\\mathbf x^T]-\\mathbf \\mu\\mathbf \\mu^T\n\\end{aligned}\n\\]\nNow, we can represent the covariance matrix as a bilinear form using matrix multiplication as:\n\\[\n\\begin{aligned}\n\\Sigma&=\\operatorname{E}[(\\mathbf x-\\mathbf \\mu)(\\mathbf x-\\mathbf \\mu)^T]\\\\\n&=\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\mathbf x_i-\\mathbf\\mu_i)(\\mathbf x_i-\\mathbf \\mu_j)\n\\end{aligned}\n\\]\nwhere \\([\\mathbf{x}-\\mathbf{\\mu}]\\) is the deviation of the random vector \\(\\mathbf{x}\\) from its mean vector \\(\\mathbf{\\mu}\\).\ncovariance matrix, and correlation matrix\nOne of the most famous examples is the use of bilinear forms in convolutional neural networks (CNNs), which are a type of deep learning model used for image and video recognition tasks.\nIn a CNN, a bilinear form is used to compute the similarity between a filter and a local region of an input image. This similarity measure is used to determine how much the filter “matches” the local region of the image, and is used to produce an output feature map.\nMore specifically, the bilinear form used in a CNN takes the form:\n\\[\nz_{i,j} = \\sum_{m=1}^{M}\\sum_{n=1}^{N} w_{m,n}x_{i+m-1,j+n-1}\n\\]\nwhere \\(z_{i,j}\\) is the output feature map at location \\((i,j)\\), \\(x_{i+m-1,j+n-1}\\) is the input image pixel at location \\((i+m-1,j+n-1)\\), and \\(w_{m,n}\\) is the weight of the filter at position \\((m,n)\\). This computation is performed for each location \\((i,j)\\) in the output feature map.\nThe bilinear form used in CNNs is a type of convolution operation, and is used to learn features such as edges, corners, and other patterns in the input image. CNNs with bilinear forms have achieved state-of-the-art performance on many image recognition tasks, including object detection, face recognition, and scene classification.\nBilinear forms also have applications in other areas of machine learning, such as natural language processing (NLP). In NLP, bilinear forms can be used to compute the similarity between two word embeddings, which are vector representations of words. This similarity measure can be used for tasks such as sentiment analysis, text classification, and machine translation.\n\\[\nB(\\mathbf{u},\\mathbf{v})=\\mathbf{u}^T \\mathbf{W}\\mathbf{v}\n\\]\nwhere \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are word embeddings, \\(\\mathbf{W}\\) is a weight matrix, and \\(B(\\mathbf{u},\\mathbf{v})\\) represents the bilinear form used to compute the similarity between the two embeddings.\nOverall, bilinear forms are a powerful tool for learning features from complex data such as images and text, and have many applications in deep learning and machine learning."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/11.linear_form.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/11.linear_form.html",
    "title": "Matrix Transformation (3) - Linear Form",
    "section": "",
    "text": "A linear form is a linear function that maps a vector space to its underlying field. Let \\(V\\) be a vector space over a field \\(\\mathbb{F}\\), and let \\(\\mathcal{L}(V,\\mathbb{F})\\) denote the set of all linear functions from \\(V\\) to \\(\\mathbb{F}\\). A linear form on \\(V\\) is an element of \\(\\mathcal{L}(V,\\mathbb{F})\\).\nA linear form \\(\\varphi\\) can be represented by a row vector of dimension \\(1\\times n\\), where \\(n\\) is the dimension of \\(V\\). Let \\({\\mathbf{e}_1, \\mathbf{e}_2, \\dots, \\mathbf{e}_n}\\) be a basis for \\(V\\), and let \\({\\alpha_1, \\alpha_2, \\dots, \\alpha_n}\\) be the corresponding dual basis for \\(\\mathcal{L}(V,\\mathbb{F})\\), such that \\(\\alpha_i(\\mathbf{e}j) = \\delta{ij}\\) (the Kronecker delta). Then, any linear form \\(\\varphi\\in\\mathcal{L}(V,\\mathbb{F})\\) can be written as: \\[\n\\varphi(x)=\\sum_{i=1}^{n}a_ix_i=\\mathbf a \\mathbf x^T=\\mathbf x \\mathbf a\n\\]\nwhere \\(\\mathbf{x}\\in V\\) is a column vector of dimension \\(n\\times 1\\), \\([\\mathbf{a}]\\) is the row vector representing \\(\\varphi\\), and \\([\\mathbf{x}]\\) is the column vector representing \\(\\mathbf{x}\\).\nFor example, let \\(V = \\mathbb{R}^2\\) be the vector space of 2-dimensional column vectors, and let \\(\\varphi\\in\\mathcal{L}(V,\\mathbb{R})\\) be the linear form defined by \\(\\varphi(\\begin{bmatrix}x\\y\\end{bmatrix}) = 3x - 2y\\). Then, we can represent \\(\\varphi\\) as:\n\\[\n[\\mathbf a]=\\begin{bmatrix} 3 & -2\\end{bmatrix} [\\mathbf x]=\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\varphi(x)=\\sum_{i=1}^{n}\\mathbf a\\mathbf x^T=3x_1-2x_2\n\\]\nwhich shows that \\(\\varphi\\) is a linear form on \\(V\\).\nconsider a linear regression model that predicts the price of a house based on its size and location. The model can be represented by the linear form:\n\\[\n\\varphi(x)=\\mathbf w\\mathbf x^T=\\sum_{i=1}^{n}w_ix_i=w_0+w_1x_1+w_2x_2\n\\]\nwhere \\(\\varphi(\\mathbf{x})\\) is the predicted price, \\(x_1\\) is the size of the house, \\(x_2\\) is a measure of the location (such as the distance from the city center), and \\(w_0\\), \\(w_1\\), and \\(w_2\\) are the model parameters that control the intercept and the weights of the features. This linear form can be written in matrix form as:\n\\[\n\\varphi(x)=\\mathbf x\\mathbf w=\\mathbf w \\mathbf x^T\n\\]\nwhere \\([\\mathbf{w}]\\) is a row vector of the model parameters and \\([\\mathbf{x}]\\) is a row vector of the features.\nLinear forms can also be used in deep learning and machine learning models that involve linear transformations, such as fully connected layers in neural networks or linear classifiers. For example, consider a simple linear classifier that classifies images of digits into one of 10 classes. The classifier can be represented by the linear form:\n\\[\n\\varphi(x)=\\mathbf x\\mathbf w + b =\\mathbf w \\mathbf x^T +b\n\\]\nwhere \\(\\varphi(\\mathbf{x})\\) is the predicted class score, \\([\\mathbf{x}]\\) is a row vector of the pixel values of the image, \\([\\mathbf{w}]\\) is a row vector of the weights of the classifier, and \\(b\\) is the bias term. This linear form can be used to classify the image by selecting the class with the highest score.\nIn both of these examples, linear forms are used to represent linear relationships between variables or features, and the model parameters are learned through training on a set of labeled examples."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html",
    "title": "Basics (3) - Special Matrices",
    "section": "",
    "text": "A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, it can be represented as: \\[\n\\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\] where \\(a_{ij}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column of the matrix \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#basic-matrix-operations",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#basic-matrix-operations",
    "title": "Basics (3) - Special Matrices",
    "section": "2 Basic Matrix Operations",
    "text": "2 Basic Matrix Operations\n\n2.1 Matrix addition\nThe sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.\nGiven two \\(m \\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is defined by:\n\\[\nc_{i,j}=a_{i,j}+b_{i,j}​\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\n\\[\n\\begin{bmatrix}\n  1 & 2 \\\\\n  3 & 4 \\\\\n  5 & 6\n\\end{bmatrix} +\n\\begin{bmatrix}\n  -1 & 0 \\\\\n  2 & -3 \\\\\n  -5 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n  0 & 2 \\\\\n  5 & 1 \\\\\n  0 & 10\n\\end{bmatrix}\n\\]\n\n\n2.2 Scalar multiplication\nThe product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.\nGiven a scalar \\(k\\) and an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), their product \\(k\\mathbf{A}\\) is defined by: \\[\n(k\\mathbf{A})_{i,j} = k(a_{i,j})\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\nExample: \\[\n2\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 & 4 \\\\\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\]\n\n\n2.3 Matrix multiplication\nThe product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is a matrix obtained by multiplying the rows of \\(\\mathbf{A}\\) by the columns of \\(\\mathbf{B}\\).\nGiven two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with dimensions \\(m \\times n\\) and \\(n \\times p\\), respectively, their product \\(\\mathbf{C} = \\mathbf{AB}\\) is an \\(m \\times p\\) matrix defined by: \\[\nc_{i,j} = \\sum_{k=1}^n a_{i,k}b_{k,j}\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq p\\).\nExample: \\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} \\begin{bmatrix}\n-1 & 0 & 2 \\\\\n2 & -3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 & -6 & 4 \\\\\n5 & -12 & 10 \\\\\n7 & -18 & 16\n\\end{bmatrix}\n\\]\n\n\n2.4 Transpose\nThe transpose of an \\(m x n\\) matrix A, denoted by \\(A^T\\), is the \\(n x m\\) matrix obtained by interchanging the rows and columns of A. Formally, if \\(\\mathbf{A} = [a_{ij}]\\) is an m x n matrix, then its transpose \\(\\mathbf{A}^T = [b_{ij}]\\) is an \\(n x m\\) matrix where \\(b_{ij}\\) = \\(a_{ji}\\) for all \\(i\\) and \\(j\\). In other words, the element in the \\(i\\) th row and \\(j\\) th column of \\(A^T\\) is equal to the element in the \\(j\\) th row and ith column of \\(\\mathbf{A}\\).\nGiven an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix defined by: $$ {i,j}^T = {j,i}\n$$\n\nWhen \\(\\mathbf{A}\\) is transposed, diagnoal entries(\\(a_{ii}\\)) do not change but off-diagnoal elements(\\(a_{ij} \\; i \\neq j\\)) change.\nA column vector is tranposed into a row vector, and vice versa.\nsymmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)\n\nExample:\nLet A be the matrix \\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\] The transpose of A, denoted by A^T, is the matrix \\[\n\\mathbf{A}^T = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}\n\\]\n\n\n2.5 Determinant\nLet \\(\\mathbf{A}\\) be an \\(n \\times n\\) square matrix. The determinant of \\(\\mathbf{A}\\), denoted by \\(|\\mathbf{A}|\\) or \\(\\det(\\mathbf{A})\\), is a scalar value calculated as the sum of the products of the elements in any row or column of \\(\\mathbf{A}\\) with their corresponding cofactors, that is,\n\\[\n|\\mathbf{A}|=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n\\]\nwhere \\(a_{ij}\\) is the element of \\(\\mathbf{A}\\) in the \\(i\\)-th row and \\(j\\)-th column, and \\(C_{ij}\\) is the cofactor of \\(a_{ij}\\). The cofactor of \\(a_{ij}\\), denoted by \\(C_{ij}\\), is given by \\((-1)^{i+j}\\) times the determinant of the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)-th row and \\(j\\)-th column of \\(\\mathbf{A}\\).\nThe determinant of an \\(n x n\\) matrix \\(\\mathbf{A}\\) is a scalar value denoted as \\(|\\mathbf{A}|\\). It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a \\(3 x 3\\) matrix \\(\\mathbf{A} =\\) \\[\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\]\nis given by:\n\\[\n|\\mathbf{A}| = a_{11}\n\\begin{vmatrix}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{vmatrix}\n- a_{12}\n\\begin{vmatrix}\na_{21} & a_{23} \\\\\na_{31} & a_{33}\n\\end{vmatrix}\n+ a_{13}\n\\begin{vmatrix}\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{vmatrix}\n\\]\nFor example, consider the \\(3 \\times 3\\) matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\). We can calculate the determinant of \\(\\mathbf{A}\\) using any row or column. Let’s use the first column:\n\\[\n|\\mathbf{A}| = 1\n\\begin{vmatrix}\n5 & 6 \\\\\n8 & 9\n\\end{vmatrix}\n- 4\n\\begin{vmatrix}\n2 & 3\\\\\n8 & 9\n\\end{vmatrix}\n+ 7\n\\begin{vmatrix}\n2 & 5 \\\\\n3 & 6\n\\end{vmatrix} = 0\n\\]\nTherefore, the determinant of \\(\\mathbf{A}\\) is zero.\n\n\n2.6 Inverse\nThe inverse of a square matrix \\(A\\) of size \\(n\\) is a matrix \\(A^{-1}\\) such that the product of \\(A\\) and \\(A^{-1}\\) is the identity matrix \\(I_n\\), i.e. \\(A \\times A^{-1} = I_n\\). If such a matrix exists, then \\(A\\) is said to be invertible or non-singular.\nThe inverse of a square matrix \\(\\mathbf{A}\\) is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the unique matrix that satisfies the following equation: \\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\nFor example, consider the \\(2\\times 2\\) matrix \\[\n\\mathbf{A} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}.\n\\] The inverse of \\(\\mathbf{A}\\) is given by:\n\\[\n\\mathbf{A}^{-1} =\n\\frac{1}{-2}\n\\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\]\nWe can verify that \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\) by computing:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} =\n\\begin{bmatrix} -2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\nLet me give another example and A be a \\(3x3\\) square matrix:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, the inverse of \\(\\mathbf{A}\\), denoted as \\(\\mathbf{A}^{-1}\\), is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\na_{22}a_{33}-a_{23}a_{32} & a_{13}a_{32}-a_{12}a_{33} & a_{12}a_{23}-a_{13}a_{22} \\\\\na_{23}a_{31}-a_{21}a_{33} & a_{11}a_{33}-a_{13}a_{31} & a_{13}a_{21}-a_{11}a_{23} \\\\\na_{21}a_{32}-a_{22}a_{31} & a_{12}a_{31}-a_{11}a_{32} & a_{11}a_{22}-a_{12}a_{21} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nwhere \\(det(\\mathbf{A})\\) is the determinant of \\(\\mathbf{A}\\).\nFor example, let:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, \\(det(A)\\) = -57, and the inverse of \\(\\mathbf{A}\\) is:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{-57}\\begin{bmatrix}\n-24 & 18 & 5 \\\\\n20 & -15 & -4 \\\\\n-3 & 2 & 1 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThere are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.\n\n\n2.7 Rank\nThe rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by \\(\\text{rank}(\\mathbf{A})\\).\nFor example, consider the following matrix:\n\\[\n\\begin{equation*}\n  \\mathbf{A} = \\begin{bmatrix}\n    1 & 2 & 3\\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\\\\\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe columns of \\(\\mathbf{A}\\) are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of \\(\\mathbf{A}\\) is 2.\n\n\n2.8 Trace\nThe trace of a square matrix \\(\\mathbf{A}\\), denoted by \\(\\mathrm{tr}(\\mathbf{A})\\), is defined as the sum of the diagonal elements of \\(\\mathbf{A}\\). In other words, if \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix, then its trace is given by:\n\\[\n\\mathrm{tr}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}\n\\]\nwhere \\(a_{ii}\\) denotes the \\(i\\) th diagonal element of \\(\\mathbf{A}\\).\nFor example, let \\[\n\\begin{bmatrix}\n  2 & 3 & 1 \\\\\n  0 & 5 & 2 \\\\\n  1 & 1 & 4\n\\end{bmatrix}\n\\]\nThen, the trace of \\(\\mathbf{A}\\) is \\(\\mathrm{tr}(\\mathbf{A}) = 2 + 5 + 4 = 11\\)\n\n\n2.9 Eigenvalues and Eigenvectors\nLet A be an \\(n × n\\) square matrix. A scalar \\(\\lambda\\) is called an eigenvalue of \\(\\mathbf A\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that \\[\n\\mathbf{Av}=\\lambda\\mathbf{v}\n\\]\nSuch a vector \\(\\mathbf{v}\\) is called an eigenvector corresponding to \\(\\lambda\\).\nExample:\nLet \\(\\mathbf A\\) be the matrix\nTo find the eigenvalues of \\(\\mathbf A\\), we solve the characteristic equation \\(\\text{det}(\\mathbf A - \\lambda \\mathbf I ) = 0\\), where I is the n × n identity matrix.\n\\[\n\\begin{align*}\n  \\text{det}(\\mathbf A - \\lambda \\mathbf I )\n  &=\n    \\begin{vmatrix}\n    3 - \\lambda & 1 \\\\\n    1 & 3 - \\lambda\n    \\end{vmatrix} \\\\\n  &=\n  (3 - \\lambda)(3 - \\lambda) - 1 \\\\\n  &= \\lambda^2 - 6\\lambda + 8 = 0\n\\end{align*}\n\\]\nSolving this quadratic equation gives us the eigenvalues of \\(\\mathbf A\\): \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 4\\).\nTo find the eigenvectors corresponding to \\(\\lambda_1 = 2\\), we solve the equation \\((\\mathbf A - 2 \\mathbf I)\\mathbf{v} = \\mathbf{0}\\), where \\(\\mathbf I\\) is the \\(2 \\times 2\\) identity matrix.\n\\[\n\\begin{align*}\n  (\\mathbf A - 2 \\mathbf I)\\mathbf{v} =\n    \\begin{bmatrix}\n      1 & 1 \\\\\n      1 & 1\n    \\end{bmatrix}\n  \\begin{bmatrix}\n    x \\\\\n    y\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    0 \\\\\n    0\n  \\end{bmatrix}\n\\end{align*}\n\\]\nSolving this system of equations gives us the eigenvectors corresponding to \\(\\lambda_1 = 2\\): \\(\\mathbf{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\)\nSimilarly, for \\(\\lambda_2 = 4\\), we solve the equation \\((\\mathbf A - 4\\mathbf I)\\mathbf{v}\\) = \\(\\mathbf{0}\\) to get the eigenvectors corresponding to \\(\\lambda_2 = 4\\): \\(\\mathbf{v_2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)\n\n\n2.10 Singular value and Singluar Vectors\nThe singular value decomposition (SVD) of a matrix \\(\\mathbf A\\) is a factorization of \\(\\mathbf A\\) into the product of three matrices as follows:\n\\[\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n\\] where \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) rectangular diagonal matrix with non-negative real numbers on the diagonal, and \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix.\nThe diagonal entries of \\(\\mathbf{\\Sigma}\\) are called the singular values of \\(\\mathbf{A}\\), denoted as \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_r\\) (where \\(r\\) is the rank of \\(\\mathbf{A}\\)), and are arranged in descending order. The columns of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are called the left and right singular vectors of \\(\\mathbf{A}\\), respectively, and are orthonormal vectors.\nFor example, let \\(\\mathbf{A}\\) be a 3 by 2 matrix given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} =\n  \\begin{bmatrix}\n    1 & 2\\\\\n    3 & 4\\\\\n    5 & 6\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe SVD of \\(\\mathbf{A}\\) is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T =\n\\begin{bmatrix}\n  -0.23 & -0.53 & -0.81\\\\\n  -0.53 & -0.72 & 0.45\\\\\n  -0.81 & 0.45 & -0.38\n\\end{bmatrix}\n\\begin{bmatrix}\n  9.53 & 0\\\\\n  0 & 0.90\\\\\n  0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  -0.62 & -0.78\\\\\n  -0.78 & 0.62\n\\end{bmatrix}^T\n\\end{equation*}\n\\]\nwhere the left singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{U}\\), the right singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{V}\\), and the singular values of \\(\\mathbf{A}\\) are the diagonal entries of \\(\\boldsymbol{\\Sigma}\\).\n\n연립 방정식을 행렬의 곱으로 나타내보기 \\[\\begin{matrix}x_1+2y_1=4\\\\2x_1+5y_1=9\\end{matrix} \\quad \\quad \\quad \\begin{matrix}x_2+2y_2=3\\\\2x_2+5y_2=7\\end{matrix}\\] \\[ \\begin{bmatrix} 1 & 2 \\\\ 2 & 5 \\end{bmatrix} \\begin{bmatrix} x_1 & x_2 \\\\ y_1 & y_2 \\end{bmatrix} = \\begin{bmatrix} 4 & 9 \\\\ 3 & 7 \\end{bmatrix}\\]\n중요한 사실(….당연한 사실?)\n\n곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함\n\n\\(A_{m \\times n} \\times B_{o \\times p}\\) 에서 \\(n = o\\) 여야 곱셈 성립\n\n곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 \\(\\times\\) 곱셈의 오른쪽 행렬의 열 수\n\n\\(A_{m \\times n} \\times B_{o \\times p} = C_{m \\times p}\\)\n\n교환법칙(Commutative property)이 성립하지 않음\n\n\\(AB \\neq BA\\)\n\n\n행렬 곱셈의 여러가지 관점\n\n내적으로 바라보기 \\[ A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_1^T b_1} & \\mathbf{a_1^T b_2} & \\cdots & \\mathbf{a_1^T b_m} \\\\ \\mathbf{a_2^T b_1} & \\mathbf{a_2^T b_2} & \\cdots & \\mathbf{a_2^T b_m} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ \\mathbf{a_m^T b_1} & \\mathbf{a_m^T b_2} & \\cdots & \\mathbf{a_m^T b_m} \\end{bmatrix}\\]\nrank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) \\[AB = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1^T} \\\\ \\mathbf{b_2^T} \\\\ \\vdots \\\\ \\mathbf{b_m^T} \\end{bmatrix} = \\mathbf{a_1 b_1^T} + \\mathbf{a_2 b_2^T} + \\cdots + \\mathbf{a_m b_m^T}\\]\ncolumn space로 바라보기 \\[A\\mathbf{x} = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} = \\mathbf{a_1} x_1 + \\mathbf{a_2} x_2 + \\cdots + \\mathbf{a_m} x_m \\] (스칼라배의 합)\n\n\\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) 는 2차원 좌표평면의 모든 점을, \\(A=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)은 3차원 좌표평면의 모든 점 표현 가능\n\\(AB = A \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} A \\mathbf{b_1} & A \\mathbf{b_2} & \\cdots & A \\mathbf{b_m} \\end{bmatrix}\\)\ncolumn space: A의 column vector로 만들 수 있는 부분 공간\n\nrow space로 바라보기 \\[\\mathbf{x^T}A = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_m \\end{bmatrix} \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} = x_1 \\mathbf{a_1^T} + x_2 \\mathbf{a_2^T} + \\cdots + x_m \\mathbf{a_m^T} \\]"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#열공간column-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#열공간column-space",
    "title": "Basics (3) - Special Matrices",
    "section": "3 열공간(Column Space)",
    "text": "3 열공간(Column Space)\n\n\ncolumn space: column vector 들이 span 하는 space\n\n\\(A\\)의 column space = \\(C(A)\\) 또는 \\(range(A)\\)\n\nspan: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합\n\nvector에 따라, 점일수도 선일수도 평면일 수도 있음\nvector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space\n\nvector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것\n\n\\(\\mathbf{v_1}\\) 과 \\(\\mathbf{v_2}\\)의 linear combination으로 2차원 좌표평면 나타내기"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#선형-독립linear-independent",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#선형-독립linear-independent",
    "title": "Basics (3) - Special Matrices",
    "section": "4 선형 독립(Linear Independent)",
    "text": "4 선형 독립(Linear Independent)\n\n…and also see\n\n\n선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌\northogonal 하면 independent\n\nbut independent해도 항상 orthogonal하지는 않음 (Independent > Orthogonal)\n\ndefinition: \\(a_1 \\mathbf{v_1} + a_2 \\mathbf{v_2} + a_3 \\mathbf{v_3} \\cdots a_n \\mathbf{v_n} = \\mathbf{0}\\) 를 만족하는 \\(a_1, a_2, a_3, \\cdots a_n\\) 이 \\(a_1 = a_2 = a_3 = \\cdots = a_n = 0\\) 밖에 없을때\n\n\\(\\mathbf{0}\\)는 모든 elements가 \\(0\\)인 벡터\n예: \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\\) 는 \\(-2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) 이 되므로, linearly independent 하지 않음\nindependent한 vector 들의 수 = 표현할 수 있는 차원의 dimension"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#기저basis",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#기저basis",
    "title": "Basics (3) - Special Matrices",
    "section": "5 기저(basis)",
    "text": "5 기저(basis)\n\n주어진 vector space를 span하는 linearly independent한 vectors\n어떤 공간을 이루는 필수적인 구성요소\northogonal 하면 orthogonal basis\n예: 2차원 좌표평면에 대해\n\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) : orthogonal basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) : orthogonal 하지 않은 basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}\\) : linearly independent 하지 않으므로 basis 아님"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "title": "Basics (3) - Special Matrices",
    "section": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)",
    "text": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)\n\n\n6.1 Identity matrix(항등행렬)\n\n항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)\n\n실수에서 곱셈의 항등원은 1\n\n행렬의 항등원: 항등행렬(\\(I\\)) \\[I = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\\]\n\n\\(A_{m \\times n} \\times I_{n \\times n = n} = A_{m \\times n}\\)\n\\(I_{m \\times m = m} \\times A_{m \\times n} = A_{m \\times n}\\)\n\n\n\n\n6.2 Inverse matrix(역행렬)\n\n역원: 연산 결과 항등원이 나오게 하는 연소\n\n실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): \\(a \\times a^{-1} = 1\\)\n\n행렬의 역원: 역행렬(\\(A^{-1}\\)) \\[A \\times A^{-1} = I , A^{-1} \\times A = I\\]\n\n존재하지 않는 경우도 있음\n존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림\n\n존재하지 않으면 singular, degenerate라고 불림\n\nsquare matrix(정사각행렬, \\(m = n\\))은 특수한 경우를 제외하면 역행렬이 항상 존재\n\n역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우\n\n\\(m \\neq n\\)인 행렬의 경우에는 역행렬이 존재하지 않음\n\n다만, 경우에 따라 \\(A \\times A^{-1} = I\\) 를 만족하거나(right inverse), \\(A^{-1} \\times A = I\\)를 만족하는(left inverse)는 \\(A^{-1}\\)이 존재함\n\n연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 \\[A\\mathbf{x} = \\mathbf{b} \\Rightarrow A^{-1}A\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow I\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow \\mathbf{x} = A^{-1}\\mathbf{b}\\]\n\n\n\n\n6.3 Diagonal Matrix(대각행렬)\n\ndiagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix \\[ D = Diag(\\mathbf{a}) = \\begin{bmatrix} a_{1,1} & 0 & \\cdots & 0 \\\\ 0 & a_{2,2} & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & a_{n,n} \\end{bmatrix}\\]\n\nidentity matrix는 diagonal matrix\ndiagnomal matrix는 symmetric matrix 이기도 함\n보통은 square matrix에서 주로 사용됨\n\nsquare matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix\n\n\n\n\n\n6.4 Orthogonal matrix(직교행렬, orthonomal matrix)\n\n행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) \\[A A^T = A^T A = I\\]\nidentity matrix는 orthogonal matrix\nsquare matrix에서만 정의됨\nOrthogonal matrix인 \\(A\\)이면 \\(A^{-1} = A^{T}\\)\n\n각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임\n\ncomplex matrix(복소수 행렬)에서는 unitary matrix라고 부름"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#계수rank",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#계수rank",
    "title": "Basics (3) - Special Matrices",
    "section": "7 계수(Rank)",
    "text": "7 계수(Rank)\n\n\nrank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension\nindependent한 column의 수 = independent한 행의 수: \\(rank(A) = rank(A^T)\\)\n\nproof: Wikipedia\n\n예: \\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix} \\Rightarrow rank=1\\] \\[\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\Rightarrow rank=2\\]\n\\(A_{m \\times n}\\) 의 최대 랭크는 \\(min\\{m,n\\}\\)\n\n\\(rank(A) < min\\{m,n\\}\\) 면 rank-deficient, \\(rank(A) = min\\{m,n\\}\\)면 full (row/column) rank"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#영공간null-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#영공간null-space",
    "title": "Basics (3) - Special Matrices",
    "section": "8 영공간(Null space)",
    "text": "8 영공간(Null space)\n\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) 을 만족하는 \\(\\mathbf{x}\\)의 집합\n\ncolumn space 관점에서 보기: \\(A\\mathbf{x} = x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{0}\\)\nnull space에 항상 들어가는 \\(\\mathbf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\) : trivial solution\n\n모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 \\(\\mathbf{x}=\\mathbf{0}\\)하나 밖에 없음\n\n\n\\(\\mathbf{x}=\\mathbf{0}\\) 가 아닌 vector가 null space에 있으면, 스칼라배(constant \\(c\\)에 대해 \\(c \\mathbf{x}\\)) 역시 null space에 포함됨\n혼동 주의! null space는 column space의 일부가 아님\n\nrow vector의 차원이 null space가 존재하는 공간\n\nrank와 null space의 dimension의 합은 항상 matrix의 column의 수\n\n\\(A_{m \\times n}\\)에 대해, \\(dim(N(A)) = n - r\\)\n\n모든 columns이 다 lienarly independent 하면 null space는 0차원(점)\n\nnull space는 row space와 수직한 space\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0\nrank는 row space의 dimension → row space의 dimension(\\(dim(R(A))\\))과 null space의 dimension(\\(dim(N(A))\\))의 합이 \\(n\\)\n\\(\\mathbb{R^n}\\) 공간에 표현: \n\n겹친 점: 영벡터\n\n\n\nleft null space: \\(\\mathbf{x^T} A = \\mathbf{0^T}\\) 인 \\(\\mathbf{x}\\)\n\n위의 성질을 row에 대해 적용\n\nm 차원에 놓인 벡터\ndimension: \\(dim(N_L(A)) = m - r\\)\ncolumn space와 수직: \\(dim(N_L(A)) +dim(C(A)) = m\\)\n\n\n\\(R(A)\\)에 있는 vector \\(\\mathbf{x_r}\\) 와 \\(N(A)\\)에 있는 vector \\(\\mathbf{x_n}\\)에 대해:\n\n\\(\\mathbf{x_r}\\)에 \\(A\\)를 곱하면 column space로 감\n\\(\\mathbf{x_n}\\)에 \\(A\\)를 곱하면 $\n\\(A(\\mathbf{x_r} +\\mathbf{x_n}) = A\\mathbf{x_r} + A\\mathbf{x_n} = A\\mathbf{x_r} = \\mathbf{b}\\)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#ax-b의-해의-수",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/11.matrix_engineering.html#ax-b의-해의-수",
    "title": "Basics (3) - Special Matrices",
    "section": "9 Ax = b의 해의 수",
    "text": "9 Ax = b의 해의 수\n\n\nfull column rank 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 해가 하나\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음 \n\nfull row rank 일때\n\n\\(\\mathbf{b}\\)는 항상 column space 안에 있음: 무한의 해를 가짐\n임의의 특정한 해(particular solution) \\(\\mathbf{x_p}\\)와 null space의 vector \\(\\mathbf{x_n}\\)에 대해, \\(A(\\mathbf{x_p} +\\mathbf{x_n})=\\mathbf{b}\\)\n\n즉, \\(\\mathbf{x_p} +\\mathbf{x_n}\\) 도 해가 됨: complete solution\n\nnull space는 무한하므로, 해도 무한함\n\n\n\nfull rank 일때(square matrix): 해가 하나 존재 (\\(\\mathbf{x} = A^{-1}\\)$)\nrank-deficient 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 무한한 해를 가짐\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html",
    "title": "Basics (3) - Special Matrices",
    "section": "",
    "text": "A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, it can be represented as: \\[\n\\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\] where \\(a_{ij}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column of the matrix \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#basic-matrix-operations",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#basic-matrix-operations",
    "title": "Basics (3) - Special Matrices",
    "section": "2 Basic Matrix Operations",
    "text": "2 Basic Matrix Operations\n\n2.1 Matrix addition\nThe sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.\nGiven two \\(m \\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is defined by:\n\\[\nc_{i,j}=a_{i,j}+b_{i,j}​\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\n\\[\n\\begin{bmatrix}\n  1 & 2 \\\\\n  3 & 4 \\\\\n  5 & 6\n\\end{bmatrix} +\n\\begin{bmatrix}\n  -1 & 0 \\\\\n  2 & -3 \\\\\n  -5 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n  0 & 2 \\\\\n  5 & 1 \\\\\n  0 & 10\n\\end{bmatrix}\n\\]\n\n\n2.2 Scalar multiplication\nThe product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.\nGiven a scalar \\(k\\) and an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), their product \\(k\\mathbf{A}\\) is defined by: \\[\n(k\\mathbf{A})_{i,j} = k(a_{i,j})\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\nExample: \\[\n2\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 & 4 \\\\\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\]\n\n\n2.3 Matrix multiplication\nThe product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is a matrix obtained by multiplying the rows of \\(\\mathbf{A}\\) by the columns of \\(\\mathbf{B}\\).\nGiven two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with dimensions \\(m \\times n\\) and \\(n \\times p\\), respectively, their product \\(\\mathbf{C} = \\mathbf{AB}\\) is an \\(m \\times p\\) matrix defined by: \\[\nc_{i,j} = \\sum_{k=1}^n a_{i,k}b_{k,j}\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq p\\).\nExample: \\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} \\begin{bmatrix}\n-1 & 0 & 2 \\\\\n2 & -3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 & -6 & 4 \\\\\n5 & -12 & 10 \\\\\n7 & -18 & 16\n\\end{bmatrix}\n\\]\n\n\n2.4 Transpose\nThe transpose of an \\(m x n\\) matrix A, denoted by \\(A^T\\), is the \\(n x m\\) matrix obtained by interchanging the rows and columns of A. Formally, if \\(\\mathbf{A} = [a_{ij}]\\) is an m x n matrix, then its transpose \\(\\mathbf{A}^T = [b_{ij}]\\) is an \\(n x m\\) matrix where \\(b_{ij}\\) = \\(a_{ji}\\) for all \\(i\\) and \\(j\\). In other words, the element in the \\(i\\) th row and \\(j\\) th column of \\(A^T\\) is equal to the element in the \\(j\\) th row and ith column of \\(\\mathbf{A}\\).\nGiven an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix defined by: $$ {i,j}^T = {j,i}\n$$\n\nWhen \\(\\mathbf{A}\\) is transposed, diagnoal entries(\\(a_{ii}\\)) do not change but off-diagnoal elements(\\(a_{ij} \\; i \\neq j\\)) change.\nA column vector is tranposed into a row vector, and vice versa.\nsymmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)\n\nExample:\nLet A be the matrix \\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\] The transpose of A, denoted by A^T, is the matrix \\[\n\\mathbf{A}^T = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}\n\\]\n\n\n2.5 Determinant\nLet \\(\\mathbf{A}\\) be an \\(n \\times n\\) square matrix. The determinant of \\(\\mathbf{A}\\), denoted by \\(|\\mathbf{A}|\\) or \\(\\det(\\mathbf{A})\\), is a scalar value calculated as the sum of the products of the elements in any row or column of \\(\\mathbf{A}\\) with their corresponding cofactors, that is,\n\\[\n|\\mathbf{A}|=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n\\]\nwhere \\(a_{ij}\\) is the element of \\(\\mathbf{A}\\) in the \\(i\\)-th row and \\(j\\)-th column, and \\(C_{ij}\\) is the cofactor of \\(a_{ij}\\). The cofactor of \\(a_{ij}\\), denoted by \\(C_{ij}\\), is given by \\((-1)^{i+j}\\) times the determinant of the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)-th row and \\(j\\)-th column of \\(\\mathbf{A}\\).\nThe determinant of an \\(n x n\\) matrix \\(\\mathbf{A}\\) is a scalar value denoted as \\(|\\mathbf{A}|\\). It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a \\(3 x 3\\) matrix \\(\\mathbf{A} =\\) \\[\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\]\nis given by:\n\\[\n|\\mathbf{A}| = a_{11}\n\\begin{vmatrix}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{vmatrix}\n- a_{12}\n\\begin{vmatrix}\na_{21} & a_{23} \\\\\na_{31} & a_{33}\n\\end{vmatrix}\n+ a_{13}\n\\begin{vmatrix}\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{vmatrix}\n\\]\nFor example, consider the \\(3 \\times 3\\) matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\). We can calculate the determinant of \\(\\mathbf{A}\\) using any row or column. Let’s use the first column:\n\\[\n|\\mathbf{A}| = 1\n\\begin{vmatrix}\n5 & 6 \\\\\n8 & 9\n\\end{vmatrix}\n- 4\n\\begin{vmatrix}\n2 & 3\\\\\n8 & 9\n\\end{vmatrix}\n+ 7\n\\begin{vmatrix}\n2 & 5 \\\\\n3 & 6\n\\end{vmatrix} = 0\n\\]\nTherefore, the determinant of \\(\\mathbf{A}\\) is zero.\n\n\n2.6 Inverse\nThe inverse of a square matrix \\(A\\) of size \\(n\\) is a matrix \\(A^{-1}\\) such that the product of \\(A\\) and \\(A^{-1}\\) is the identity matrix \\(I_n\\), i.e. \\(A \\times A^{-1} = I_n\\). If such a matrix exists, then \\(A\\) is said to be invertible or non-singular.\nThe inverse of a square matrix \\(\\mathbf{A}\\) is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the unique matrix that satisfies the following equation: \\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\nFor example, consider the \\(2\\times 2\\) matrix \\[\n\\mathbf{A} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}.\n\\] The inverse of \\(\\mathbf{A}\\) is given by:\n\\[\n\\mathbf{A}^{-1} =\n\\frac{1}{-2}\n\\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\]\nWe can verify that \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\) by computing:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} =\n\\begin{bmatrix} -2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\nLet me give another example and A be a \\(3x3\\) square matrix:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, the inverse of \\(\\mathbf{A}\\), denoted as \\(\\mathbf{A}^{-1}\\), is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\na_{22}a_{33}-a_{23}a_{32} & a_{13}a_{32}-a_{12}a_{33} & a_{12}a_{23}-a_{13}a_{22} \\\\\na_{23}a_{31}-a_{21}a_{33} & a_{11}a_{33}-a_{13}a_{31} & a_{13}a_{21}-a_{11}a_{23} \\\\\na_{21}a_{32}-a_{22}a_{31} & a_{12}a_{31}-a_{11}a_{32} & a_{11}a_{22}-a_{12}a_{21} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nwhere \\(det(\\mathbf{A})\\) is the determinant of \\(\\mathbf{A}\\).\nFor example, let:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, \\(det(A)\\) = -57, and the inverse of \\(\\mathbf{A}\\) is:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{-57}\\begin{bmatrix}\n-24 & 18 & 5 \\\\\n20 & -15 & -4 \\\\\n-3 & 2 & 1 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThere are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.\n\n\n2.7 Rank\nThe rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by \\(\\text{rank}(\\mathbf{A})\\).\nFor example, consider the following matrix:\n\\[\n\\begin{equation*}\n  \\mathbf{A} = \\begin{bmatrix}\n    1 & 2 & 3\\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\\\\\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe columns of \\(\\mathbf{A}\\) are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of \\(\\mathbf{A}\\) is 2.\n\n\n2.8 Trace\nThe trace of a square matrix \\(\\mathbf{A}\\), denoted by \\(\\mathrm{tr}(\\mathbf{A})\\), is defined as the sum of the diagonal elements of \\(\\mathbf{A}\\). In other words, if \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix, then its trace is given by:\n\\[\n\\mathrm{tr}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}\n\\]\nwhere \\(a_{ii}\\) denotes the \\(i\\) th diagonal element of \\(\\mathbf{A}\\).\nFor example, let \\[\n\\begin{bmatrix}\n  2 & 3 & 1 \\\\\n  0 & 5 & 2 \\\\\n  1 & 1 & 4\n\\end{bmatrix}\n\\]\nThen, the trace of \\(\\mathbf{A}\\) is \\(\\mathrm{tr}(\\mathbf{A}) = 2 + 5 + 4 = 11\\)\n\n\n2.9 Eigenvalues and Eigenvectors\nLet A be an \\(n × n\\) square matrix. A scalar \\(\\lambda\\) is called an eigenvalue of \\(\\mathbf A\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that \\[\n\\mathbf{Av}=\\lambda\\mathbf{v}\n\\]\nSuch a vector \\(\\mathbf{v}\\) is called an eigenvector corresponding to \\(\\lambda\\).\nExample:\nLet \\(\\mathbf A\\) be the matrix\nTo find the eigenvalues of \\(\\mathbf A\\), we solve the characteristic equation \\(\\text{det}(\\mathbf A - \\lambda \\mathbf I ) = 0\\), where I is the n × n identity matrix.\n\\[\n\\begin{align*}\n  \\text{det}(\\mathbf A - \\lambda \\mathbf I )\n  &=\n    \\begin{vmatrix}\n    3 - \\lambda & 1 \\\\\n    1 & 3 - \\lambda\n    \\end{vmatrix} \\\\\n  &=\n  (3 - \\lambda)(3 - \\lambda) - 1 \\\\\n  &= \\lambda^2 - 6\\lambda + 8 = 0\n\\end{align*}\n\\]\nSolving this quadratic equation gives us the eigenvalues of \\(\\mathbf A\\): \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 4\\).\nTo find the eigenvectors corresponding to \\(\\lambda_1 = 2\\), we solve the equation \\((\\mathbf A - 2 \\mathbf I)\\mathbf{v} = \\mathbf{0}\\), where \\(\\mathbf I\\) is the \\(2 \\times 2\\) identity matrix.\n\\[\n\\begin{align*}\n  (\\mathbf A - 2 \\mathbf I)\\mathbf{v} =\n    \\begin{bmatrix}\n      1 & 1 \\\\\n      1 & 1\n    \\end{bmatrix}\n  \\begin{bmatrix}\n    x \\\\\n    y\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    0 \\\\\n    0\n  \\end{bmatrix}\n\\end{align*}\n\\]\nSolving this system of equations gives us the eigenvectors corresponding to \\(\\lambda_1 = 2\\): \\(\\mathbf{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\)\nSimilarly, for \\(\\lambda_2 = 4\\), we solve the equation \\((\\mathbf A - 4\\mathbf I)\\mathbf{v}\\) = \\(\\mathbf{0}\\) to get the eigenvectors corresponding to \\(\\lambda_2 = 4\\): \\(\\mathbf{v_2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)\n\n\n2.10 Singular value and Singluar Vectors\nThe singular value decomposition (SVD) of a matrix \\(\\mathbf A\\) is a factorization of \\(\\mathbf A\\) into the product of three matrices as follows:\n\\[\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n\\] where \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) rectangular diagonal matrix with non-negative real numbers on the diagonal, and \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix.\nThe diagonal entries of \\(\\mathbf{\\Sigma}\\) are called the singular values of \\(\\mathbf{A}\\), denoted as \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_r\\) (where \\(r\\) is the rank of \\(\\mathbf{A}\\)), and are arranged in descending order. The columns of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are called the left and right singular vectors of \\(\\mathbf{A}\\), respectively, and are orthonormal vectors.\nFor example, let \\(\\mathbf{A}\\) be a 3 by 2 matrix given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} =\n  \\begin{bmatrix}\n    1 & 2\\\\\n    3 & 4\\\\\n    5 & 6\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe SVD of \\(\\mathbf{A}\\) is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T =\n\\begin{bmatrix}\n  -0.23 & -0.53 & -0.81\\\\\n  -0.53 & -0.72 & 0.45\\\\\n  -0.81 & 0.45 & -0.38\n\\end{bmatrix}\n\\begin{bmatrix}\n  9.53 & 0\\\\\n  0 & 0.90\\\\\n  0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  -0.62 & -0.78\\\\\n  -0.78 & 0.62\n\\end{bmatrix}^T\n\\end{equation*}\n\\]\nwhere the left singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{U}\\), the right singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{V}\\), and the singular values of \\(\\mathbf{A}\\) are the diagonal entries of \\(\\boldsymbol{\\Sigma}\\).\n\n연립 방정식을 행렬의 곱으로 나타내보기 \\[\\begin{matrix}x_1+2y_1=4\\\\2x_1+5y_1=9\\end{matrix} \\quad \\quad \\quad \\begin{matrix}x_2+2y_2=3\\\\2x_2+5y_2=7\\end{matrix}\\] \\[ \\begin{bmatrix} 1 & 2 \\\\ 2 & 5 \\end{bmatrix} \\begin{bmatrix} x_1 & x_2 \\\\ y_1 & y_2 \\end{bmatrix} = \\begin{bmatrix} 4 & 9 \\\\ 3 & 7 \\end{bmatrix}\\]\n중요한 사실(….당연한 사실?)\n\n곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함\n\n\\(A_{m \\times n} \\times B_{o \\times p}\\) 에서 \\(n = o\\) 여야 곱셈 성립\n\n곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 \\(\\times\\) 곱셈의 오른쪽 행렬의 열 수\n\n\\(A_{m \\times n} \\times B_{o \\times p} = C_{m \\times p}\\)\n\n교환법칙(Commutative property)이 성립하지 않음\n\n\\(AB \\neq BA\\)\n\n\n행렬 곱셈의 여러가지 관점\n\n내적으로 바라보기 \\[ A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_1^T b_1} & \\mathbf{a_1^T b_2} & \\cdots & \\mathbf{a_1^T b_m} \\\\ \\mathbf{a_2^T b_1} & \\mathbf{a_2^T b_2} & \\cdots & \\mathbf{a_2^T b_m} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ \\mathbf{a_m^T b_1} & \\mathbf{a_m^T b_2} & \\cdots & \\mathbf{a_m^T b_m} \\end{bmatrix}\\]\nrank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) \\[AB = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1^T} \\\\ \\mathbf{b_2^T} \\\\ \\vdots \\\\ \\mathbf{b_m^T} \\end{bmatrix} = \\mathbf{a_1 b_1^T} + \\mathbf{a_2 b_2^T} + \\cdots + \\mathbf{a_m b_m^T}\\]\ncolumn space로 바라보기 \\[A\\mathbf{x} = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} = \\mathbf{a_1} x_1 + \\mathbf{a_2} x_2 + \\cdots + \\mathbf{a_m} x_m \\] (스칼라배의 합)\n\n\\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) 는 2차원 좌표평면의 모든 점을, \\(A=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)은 3차원 좌표평면의 모든 점 표현 가능\n\\(AB = A \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} A \\mathbf{b_1} & A \\mathbf{b_2} & \\cdots & A \\mathbf{b_m} \\end{bmatrix}\\)\ncolumn space: A의 column vector로 만들 수 있는 부분 공간\n\nrow space로 바라보기 \\[\\mathbf{x^T}A = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_m \\end{bmatrix} \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} = x_1 \\mathbf{a_1^T} + x_2 \\mathbf{a_2^T} + \\cdots + x_m \\mathbf{a_m^T} \\]"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#열공간column-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#열공간column-space",
    "title": "Basics (3) - Special Matrices",
    "section": "3 열공간(Column Space)",
    "text": "3 열공간(Column Space)\n\n\ncolumn space: column vector 들이 span 하는 space\n\n\\(A\\)의 column space = \\(C(A)\\) 또는 \\(range(A)\\)\n\nspan: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합\n\nvector에 따라, 점일수도 선일수도 평면일 수도 있음\nvector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space\n\nvector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것\n\n\\(\\mathbf{v_1}\\) 과 \\(\\mathbf{v_2}\\)의 linear combination으로 2차원 좌표평면 나타내기"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#선형-독립linear-independent",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#선형-독립linear-independent",
    "title": "Basics (3) - Special Matrices",
    "section": "4 선형 독립(Linear Independent)",
    "text": "4 선형 독립(Linear Independent)\n\n…and also see\n\n\n선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌\northogonal 하면 independent\n\nbut independent해도 항상 orthogonal하지는 않음 (Independent > Orthogonal)\n\ndefinition: \\(a_1 \\mathbf{v_1} + a_2 \\mathbf{v_2} + a_3 \\mathbf{v_3} \\cdots a_n \\mathbf{v_n} = \\mathbf{0}\\) 를 만족하는 \\(a_1, a_2, a_3, \\cdots a_n\\) 이 \\(a_1 = a_2 = a_3 = \\cdots = a_n = 0\\) 밖에 없을때\n\n\\(\\mathbf{0}\\)는 모든 elements가 \\(0\\)인 벡터\n예: \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\\) 는 \\(-2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) 이 되므로, linearly independent 하지 않음\nindependent한 vector 들의 수 = 표현할 수 있는 차원의 dimension"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#기저basis",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#기저basis",
    "title": "Basics (3) - Special Matrices",
    "section": "5 기저(basis)",
    "text": "5 기저(basis)\n\n주어진 vector space를 span하는 linearly independent한 vectors\n어떤 공간을 이루는 필수적인 구성요소\northogonal 하면 orthogonal basis\n예: 2차원 좌표평면에 대해\n\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) : orthogonal basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) : orthogonal 하지 않은 basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}\\) : linearly independent 하지 않으므로 basis 아님"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "title": "Basics (3) - Special Matrices",
    "section": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)",
    "text": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)\n\n\n6.1 Identity matrix(항등행렬)\n\n항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)\n\n실수에서 곱셈의 항등원은 1\n\n행렬의 항등원: 항등행렬(\\(I\\)) \\[I = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\\]\n\n\\(A_{m \\times n} \\times I_{n \\times n = n} = A_{m \\times n}\\)\n\\(I_{m \\times m = m} \\times A_{m \\times n} = A_{m \\times n}\\)\n\n\n\n\n6.2 Inverse matrix(역행렬)\n\n역원: 연산 결과 항등원이 나오게 하는 연소\n\n실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): \\(a \\times a^{-1} = 1\\)\n\n행렬의 역원: 역행렬(\\(A^{-1}\\)) \\[A \\times A^{-1} = I , A^{-1} \\times A = I\\]\n\n존재하지 않는 경우도 있음\n존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림\n\n존재하지 않으면 singular, degenerate라고 불림\n\nsquare matrix(정사각행렬, \\(m = n\\))은 특수한 경우를 제외하면 역행렬이 항상 존재\n\n역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우\n\n\\(m \\neq n\\)인 행렬의 경우에는 역행렬이 존재하지 않음\n\n다만, 경우에 따라 \\(A \\times A^{-1} = I\\) 를 만족하거나(right inverse), \\(A^{-1} \\times A = I\\)를 만족하는(left inverse)는 \\(A^{-1}\\)이 존재함\n\n연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 \\[A\\mathbf{x} = \\mathbf{b} \\Rightarrow A^{-1}A\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow I\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow \\mathbf{x} = A^{-1}\\mathbf{b}\\]\n\n\n\n\n6.3 Diagonal Matrix(대각행렬)\n\ndiagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix \\[ D = Diag(\\mathbf{a}) = \\begin{bmatrix} a_{1,1} & 0 & \\cdots & 0 \\\\ 0 & a_{2,2} & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & a_{n,n} \\end{bmatrix}\\]\n\nidentity matrix는 diagonal matrix\ndiagnomal matrix는 symmetric matrix 이기도 함\n보통은 square matrix에서 주로 사용됨\n\nsquare matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix\n\n\n\n\n\n6.4 Orthogonal matrix(직교행렬, orthonomal matrix)\n\n행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) \\[A A^T = A^T A = I\\]\nidentity matrix는 orthogonal matrix\nsquare matrix에서만 정의됨\nOrthogonal matrix인 \\(A\\)이면 \\(A^{-1} = A^{T}\\)\n\n각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임\n\ncomplex matrix(복소수 행렬)에서는 unitary matrix라고 부름"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#계수rank",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#계수rank",
    "title": "Basics (3) - Special Matrices",
    "section": "7 계수(Rank)",
    "text": "7 계수(Rank)\n\n\nrank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension\nindependent한 column의 수 = independent한 행의 수: \\(rank(A) = rank(A^T)\\)\n\nproof: Wikipedia\n\n예: \\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix} \\Rightarrow rank=1\\] \\[\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\Rightarrow rank=2\\]\n\\(A_{m \\times n}\\) 의 최대 랭크는 \\(min\\{m,n\\}\\)\n\n\\(rank(A) < min\\{m,n\\}\\) 면 rank-deficient, \\(rank(A) = min\\{m,n\\}\\)면 full (row/column) rank"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#영공간null-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#영공간null-space",
    "title": "Basics (3) - Special Matrices",
    "section": "8 영공간(Null space)",
    "text": "8 영공간(Null space)\n\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) 을 만족하는 \\(\\mathbf{x}\\)의 집합\n\ncolumn space 관점에서 보기: \\(A\\mathbf{x} = x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{0}\\)\nnull space에 항상 들어가는 \\(\\mathbf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\) : trivial solution\n\n모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 \\(\\mathbf{x}=\\mathbf{0}\\)하나 밖에 없음\n\n\n\\(\\mathbf{x}=\\mathbf{0}\\) 가 아닌 vector가 null space에 있으면, 스칼라배(constant \\(c\\)에 대해 \\(c \\mathbf{x}\\)) 역시 null space에 포함됨\n혼동 주의! null space는 column space의 일부가 아님\n\nrow vector의 차원이 null space가 존재하는 공간\n\nrank와 null space의 dimension의 합은 항상 matrix의 column의 수\n\n\\(A_{m \\times n}\\)에 대해, \\(dim(N(A)) = n - r\\)\n\n모든 columns이 다 lienarly independent 하면 null space는 0차원(점)\n\nnull space는 row space와 수직한 space\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0\nrank는 row space의 dimension → row space의 dimension(\\(dim(R(A))\\))과 null space의 dimension(\\(dim(N(A))\\))의 합이 \\(n\\)\n\\(\\mathbb{R^n}\\) 공간에 표현: \n\n겹친 점: 영벡터\n\n\n\nleft null space: \\(\\mathbf{x^T} A = \\mathbf{0^T}\\) 인 \\(\\mathbf{x}\\)\n\n위의 성질을 row에 대해 적용\n\nm 차원에 놓인 벡터\ndimension: \\(dim(N_L(A)) = m - r\\)\ncolumn space와 수직: \\(dim(N_L(A)) +dim(C(A)) = m\\)\n\n\n\\(R(A)\\)에 있는 vector \\(\\mathbf{x_r}\\) 와 \\(N(A)\\)에 있는 vector \\(\\mathbf{x_n}\\)에 대해:\n\n\\(\\mathbf{x_r}\\)에 \\(A\\)를 곱하면 column space로 감\n\\(\\mathbf{x_n}\\)에 \\(A\\)를 곱하면 $\n\\(A(\\mathbf{x_r} +\\mathbf{x_n}) = A\\mathbf{x_r} + A\\mathbf{x_n} = A\\mathbf{x_r} = \\mathbf{b}\\)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#ax-b의-해의-수",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.graphs.html#ax-b의-해의-수",
    "title": "Basics (3) - Special Matrices",
    "section": "9 Ax = b의 해의 수",
    "text": "9 Ax = b의 해의 수\n\n\nfull column rank 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 해가 하나\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음 \n\nfull row rank 일때\n\n\\(\\mathbf{b}\\)는 항상 column space 안에 있음: 무한의 해를 가짐\n임의의 특정한 해(particular solution) \\(\\mathbf{x_p}\\)와 null space의 vector \\(\\mathbf{x_n}\\)에 대해, \\(A(\\mathbf{x_p} +\\mathbf{x_n})=\\mathbf{b}\\)\n\n즉, \\(\\mathbf{x_p} +\\mathbf{x_n}\\) 도 해가 됨: complete solution\n\nnull space는 무한하므로, 해도 무한함\n\n\n\nfull rank 일때(square matrix): 해가 하나 존재 (\\(\\mathbf{x} = A^{-1}\\)$)\nrank-deficient 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 무한한 해를 가짐\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html",
    "title": "Basics (3) - Special Matrices",
    "section": "",
    "text": "A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, it can be represented as: \\[\n\\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\] where \\(a_{ij}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column of the matrix \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#basic-matrix-operations",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#basic-matrix-operations",
    "title": "Basics (3) - Special Matrices",
    "section": "2 Basic Matrix Operations",
    "text": "2 Basic Matrix Operations\n\n2.1 Matrix addition\nThe sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.\nGiven two \\(m \\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is defined by:\n\\[\nc_{i,j}=a_{i,j}+b_{i,j}​\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\n\\[\n\\begin{bmatrix}\n  1 & 2 \\\\\n  3 & 4 \\\\\n  5 & 6\n\\end{bmatrix} +\n\\begin{bmatrix}\n  -1 & 0 \\\\\n  2 & -3 \\\\\n  -5 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n  0 & 2 \\\\\n  5 & 1 \\\\\n  0 & 10\n\\end{bmatrix}\n\\]\n\n\n2.2 Scalar multiplication\nThe product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.\nGiven a scalar \\(k\\) and an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), their product \\(k\\mathbf{A}\\) is defined by: \\[\n(k\\mathbf{A})_{i,j} = k(a_{i,j})\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\nExample: \\[\n2\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 & 4 \\\\\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\]\n\n\n2.3 Matrix multiplication\nThe product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is a matrix obtained by multiplying the rows of \\(\\mathbf{A}\\) by the columns of \\(\\mathbf{B}\\).\nGiven two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with dimensions \\(m \\times n\\) and \\(n \\times p\\), respectively, their product \\(\\mathbf{C} = \\mathbf{AB}\\) is an \\(m \\times p\\) matrix defined by: \\[\nc_{i,j} = \\sum_{k=1}^n a_{i,k}b_{k,j}\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq p\\).\nExample: \\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} \\begin{bmatrix}\n-1 & 0 & 2 \\\\\n2 & -3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 & -6 & 4 \\\\\n5 & -12 & 10 \\\\\n7 & -18 & 16\n\\end{bmatrix}\n\\]\n\n\n2.4 Transpose\nThe transpose of an \\(m x n\\) matrix A, denoted by \\(A^T\\), is the \\(n x m\\) matrix obtained by interchanging the rows and columns of A. Formally, if \\(\\mathbf{A} = [a_{ij}]\\) is an m x n matrix, then its transpose \\(\\mathbf{A}^T = [b_{ij}]\\) is an \\(n x m\\) matrix where \\(b_{ij}\\) = \\(a_{ji}\\) for all \\(i\\) and \\(j\\). In other words, the element in the \\(i\\) th row and \\(j\\) th column of \\(A^T\\) is equal to the element in the \\(j\\) th row and ith column of \\(\\mathbf{A}\\).\nGiven an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix defined by: $$ {i,j}^T = {j,i}\n$$\n\nWhen \\(\\mathbf{A}\\) is transposed, diagnoal entries(\\(a_{ii}\\)) do not change but off-diagnoal elements(\\(a_{ij} \\; i \\neq j\\)) change.\nA column vector is tranposed into a row vector, and vice versa.\nsymmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)\n\nExample:\nLet A be the matrix \\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\] The transpose of A, denoted by A^T, is the matrix \\[\n\\mathbf{A}^T = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}\n\\]\n\n\n2.5 Determinant\nLet \\(\\mathbf{A}\\) be an \\(n \\times n\\) square matrix. The determinant of \\(\\mathbf{A}\\), denoted by \\(|\\mathbf{A}|\\) or \\(\\det(\\mathbf{A})\\), is a scalar value calculated as the sum of the products of the elements in any row or column of \\(\\mathbf{A}\\) with their corresponding cofactors, that is,\n\\[\n|\\mathbf{A}|=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n\\]\nwhere \\(a_{ij}\\) is the element of \\(\\mathbf{A}\\) in the \\(i\\)-th row and \\(j\\)-th column, and \\(C_{ij}\\) is the cofactor of \\(a_{ij}\\). The cofactor of \\(a_{ij}\\), denoted by \\(C_{ij}\\), is given by \\((-1)^{i+j}\\) times the determinant of the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)-th row and \\(j\\)-th column of \\(\\mathbf{A}\\).\nThe determinant of an \\(n x n\\) matrix \\(\\mathbf{A}\\) is a scalar value denoted as \\(|\\mathbf{A}|\\). It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a \\(3 x 3\\) matrix \\(\\mathbf{A} =\\) \\[\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\]\nis given by:\n\\[\n|\\mathbf{A}| = a_{11}\n\\begin{vmatrix}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{vmatrix}\n- a_{12}\n\\begin{vmatrix}\na_{21} & a_{23} \\\\\na_{31} & a_{33}\n\\end{vmatrix}\n+ a_{13}\n\\begin{vmatrix}\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{vmatrix}\n\\]\nFor example, consider the \\(3 \\times 3\\) matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\). We can calculate the determinant of \\(\\mathbf{A}\\) using any row or column. Let’s use the first column:\n\\[\n|\\mathbf{A}| = 1\n\\begin{vmatrix}\n5 & 6 \\\\\n8 & 9\n\\end{vmatrix}\n- 4\n\\begin{vmatrix}\n2 & 3\\\\\n8 & 9\n\\end{vmatrix}\n+ 7\n\\begin{vmatrix}\n2 & 5 \\\\\n3 & 6\n\\end{vmatrix} = 0\n\\]\nTherefore, the determinant of \\(\\mathbf{A}\\) is zero.\n\n\n2.6 Inverse\nThe inverse of a square matrix \\(A\\) of size \\(n\\) is a matrix \\(A^{-1}\\) such that the product of \\(A\\) and \\(A^{-1}\\) is the identity matrix \\(I_n\\), i.e. \\(A \\times A^{-1} = I_n\\). If such a matrix exists, then \\(A\\) is said to be invertible or non-singular.\nThe inverse of a square matrix \\(\\mathbf{A}\\) is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the unique matrix that satisfies the following equation: \\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\nFor example, consider the \\(2\\times 2\\) matrix \\[\n\\mathbf{A} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}.\n\\] The inverse of \\(\\mathbf{A}\\) is given by:\n\\[\n\\mathbf{A}^{-1} =\n\\frac{1}{-2}\n\\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\]\nWe can verify that \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\) by computing:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} =\n\\begin{bmatrix} -2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\nLet me give another example and A be a \\(3x3\\) square matrix:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, the inverse of \\(\\mathbf{A}\\), denoted as \\(\\mathbf{A}^{-1}\\), is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\na_{22}a_{33}-a_{23}a_{32} & a_{13}a_{32}-a_{12}a_{33} & a_{12}a_{23}-a_{13}a_{22} \\\\\na_{23}a_{31}-a_{21}a_{33} & a_{11}a_{33}-a_{13}a_{31} & a_{13}a_{21}-a_{11}a_{23} \\\\\na_{21}a_{32}-a_{22}a_{31} & a_{12}a_{31}-a_{11}a_{32} & a_{11}a_{22}-a_{12}a_{21} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nwhere \\(det(\\mathbf{A})\\) is the determinant of \\(\\mathbf{A}\\).\nFor example, let:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, \\(det(A)\\) = -57, and the inverse of \\(\\mathbf{A}\\) is:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{-57}\\begin{bmatrix}\n-24 & 18 & 5 \\\\\n20 & -15 & -4 \\\\\n-3 & 2 & 1 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThere are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.\n\n\n2.7 Rank\nThe rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by \\(\\text{rank}(\\mathbf{A})\\).\nFor example, consider the following matrix:\n\\[\n\\begin{equation*}\n  \\mathbf{A} = \\begin{bmatrix}\n    1 & 2 & 3\\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\\\\\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe columns of \\(\\mathbf{A}\\) are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of \\(\\mathbf{A}\\) is 2.\n\n\n2.8 Trace\nThe trace of a square matrix \\(\\mathbf{A}\\), denoted by \\(\\mathrm{tr}(\\mathbf{A})\\), is defined as the sum of the diagonal elements of \\(\\mathbf{A}\\). In other words, if \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix, then its trace is given by:\n\\[\n\\mathrm{tr}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}\n\\]\nwhere \\(a_{ii}\\) denotes the \\(i\\) th diagonal element of \\(\\mathbf{A}\\).\nFor example, let \\[\n\\begin{bmatrix}\n  2 & 3 & 1 \\\\\n  0 & 5 & 2 \\\\\n  1 & 1 & 4\n\\end{bmatrix}\n\\]\nThen, the trace of \\(\\mathbf{A}\\) is \\(\\mathrm{tr}(\\mathbf{A}) = 2 + 5 + 4 = 11\\)\n\n\n2.9 Eigenvalues and Eigenvectors\nLet A be an \\(n × n\\) square matrix. A scalar \\(\\lambda\\) is called an eigenvalue of \\(\\mathbf A\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that \\[\n\\mathbf{Av}=\\lambda\\mathbf{v}\n\\]\nSuch a vector \\(\\mathbf{v}\\) is called an eigenvector corresponding to \\(\\lambda\\).\nExample:\nLet \\(\\mathbf A\\) be the matrix\nTo find the eigenvalues of \\(\\mathbf A\\), we solve the characteristic equation \\(\\text{det}(\\mathbf A - \\lambda \\mathbf I ) = 0\\), where I is the n × n identity matrix.\n\\[\n\\begin{align*}\n  \\text{det}(\\mathbf A - \\lambda \\mathbf I )\n  &=\n    \\begin{vmatrix}\n    3 - \\lambda & 1 \\\\\n    1 & 3 - \\lambda\n    \\end{vmatrix} \\\\\n  &=\n  (3 - \\lambda)(3 - \\lambda) - 1 \\\\\n  &= \\lambda^2 - 6\\lambda + 8 = 0\n\\end{align*}\n\\]\nSolving this quadratic equation gives us the eigenvalues of \\(\\mathbf A\\): \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 4\\).\nTo find the eigenvectors corresponding to \\(\\lambda_1 = 2\\), we solve the equation \\((\\mathbf A - 2 \\mathbf I)\\mathbf{v} = \\mathbf{0}\\), where \\(\\mathbf I\\) is the \\(2 \\times 2\\) identity matrix.\n\\[\n\\begin{align*}\n  (\\mathbf A - 2 \\mathbf I)\\mathbf{v} =\n    \\begin{bmatrix}\n      1 & 1 \\\\\n      1 & 1\n    \\end{bmatrix}\n  \\begin{bmatrix}\n    x \\\\\n    y\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    0 \\\\\n    0\n  \\end{bmatrix}\n\\end{align*}\n\\]\nSolving this system of equations gives us the eigenvectors corresponding to \\(\\lambda_1 = 2\\): \\(\\mathbf{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\)\nSimilarly, for \\(\\lambda_2 = 4\\), we solve the equation \\((\\mathbf A - 4\\mathbf I)\\mathbf{v}\\) = \\(\\mathbf{0}\\) to get the eigenvectors corresponding to \\(\\lambda_2 = 4\\): \\(\\mathbf{v_2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)\n\n\n2.10 Singular value and Singluar Vectors\nThe singular value decomposition (SVD) of a matrix \\(\\mathbf A\\) is a factorization of \\(\\mathbf A\\) into the product of three matrices as follows:\n\\[\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n\\] where \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) rectangular diagonal matrix with non-negative real numbers on the diagonal, and \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix.\nThe diagonal entries of \\(\\mathbf{\\Sigma}\\) are called the singular values of \\(\\mathbf{A}\\), denoted as \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_r\\) (where \\(r\\) is the rank of \\(\\mathbf{A}\\)), and are arranged in descending order. The columns of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are called the left and right singular vectors of \\(\\mathbf{A}\\), respectively, and are orthonormal vectors.\nFor example, let \\(\\mathbf{A}\\) be a 3 by 2 matrix given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} =\n  \\begin{bmatrix}\n    1 & 2\\\\\n    3 & 4\\\\\n    5 & 6\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe SVD of \\(\\mathbf{A}\\) is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T =\n\\begin{bmatrix}\n  -0.23 & -0.53 & -0.81\\\\\n  -0.53 & -0.72 & 0.45\\\\\n  -0.81 & 0.45 & -0.38\n\\end{bmatrix}\n\\begin{bmatrix}\n  9.53 & 0\\\\\n  0 & 0.90\\\\\n  0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  -0.62 & -0.78\\\\\n  -0.78 & 0.62\n\\end{bmatrix}^T\n\\end{equation*}\n\\]\nwhere the left singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{U}\\), the right singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{V}\\), and the singular values of \\(\\mathbf{A}\\) are the diagonal entries of \\(\\boldsymbol{\\Sigma}\\).\n\n연립 방정식을 행렬의 곱으로 나타내보기 \\[\\begin{matrix}x_1+2y_1=4\\\\2x_1+5y_1=9\\end{matrix} \\quad \\quad \\quad \\begin{matrix}x_2+2y_2=3\\\\2x_2+5y_2=7\\end{matrix}\\] \\[ \\begin{bmatrix} 1 & 2 \\\\ 2 & 5 \\end{bmatrix} \\begin{bmatrix} x_1 & x_2 \\\\ y_1 & y_2 \\end{bmatrix} = \\begin{bmatrix} 4 & 9 \\\\ 3 & 7 \\end{bmatrix}\\]\n중요한 사실(….당연한 사실?)\n\n곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함\n\n\\(A_{m \\times n} \\times B_{o \\times p}\\) 에서 \\(n = o\\) 여야 곱셈 성립\n\n곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 \\(\\times\\) 곱셈의 오른쪽 행렬의 열 수\n\n\\(A_{m \\times n} \\times B_{o \\times p} = C_{m \\times p}\\)\n\n교환법칙(Commutative property)이 성립하지 않음\n\n\\(AB \\neq BA\\)\n\n\n행렬 곱셈의 여러가지 관점\n\n내적으로 바라보기 \\[ A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_1^T b_1} & \\mathbf{a_1^T b_2} & \\cdots & \\mathbf{a_1^T b_m} \\\\ \\mathbf{a_2^T b_1} & \\mathbf{a_2^T b_2} & \\cdots & \\mathbf{a_2^T b_m} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ \\mathbf{a_m^T b_1} & \\mathbf{a_m^T b_2} & \\cdots & \\mathbf{a_m^T b_m} \\end{bmatrix}\\]\nrank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) \\[AB = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1^T} \\\\ \\mathbf{b_2^T} \\\\ \\vdots \\\\ \\mathbf{b_m^T} \\end{bmatrix} = \\mathbf{a_1 b_1^T} + \\mathbf{a_2 b_2^T} + \\cdots + \\mathbf{a_m b_m^T}\\]\ncolumn space로 바라보기 \\[A\\mathbf{x} = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} = \\mathbf{a_1} x_1 + \\mathbf{a_2} x_2 + \\cdots + \\mathbf{a_m} x_m \\] (스칼라배의 합)\n\n\\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) 는 2차원 좌표평면의 모든 점을, \\(A=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)은 3차원 좌표평면의 모든 점 표현 가능\n\\(AB = A \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} A \\mathbf{b_1} & A \\mathbf{b_2} & \\cdots & A \\mathbf{b_m} \\end{bmatrix}\\)\ncolumn space: A의 column vector로 만들 수 있는 부분 공간\n\nrow space로 바라보기 \\[\\mathbf{x^T}A = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_m \\end{bmatrix} \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} = x_1 \\mathbf{a_1^T} + x_2 \\mathbf{a_2^T} + \\cdots + x_m \\mathbf{a_m^T} \\]"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#열공간column-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#열공간column-space",
    "title": "Basics (3) - Special Matrices",
    "section": "3 열공간(Column Space)",
    "text": "3 열공간(Column Space)\n\n\ncolumn space: column vector 들이 span 하는 space\n\n\\(A\\)의 column space = \\(C(A)\\) 또는 \\(range(A)\\)\n\nspan: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합\n\nvector에 따라, 점일수도 선일수도 평면일 수도 있음\nvector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space\n\nvector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것\n\n\\(\\mathbf{v_1}\\) 과 \\(\\mathbf{v_2}\\)의 linear combination으로 2차원 좌표평면 나타내기"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#선형-독립linear-independent",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#선형-독립linear-independent",
    "title": "Basics (3) - Special Matrices",
    "section": "4 선형 독립(Linear Independent)",
    "text": "4 선형 독립(Linear Independent)\n\n…and also see\n\n\n선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌\northogonal 하면 independent\n\nbut independent해도 항상 orthogonal하지는 않음 (Independent > Orthogonal)\n\ndefinition: \\(a_1 \\mathbf{v_1} + a_2 \\mathbf{v_2} + a_3 \\mathbf{v_3} \\cdots a_n \\mathbf{v_n} = \\mathbf{0}\\) 를 만족하는 \\(a_1, a_2, a_3, \\cdots a_n\\) 이 \\(a_1 = a_2 = a_3 = \\cdots = a_n = 0\\) 밖에 없을때\n\n\\(\\mathbf{0}\\)는 모든 elements가 \\(0\\)인 벡터\n예: \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\\) 는 \\(-2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) 이 되므로, linearly independent 하지 않음\nindependent한 vector 들의 수 = 표현할 수 있는 차원의 dimension"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#기저basis",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#기저basis",
    "title": "Basics (3) - Special Matrices",
    "section": "5 기저(basis)",
    "text": "5 기저(basis)\n\n주어진 vector space를 span하는 linearly independent한 vectors\n어떤 공간을 이루는 필수적인 구성요소\northogonal 하면 orthogonal basis\n예: 2차원 좌표평면에 대해\n\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) : orthogonal basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) : orthogonal 하지 않은 basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}\\) : linearly independent 하지 않으므로 basis 아님"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "title": "Basics (3) - Special Matrices",
    "section": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)",
    "text": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)\n\n\n6.1 Identity matrix(항등행렬)\n\n항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)\n\n실수에서 곱셈의 항등원은 1\n\n행렬의 항등원: 항등행렬(\\(I\\)) \\[I = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\\]\n\n\\(A_{m \\times n} \\times I_{n \\times n = n} = A_{m \\times n}\\)\n\\(I_{m \\times m = m} \\times A_{m \\times n} = A_{m \\times n}\\)\n\n\n\n\n6.2 Inverse matrix(역행렬)\n\n역원: 연산 결과 항등원이 나오게 하는 연소\n\n실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): \\(a \\times a^{-1} = 1\\)\n\n행렬의 역원: 역행렬(\\(A^{-1}\\)) \\[A \\times A^{-1} = I , A^{-1} \\times A = I\\]\n\n존재하지 않는 경우도 있음\n존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림\n\n존재하지 않으면 singular, degenerate라고 불림\n\nsquare matrix(정사각행렬, \\(m = n\\))은 특수한 경우를 제외하면 역행렬이 항상 존재\n\n역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우\n\n\\(m \\neq n\\)인 행렬의 경우에는 역행렬이 존재하지 않음\n\n다만, 경우에 따라 \\(A \\times A^{-1} = I\\) 를 만족하거나(right inverse), \\(A^{-1} \\times A = I\\)를 만족하는(left inverse)는 \\(A^{-1}\\)이 존재함\n\n연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 \\[A\\mathbf{x} = \\mathbf{b} \\Rightarrow A^{-1}A\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow I\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow \\mathbf{x} = A^{-1}\\mathbf{b}\\]\n\n\n\n\n6.3 Diagonal Matrix(대각행렬)\n\ndiagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix \\[ D = Diag(\\mathbf{a}) = \\begin{bmatrix} a_{1,1} & 0 & \\cdots & 0 \\\\ 0 & a_{2,2} & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & a_{n,n} \\end{bmatrix}\\]\n\nidentity matrix는 diagonal matrix\ndiagnomal matrix는 symmetric matrix 이기도 함\n보통은 square matrix에서 주로 사용됨\n\nsquare matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix\n\n\n\n\n\n6.4 Orthogonal matrix(직교행렬, orthonomal matrix)\n\n행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) \\[A A^T = A^T A = I\\]\nidentity matrix는 orthogonal matrix\nsquare matrix에서만 정의됨\nOrthogonal matrix인 \\(A\\)이면 \\(A^{-1} = A^{T}\\)\n\n각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임\n\ncomplex matrix(복소수 행렬)에서는 unitary matrix라고 부름"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#계수rank",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#계수rank",
    "title": "Basics (3) - Special Matrices",
    "section": "7 계수(Rank)",
    "text": "7 계수(Rank)\n\n\nrank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension\nindependent한 column의 수 = independent한 행의 수: \\(rank(A) = rank(A^T)\\)\n\nproof: Wikipedia\n\n예: \\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix} \\Rightarrow rank=1\\] \\[\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\Rightarrow rank=2\\]\n\\(A_{m \\times n}\\) 의 최대 랭크는 \\(min\\{m,n\\}\\)\n\n\\(rank(A) < min\\{m,n\\}\\) 면 rank-deficient, \\(rank(A) = min\\{m,n\\}\\)면 full (row/column) rank"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#영공간null-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#영공간null-space",
    "title": "Basics (3) - Special Matrices",
    "section": "8 영공간(Null space)",
    "text": "8 영공간(Null space)\n\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) 을 만족하는 \\(\\mathbf{x}\\)의 집합\n\ncolumn space 관점에서 보기: \\(A\\mathbf{x} = x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{0}\\)\nnull space에 항상 들어가는 \\(\\mathbf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\) : trivial solution\n\n모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 \\(\\mathbf{x}=\\mathbf{0}\\)하나 밖에 없음\n\n\n\\(\\mathbf{x}=\\mathbf{0}\\) 가 아닌 vector가 null space에 있으면, 스칼라배(constant \\(c\\)에 대해 \\(c \\mathbf{x}\\)) 역시 null space에 포함됨\n혼동 주의! null space는 column space의 일부가 아님\n\nrow vector의 차원이 null space가 존재하는 공간\n\nrank와 null space의 dimension의 합은 항상 matrix의 column의 수\n\n\\(A_{m \\times n}\\)에 대해, \\(dim(N(A)) = n - r\\)\n\n모든 columns이 다 lienarly independent 하면 null space는 0차원(점)\n\nnull space는 row space와 수직한 space\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0\nrank는 row space의 dimension → row space의 dimension(\\(dim(R(A))\\))과 null space의 dimension(\\(dim(N(A))\\))의 합이 \\(n\\)\n\\(\\mathbb{R^n}\\) 공간에 표현: \n\n겹친 점: 영벡터\n\n\n\nleft null space: \\(\\mathbf{x^T} A = \\mathbf{0^T}\\) 인 \\(\\mathbf{x}\\)\n\n위의 성질을 row에 대해 적용\n\nm 차원에 놓인 벡터\ndimension: \\(dim(N_L(A)) = m - r\\)\ncolumn space와 수직: \\(dim(N_L(A)) +dim(C(A)) = m\\)\n\n\n\\(R(A)\\)에 있는 vector \\(\\mathbf{x_r}\\) 와 \\(N(A)\\)에 있는 vector \\(\\mathbf{x_n}\\)에 대해:\n\n\\(\\mathbf{x_r}\\)에 \\(A\\)를 곱하면 column space로 감\n\\(\\mathbf{x_n}\\)에 \\(A\\)를 곱하면 $\n\\(A(\\mathbf{x_r} +\\mathbf{x_n}) = A\\mathbf{x_r} + A\\mathbf{x_n} = A\\mathbf{x_r} = \\mathbf{b}\\)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#ax-b의-해의-수",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/12.vector_differentiation.html#ax-b의-해의-수",
    "title": "Basics (3) - Special Matrices",
    "section": "9 Ax = b의 해의 수",
    "text": "9 Ax = b의 해의 수\n\n\nfull column rank 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 해가 하나\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음 \n\nfull row rank 일때\n\n\\(\\mathbf{b}\\)는 항상 column space 안에 있음: 무한의 해를 가짐\n임의의 특정한 해(particular solution) \\(\\mathbf{x_p}\\)와 null space의 vector \\(\\mathbf{x_n}\\)에 대해, \\(A(\\mathbf{x_p} +\\mathbf{x_n})=\\mathbf{b}\\)\n\n즉, \\(\\mathbf{x_p} +\\mathbf{x_n}\\) 도 해가 됨: complete solution\n\nnull space는 무한하므로, 해도 무한함\n\n\n\nfull rank 일때(square matrix): 해가 하나 존재 (\\(\\mathbf{x} = A^{-1}\\)$)\nrank-deficient 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 무한한 해를 가짐\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html",
    "title": "Basics (3) - Special Matrices",
    "section": "",
    "text": "A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, it can be represented as: \\[\n\\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\] where \\(a_{ij}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column of the matrix \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#basic-matrix-operations",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#basic-matrix-operations",
    "title": "Basics (3) - Special Matrices",
    "section": "2 Basic Matrix Operations",
    "text": "2 Basic Matrix Operations\n\n2.1 Matrix addition\nThe sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.\nGiven two \\(m \\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is defined by:\n\\[\nc_{i,j}=a_{i,j}+b_{i,j}​\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\n\\[\n\\begin{bmatrix}\n  1 & 2 \\\\\n  3 & 4 \\\\\n  5 & 6\n\\end{bmatrix} +\n\\begin{bmatrix}\n  -1 & 0 \\\\\n  2 & -3 \\\\\n  -5 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n  0 & 2 \\\\\n  5 & 1 \\\\\n  0 & 10\n\\end{bmatrix}\n\\]\n\n\n2.2 Scalar multiplication\nThe product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.\nGiven a scalar \\(k\\) and an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), their product \\(k\\mathbf{A}\\) is defined by: \\[\n(k\\mathbf{A})_{i,j} = k(a_{i,j})\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\nExample: \\[\n2\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 & 4 \\\\\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\]\n\n\n2.3 Matrix multiplication\nThe product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is a matrix obtained by multiplying the rows of \\(\\mathbf{A}\\) by the columns of \\(\\mathbf{B}\\).\nGiven two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with dimensions \\(m \\times n\\) and \\(n \\times p\\), respectively, their product \\(\\mathbf{C} = \\mathbf{AB}\\) is an \\(m \\times p\\) matrix defined by: \\[\nc_{i,j} = \\sum_{k=1}^n a_{i,k}b_{k,j}\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq p\\).\nExample: \\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} \\begin{bmatrix}\n-1 & 0 & 2 \\\\\n2 & -3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 & -6 & 4 \\\\\n5 & -12 & 10 \\\\\n7 & -18 & 16\n\\end{bmatrix}\n\\]\n\n\n2.4 Transpose\nThe transpose of an \\(m x n\\) matrix A, denoted by \\(A^T\\), is the \\(n x m\\) matrix obtained by interchanging the rows and columns of A. Formally, if \\(\\mathbf{A} = [a_{ij}]\\) is an m x n matrix, then its transpose \\(\\mathbf{A}^T = [b_{ij}]\\) is an \\(n x m\\) matrix where \\(b_{ij}\\) = \\(a_{ji}\\) for all \\(i\\) and \\(j\\). In other words, the element in the \\(i\\) th row and \\(j\\) th column of \\(A^T\\) is equal to the element in the \\(j\\) th row and ith column of \\(\\mathbf{A}\\).\nGiven an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix defined by: $$ {i,j}^T = {j,i}\n$$\n\nWhen \\(\\mathbf{A}\\) is transposed, diagnoal entries(\\(a_{ii}\\)) do not change but off-diagnoal elements(\\(a_{ij} \\; i \\neq j\\)) change.\nA column vector is tranposed into a row vector, and vice versa.\nsymmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)\n\nExample:\nLet A be the matrix \\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\] The transpose of A, denoted by A^T, is the matrix \\[\n\\mathbf{A}^T = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}\n\\]\n\n\n2.5 Determinant\nLet \\(\\mathbf{A}\\) be an \\(n \\times n\\) square matrix. The determinant of \\(\\mathbf{A}\\), denoted by \\(|\\mathbf{A}|\\) or \\(\\det(\\mathbf{A})\\), is a scalar value calculated as the sum of the products of the elements in any row or column of \\(\\mathbf{A}\\) with their corresponding cofactors, that is,\n\\[\n|\\mathbf{A}|=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n\\]\nwhere \\(a_{ij}\\) is the element of \\(\\mathbf{A}\\) in the \\(i\\)-th row and \\(j\\)-th column, and \\(C_{ij}\\) is the cofactor of \\(a_{ij}\\). The cofactor of \\(a_{ij}\\), denoted by \\(C_{ij}\\), is given by \\((-1)^{i+j}\\) times the determinant of the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)-th row and \\(j\\)-th column of \\(\\mathbf{A}\\).\nThe determinant of an \\(n x n\\) matrix \\(\\mathbf{A}\\) is a scalar value denoted as \\(|\\mathbf{A}|\\). It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a \\(3 x 3\\) matrix \\(\\mathbf{A} =\\) \\[\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\]\nis given by:\n\\[\n|\\mathbf{A}| = a_{11}\n\\begin{vmatrix}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{vmatrix}\n- a_{12}\n\\begin{vmatrix}\na_{21} & a_{23} \\\\\na_{31} & a_{33}\n\\end{vmatrix}\n+ a_{13}\n\\begin{vmatrix}\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{vmatrix}\n\\]\nFor example, consider the \\(3 \\times 3\\) matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\). We can calculate the determinant of \\(\\mathbf{A}\\) using any row or column. Let’s use the first column:\n\\[\n|\\mathbf{A}| = 1\n\\begin{vmatrix}\n5 & 6 \\\\\n8 & 9\n\\end{vmatrix}\n- 4\n\\begin{vmatrix}\n2 & 3\\\\\n8 & 9\n\\end{vmatrix}\n+ 7\n\\begin{vmatrix}\n2 & 5 \\\\\n3 & 6\n\\end{vmatrix} = 0\n\\]\nTherefore, the determinant of \\(\\mathbf{A}\\) is zero.\n\n\n2.6 Inverse\nThe inverse of a square matrix \\(A\\) of size \\(n\\) is a matrix \\(A^{-1}\\) such that the product of \\(A\\) and \\(A^{-1}\\) is the identity matrix \\(I_n\\), i.e. \\(A \\times A^{-1} = I_n\\). If such a matrix exists, then \\(A\\) is said to be invertible or non-singular.\nThe inverse of a square matrix \\(\\mathbf{A}\\) is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the unique matrix that satisfies the following equation: \\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\nFor example, consider the \\(2\\times 2\\) matrix \\[\n\\mathbf{A} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}.\n\\] The inverse of \\(\\mathbf{A}\\) is given by:\n\\[\n\\mathbf{A}^{-1} =\n\\frac{1}{-2}\n\\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\]\nWe can verify that \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\) by computing:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} =\n\\begin{bmatrix} -2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\nLet me give another example and A be a \\(3x3\\) square matrix:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, the inverse of \\(\\mathbf{A}\\), denoted as \\(\\mathbf{A}^{-1}\\), is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\na_{22}a_{33}-a_{23}a_{32} & a_{13}a_{32}-a_{12}a_{33} & a_{12}a_{23}-a_{13}a_{22} \\\\\na_{23}a_{31}-a_{21}a_{33} & a_{11}a_{33}-a_{13}a_{31} & a_{13}a_{21}-a_{11}a_{23} \\\\\na_{21}a_{32}-a_{22}a_{31} & a_{12}a_{31}-a_{11}a_{32} & a_{11}a_{22}-a_{12}a_{21} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nwhere \\(det(\\mathbf{A})\\) is the determinant of \\(\\mathbf{A}\\).\nFor example, let:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, \\(det(A)\\) = -57, and the inverse of \\(\\mathbf{A}\\) is:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{-57}\\begin{bmatrix}\n-24 & 18 & 5 \\\\\n20 & -15 & -4 \\\\\n-3 & 2 & 1 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThere are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.\n\n\n2.7 Rank\nThe rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by \\(\\text{rank}(\\mathbf{A})\\).\nFor example, consider the following matrix:\n\\[\n\\begin{equation*}\n  \\mathbf{A} = \\begin{bmatrix}\n    1 & 2 & 3\\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\\\\\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe columns of \\(\\mathbf{A}\\) are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of \\(\\mathbf{A}\\) is 2.\n\n\n2.8 Trace\nThe trace of a square matrix \\(\\mathbf{A}\\), denoted by \\(\\mathrm{tr}(\\mathbf{A})\\), is defined as the sum of the diagonal elements of \\(\\mathbf{A}\\). In other words, if \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix, then its trace is given by:\n\\[\n\\mathrm{tr}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}\n\\]\nwhere \\(a_{ii}\\) denotes the \\(i\\) th diagonal element of \\(\\mathbf{A}\\).\nFor example, let \\[\n\\begin{bmatrix}\n  2 & 3 & 1 \\\\\n  0 & 5 & 2 \\\\\n  1 & 1 & 4\n\\end{bmatrix}\n\\]\nThen, the trace of \\(\\mathbf{A}\\) is \\(\\mathrm{tr}(\\mathbf{A}) = 2 + 5 + 4 = 11\\)\n\n\n2.9 Eigenvalues and Eigenvectors\nLet A be an \\(n × n\\) square matrix. A scalar \\(\\lambda\\) is called an eigenvalue of \\(\\mathbf A\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that \\[\n\\mathbf{Av}=\\lambda\\mathbf{v}\n\\]\nSuch a vector \\(\\mathbf{v}\\) is called an eigenvector corresponding to \\(\\lambda\\).\nExample:\nLet \\(\\mathbf A\\) be the matrix\nTo find the eigenvalues of \\(\\mathbf A\\), we solve the characteristic equation \\(\\text{det}(\\mathbf A - \\lambda \\mathbf I ) = 0\\), where I is the n × n identity matrix.\n\\[\n\\begin{align*}\n  \\text{det}(\\mathbf A - \\lambda \\mathbf I )\n  &=\n    \\begin{vmatrix}\n    3 - \\lambda & 1 \\\\\n    1 & 3 - \\lambda\n    \\end{vmatrix} \\\\\n  &=\n  (3 - \\lambda)(3 - \\lambda) - 1 \\\\\n  &= \\lambda^2 - 6\\lambda + 8 = 0\n\\end{align*}\n\\]\nSolving this quadratic equation gives us the eigenvalues of \\(\\mathbf A\\): \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 4\\).\nTo find the eigenvectors corresponding to \\(\\lambda_1 = 2\\), we solve the equation \\((\\mathbf A - 2 \\mathbf I)\\mathbf{v} = \\mathbf{0}\\), where \\(\\mathbf I\\) is the \\(2 \\times 2\\) identity matrix.\n\\[\n\\begin{align*}\n  (\\mathbf A - 2 \\mathbf I)\\mathbf{v} =\n    \\begin{bmatrix}\n      1 & 1 \\\\\n      1 & 1\n    \\end{bmatrix}\n  \\begin{bmatrix}\n    x \\\\\n    y\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    0 \\\\\n    0\n  \\end{bmatrix}\n\\end{align*}\n\\]\nSolving this system of equations gives us the eigenvectors corresponding to \\(\\lambda_1 = 2\\): \\(\\mathbf{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\)\nSimilarly, for \\(\\lambda_2 = 4\\), we solve the equation \\((\\mathbf A - 4\\mathbf I)\\mathbf{v}\\) = \\(\\mathbf{0}\\) to get the eigenvectors corresponding to \\(\\lambda_2 = 4\\): \\(\\mathbf{v_2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)\n\n\n2.10 Singular value and Singluar Vectors\nThe singular value decomposition (SVD) of a matrix \\(\\mathbf A\\) is a factorization of \\(\\mathbf A\\) into the product of three matrices as follows:\n\\[\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n\\] where \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) rectangular diagonal matrix with non-negative real numbers on the diagonal, and \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix.\nThe diagonal entries of \\(\\mathbf{\\Sigma}\\) are called the singular values of \\(\\mathbf{A}\\), denoted as \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_r\\) (where \\(r\\) is the rank of \\(\\mathbf{A}\\)), and are arranged in descending order. The columns of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are called the left and right singular vectors of \\(\\mathbf{A}\\), respectively, and are orthonormal vectors.\nFor example, let \\(\\mathbf{A}\\) be a 3 by 2 matrix given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} =\n  \\begin{bmatrix}\n    1 & 2\\\\\n    3 & 4\\\\\n    5 & 6\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe SVD of \\(\\mathbf{A}\\) is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T =\n\\begin{bmatrix}\n  -0.23 & -0.53 & -0.81\\\\\n  -0.53 & -0.72 & 0.45\\\\\n  -0.81 & 0.45 & -0.38\n\\end{bmatrix}\n\\begin{bmatrix}\n  9.53 & 0\\\\\n  0 & 0.90\\\\\n  0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  -0.62 & -0.78\\\\\n  -0.78 & 0.62\n\\end{bmatrix}^T\n\\end{equation*}\n\\]\nwhere the left singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{U}\\), the right singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{V}\\), and the singular values of \\(\\mathbf{A}\\) are the diagonal entries of \\(\\boldsymbol{\\Sigma}\\).\n\n연립 방정식을 행렬의 곱으로 나타내보기 \\[\\begin{matrix}x_1+2y_1=4\\\\2x_1+5y_1=9\\end{matrix} \\quad \\quad \\quad \\begin{matrix}x_2+2y_2=3\\\\2x_2+5y_2=7\\end{matrix}\\] \\[ \\begin{bmatrix} 1 & 2 \\\\ 2 & 5 \\end{bmatrix} \\begin{bmatrix} x_1 & x_2 \\\\ y_1 & y_2 \\end{bmatrix} = \\begin{bmatrix} 4 & 9 \\\\ 3 & 7 \\end{bmatrix}\\]\n중요한 사실(….당연한 사실?)\n\n곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함\n\n\\(A_{m \\times n} \\times B_{o \\times p}\\) 에서 \\(n = o\\) 여야 곱셈 성립\n\n곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 \\(\\times\\) 곱셈의 오른쪽 행렬의 열 수\n\n\\(A_{m \\times n} \\times B_{o \\times p} = C_{m \\times p}\\)\n\n교환법칙(Commutative property)이 성립하지 않음\n\n\\(AB \\neq BA\\)\n\n\n행렬 곱셈의 여러가지 관점\n\n내적으로 바라보기 \\[ A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_1^T b_1} & \\mathbf{a_1^T b_2} & \\cdots & \\mathbf{a_1^T b_m} \\\\ \\mathbf{a_2^T b_1} & \\mathbf{a_2^T b_2} & \\cdots & \\mathbf{a_2^T b_m} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ \\mathbf{a_m^T b_1} & \\mathbf{a_m^T b_2} & \\cdots & \\mathbf{a_m^T b_m} \\end{bmatrix}\\]\nrank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) \\[AB = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1^T} \\\\ \\mathbf{b_2^T} \\\\ \\vdots \\\\ \\mathbf{b_m^T} \\end{bmatrix} = \\mathbf{a_1 b_1^T} + \\mathbf{a_2 b_2^T} + \\cdots + \\mathbf{a_m b_m^T}\\]\ncolumn space로 바라보기 \\[A\\mathbf{x} = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} = \\mathbf{a_1} x_1 + \\mathbf{a_2} x_2 + \\cdots + \\mathbf{a_m} x_m \\] (스칼라배의 합)\n\n\\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) 는 2차원 좌표평면의 모든 점을, \\(A=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)은 3차원 좌표평면의 모든 점 표현 가능\n\\(AB = A \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} A \\mathbf{b_1} & A \\mathbf{b_2} & \\cdots & A \\mathbf{b_m} \\end{bmatrix}\\)\ncolumn space: A의 column vector로 만들 수 있는 부분 공간\n\nrow space로 바라보기 \\[\\mathbf{x^T}A = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_m \\end{bmatrix} \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} = x_1 \\mathbf{a_1^T} + x_2 \\mathbf{a_2^T} + \\cdots + x_m \\mathbf{a_m^T} \\]"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#열공간column-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#열공간column-space",
    "title": "Basics (3) - Special Matrices",
    "section": "3 열공간(Column Space)",
    "text": "3 열공간(Column Space)\n\n\ncolumn space: column vector 들이 span 하는 space\n\n\\(A\\)의 column space = \\(C(A)\\) 또는 \\(range(A)\\)\n\nspan: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합\n\nvector에 따라, 점일수도 선일수도 평면일 수도 있음\nvector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space\n\nvector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것\n\n\\(\\mathbf{v_1}\\) 과 \\(\\mathbf{v_2}\\)의 linear combination으로 2차원 좌표평면 나타내기"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#선형-독립linear-independent",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#선형-독립linear-independent",
    "title": "Basics (3) - Special Matrices",
    "section": "4 선형 독립(Linear Independent)",
    "text": "4 선형 독립(Linear Independent)\n\n…and also see\n\n\n선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌\northogonal 하면 independent\n\nbut independent해도 항상 orthogonal하지는 않음 (Independent > Orthogonal)\n\ndefinition: \\(a_1 \\mathbf{v_1} + a_2 \\mathbf{v_2} + a_3 \\mathbf{v_3} \\cdots a_n \\mathbf{v_n} = \\mathbf{0}\\) 를 만족하는 \\(a_1, a_2, a_3, \\cdots a_n\\) 이 \\(a_1 = a_2 = a_3 = \\cdots = a_n = 0\\) 밖에 없을때\n\n\\(\\mathbf{0}\\)는 모든 elements가 \\(0\\)인 벡터\n예: \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\\) 는 \\(-2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) 이 되므로, linearly independent 하지 않음\nindependent한 vector 들의 수 = 표현할 수 있는 차원의 dimension"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#기저basis",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#기저basis",
    "title": "Basics (3) - Special Matrices",
    "section": "5 기저(basis)",
    "text": "5 기저(basis)\n\n주어진 vector space를 span하는 linearly independent한 vectors\n어떤 공간을 이루는 필수적인 구성요소\northogonal 하면 orthogonal basis\n예: 2차원 좌표평면에 대해\n\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) : orthogonal basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) : orthogonal 하지 않은 basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}\\) : linearly independent 하지 않으므로 basis 아님"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "title": "Basics (3) - Special Matrices",
    "section": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)",
    "text": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)\n\n\n6.1 Identity matrix(항등행렬)\n\n항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)\n\n실수에서 곱셈의 항등원은 1\n\n행렬의 항등원: 항등행렬(\\(I\\)) \\[I = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\\]\n\n\\(A_{m \\times n} \\times I_{n \\times n = n} = A_{m \\times n}\\)\n\\(I_{m \\times m = m} \\times A_{m \\times n} = A_{m \\times n}\\)\n\n\n\n\n6.2 Inverse matrix(역행렬)\n\n역원: 연산 결과 항등원이 나오게 하는 연소\n\n실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): \\(a \\times a^{-1} = 1\\)\n\n행렬의 역원: 역행렬(\\(A^{-1}\\)) \\[A \\times A^{-1} = I , A^{-1} \\times A = I\\]\n\n존재하지 않는 경우도 있음\n존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림\n\n존재하지 않으면 singular, degenerate라고 불림\n\nsquare matrix(정사각행렬, \\(m = n\\))은 특수한 경우를 제외하면 역행렬이 항상 존재\n\n역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우\n\n\\(m \\neq n\\)인 행렬의 경우에는 역행렬이 존재하지 않음\n\n다만, 경우에 따라 \\(A \\times A^{-1} = I\\) 를 만족하거나(right inverse), \\(A^{-1} \\times A = I\\)를 만족하는(left inverse)는 \\(A^{-1}\\)이 존재함\n\n연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 \\[A\\mathbf{x} = \\mathbf{b} \\Rightarrow A^{-1}A\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow I\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow \\mathbf{x} = A^{-1}\\mathbf{b}\\]\n\n\n\n\n6.3 Diagonal Matrix(대각행렬)\n\ndiagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix \\[ D = Diag(\\mathbf{a}) = \\begin{bmatrix} a_{1,1} & 0 & \\cdots & 0 \\\\ 0 & a_{2,2} & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & a_{n,n} \\end{bmatrix}\\]\n\nidentity matrix는 diagonal matrix\ndiagnomal matrix는 symmetric matrix 이기도 함\n보통은 square matrix에서 주로 사용됨\n\nsquare matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix\n\n\n\n\n\n6.4 Orthogonal matrix(직교행렬, orthonomal matrix)\n\n행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) \\[A A^T = A^T A = I\\]\nidentity matrix는 orthogonal matrix\nsquare matrix에서만 정의됨\nOrthogonal matrix인 \\(A\\)이면 \\(A^{-1} = A^{T}\\)\n\n각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임\n\ncomplex matrix(복소수 행렬)에서는 unitary matrix라고 부름"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#계수rank",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#계수rank",
    "title": "Basics (3) - Special Matrices",
    "section": "7 계수(Rank)",
    "text": "7 계수(Rank)\n\n\nrank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension\nindependent한 column의 수 = independent한 행의 수: \\(rank(A) = rank(A^T)\\)\n\nproof: Wikipedia\n\n예: \\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix} \\Rightarrow rank=1\\] \\[\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\Rightarrow rank=2\\]\n\\(A_{m \\times n}\\) 의 최대 랭크는 \\(min\\{m,n\\}\\)\n\n\\(rank(A) < min\\{m,n\\}\\) 면 rank-deficient, \\(rank(A) = min\\{m,n\\}\\)면 full (row/column) rank"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#영공간null-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#영공간null-space",
    "title": "Basics (3) - Special Matrices",
    "section": "8 영공간(Null space)",
    "text": "8 영공간(Null space)\n\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) 을 만족하는 \\(\\mathbf{x}\\)의 집합\n\ncolumn space 관점에서 보기: \\(A\\mathbf{x} = x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{0}\\)\nnull space에 항상 들어가는 \\(\\mathbf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\) : trivial solution\n\n모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 \\(\\mathbf{x}=\\mathbf{0}\\)하나 밖에 없음\n\n\n\\(\\mathbf{x}=\\mathbf{0}\\) 가 아닌 vector가 null space에 있으면, 스칼라배(constant \\(c\\)에 대해 \\(c \\mathbf{x}\\)) 역시 null space에 포함됨\n혼동 주의! null space는 column space의 일부가 아님\n\nrow vector의 차원이 null space가 존재하는 공간\n\nrank와 null space의 dimension의 합은 항상 matrix의 column의 수\n\n\\(A_{m \\times n}\\)에 대해, \\(dim(N(A)) = n - r\\)\n\n모든 columns이 다 lienarly independent 하면 null space는 0차원(점)\n\nnull space는 row space와 수직한 space\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0\nrank는 row space의 dimension → row space의 dimension(\\(dim(R(A))\\))과 null space의 dimension(\\(dim(N(A))\\))의 합이 \\(n\\)\n\\(\\mathbb{R^n}\\) 공간에 표현: \n\n겹친 점: 영벡터\n\n\n\nleft null space: \\(\\mathbf{x^T} A = \\mathbf{0^T}\\) 인 \\(\\mathbf{x}\\)\n\n위의 성질을 row에 대해 적용\n\nm 차원에 놓인 벡터\ndimension: \\(dim(N_L(A)) = m - r\\)\ncolumn space와 수직: \\(dim(N_L(A)) +dim(C(A)) = m\\)\n\n\n\\(R(A)\\)에 있는 vector \\(\\mathbf{x_r}\\) 와 \\(N(A)\\)에 있는 vector \\(\\mathbf{x_n}\\)에 대해:\n\n\\(\\mathbf{x_r}\\)에 \\(A\\)를 곱하면 column space로 감\n\\(\\mathbf{x_n}\\)에 \\(A\\)를 곱하면 $\n\\(A(\\mathbf{x_r} +\\mathbf{x_n}) = A\\mathbf{x_r} + A\\mathbf{x_n} = A\\mathbf{x_r} = \\mathbf{b}\\)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#ax-b의-해의-수",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/13.markov_matrices.html#ax-b의-해의-수",
    "title": "Basics (3) - Special Matrices",
    "section": "9 Ax = b의 해의 수",
    "text": "9 Ax = b의 해의 수\n\n\nfull column rank 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 해가 하나\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음 \n\nfull row rank 일때\n\n\\(\\mathbf{b}\\)는 항상 column space 안에 있음: 무한의 해를 가짐\n임의의 특정한 해(particular solution) \\(\\mathbf{x_p}\\)와 null space의 vector \\(\\mathbf{x_n}\\)에 대해, \\(A(\\mathbf{x_p} +\\mathbf{x_n})=\\mathbf{b}\\)\n\n즉, \\(\\mathbf{x_p} +\\mathbf{x_n}\\) 도 해가 됨: complete solution\n\nnull space는 무한하므로, 해도 무한함\n\n\n\nfull rank 일때(square matrix): 해가 하나 존재 (\\(\\mathbf{x} = A^{-1}\\)$)\nrank-deficient 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 무한한 해를 가짐\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html",
    "title": "Basics (3) - Special Matrices",
    "section": "",
    "text": "A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, it can be represented as: \\[\n\\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\] where \\(a_{ij}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column of the matrix \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#basic-matrix-operations",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#basic-matrix-operations",
    "title": "Basics (3) - Special Matrices",
    "section": "2 Basic Matrix Operations",
    "text": "2 Basic Matrix Operations\n\n2.1 Matrix addition\nThe sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.\nGiven two \\(m \\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is defined by:\n\\[\nc_{i,j}=a_{i,j}+b_{i,j}​\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\n\\[\n\\begin{bmatrix}\n  1 & 2 \\\\\n  3 & 4 \\\\\n  5 & 6\n\\end{bmatrix} +\n\\begin{bmatrix}\n  -1 & 0 \\\\\n  2 & -3 \\\\\n  -5 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n  0 & 2 \\\\\n  5 & 1 \\\\\n  0 & 10\n\\end{bmatrix}\n\\]\n\n\n2.2 Scalar multiplication\nThe product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.\nGiven a scalar \\(k\\) and an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), their product \\(k\\mathbf{A}\\) is defined by: \\[\n(k\\mathbf{A})_{i,j} = k(a_{i,j})\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\nExample: \\[\n2\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 & 4 \\\\\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\]\n\n\n2.3 Matrix multiplication\nThe product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is a matrix obtained by multiplying the rows of \\(\\mathbf{A}\\) by the columns of \\(\\mathbf{B}\\).\nGiven two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with dimensions \\(m \\times n\\) and \\(n \\times p\\), respectively, their product \\(\\mathbf{C} = \\mathbf{AB}\\) is an \\(m \\times p\\) matrix defined by: \\[\nc_{i,j} = \\sum_{k=1}^n a_{i,k}b_{k,j}\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq p\\).\nExample: \\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} \\begin{bmatrix}\n-1 & 0 & 2 \\\\\n2 & -3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 & -6 & 4 \\\\\n5 & -12 & 10 \\\\\n7 & -18 & 16\n\\end{bmatrix}\n\\]\n\n\n2.4 Transpose\nThe transpose of an \\(m x n\\) matrix A, denoted by \\(A^T\\), is the \\(n x m\\) matrix obtained by interchanging the rows and columns of A. Formally, if \\(\\mathbf{A} = [a_{ij}]\\) is an m x n matrix, then its transpose \\(\\mathbf{A}^T = [b_{ij}]\\) is an \\(n x m\\) matrix where \\(b_{ij}\\) = \\(a_{ji}\\) for all \\(i\\) and \\(j\\). In other words, the element in the \\(i\\) th row and \\(j\\) th column of \\(A^T\\) is equal to the element in the \\(j\\) th row and ith column of \\(\\mathbf{A}\\).\nGiven an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix defined by: $$ {i,j}^T = {j,i}\n$$\n\nWhen \\(\\mathbf{A}\\) is transposed, diagnoal entries(\\(a_{ii}\\)) do not change but off-diagnoal elements(\\(a_{ij} \\; i \\neq j\\)) change.\nA column vector is tranposed into a row vector, and vice versa.\nsymmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)\n\nExample:\nLet A be the matrix \\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\] The transpose of A, denoted by A^T, is the matrix \\[\n\\mathbf{A}^T = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}\n\\]\n\n\n2.5 Determinant\nLet \\(\\mathbf{A}\\) be an \\(n \\times n\\) square matrix. The determinant of \\(\\mathbf{A}\\), denoted by \\(|\\mathbf{A}|\\) or \\(\\det(\\mathbf{A})\\), is a scalar value calculated as the sum of the products of the elements in any row or column of \\(\\mathbf{A}\\) with their corresponding cofactors, that is,\n\\[\n|\\mathbf{A}|=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n\\]\nwhere \\(a_{ij}\\) is the element of \\(\\mathbf{A}\\) in the \\(i\\)-th row and \\(j\\)-th column, and \\(C_{ij}\\) is the cofactor of \\(a_{ij}\\). The cofactor of \\(a_{ij}\\), denoted by \\(C_{ij}\\), is given by \\((-1)^{i+j}\\) times the determinant of the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)-th row and \\(j\\)-th column of \\(\\mathbf{A}\\).\nThe determinant of an \\(n x n\\) matrix \\(\\mathbf{A}\\) is a scalar value denoted as \\(|\\mathbf{A}|\\). It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a \\(3 x 3\\) matrix \\(\\mathbf{A} =\\) \\[\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\]\nis given by:\n\\[\n|\\mathbf{A}| = a_{11}\n\\begin{vmatrix}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{vmatrix}\n- a_{12}\n\\begin{vmatrix}\na_{21} & a_{23} \\\\\na_{31} & a_{33}\n\\end{vmatrix}\n+ a_{13}\n\\begin{vmatrix}\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{vmatrix}\n\\]\nFor example, consider the \\(3 \\times 3\\) matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\). We can calculate the determinant of \\(\\mathbf{A}\\) using any row or column. Let’s use the first column:\n\\[\n|\\mathbf{A}| = 1\n\\begin{vmatrix}\n5 & 6 \\\\\n8 & 9\n\\end{vmatrix}\n- 4\n\\begin{vmatrix}\n2 & 3\\\\\n8 & 9\n\\end{vmatrix}\n+ 7\n\\begin{vmatrix}\n2 & 5 \\\\\n3 & 6\n\\end{vmatrix} = 0\n\\]\nTherefore, the determinant of \\(\\mathbf{A}\\) is zero.\n\n\n2.6 Inverse\nThe inverse of a square matrix \\(A\\) of size \\(n\\) is a matrix \\(A^{-1}\\) such that the product of \\(A\\) and \\(A^{-1}\\) is the identity matrix \\(I_n\\), i.e. \\(A \\times A^{-1} = I_n\\). If such a matrix exists, then \\(A\\) is said to be invertible or non-singular.\nThe inverse of a square matrix \\(\\mathbf{A}\\) is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the unique matrix that satisfies the following equation: \\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\nFor example, consider the \\(2\\times 2\\) matrix \\[\n\\mathbf{A} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}.\n\\] The inverse of \\(\\mathbf{A}\\) is given by:\n\\[\n\\mathbf{A}^{-1} =\n\\frac{1}{-2}\n\\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\]\nWe can verify that \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\) by computing:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} =\n\\begin{bmatrix} -2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\nLet me give another example and A be a \\(3x3\\) square matrix:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, the inverse of \\(\\mathbf{A}\\), denoted as \\(\\mathbf{A}^{-1}\\), is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\na_{22}a_{33}-a_{23}a_{32} & a_{13}a_{32}-a_{12}a_{33} & a_{12}a_{23}-a_{13}a_{22} \\\\\na_{23}a_{31}-a_{21}a_{33} & a_{11}a_{33}-a_{13}a_{31} & a_{13}a_{21}-a_{11}a_{23} \\\\\na_{21}a_{32}-a_{22}a_{31} & a_{12}a_{31}-a_{11}a_{32} & a_{11}a_{22}-a_{12}a_{21} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nwhere \\(det(\\mathbf{A})\\) is the determinant of \\(\\mathbf{A}\\).\nFor example, let:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, \\(det(A)\\) = -57, and the inverse of \\(\\mathbf{A}\\) is:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{-57}\\begin{bmatrix}\n-24 & 18 & 5 \\\\\n20 & -15 & -4 \\\\\n-3 & 2 & 1 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThere are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.\n\n\n2.7 Rank\nThe rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by \\(\\text{rank}(\\mathbf{A})\\).\nFor example, consider the following matrix:\n\\[\n\\begin{equation*}\n  \\mathbf{A} = \\begin{bmatrix}\n    1 & 2 & 3\\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\\\\\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe columns of \\(\\mathbf{A}\\) are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of \\(\\mathbf{A}\\) is 2.\n\n\n2.8 Trace\nThe trace of a square matrix \\(\\mathbf{A}\\), denoted by \\(\\mathrm{tr}(\\mathbf{A})\\), is defined as the sum of the diagonal elements of \\(\\mathbf{A}\\). In other words, if \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix, then its trace is given by:\n\\[\n\\mathrm{tr}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}\n\\]\nwhere \\(a_{ii}\\) denotes the \\(i\\) th diagonal element of \\(\\mathbf{A}\\).\nFor example, let \\[\n\\begin{bmatrix}\n  2 & 3 & 1 \\\\\n  0 & 5 & 2 \\\\\n  1 & 1 & 4\n\\end{bmatrix}\n\\]\nThen, the trace of \\(\\mathbf{A}\\) is \\(\\mathrm{tr}(\\mathbf{A}) = 2 + 5 + 4 = 11\\)\n\n\n2.9 Eigenvalues and Eigenvectors\nLet A be an \\(n × n\\) square matrix. A scalar \\(\\lambda\\) is called an eigenvalue of \\(\\mathbf A\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that \\[\n\\mathbf{Av}=\\lambda\\mathbf{v}\n\\]\nSuch a vector \\(\\mathbf{v}\\) is called an eigenvector corresponding to \\(\\lambda\\).\nExample:\nLet \\(\\mathbf A\\) be the matrix\nTo find the eigenvalues of \\(\\mathbf A\\), we solve the characteristic equation \\(\\text{det}(\\mathbf A - \\lambda \\mathbf I ) = 0\\), where I is the n × n identity matrix.\n\\[\n\\begin{align*}\n  \\text{det}(\\mathbf A - \\lambda \\mathbf I )\n  &=\n    \\begin{vmatrix}\n    3 - \\lambda & 1 \\\\\n    1 & 3 - \\lambda\n    \\end{vmatrix} \\\\\n  &=\n  (3 - \\lambda)(3 - \\lambda) - 1 \\\\\n  &= \\lambda^2 - 6\\lambda + 8 = 0\n\\end{align*}\n\\]\nSolving this quadratic equation gives us the eigenvalues of \\(\\mathbf A\\): \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 4\\).\nTo find the eigenvectors corresponding to \\(\\lambda_1 = 2\\), we solve the equation \\((\\mathbf A - 2 \\mathbf I)\\mathbf{v} = \\mathbf{0}\\), where \\(\\mathbf I\\) is the \\(2 \\times 2\\) identity matrix.\n\\[\n\\begin{align*}\n  (\\mathbf A - 2 \\mathbf I)\\mathbf{v} =\n    \\begin{bmatrix}\n      1 & 1 \\\\\n      1 & 1\n    \\end{bmatrix}\n  \\begin{bmatrix}\n    x \\\\\n    y\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    0 \\\\\n    0\n  \\end{bmatrix}\n\\end{align*}\n\\]\nSolving this system of equations gives us the eigenvectors corresponding to \\(\\lambda_1 = 2\\): \\(\\mathbf{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\)\nSimilarly, for \\(\\lambda_2 = 4\\), we solve the equation \\((\\mathbf A - 4\\mathbf I)\\mathbf{v}\\) = \\(\\mathbf{0}\\) to get the eigenvectors corresponding to \\(\\lambda_2 = 4\\): \\(\\mathbf{v_2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)\n\n\n2.10 Singular value and Singluar Vectors\nThe singular value decomposition (SVD) of a matrix \\(\\mathbf A\\) is a factorization of \\(\\mathbf A\\) into the product of three matrices as follows:\n\\[\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n\\] where \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) rectangular diagonal matrix with non-negative real numbers on the diagonal, and \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix.\nThe diagonal entries of \\(\\mathbf{\\Sigma}\\) are called the singular values of \\(\\mathbf{A}\\), denoted as \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_r\\) (where \\(r\\) is the rank of \\(\\mathbf{A}\\)), and are arranged in descending order. The columns of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are called the left and right singular vectors of \\(\\mathbf{A}\\), respectively, and are orthonormal vectors.\nFor example, let \\(\\mathbf{A}\\) be a 3 by 2 matrix given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} =\n  \\begin{bmatrix}\n    1 & 2\\\\\n    3 & 4\\\\\n    5 & 6\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe SVD of \\(\\mathbf{A}\\) is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T =\n\\begin{bmatrix}\n  -0.23 & -0.53 & -0.81\\\\\n  -0.53 & -0.72 & 0.45\\\\\n  -0.81 & 0.45 & -0.38\n\\end{bmatrix}\n\\begin{bmatrix}\n  9.53 & 0\\\\\n  0 & 0.90\\\\\n  0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  -0.62 & -0.78\\\\\n  -0.78 & 0.62\n\\end{bmatrix}^T\n\\end{equation*}\n\\]\nwhere the left singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{U}\\), the right singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{V}\\), and the singular values of \\(\\mathbf{A}\\) are the diagonal entries of \\(\\boldsymbol{\\Sigma}\\).\n\n연립 방정식을 행렬의 곱으로 나타내보기 \\[\\begin{matrix}x_1+2y_1=4\\\\2x_1+5y_1=9\\end{matrix} \\quad \\quad \\quad \\begin{matrix}x_2+2y_2=3\\\\2x_2+5y_2=7\\end{matrix}\\] \\[ \\begin{bmatrix} 1 & 2 \\\\ 2 & 5 \\end{bmatrix} \\begin{bmatrix} x_1 & x_2 \\\\ y_1 & y_2 \\end{bmatrix} = \\begin{bmatrix} 4 & 9 \\\\ 3 & 7 \\end{bmatrix}\\]\n중요한 사실(….당연한 사실?)\n\n곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함\n\n\\(A_{m \\times n} \\times B_{o \\times p}\\) 에서 \\(n = o\\) 여야 곱셈 성립\n\n곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 \\(\\times\\) 곱셈의 오른쪽 행렬의 열 수\n\n\\(A_{m \\times n} \\times B_{o \\times p} = C_{m \\times p}\\)\n\n교환법칙(Commutative property)이 성립하지 않음\n\n\\(AB \\neq BA\\)\n\n\n행렬 곱셈의 여러가지 관점\n\n내적으로 바라보기 \\[ A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_1^T b_1} & \\mathbf{a_1^T b_2} & \\cdots & \\mathbf{a_1^T b_m} \\\\ \\mathbf{a_2^T b_1} & \\mathbf{a_2^T b_2} & \\cdots & \\mathbf{a_2^T b_m} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ \\mathbf{a_m^T b_1} & \\mathbf{a_m^T b_2} & \\cdots & \\mathbf{a_m^T b_m} \\end{bmatrix}\\]\nrank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) \\[AB = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1^T} \\\\ \\mathbf{b_2^T} \\\\ \\vdots \\\\ \\mathbf{b_m^T} \\end{bmatrix} = \\mathbf{a_1 b_1^T} + \\mathbf{a_2 b_2^T} + \\cdots + \\mathbf{a_m b_m^T}\\]\ncolumn space로 바라보기 \\[A\\mathbf{x} = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} = \\mathbf{a_1} x_1 + \\mathbf{a_2} x_2 + \\cdots + \\mathbf{a_m} x_m \\] (스칼라배의 합)\n\n\\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) 는 2차원 좌표평면의 모든 점을, \\(A=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)은 3차원 좌표평면의 모든 점 표현 가능\n\\(AB = A \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} A \\mathbf{b_1} & A \\mathbf{b_2} & \\cdots & A \\mathbf{b_m} \\end{bmatrix}\\)\ncolumn space: A의 column vector로 만들 수 있는 부분 공간\n\nrow space로 바라보기 \\[\\mathbf{x^T}A = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_m \\end{bmatrix} \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} = x_1 \\mathbf{a_1^T} + x_2 \\mathbf{a_2^T} + \\cdots + x_m \\mathbf{a_m^T} \\]"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#열공간column-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#열공간column-space",
    "title": "Basics (3) - Special Matrices",
    "section": "3 열공간(Column Space)",
    "text": "3 열공간(Column Space)\n\n\ncolumn space: column vector 들이 span 하는 space\n\n\\(A\\)의 column space = \\(C(A)\\) 또는 \\(range(A)\\)\n\nspan: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합\n\nvector에 따라, 점일수도 선일수도 평면일 수도 있음\nvector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space\n\nvector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것\n\n\\(\\mathbf{v_1}\\) 과 \\(\\mathbf{v_2}\\)의 linear combination으로 2차원 좌표평면 나타내기"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#선형-독립linear-independent",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#선형-독립linear-independent",
    "title": "Basics (3) - Special Matrices",
    "section": "4 선형 독립(Linear Independent)",
    "text": "4 선형 독립(Linear Independent)\n\n…and also see\n\n\n선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌\northogonal 하면 independent\n\nbut independent해도 항상 orthogonal하지는 않음 (Independent > Orthogonal)\n\ndefinition: \\(a_1 \\mathbf{v_1} + a_2 \\mathbf{v_2} + a_3 \\mathbf{v_3} \\cdots a_n \\mathbf{v_n} = \\mathbf{0}\\) 를 만족하는 \\(a_1, a_2, a_3, \\cdots a_n\\) 이 \\(a_1 = a_2 = a_3 = \\cdots = a_n = 0\\) 밖에 없을때\n\n\\(\\mathbf{0}\\)는 모든 elements가 \\(0\\)인 벡터\n예: \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\\) 는 \\(-2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) 이 되므로, linearly independent 하지 않음\nindependent한 vector 들의 수 = 표현할 수 있는 차원의 dimension"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#기저basis",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#기저basis",
    "title": "Basics (3) - Special Matrices",
    "section": "5 기저(basis)",
    "text": "5 기저(basis)\n\n주어진 vector space를 span하는 linearly independent한 vectors\n어떤 공간을 이루는 필수적인 구성요소\northogonal 하면 orthogonal basis\n예: 2차원 좌표평면에 대해\n\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) : orthogonal basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) : orthogonal 하지 않은 basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}\\) : linearly independent 하지 않으므로 basis 아님"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "title": "Basics (3) - Special Matrices",
    "section": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)",
    "text": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)\n\n\n6.1 Identity matrix(항등행렬)\n\n항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)\n\n실수에서 곱셈의 항등원은 1\n\n행렬의 항등원: 항등행렬(\\(I\\)) \\[I = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\\]\n\n\\(A_{m \\times n} \\times I_{n \\times n = n} = A_{m \\times n}\\)\n\\(I_{m \\times m = m} \\times A_{m \\times n} = A_{m \\times n}\\)\n\n\n\n\n6.2 Inverse matrix(역행렬)\n\n역원: 연산 결과 항등원이 나오게 하는 연소\n\n실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): \\(a \\times a^{-1} = 1\\)\n\n행렬의 역원: 역행렬(\\(A^{-1}\\)) \\[A \\times A^{-1} = I , A^{-1} \\times A = I\\]\n\n존재하지 않는 경우도 있음\n존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림\n\n존재하지 않으면 singular, degenerate라고 불림\n\nsquare matrix(정사각행렬, \\(m = n\\))은 특수한 경우를 제외하면 역행렬이 항상 존재\n\n역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우\n\n\\(m \\neq n\\)인 행렬의 경우에는 역행렬이 존재하지 않음\n\n다만, 경우에 따라 \\(A \\times A^{-1} = I\\) 를 만족하거나(right inverse), \\(A^{-1} \\times A = I\\)를 만족하는(left inverse)는 \\(A^{-1}\\)이 존재함\n\n연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 \\[A\\mathbf{x} = \\mathbf{b} \\Rightarrow A^{-1}A\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow I\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow \\mathbf{x} = A^{-1}\\mathbf{b}\\]\n\n\n\n\n6.3 Diagonal Matrix(대각행렬)\n\ndiagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix \\[ D = Diag(\\mathbf{a}) = \\begin{bmatrix} a_{1,1} & 0 & \\cdots & 0 \\\\ 0 & a_{2,2} & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & a_{n,n} \\end{bmatrix}\\]\n\nidentity matrix는 diagonal matrix\ndiagnomal matrix는 symmetric matrix 이기도 함\n보통은 square matrix에서 주로 사용됨\n\nsquare matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix\n\n\n\n\n\n6.4 Orthogonal matrix(직교행렬, orthonomal matrix)\n\n행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) \\[A A^T = A^T A = I\\]\nidentity matrix는 orthogonal matrix\nsquare matrix에서만 정의됨\nOrthogonal matrix인 \\(A\\)이면 \\(A^{-1} = A^{T}\\)\n\n각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임\n\ncomplex matrix(복소수 행렬)에서는 unitary matrix라고 부름"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#계수rank",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#계수rank",
    "title": "Basics (3) - Special Matrices",
    "section": "7 계수(Rank)",
    "text": "7 계수(Rank)\n\n\nrank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension\nindependent한 column의 수 = independent한 행의 수: \\(rank(A) = rank(A^T)\\)\n\nproof: Wikipedia\n\n예: \\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix} \\Rightarrow rank=1\\] \\[\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\Rightarrow rank=2\\]\n\\(A_{m \\times n}\\) 의 최대 랭크는 \\(min\\{m,n\\}\\)\n\n\\(rank(A) < min\\{m,n\\}\\) 면 rank-deficient, \\(rank(A) = min\\{m,n\\}\\)면 full (row/column) rank"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#영공간null-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#영공간null-space",
    "title": "Basics (3) - Special Matrices",
    "section": "8 영공간(Null space)",
    "text": "8 영공간(Null space)\n\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) 을 만족하는 \\(\\mathbf{x}\\)의 집합\n\ncolumn space 관점에서 보기: \\(A\\mathbf{x} = x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{0}\\)\nnull space에 항상 들어가는 \\(\\mathbf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\) : trivial solution\n\n모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 \\(\\mathbf{x}=\\mathbf{0}\\)하나 밖에 없음\n\n\n\\(\\mathbf{x}=\\mathbf{0}\\) 가 아닌 vector가 null space에 있으면, 스칼라배(constant \\(c\\)에 대해 \\(c \\mathbf{x}\\)) 역시 null space에 포함됨\n혼동 주의! null space는 column space의 일부가 아님\n\nrow vector의 차원이 null space가 존재하는 공간\n\nrank와 null space의 dimension의 합은 항상 matrix의 column의 수\n\n\\(A_{m \\times n}\\)에 대해, \\(dim(N(A)) = n - r\\)\n\n모든 columns이 다 lienarly independent 하면 null space는 0차원(점)\n\nnull space는 row space와 수직한 space\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0\nrank는 row space의 dimension → row space의 dimension(\\(dim(R(A))\\))과 null space의 dimension(\\(dim(N(A))\\))의 합이 \\(n\\)\n\\(\\mathbb{R^n}\\) 공간에 표현: \n\n겹친 점: 영벡터\n\n\n\nleft null space: \\(\\mathbf{x^T} A = \\mathbf{0^T}\\) 인 \\(\\mathbf{x}\\)\n\n위의 성질을 row에 대해 적용\n\nm 차원에 놓인 벡터\ndimension: \\(dim(N_L(A)) = m - r\\)\ncolumn space와 수직: \\(dim(N_L(A)) +dim(C(A)) = m\\)\n\n\n\\(R(A)\\)에 있는 vector \\(\\mathbf{x_r}\\) 와 \\(N(A)\\)에 있는 vector \\(\\mathbf{x_n}\\)에 대해:\n\n\\(\\mathbf{x_r}\\)에 \\(A\\)를 곱하면 column space로 감\n\\(\\mathbf{x_n}\\)에 \\(A\\)를 곱하면 $\n\\(A(\\mathbf{x_r} +\\mathbf{x_n}) = A\\mathbf{x_r} + A\\mathbf{x_n} = A\\mathbf{x_r} = \\mathbf{b}\\)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#ax-b의-해의-수",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/14.linear_programming.html#ax-b의-해의-수",
    "title": "Basics (3) - Special Matrices",
    "section": "9 Ax = b의 해의 수",
    "text": "9 Ax = b의 해의 수\n\n\nfull column rank 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 해가 하나\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음 \n\nfull row rank 일때\n\n\\(\\mathbf{b}\\)는 항상 column space 안에 있음: 무한의 해를 가짐\n임의의 특정한 해(particular solution) \\(\\mathbf{x_p}\\)와 null space의 vector \\(\\mathbf{x_n}\\)에 대해, \\(A(\\mathbf{x_p} +\\mathbf{x_n})=\\mathbf{b}\\)\n\n즉, \\(\\mathbf{x_p} +\\mathbf{x_n}\\) 도 해가 됨: complete solution\n\nnull space는 무한하므로, 해도 무한함\n\n\n\nfull rank 일때(square matrix): 해가 하나 존재 (\\(\\mathbf{x} = A^{-1}\\)$)\nrank-deficient 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 무한한 해를 가짐\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html",
    "title": "Basics (3) - Special Matrices",
    "section": "",
    "text": "A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, it can be represented as: \\[\n\\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\] where \\(a_{ij}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column of the matrix \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#basic-matrix-operations",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#basic-matrix-operations",
    "title": "Basics (3) - Special Matrices",
    "section": "2 Basic Matrix Operations",
    "text": "2 Basic Matrix Operations\n\n2.1 Matrix addition\nThe sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.\nGiven two \\(m \\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is defined by:\n\\[\nc_{i,j}=a_{i,j}+b_{i,j}​\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\n\\[\n\\begin{bmatrix}\n  1 & 2 \\\\\n  3 & 4 \\\\\n  5 & 6\n\\end{bmatrix} +\n\\begin{bmatrix}\n  -1 & 0 \\\\\n  2 & -3 \\\\\n  -5 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n  0 & 2 \\\\\n  5 & 1 \\\\\n  0 & 10\n\\end{bmatrix}\n\\]\n\n\n2.2 Scalar multiplication\nThe product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.\nGiven a scalar \\(k\\) and an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), their product \\(k\\mathbf{A}\\) is defined by: \\[\n(k\\mathbf{A})_{i,j} = k(a_{i,j})\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\nExample: \\[\n2\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 & 4 \\\\\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\]\n\n\n2.3 Matrix multiplication\nThe product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is a matrix obtained by multiplying the rows of \\(\\mathbf{A}\\) by the columns of \\(\\mathbf{B}\\).\nGiven two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with dimensions \\(m \\times n\\) and \\(n \\times p\\), respectively, their product \\(\\mathbf{C} = \\mathbf{AB}\\) is an \\(m \\times p\\) matrix defined by: \\[\nc_{i,j} = \\sum_{k=1}^n a_{i,k}b_{k,j}\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq p\\).\nExample: \\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} \\begin{bmatrix}\n-1 & 0 & 2 \\\\\n2 & -3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 & -6 & 4 \\\\\n5 & -12 & 10 \\\\\n7 & -18 & 16\n\\end{bmatrix}\n\\]\n\n\n2.4 Transpose\nThe transpose of an \\(m x n\\) matrix A, denoted by \\(A^T\\), is the \\(n x m\\) matrix obtained by interchanging the rows and columns of A. Formally, if \\(\\mathbf{A} = [a_{ij}]\\) is an m x n matrix, then its transpose \\(\\mathbf{A}^T = [b_{ij}]\\) is an \\(n x m\\) matrix where \\(b_{ij}\\) = \\(a_{ji}\\) for all \\(i\\) and \\(j\\). In other words, the element in the \\(i\\) th row and \\(j\\) th column of \\(A^T\\) is equal to the element in the \\(j\\) th row and ith column of \\(\\mathbf{A}\\).\nGiven an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix defined by: $$ {i,j}^T = {j,i}\n$$\n\nWhen \\(\\mathbf{A}\\) is transposed, diagnoal entries(\\(a_{ii}\\)) do not change but off-diagnoal elements(\\(a_{ij} \\; i \\neq j\\)) change.\nA column vector is tranposed into a row vector, and vice versa.\nsymmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)\n\nExample:\nLet A be the matrix \\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\] The transpose of A, denoted by A^T, is the matrix \\[\n\\mathbf{A}^T = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}\n\\]\n\n\n2.5 Determinant\nLet \\(\\mathbf{A}\\) be an \\(n \\times n\\) square matrix. The determinant of \\(\\mathbf{A}\\), denoted by \\(|\\mathbf{A}|\\) or \\(\\det(\\mathbf{A})\\), is a scalar value calculated as the sum of the products of the elements in any row or column of \\(\\mathbf{A}\\) with their corresponding cofactors, that is,\n\\[\n|\\mathbf{A}|=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n\\]\nwhere \\(a_{ij}\\) is the element of \\(\\mathbf{A}\\) in the \\(i\\)-th row and \\(j\\)-th column, and \\(C_{ij}\\) is the cofactor of \\(a_{ij}\\). The cofactor of \\(a_{ij}\\), denoted by \\(C_{ij}\\), is given by \\((-1)^{i+j}\\) times the determinant of the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)-th row and \\(j\\)-th column of \\(\\mathbf{A}\\).\nThe determinant of an \\(n x n\\) matrix \\(\\mathbf{A}\\) is a scalar value denoted as \\(|\\mathbf{A}|\\). It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a \\(3 x 3\\) matrix \\(\\mathbf{A} =\\) \\[\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\]\nis given by:\n\\[\n|\\mathbf{A}| = a_{11}\n\\begin{vmatrix}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{vmatrix}\n- a_{12}\n\\begin{vmatrix}\na_{21} & a_{23} \\\\\na_{31} & a_{33}\n\\end{vmatrix}\n+ a_{13}\n\\begin{vmatrix}\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{vmatrix}\n\\]\nFor example, consider the \\(3 \\times 3\\) matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\). We can calculate the determinant of \\(\\mathbf{A}\\) using any row or column. Let’s use the first column:\n\\[\n|\\mathbf{A}| = 1\n\\begin{vmatrix}\n5 & 6 \\\\\n8 & 9\n\\end{vmatrix}\n- 4\n\\begin{vmatrix}\n2 & 3\\\\\n8 & 9\n\\end{vmatrix}\n+ 7\n\\begin{vmatrix}\n2 & 5 \\\\\n3 & 6\n\\end{vmatrix} = 0\n\\]\nTherefore, the determinant of \\(\\mathbf{A}\\) is zero.\n\n\n2.6 Inverse\nThe inverse of a square matrix \\(A\\) of size \\(n\\) is a matrix \\(A^{-1}\\) such that the product of \\(A\\) and \\(A^{-1}\\) is the identity matrix \\(I_n\\), i.e. \\(A \\times A^{-1} = I_n\\). If such a matrix exists, then \\(A\\) is said to be invertible or non-singular.\nThe inverse of a square matrix \\(\\mathbf{A}\\) is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the unique matrix that satisfies the following equation: \\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\nFor example, consider the \\(2\\times 2\\) matrix \\[\n\\mathbf{A} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}.\n\\] The inverse of \\(\\mathbf{A}\\) is given by:\n\\[\n\\mathbf{A}^{-1} =\n\\frac{1}{-2}\n\\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\]\nWe can verify that \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\) by computing:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} =\n\\begin{bmatrix} -2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\nLet me give another example and A be a \\(3x3\\) square matrix:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, the inverse of \\(\\mathbf{A}\\), denoted as \\(\\mathbf{A}^{-1}\\), is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\na_{22}a_{33}-a_{23}a_{32} & a_{13}a_{32}-a_{12}a_{33} & a_{12}a_{23}-a_{13}a_{22} \\\\\na_{23}a_{31}-a_{21}a_{33} & a_{11}a_{33}-a_{13}a_{31} & a_{13}a_{21}-a_{11}a_{23} \\\\\na_{21}a_{32}-a_{22}a_{31} & a_{12}a_{31}-a_{11}a_{32} & a_{11}a_{22}-a_{12}a_{21} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nwhere \\(det(\\mathbf{A})\\) is the determinant of \\(\\mathbf{A}\\).\nFor example, let:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, \\(det(A)\\) = -57, and the inverse of \\(\\mathbf{A}\\) is:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{-57}\\begin{bmatrix}\n-24 & 18 & 5 \\\\\n20 & -15 & -4 \\\\\n-3 & 2 & 1 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThere are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.\n\n\n2.7 Rank\nThe rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by \\(\\text{rank}(\\mathbf{A})\\).\nFor example, consider the following matrix:\n\\[\n\\begin{equation*}\n  \\mathbf{A} = \\begin{bmatrix}\n    1 & 2 & 3\\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\\\\\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe columns of \\(\\mathbf{A}\\) are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of \\(\\mathbf{A}\\) is 2.\n\n\n2.8 Trace\nThe trace of a square matrix \\(\\mathbf{A}\\), denoted by \\(\\mathrm{tr}(\\mathbf{A})\\), is defined as the sum of the diagonal elements of \\(\\mathbf{A}\\). In other words, if \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix, then its trace is given by:\n\\[\n\\mathrm{tr}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}\n\\]\nwhere \\(a_{ii}\\) denotes the \\(i\\) th diagonal element of \\(\\mathbf{A}\\).\nFor example, let \\[\n\\begin{bmatrix}\n  2 & 3 & 1 \\\\\n  0 & 5 & 2 \\\\\n  1 & 1 & 4\n\\end{bmatrix}\n\\]\nThen, the trace of \\(\\mathbf{A}\\) is \\(\\mathrm{tr}(\\mathbf{A}) = 2 + 5 + 4 = 11\\)\n\n\n2.9 Eigenvalues and Eigenvectors\nLet A be an \\(n × n\\) square matrix. A scalar \\(\\lambda\\) is called an eigenvalue of \\(\\mathbf A\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that \\[\n\\mathbf{Av}=\\lambda\\mathbf{v}\n\\]\nSuch a vector \\(\\mathbf{v}\\) is called an eigenvector corresponding to \\(\\lambda\\).\nExample:\nLet \\(\\mathbf A\\) be the matrix\nTo find the eigenvalues of \\(\\mathbf A\\), we solve the characteristic equation \\(\\text{det}(\\mathbf A - \\lambda \\mathbf I ) = 0\\), where I is the n × n identity matrix.\n\\[\n\\begin{align*}\n  \\text{det}(\\mathbf A - \\lambda \\mathbf I )\n  &=\n    \\begin{vmatrix}\n    3 - \\lambda & 1 \\\\\n    1 & 3 - \\lambda\n    \\end{vmatrix} \\\\\n  &=\n  (3 - \\lambda)(3 - \\lambda) - 1 \\\\\n  &= \\lambda^2 - 6\\lambda + 8 = 0\n\\end{align*}\n\\]\nSolving this quadratic equation gives us the eigenvalues of \\(\\mathbf A\\): \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 4\\).\nTo find the eigenvectors corresponding to \\(\\lambda_1 = 2\\), we solve the equation \\((\\mathbf A - 2 \\mathbf I)\\mathbf{v} = \\mathbf{0}\\), where \\(\\mathbf I\\) is the \\(2 \\times 2\\) identity matrix.\n\\[\n\\begin{align*}\n  (\\mathbf A - 2 \\mathbf I)\\mathbf{v} =\n    \\begin{bmatrix}\n      1 & 1 \\\\\n      1 & 1\n    \\end{bmatrix}\n  \\begin{bmatrix}\n    x \\\\\n    y\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    0 \\\\\n    0\n  \\end{bmatrix}\n\\end{align*}\n\\]\nSolving this system of equations gives us the eigenvectors corresponding to \\(\\lambda_1 = 2\\): \\(\\mathbf{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\)\nSimilarly, for \\(\\lambda_2 = 4\\), we solve the equation \\((\\mathbf A - 4\\mathbf I)\\mathbf{v}\\) = \\(\\mathbf{0}\\) to get the eigenvectors corresponding to \\(\\lambda_2 = 4\\): \\(\\mathbf{v_2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)\n\n\n2.10 Singular value and Singluar Vectors\nThe singular value decomposition (SVD) of a matrix \\(\\mathbf A\\) is a factorization of \\(\\mathbf A\\) into the product of three matrices as follows:\n\\[\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n\\] where \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) rectangular diagonal matrix with non-negative real numbers on the diagonal, and \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix.\nThe diagonal entries of \\(\\mathbf{\\Sigma}\\) are called the singular values of \\(\\mathbf{A}\\), denoted as \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_r\\) (where \\(r\\) is the rank of \\(\\mathbf{A}\\)), and are arranged in descending order. The columns of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are called the left and right singular vectors of \\(\\mathbf{A}\\), respectively, and are orthonormal vectors.\nFor example, let \\(\\mathbf{A}\\) be a 3 by 2 matrix given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} =\n  \\begin{bmatrix}\n    1 & 2\\\\\n    3 & 4\\\\\n    5 & 6\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe SVD of \\(\\mathbf{A}\\) is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T =\n\\begin{bmatrix}\n  -0.23 & -0.53 & -0.81\\\\\n  -0.53 & -0.72 & 0.45\\\\\n  -0.81 & 0.45 & -0.38\n\\end{bmatrix}\n\\begin{bmatrix}\n  9.53 & 0\\\\\n  0 & 0.90\\\\\n  0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  -0.62 & -0.78\\\\\n  -0.78 & 0.62\n\\end{bmatrix}^T\n\\end{equation*}\n\\]\nwhere the left singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{U}\\), the right singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{V}\\), and the singular values of \\(\\mathbf{A}\\) are the diagonal entries of \\(\\boldsymbol{\\Sigma}\\).\n\n연립 방정식을 행렬의 곱으로 나타내보기 \\[\\begin{matrix}x_1+2y_1=4\\\\2x_1+5y_1=9\\end{matrix} \\quad \\quad \\quad \\begin{matrix}x_2+2y_2=3\\\\2x_2+5y_2=7\\end{matrix}\\] \\[ \\begin{bmatrix} 1 & 2 \\\\ 2 & 5 \\end{bmatrix} \\begin{bmatrix} x_1 & x_2 \\\\ y_1 & y_2 \\end{bmatrix} = \\begin{bmatrix} 4 & 9 \\\\ 3 & 7 \\end{bmatrix}\\]\n중요한 사실(….당연한 사실?)\n\n곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함\n\n\\(A_{m \\times n} \\times B_{o \\times p}\\) 에서 \\(n = o\\) 여야 곱셈 성립\n\n곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 \\(\\times\\) 곱셈의 오른쪽 행렬의 열 수\n\n\\(A_{m \\times n} \\times B_{o \\times p} = C_{m \\times p}\\)\n\n교환법칙(Commutative property)이 성립하지 않음\n\n\\(AB \\neq BA\\)\n\n\n행렬 곱셈의 여러가지 관점\n\n내적으로 바라보기 \\[ A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_1^T b_1} & \\mathbf{a_1^T b_2} & \\cdots & \\mathbf{a_1^T b_m} \\\\ \\mathbf{a_2^T b_1} & \\mathbf{a_2^T b_2} & \\cdots & \\mathbf{a_2^T b_m} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ \\mathbf{a_m^T b_1} & \\mathbf{a_m^T b_2} & \\cdots & \\mathbf{a_m^T b_m} \\end{bmatrix}\\]\nrank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) \\[AB = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1^T} \\\\ \\mathbf{b_2^T} \\\\ \\vdots \\\\ \\mathbf{b_m^T} \\end{bmatrix} = \\mathbf{a_1 b_1^T} + \\mathbf{a_2 b_2^T} + \\cdots + \\mathbf{a_m b_m^T}\\]\ncolumn space로 바라보기 \\[A\\mathbf{x} = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} = \\mathbf{a_1} x_1 + \\mathbf{a_2} x_2 + \\cdots + \\mathbf{a_m} x_m \\] (스칼라배의 합)\n\n\\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) 는 2차원 좌표평면의 모든 점을, \\(A=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)은 3차원 좌표평면의 모든 점 표현 가능\n\\(AB = A \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} A \\mathbf{b_1} & A \\mathbf{b_2} & \\cdots & A \\mathbf{b_m} \\end{bmatrix}\\)\ncolumn space: A의 column vector로 만들 수 있는 부분 공간\n\nrow space로 바라보기 \\[\\mathbf{x^T}A = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_m \\end{bmatrix} \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} = x_1 \\mathbf{a_1^T} + x_2 \\mathbf{a_2^T} + \\cdots + x_m \\mathbf{a_m^T} \\]"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#열공간column-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#열공간column-space",
    "title": "Basics (3) - Special Matrices",
    "section": "3 열공간(Column Space)",
    "text": "3 열공간(Column Space)\n\n\ncolumn space: column vector 들이 span 하는 space\n\n\\(A\\)의 column space = \\(C(A)\\) 또는 \\(range(A)\\)\n\nspan: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합\n\nvector에 따라, 점일수도 선일수도 평면일 수도 있음\nvector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space\n\nvector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것\n\n\\(\\mathbf{v_1}\\) 과 \\(\\mathbf{v_2}\\)의 linear combination으로 2차원 좌표평면 나타내기"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#선형-독립linear-independent",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#선형-독립linear-independent",
    "title": "Basics (3) - Special Matrices",
    "section": "4 선형 독립(Linear Independent)",
    "text": "4 선형 독립(Linear Independent)\n\n…and also see\n\n\n선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌\northogonal 하면 independent\n\nbut independent해도 항상 orthogonal하지는 않음 (Independent > Orthogonal)\n\ndefinition: \\(a_1 \\mathbf{v_1} + a_2 \\mathbf{v_2} + a_3 \\mathbf{v_3} \\cdots a_n \\mathbf{v_n} = \\mathbf{0}\\) 를 만족하는 \\(a_1, a_2, a_3, \\cdots a_n\\) 이 \\(a_1 = a_2 = a_3 = \\cdots = a_n = 0\\) 밖에 없을때\n\n\\(\\mathbf{0}\\)는 모든 elements가 \\(0\\)인 벡터\n예: \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\\) 는 \\(-2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) 이 되므로, linearly independent 하지 않음\nindependent한 vector 들의 수 = 표현할 수 있는 차원의 dimension"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#기저basis",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#기저basis",
    "title": "Basics (3) - Special Matrices",
    "section": "5 기저(basis)",
    "text": "5 기저(basis)\n\n주어진 vector space를 span하는 linearly independent한 vectors\n어떤 공간을 이루는 필수적인 구성요소\northogonal 하면 orthogonal basis\n예: 2차원 좌표평면에 대해\n\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) : orthogonal basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) : orthogonal 하지 않은 basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}\\) : linearly independent 하지 않으므로 basis 아님"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "title": "Basics (3) - Special Matrices",
    "section": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)",
    "text": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)\n\n\n6.1 Identity matrix(항등행렬)\n\n항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)\n\n실수에서 곱셈의 항등원은 1\n\n행렬의 항등원: 항등행렬(\\(I\\)) \\[I = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\\]\n\n\\(A_{m \\times n} \\times I_{n \\times n = n} = A_{m \\times n}\\)\n\\(I_{m \\times m = m} \\times A_{m \\times n} = A_{m \\times n}\\)\n\n\n\n\n6.2 Inverse matrix(역행렬)\n\n역원: 연산 결과 항등원이 나오게 하는 연소\n\n실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): \\(a \\times a^{-1} = 1\\)\n\n행렬의 역원: 역행렬(\\(A^{-1}\\)) \\[A \\times A^{-1} = I , A^{-1} \\times A = I\\]\n\n존재하지 않는 경우도 있음\n존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림\n\n존재하지 않으면 singular, degenerate라고 불림\n\nsquare matrix(정사각행렬, \\(m = n\\))은 특수한 경우를 제외하면 역행렬이 항상 존재\n\n역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우\n\n\\(m \\neq n\\)인 행렬의 경우에는 역행렬이 존재하지 않음\n\n다만, 경우에 따라 \\(A \\times A^{-1} = I\\) 를 만족하거나(right inverse), \\(A^{-1} \\times A = I\\)를 만족하는(left inverse)는 \\(A^{-1}\\)이 존재함\n\n연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 \\[A\\mathbf{x} = \\mathbf{b} \\Rightarrow A^{-1}A\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow I\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow \\mathbf{x} = A^{-1}\\mathbf{b}\\]\n\n\n\n\n6.3 Diagonal Matrix(대각행렬)\n\ndiagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix \\[ D = Diag(\\mathbf{a}) = \\begin{bmatrix} a_{1,1} & 0 & \\cdots & 0 \\\\ 0 & a_{2,2} & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & a_{n,n} \\end{bmatrix}\\]\n\nidentity matrix는 diagonal matrix\ndiagnomal matrix는 symmetric matrix 이기도 함\n보통은 square matrix에서 주로 사용됨\n\nsquare matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix\n\n\n\n\n\n6.4 Orthogonal matrix(직교행렬, orthonomal matrix)\n\n행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) \\[A A^T = A^T A = I\\]\nidentity matrix는 orthogonal matrix\nsquare matrix에서만 정의됨\nOrthogonal matrix인 \\(A\\)이면 \\(A^{-1} = A^{T}\\)\n\n각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임\n\ncomplex matrix(복소수 행렬)에서는 unitary matrix라고 부름"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#계수rank",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#계수rank",
    "title": "Basics (3) - Special Matrices",
    "section": "7 계수(Rank)",
    "text": "7 계수(Rank)\n\n\nrank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension\nindependent한 column의 수 = independent한 행의 수: \\(rank(A) = rank(A^T)\\)\n\nproof: Wikipedia\n\n예: \\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix} \\Rightarrow rank=1\\] \\[\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\Rightarrow rank=2\\]\n\\(A_{m \\times n}\\) 의 최대 랭크는 \\(min\\{m,n\\}\\)\n\n\\(rank(A) < min\\{m,n\\}\\) 면 rank-deficient, \\(rank(A) = min\\{m,n\\}\\)면 full (row/column) rank"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#영공간null-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#영공간null-space",
    "title": "Basics (3) - Special Matrices",
    "section": "8 영공간(Null space)",
    "text": "8 영공간(Null space)\n\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) 을 만족하는 \\(\\mathbf{x}\\)의 집합\n\ncolumn space 관점에서 보기: \\(A\\mathbf{x} = x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{0}\\)\nnull space에 항상 들어가는 \\(\\mathbf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\) : trivial solution\n\n모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 \\(\\mathbf{x}=\\mathbf{0}\\)하나 밖에 없음\n\n\n\\(\\mathbf{x}=\\mathbf{0}\\) 가 아닌 vector가 null space에 있으면, 스칼라배(constant \\(c\\)에 대해 \\(c \\mathbf{x}\\)) 역시 null space에 포함됨\n혼동 주의! null space는 column space의 일부가 아님\n\nrow vector의 차원이 null space가 존재하는 공간\n\nrank와 null space의 dimension의 합은 항상 matrix의 column의 수\n\n\\(A_{m \\times n}\\)에 대해, \\(dim(N(A)) = n - r\\)\n\n모든 columns이 다 lienarly independent 하면 null space는 0차원(점)\n\nnull space는 row space와 수직한 space\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0\nrank는 row space의 dimension → row space의 dimension(\\(dim(R(A))\\))과 null space의 dimension(\\(dim(N(A))\\))의 합이 \\(n\\)\n\\(\\mathbb{R^n}\\) 공간에 표현: \n\n겹친 점: 영벡터\n\n\n\nleft null space: \\(\\mathbf{x^T} A = \\mathbf{0^T}\\) 인 \\(\\mathbf{x}\\)\n\n위의 성질을 row에 대해 적용\n\nm 차원에 놓인 벡터\ndimension: \\(dim(N_L(A)) = m - r\\)\ncolumn space와 수직: \\(dim(N_L(A)) +dim(C(A)) = m\\)\n\n\n\\(R(A)\\)에 있는 vector \\(\\mathbf{x_r}\\) 와 \\(N(A)\\)에 있는 vector \\(\\mathbf{x_n}\\)에 대해:\n\n\\(\\mathbf{x_r}\\)에 \\(A\\)를 곱하면 column space로 감\n\\(\\mathbf{x_n}\\)에 \\(A\\)를 곱하면 $\n\\(A(\\mathbf{x_r} +\\mathbf{x_n}) = A\\mathbf{x_r} + A\\mathbf{x_n} = A\\mathbf{x_r} = \\mathbf{b}\\)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#ax-b의-해의-수",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/15.fourier_series.html#ax-b의-해의-수",
    "title": "Basics (3) - Special Matrices",
    "section": "9 Ax = b의 해의 수",
    "text": "9 Ax = b의 해의 수\n\n\nfull column rank 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 해가 하나\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음 \n\nfull row rank 일때\n\n\\(\\mathbf{b}\\)는 항상 column space 안에 있음: 무한의 해를 가짐\n임의의 특정한 해(particular solution) \\(\\mathbf{x_p}\\)와 null space의 vector \\(\\mathbf{x_n}\\)에 대해, \\(A(\\mathbf{x_p} +\\mathbf{x_n})=\\mathbf{b}\\)\n\n즉, \\(\\mathbf{x_p} +\\mathbf{x_n}\\) 도 해가 됨: complete solution\n\nnull space는 무한하므로, 해도 무한함\n\n\n\nfull rank 일때(square matrix): 해가 하나 존재 (\\(\\mathbf{x} = A^{-1}\\)$)\nrank-deficient 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 무한한 해를 가짐\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html",
    "title": "Basics (3) - Special Matrices",
    "section": "",
    "text": "A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, it can be represented as: \\[\n\\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\] where \\(a_{ij}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column of the matrix \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#basic-matrix-operations",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#basic-matrix-operations",
    "title": "Basics (3) - Special Matrices",
    "section": "2 Basic Matrix Operations",
    "text": "2 Basic Matrix Operations\n\n2.1 Matrix addition\nThe sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.\nGiven two \\(m \\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is defined by:\n\\[\nc_{i,j}=a_{i,j}+b_{i,j}​\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\n\\[\n\\begin{bmatrix}\n  1 & 2 \\\\\n  3 & 4 \\\\\n  5 & 6\n\\end{bmatrix} +\n\\begin{bmatrix}\n  -1 & 0 \\\\\n  2 & -3 \\\\\n  -5 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n  0 & 2 \\\\\n  5 & 1 \\\\\n  0 & 10\n\\end{bmatrix}\n\\]\n\n\n2.2 Scalar multiplication\nThe product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.\nGiven a scalar \\(k\\) and an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), their product \\(k\\mathbf{A}\\) is defined by: \\[\n(k\\mathbf{A})_{i,j} = k(a_{i,j})\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\nExample: \\[\n2\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 & 4 \\\\\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\]\n\n\n2.3 Matrix multiplication\nThe product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is a matrix obtained by multiplying the rows of \\(\\mathbf{A}\\) by the columns of \\(\\mathbf{B}\\).\nGiven two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with dimensions \\(m \\times n\\) and \\(n \\times p\\), respectively, their product \\(\\mathbf{C} = \\mathbf{AB}\\) is an \\(m \\times p\\) matrix defined by: \\[\nc_{i,j} = \\sum_{k=1}^n a_{i,k}b_{k,j}\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq p\\).\nExample: \\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} \\begin{bmatrix}\n-1 & 0 & 2 \\\\\n2 & -3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 & -6 & 4 \\\\\n5 & -12 & 10 \\\\\n7 & -18 & 16\n\\end{bmatrix}\n\\]\n\n\n2.4 Transpose\nThe transpose of an \\(m x n\\) matrix A, denoted by \\(A^T\\), is the \\(n x m\\) matrix obtained by interchanging the rows and columns of A. Formally, if \\(\\mathbf{A} = [a_{ij}]\\) is an m x n matrix, then its transpose \\(\\mathbf{A}^T = [b_{ij}]\\) is an \\(n x m\\) matrix where \\(b_{ij}\\) = \\(a_{ji}\\) for all \\(i\\) and \\(j\\). In other words, the element in the \\(i\\) th row and \\(j\\) th column of \\(A^T\\) is equal to the element in the \\(j\\) th row and ith column of \\(\\mathbf{A}\\).\nGiven an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix defined by: $$ {i,j}^T = {j,i}\n$$\n\nWhen \\(\\mathbf{A}\\) is transposed, diagnoal entries(\\(a_{ii}\\)) do not change but off-diagnoal elements(\\(a_{ij} \\; i \\neq j\\)) change.\nA column vector is tranposed into a row vector, and vice versa.\nsymmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)\n\nExample:\nLet A be the matrix \\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\] The transpose of A, denoted by A^T, is the matrix \\[\n\\mathbf{A}^T = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}\n\\]\n\n\n2.5 Determinant\nLet \\(\\mathbf{A}\\) be an \\(n \\times n\\) square matrix. The determinant of \\(\\mathbf{A}\\), denoted by \\(|\\mathbf{A}|\\) or \\(\\det(\\mathbf{A})\\), is a scalar value calculated as the sum of the products of the elements in any row or column of \\(\\mathbf{A}\\) with their corresponding cofactors, that is,\n\\[\n|\\mathbf{A}|=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n\\]\nwhere \\(a_{ij}\\) is the element of \\(\\mathbf{A}\\) in the \\(i\\)-th row and \\(j\\)-th column, and \\(C_{ij}\\) is the cofactor of \\(a_{ij}\\). The cofactor of \\(a_{ij}\\), denoted by \\(C_{ij}\\), is given by \\((-1)^{i+j}\\) times the determinant of the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)-th row and \\(j\\)-th column of \\(\\mathbf{A}\\).\nThe determinant of an \\(n x n\\) matrix \\(\\mathbf{A}\\) is a scalar value denoted as \\(|\\mathbf{A}|\\). It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a \\(3 x 3\\) matrix \\(\\mathbf{A} =\\) \\[\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\]\nis given by:\n\\[\n|\\mathbf{A}| = a_{11}\n\\begin{vmatrix}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{vmatrix}\n- a_{12}\n\\begin{vmatrix}\na_{21} & a_{23} \\\\\na_{31} & a_{33}\n\\end{vmatrix}\n+ a_{13}\n\\begin{vmatrix}\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{vmatrix}\n\\]\nFor example, consider the \\(3 \\times 3\\) matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\). We can calculate the determinant of \\(\\mathbf{A}\\) using any row or column. Let’s use the first column:\n\\[\n|\\mathbf{A}| = 1\n\\begin{vmatrix}\n5 & 6 \\\\\n8 & 9\n\\end{vmatrix}\n- 4\n\\begin{vmatrix}\n2 & 3\\\\\n8 & 9\n\\end{vmatrix}\n+ 7\n\\begin{vmatrix}\n2 & 5 \\\\\n3 & 6\n\\end{vmatrix} = 0\n\\]\nTherefore, the determinant of \\(\\mathbf{A}\\) is zero.\n\n\n2.6 Inverse\nThe inverse of a square matrix \\(A\\) of size \\(n\\) is a matrix \\(A^{-1}\\) such that the product of \\(A\\) and \\(A^{-1}\\) is the identity matrix \\(I_n\\), i.e. \\(A \\times A^{-1} = I_n\\). If such a matrix exists, then \\(A\\) is said to be invertible or non-singular.\nThe inverse of a square matrix \\(\\mathbf{A}\\) is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the unique matrix that satisfies the following equation: \\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\nFor example, consider the \\(2\\times 2\\) matrix \\[\n\\mathbf{A} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}.\n\\] The inverse of \\(\\mathbf{A}\\) is given by:\n\\[\n\\mathbf{A}^{-1} =\n\\frac{1}{-2}\n\\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\]\nWe can verify that \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\) by computing:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} =\n\\begin{bmatrix} -2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\nLet me give another example and A be a \\(3x3\\) square matrix:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, the inverse of \\(\\mathbf{A}\\), denoted as \\(\\mathbf{A}^{-1}\\), is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\na_{22}a_{33}-a_{23}a_{32} & a_{13}a_{32}-a_{12}a_{33} & a_{12}a_{23}-a_{13}a_{22} \\\\\na_{23}a_{31}-a_{21}a_{33} & a_{11}a_{33}-a_{13}a_{31} & a_{13}a_{21}-a_{11}a_{23} \\\\\na_{21}a_{32}-a_{22}a_{31} & a_{12}a_{31}-a_{11}a_{32} & a_{11}a_{22}-a_{12}a_{21} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nwhere \\(det(\\mathbf{A})\\) is the determinant of \\(\\mathbf{A}\\).\nFor example, let:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, \\(det(A)\\) = -57, and the inverse of \\(\\mathbf{A}\\) is:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{-57}\\begin{bmatrix}\n-24 & 18 & 5 \\\\\n20 & -15 & -4 \\\\\n-3 & 2 & 1 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThere are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.\n\n\n2.7 Rank\nThe rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by \\(\\text{rank}(\\mathbf{A})\\).\nFor example, consider the following matrix:\n\\[\n\\begin{equation*}\n  \\mathbf{A} = \\begin{bmatrix}\n    1 & 2 & 3\\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\\\\\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe columns of \\(\\mathbf{A}\\) are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of \\(\\mathbf{A}\\) is 2.\n\n\n2.8 Trace\nThe trace of a square matrix \\(\\mathbf{A}\\), denoted by \\(\\mathrm{tr}(\\mathbf{A})\\), is defined as the sum of the diagonal elements of \\(\\mathbf{A}\\). In other words, if \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix, then its trace is given by:\n\\[\n\\mathrm{tr}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}\n\\]\nwhere \\(a_{ii}\\) denotes the \\(i\\) th diagonal element of \\(\\mathbf{A}\\).\nFor example, let \\[\n\\begin{bmatrix}\n  2 & 3 & 1 \\\\\n  0 & 5 & 2 \\\\\n  1 & 1 & 4\n\\end{bmatrix}\n\\]\nThen, the trace of \\(\\mathbf{A}\\) is \\(\\mathrm{tr}(\\mathbf{A}) = 2 + 5 + 4 = 11\\)\n\n\n2.9 Eigenvalues and Eigenvectors\nLet A be an \\(n × n\\) square matrix. A scalar \\(\\lambda\\) is called an eigenvalue of \\(\\mathbf A\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that \\[\n\\mathbf{Av}=\\lambda\\mathbf{v}\n\\]\nSuch a vector \\(\\mathbf{v}\\) is called an eigenvector corresponding to \\(\\lambda\\).\nExample:\nLet \\(\\mathbf A\\) be the matrix\nTo find the eigenvalues of \\(\\mathbf A\\), we solve the characteristic equation \\(\\text{det}(\\mathbf A - \\lambda \\mathbf I ) = 0\\), where I is the n × n identity matrix.\n\\[\n\\begin{align*}\n  \\text{det}(\\mathbf A - \\lambda \\mathbf I )\n  &=\n    \\begin{vmatrix}\n    3 - \\lambda & 1 \\\\\n    1 & 3 - \\lambda\n    \\end{vmatrix} \\\\\n  &=\n  (3 - \\lambda)(3 - \\lambda) - 1 \\\\\n  &= \\lambda^2 - 6\\lambda + 8 = 0\n\\end{align*}\n\\]\nSolving this quadratic equation gives us the eigenvalues of \\(\\mathbf A\\): \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 4\\).\nTo find the eigenvectors corresponding to \\(\\lambda_1 = 2\\), we solve the equation \\((\\mathbf A - 2 \\mathbf I)\\mathbf{v} = \\mathbf{0}\\), where \\(\\mathbf I\\) is the \\(2 \\times 2\\) identity matrix.\n\\[\n\\begin{align*}\n  (\\mathbf A - 2 \\mathbf I)\\mathbf{v} =\n    \\begin{bmatrix}\n      1 & 1 \\\\\n      1 & 1\n    \\end{bmatrix}\n  \\begin{bmatrix}\n    x \\\\\n    y\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    0 \\\\\n    0\n  \\end{bmatrix}\n\\end{align*}\n\\]\nSolving this system of equations gives us the eigenvectors corresponding to \\(\\lambda_1 = 2\\): \\(\\mathbf{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\)\nSimilarly, for \\(\\lambda_2 = 4\\), we solve the equation \\((\\mathbf A - 4\\mathbf I)\\mathbf{v}\\) = \\(\\mathbf{0}\\) to get the eigenvectors corresponding to \\(\\lambda_2 = 4\\): \\(\\mathbf{v_2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)\n\n\n2.10 Singular value and Singluar Vectors\nThe singular value decomposition (SVD) of a matrix \\(\\mathbf A\\) is a factorization of \\(\\mathbf A\\) into the product of three matrices as follows:\n\\[\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n\\] where \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) rectangular diagonal matrix with non-negative real numbers on the diagonal, and \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix.\nThe diagonal entries of \\(\\mathbf{\\Sigma}\\) are called the singular values of \\(\\mathbf{A}\\), denoted as \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_r\\) (where \\(r\\) is the rank of \\(\\mathbf{A}\\)), and are arranged in descending order. The columns of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are called the left and right singular vectors of \\(\\mathbf{A}\\), respectively, and are orthonormal vectors.\nFor example, let \\(\\mathbf{A}\\) be a 3 by 2 matrix given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} =\n  \\begin{bmatrix}\n    1 & 2\\\\\n    3 & 4\\\\\n    5 & 6\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe SVD of \\(\\mathbf{A}\\) is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T =\n\\begin{bmatrix}\n  -0.23 & -0.53 & -0.81\\\\\n  -0.53 & -0.72 & 0.45\\\\\n  -0.81 & 0.45 & -0.38\n\\end{bmatrix}\n\\begin{bmatrix}\n  9.53 & 0\\\\\n  0 & 0.90\\\\\n  0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  -0.62 & -0.78\\\\\n  -0.78 & 0.62\n\\end{bmatrix}^T\n\\end{equation*}\n\\]\nwhere the left singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{U}\\), the right singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{V}\\), and the singular values of \\(\\mathbf{A}\\) are the diagonal entries of \\(\\boldsymbol{\\Sigma}\\).\n\n연립 방정식을 행렬의 곱으로 나타내보기 \\[\\begin{matrix}x_1+2y_1=4\\\\2x_1+5y_1=9\\end{matrix} \\quad \\quad \\quad \\begin{matrix}x_2+2y_2=3\\\\2x_2+5y_2=7\\end{matrix}\\] \\[ \\begin{bmatrix} 1 & 2 \\\\ 2 & 5 \\end{bmatrix} \\begin{bmatrix} x_1 & x_2 \\\\ y_1 & y_2 \\end{bmatrix} = \\begin{bmatrix} 4 & 9 \\\\ 3 & 7 \\end{bmatrix}\\]\n중요한 사실(….당연한 사실?)\n\n곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함\n\n\\(A_{m \\times n} \\times B_{o \\times p}\\) 에서 \\(n = o\\) 여야 곱셈 성립\n\n곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 \\(\\times\\) 곱셈의 오른쪽 행렬의 열 수\n\n\\(A_{m \\times n} \\times B_{o \\times p} = C_{m \\times p}\\)\n\n교환법칙(Commutative property)이 성립하지 않음\n\n\\(AB \\neq BA\\)\n\n\n행렬 곱셈의 여러가지 관점\n\n내적으로 바라보기 \\[ A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_1^T b_1} & \\mathbf{a_1^T b_2} & \\cdots & \\mathbf{a_1^T b_m} \\\\ \\mathbf{a_2^T b_1} & \\mathbf{a_2^T b_2} & \\cdots & \\mathbf{a_2^T b_m} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ \\mathbf{a_m^T b_1} & \\mathbf{a_m^T b_2} & \\cdots & \\mathbf{a_m^T b_m} \\end{bmatrix}\\]\nrank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) \\[AB = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1^T} \\\\ \\mathbf{b_2^T} \\\\ \\vdots \\\\ \\mathbf{b_m^T} \\end{bmatrix} = \\mathbf{a_1 b_1^T} + \\mathbf{a_2 b_2^T} + \\cdots + \\mathbf{a_m b_m^T}\\]\ncolumn space로 바라보기 \\[A\\mathbf{x} = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} = \\mathbf{a_1} x_1 + \\mathbf{a_2} x_2 + \\cdots + \\mathbf{a_m} x_m \\] (스칼라배의 합)\n\n\\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) 는 2차원 좌표평면의 모든 점을, \\(A=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)은 3차원 좌표평면의 모든 점 표현 가능\n\\(AB = A \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} A \\mathbf{b_1} & A \\mathbf{b_2} & \\cdots & A \\mathbf{b_m} \\end{bmatrix}\\)\ncolumn space: A의 column vector로 만들 수 있는 부분 공간\n\nrow space로 바라보기 \\[\\mathbf{x^T}A = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_m \\end{bmatrix} \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} = x_1 \\mathbf{a_1^T} + x_2 \\mathbf{a_2^T} + \\cdots + x_m \\mathbf{a_m^T} \\]"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#열공간column-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#열공간column-space",
    "title": "Basics (3) - Special Matrices",
    "section": "3 열공간(Column Space)",
    "text": "3 열공간(Column Space)\n\n\ncolumn space: column vector 들이 span 하는 space\n\n\\(A\\)의 column space = \\(C(A)\\) 또는 \\(range(A)\\)\n\nspan: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합\n\nvector에 따라, 점일수도 선일수도 평면일 수도 있음\nvector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space\n\nvector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것\n\n\\(\\mathbf{v_1}\\) 과 \\(\\mathbf{v_2}\\)의 linear combination으로 2차원 좌표평면 나타내기"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#선형-독립linear-independent",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#선형-독립linear-independent",
    "title": "Basics (3) - Special Matrices",
    "section": "4 선형 독립(Linear Independent)",
    "text": "4 선형 독립(Linear Independent)\n\n…and also see\n\n\n선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌\northogonal 하면 independent\n\nbut independent해도 항상 orthogonal하지는 않음 (Independent > Orthogonal)\n\ndefinition: \\(a_1 \\mathbf{v_1} + a_2 \\mathbf{v_2} + a_3 \\mathbf{v_3} \\cdots a_n \\mathbf{v_n} = \\mathbf{0}\\) 를 만족하는 \\(a_1, a_2, a_3, \\cdots a_n\\) 이 \\(a_1 = a_2 = a_3 = \\cdots = a_n = 0\\) 밖에 없을때\n\n\\(\\mathbf{0}\\)는 모든 elements가 \\(0\\)인 벡터\n예: \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\\) 는 \\(-2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) 이 되므로, linearly independent 하지 않음\nindependent한 vector 들의 수 = 표현할 수 있는 차원의 dimension"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#기저basis",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#기저basis",
    "title": "Basics (3) - Special Matrices",
    "section": "5 기저(basis)",
    "text": "5 기저(basis)\n\n주어진 vector space를 span하는 linearly independent한 vectors\n어떤 공간을 이루는 필수적인 구성요소\northogonal 하면 orthogonal basis\n예: 2차원 좌표평면에 대해\n\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) : orthogonal basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) : orthogonal 하지 않은 basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}\\) : linearly independent 하지 않으므로 basis 아님"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "title": "Basics (3) - Special Matrices",
    "section": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)",
    "text": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)\n\n\n6.1 Identity matrix(항등행렬)\n\n항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)\n\n실수에서 곱셈의 항등원은 1\n\n행렬의 항등원: 항등행렬(\\(I\\)) \\[I = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\\]\n\n\\(A_{m \\times n} \\times I_{n \\times n = n} = A_{m \\times n}\\)\n\\(I_{m \\times m = m} \\times A_{m \\times n} = A_{m \\times n}\\)\n\n\n\n\n6.2 Inverse matrix(역행렬)\n\n역원: 연산 결과 항등원이 나오게 하는 연소\n\n실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): \\(a \\times a^{-1} = 1\\)\n\n행렬의 역원: 역행렬(\\(A^{-1}\\)) \\[A \\times A^{-1} = I , A^{-1} \\times A = I\\]\n\n존재하지 않는 경우도 있음\n존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림\n\n존재하지 않으면 singular, degenerate라고 불림\n\nsquare matrix(정사각행렬, \\(m = n\\))은 특수한 경우를 제외하면 역행렬이 항상 존재\n\n역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우\n\n\\(m \\neq n\\)인 행렬의 경우에는 역행렬이 존재하지 않음\n\n다만, 경우에 따라 \\(A \\times A^{-1} = I\\) 를 만족하거나(right inverse), \\(A^{-1} \\times A = I\\)를 만족하는(left inverse)는 \\(A^{-1}\\)이 존재함\n\n연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 \\[A\\mathbf{x} = \\mathbf{b} \\Rightarrow A^{-1}A\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow I\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow \\mathbf{x} = A^{-1}\\mathbf{b}\\]\n\n\n\n\n6.3 Diagonal Matrix(대각행렬)\n\ndiagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix \\[ D = Diag(\\mathbf{a}) = \\begin{bmatrix} a_{1,1} & 0 & \\cdots & 0 \\\\ 0 & a_{2,2} & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & a_{n,n} \\end{bmatrix}\\]\n\nidentity matrix는 diagonal matrix\ndiagnomal matrix는 symmetric matrix 이기도 함\n보통은 square matrix에서 주로 사용됨\n\nsquare matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix\n\n\n\n\n\n6.4 Orthogonal matrix(직교행렬, orthonomal matrix)\n\n행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) \\[A A^T = A^T A = I\\]\nidentity matrix는 orthogonal matrix\nsquare matrix에서만 정의됨\nOrthogonal matrix인 \\(A\\)이면 \\(A^{-1} = A^{T}\\)\n\n각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임\n\ncomplex matrix(복소수 행렬)에서는 unitary matrix라고 부름"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#계수rank",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#계수rank",
    "title": "Basics (3) - Special Matrices",
    "section": "7 계수(Rank)",
    "text": "7 계수(Rank)\n\n\nrank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension\nindependent한 column의 수 = independent한 행의 수: \\(rank(A) = rank(A^T)\\)\n\nproof: Wikipedia\n\n예: \\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix} \\Rightarrow rank=1\\] \\[\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\Rightarrow rank=2\\]\n\\(A_{m \\times n}\\) 의 최대 랭크는 \\(min\\{m,n\\}\\)\n\n\\(rank(A) < min\\{m,n\\}\\) 면 rank-deficient, \\(rank(A) = min\\{m,n\\}\\)면 full (row/column) rank"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#영공간null-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#영공간null-space",
    "title": "Basics (3) - Special Matrices",
    "section": "8 영공간(Null space)",
    "text": "8 영공간(Null space)\n\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) 을 만족하는 \\(\\mathbf{x}\\)의 집합\n\ncolumn space 관점에서 보기: \\(A\\mathbf{x} = x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{0}\\)\nnull space에 항상 들어가는 \\(\\mathbf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\) : trivial solution\n\n모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 \\(\\mathbf{x}=\\mathbf{0}\\)하나 밖에 없음\n\n\n\\(\\mathbf{x}=\\mathbf{0}\\) 가 아닌 vector가 null space에 있으면, 스칼라배(constant \\(c\\)에 대해 \\(c \\mathbf{x}\\)) 역시 null space에 포함됨\n혼동 주의! null space는 column space의 일부가 아님\n\nrow vector의 차원이 null space가 존재하는 공간\n\nrank와 null space의 dimension의 합은 항상 matrix의 column의 수\n\n\\(A_{m \\times n}\\)에 대해, \\(dim(N(A)) = n - r\\)\n\n모든 columns이 다 lienarly independent 하면 null space는 0차원(점)\n\nnull space는 row space와 수직한 space\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0\nrank는 row space의 dimension → row space의 dimension(\\(dim(R(A))\\))과 null space의 dimension(\\(dim(N(A))\\))의 합이 \\(n\\)\n\\(\\mathbb{R^n}\\) 공간에 표현: \n\n겹친 점: 영벡터\n\n\n\nleft null space: \\(\\mathbf{x^T} A = \\mathbf{0^T}\\) 인 \\(\\mathbf{x}\\)\n\n위의 성질을 row에 대해 적용\n\nm 차원에 놓인 벡터\ndimension: \\(dim(N_L(A)) = m - r\\)\ncolumn space와 수직: \\(dim(N_L(A)) +dim(C(A)) = m\\)\n\n\n\\(R(A)\\)에 있는 vector \\(\\mathbf{x_r}\\) 와 \\(N(A)\\)에 있는 vector \\(\\mathbf{x_n}\\)에 대해:\n\n\\(\\mathbf{x_r}\\)에 \\(A\\)를 곱하면 column space로 감\n\\(\\mathbf{x_n}\\)에 \\(A\\)를 곱하면 $\n\\(A(\\mathbf{x_r} +\\mathbf{x_n}) = A\\mathbf{x_r} + A\\mathbf{x_n} = A\\mathbf{x_r} = \\mathbf{b}\\)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#ax-b의-해의-수",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/16.linear_algebra_statistics.html#ax-b의-해의-수",
    "title": "Basics (3) - Special Matrices",
    "section": "9 Ax = b의 해의 수",
    "text": "9 Ax = b의 해의 수\n\n\nfull column rank 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 해가 하나\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음 \n\nfull row rank 일때\n\n\\(\\mathbf{b}\\)는 항상 column space 안에 있음: 무한의 해를 가짐\n임의의 특정한 해(particular solution) \\(\\mathbf{x_p}\\)와 null space의 vector \\(\\mathbf{x_n}\\)에 대해, \\(A(\\mathbf{x_p} +\\mathbf{x_n})=\\mathbf{b}\\)\n\n즉, \\(\\mathbf{x_p} +\\mathbf{x_n}\\) 도 해가 됨: complete solution\n\nnull space는 무한하므로, 해도 무한함\n\n\n\nfull rank 일때(square matrix): 해가 하나 존재 (\\(\\mathbf{x} = A^{-1}\\)$)\nrank-deficient 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 무한한 해를 가짐\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html",
    "title": "Basics (3) - Special Matrices",
    "section": "",
    "text": "A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, it can be represented as: \\[\n\\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\] where \\(a_{ij}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column of the matrix \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#basic-matrix-operations",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#basic-matrix-operations",
    "title": "Basics (3) - Special Matrices",
    "section": "2 Basic Matrix Operations",
    "text": "2 Basic Matrix Operations\n\n2.1 Matrix addition\nThe sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.\nGiven two \\(m \\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is defined by:\n\\[\nc_{i,j}=a_{i,j}+b_{i,j}​\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\n\\[\n\\begin{bmatrix}\n  1 & 2 \\\\\n  3 & 4 \\\\\n  5 & 6\n\\end{bmatrix} +\n\\begin{bmatrix}\n  -1 & 0 \\\\\n  2 & -3 \\\\\n  -5 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n  0 & 2 \\\\\n  5 & 1 \\\\\n  0 & 10\n\\end{bmatrix}\n\\]\n\n\n2.2 Scalar multiplication\nThe product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.\nGiven a scalar \\(k\\) and an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), their product \\(k\\mathbf{A}\\) is defined by: \\[\n(k\\mathbf{A})_{i,j} = k(a_{i,j})\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\nExample: \\[\n2\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 & 4 \\\\\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\]\n\n\n2.3 Matrix multiplication\nThe product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is a matrix obtained by multiplying the rows of \\(\\mathbf{A}\\) by the columns of \\(\\mathbf{B}\\).\nGiven two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with dimensions \\(m \\times n\\) and \\(n \\times p\\), respectively, their product \\(\\mathbf{C} = \\mathbf{AB}\\) is an \\(m \\times p\\) matrix defined by: \\[\nc_{i,j} = \\sum_{k=1}^n a_{i,k}b_{k,j}\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq p\\).\nExample: \\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} \\begin{bmatrix}\n-1 & 0 & 2 \\\\\n2 & -3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 & -6 & 4 \\\\\n5 & -12 & 10 \\\\\n7 & -18 & 16\n\\end{bmatrix}\n\\]\n\n\n2.4 Transpose\nThe transpose of an \\(m x n\\) matrix A, denoted by \\(A^T\\), is the \\(n x m\\) matrix obtained by interchanging the rows and columns of A. Formally, if \\(\\mathbf{A} = [a_{ij}]\\) is an m x n matrix, then its transpose \\(\\mathbf{A}^T = [b_{ij}]\\) is an \\(n x m\\) matrix where \\(b_{ij}\\) = \\(a_{ji}\\) for all \\(i\\) and \\(j\\). In other words, the element in the \\(i\\) th row and \\(j\\) th column of \\(A^T\\) is equal to the element in the \\(j\\) th row and ith column of \\(\\mathbf{A}\\).\nGiven an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix defined by: $$ {i,j}^T = {j,i}\n$$\n\nWhen \\(\\mathbf{A}\\) is transposed, diagnoal entries(\\(a_{ii}\\)) do not change but off-diagnoal elements(\\(a_{ij} \\; i \\neq j\\)) change.\nA column vector is tranposed into a row vector, and vice versa.\nsymmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)\n\nExample:\nLet A be the matrix \\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\] The transpose of A, denoted by A^T, is the matrix \\[\n\\mathbf{A}^T = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}\n\\]\n\n\n2.5 Determinant\nLet \\(\\mathbf{A}\\) be an \\(n \\times n\\) square matrix. The determinant of \\(\\mathbf{A}\\), denoted by \\(|\\mathbf{A}|\\) or \\(\\det(\\mathbf{A})\\), is a scalar value calculated as the sum of the products of the elements in any row or column of \\(\\mathbf{A}\\) with their corresponding cofactors, that is,\n\\[\n|\\mathbf{A}|=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n\\]\nwhere \\(a_{ij}\\) is the element of \\(\\mathbf{A}\\) in the \\(i\\)-th row and \\(j\\)-th column, and \\(C_{ij}\\) is the cofactor of \\(a_{ij}\\). The cofactor of \\(a_{ij}\\), denoted by \\(C_{ij}\\), is given by \\((-1)^{i+j}\\) times the determinant of the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)-th row and \\(j\\)-th column of \\(\\mathbf{A}\\).\nThe determinant of an \\(n x n\\) matrix \\(\\mathbf{A}\\) is a scalar value denoted as \\(|\\mathbf{A}|\\). It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a \\(3 x 3\\) matrix \\(\\mathbf{A} =\\) \\[\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\]\nis given by:\n\\[\n|\\mathbf{A}| = a_{11}\n\\begin{vmatrix}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{vmatrix}\n- a_{12}\n\\begin{vmatrix}\na_{21} & a_{23} \\\\\na_{31} & a_{33}\n\\end{vmatrix}\n+ a_{13}\n\\begin{vmatrix}\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{vmatrix}\n\\]\nFor example, consider the \\(3 \\times 3\\) matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\). We can calculate the determinant of \\(\\mathbf{A}\\) using any row or column. Let’s use the first column:\n\\[\n|\\mathbf{A}| = 1\n\\begin{vmatrix}\n5 & 6 \\\\\n8 & 9\n\\end{vmatrix}\n- 4\n\\begin{vmatrix}\n2 & 3\\\\\n8 & 9\n\\end{vmatrix}\n+ 7\n\\begin{vmatrix}\n2 & 5 \\\\\n3 & 6\n\\end{vmatrix} = 0\n\\]\nTherefore, the determinant of \\(\\mathbf{A}\\) is zero.\n\n\n2.6 Inverse\nThe inverse of a square matrix \\(A\\) of size \\(n\\) is a matrix \\(A^{-1}\\) such that the product of \\(A\\) and \\(A^{-1}\\) is the identity matrix \\(I_n\\), i.e. \\(A \\times A^{-1} = I_n\\). If such a matrix exists, then \\(A\\) is said to be invertible or non-singular.\nThe inverse of a square matrix \\(\\mathbf{A}\\) is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the unique matrix that satisfies the following equation: \\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\nFor example, consider the \\(2\\times 2\\) matrix \\[\n\\mathbf{A} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}.\n\\] The inverse of \\(\\mathbf{A}\\) is given by:\n\\[\n\\mathbf{A}^{-1} =\n\\frac{1}{-2}\n\\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\]\nWe can verify that \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\) by computing:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} =\n\\begin{bmatrix} -2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\nLet me give another example and A be a \\(3x3\\) square matrix:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, the inverse of \\(\\mathbf{A}\\), denoted as \\(\\mathbf{A}^{-1}\\), is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\na_{22}a_{33}-a_{23}a_{32} & a_{13}a_{32}-a_{12}a_{33} & a_{12}a_{23}-a_{13}a_{22} \\\\\na_{23}a_{31}-a_{21}a_{33} & a_{11}a_{33}-a_{13}a_{31} & a_{13}a_{21}-a_{11}a_{23} \\\\\na_{21}a_{32}-a_{22}a_{31} & a_{12}a_{31}-a_{11}a_{32} & a_{11}a_{22}-a_{12}a_{21} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nwhere \\(det(\\mathbf{A})\\) is the determinant of \\(\\mathbf{A}\\).\nFor example, let:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, \\(det(A)\\) = -57, and the inverse of \\(\\mathbf{A}\\) is:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{-57}\\begin{bmatrix}\n-24 & 18 & 5 \\\\\n20 & -15 & -4 \\\\\n-3 & 2 & 1 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThere are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.\n\n\n2.7 Rank\nThe rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by \\(\\text{rank}(\\mathbf{A})\\).\nFor example, consider the following matrix:\n\\[\n\\begin{equation*}\n  \\mathbf{A} = \\begin{bmatrix}\n    1 & 2 & 3\\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\\\\\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe columns of \\(\\mathbf{A}\\) are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of \\(\\mathbf{A}\\) is 2.\n\n\n2.8 Trace\nThe trace of a square matrix \\(\\mathbf{A}\\), denoted by \\(\\mathrm{tr}(\\mathbf{A})\\), is defined as the sum of the diagonal elements of \\(\\mathbf{A}\\). In other words, if \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix, then its trace is given by:\n\\[\n\\mathrm{tr}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}\n\\]\nwhere \\(a_{ii}\\) denotes the \\(i\\) th diagonal element of \\(\\mathbf{A}\\).\nFor example, let \\[\n\\begin{bmatrix}\n  2 & 3 & 1 \\\\\n  0 & 5 & 2 \\\\\n  1 & 1 & 4\n\\end{bmatrix}\n\\]\nThen, the trace of \\(\\mathbf{A}\\) is \\(\\mathrm{tr}(\\mathbf{A}) = 2 + 5 + 4 = 11\\)\n\n\n2.9 Eigenvalues and Eigenvectors\nLet A be an \\(n × n\\) square matrix. A scalar \\(\\lambda\\) is called an eigenvalue of \\(\\mathbf A\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that \\[\n\\mathbf{Av}=\\lambda\\mathbf{v}\n\\]\nSuch a vector \\(\\mathbf{v}\\) is called an eigenvector corresponding to \\(\\lambda\\).\nExample:\nLet \\(\\mathbf A\\) be the matrix\nTo find the eigenvalues of \\(\\mathbf A\\), we solve the characteristic equation \\(\\text{det}(\\mathbf A - \\lambda \\mathbf I ) = 0\\), where I is the n × n identity matrix.\n\\[\n\\begin{align*}\n  \\text{det}(\\mathbf A - \\lambda \\mathbf I )\n  &=\n    \\begin{vmatrix}\n    3 - \\lambda & 1 \\\\\n    1 & 3 - \\lambda\n    \\end{vmatrix} \\\\\n  &=\n  (3 - \\lambda)(3 - \\lambda) - 1 \\\\\n  &= \\lambda^2 - 6\\lambda + 8 = 0\n\\end{align*}\n\\]\nSolving this quadratic equation gives us the eigenvalues of \\(\\mathbf A\\): \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 4\\).\nTo find the eigenvectors corresponding to \\(\\lambda_1 = 2\\), we solve the equation \\((\\mathbf A - 2 \\mathbf I)\\mathbf{v} = \\mathbf{0}\\), where \\(\\mathbf I\\) is the \\(2 \\times 2\\) identity matrix.\n\\[\n\\begin{align*}\n  (\\mathbf A - 2 \\mathbf I)\\mathbf{v} =\n    \\begin{bmatrix}\n      1 & 1 \\\\\n      1 & 1\n    \\end{bmatrix}\n  \\begin{bmatrix}\n    x \\\\\n    y\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    0 \\\\\n    0\n  \\end{bmatrix}\n\\end{align*}\n\\]\nSolving this system of equations gives us the eigenvectors corresponding to \\(\\lambda_1 = 2\\): \\(\\mathbf{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\)\nSimilarly, for \\(\\lambda_2 = 4\\), we solve the equation \\((\\mathbf A - 4\\mathbf I)\\mathbf{v}\\) = \\(\\mathbf{0}\\) to get the eigenvectors corresponding to \\(\\lambda_2 = 4\\): \\(\\mathbf{v_2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)\n\n\n2.10 Singular value and Singluar Vectors\nThe singular value decomposition (SVD) of a matrix \\(\\mathbf A\\) is a factorization of \\(\\mathbf A\\) into the product of three matrices as follows:\n\\[\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n\\] where \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) rectangular diagonal matrix with non-negative real numbers on the diagonal, and \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix.\nThe diagonal entries of \\(\\mathbf{\\Sigma}\\) are called the singular values of \\(\\mathbf{A}\\), denoted as \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_r\\) (where \\(r\\) is the rank of \\(\\mathbf{A}\\)), and are arranged in descending order. The columns of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are called the left and right singular vectors of \\(\\mathbf{A}\\), respectively, and are orthonormal vectors.\nFor example, let \\(\\mathbf{A}\\) be a 3 by 2 matrix given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} =\n  \\begin{bmatrix}\n    1 & 2\\\\\n    3 & 4\\\\\n    5 & 6\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe SVD of \\(\\mathbf{A}\\) is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T =\n\\begin{bmatrix}\n  -0.23 & -0.53 & -0.81\\\\\n  -0.53 & -0.72 & 0.45\\\\\n  -0.81 & 0.45 & -0.38\n\\end{bmatrix}\n\\begin{bmatrix}\n  9.53 & 0\\\\\n  0 & 0.90\\\\\n  0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  -0.62 & -0.78\\\\\n  -0.78 & 0.62\n\\end{bmatrix}^T\n\\end{equation*}\n\\]\nwhere the left singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{U}\\), the right singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{V}\\), and the singular values of \\(\\mathbf{A}\\) are the diagonal entries of \\(\\boldsymbol{\\Sigma}\\).\n\n연립 방정식을 행렬의 곱으로 나타내보기 \\[\\begin{matrix}x_1+2y_1=4\\\\2x_1+5y_1=9\\end{matrix} \\quad \\quad \\quad \\begin{matrix}x_2+2y_2=3\\\\2x_2+5y_2=7\\end{matrix}\\] \\[ \\begin{bmatrix} 1 & 2 \\\\ 2 & 5 \\end{bmatrix} \\begin{bmatrix} x_1 & x_2 \\\\ y_1 & y_2 \\end{bmatrix} = \\begin{bmatrix} 4 & 9 \\\\ 3 & 7 \\end{bmatrix}\\]\n중요한 사실(….당연한 사실?)\n\n곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함\n\n\\(A_{m \\times n} \\times B_{o \\times p}\\) 에서 \\(n = o\\) 여야 곱셈 성립\n\n곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 \\(\\times\\) 곱셈의 오른쪽 행렬의 열 수\n\n\\(A_{m \\times n} \\times B_{o \\times p} = C_{m \\times p}\\)\n\n교환법칙(Commutative property)이 성립하지 않음\n\n\\(AB \\neq BA\\)\n\n\n행렬 곱셈의 여러가지 관점\n\n내적으로 바라보기 \\[ A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_1^T b_1} & \\mathbf{a_1^T b_2} & \\cdots & \\mathbf{a_1^T b_m} \\\\ \\mathbf{a_2^T b_1} & \\mathbf{a_2^T b_2} & \\cdots & \\mathbf{a_2^T b_m} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ \\mathbf{a_m^T b_1} & \\mathbf{a_m^T b_2} & \\cdots & \\mathbf{a_m^T b_m} \\end{bmatrix}\\]\nrank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) \\[AB = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1^T} \\\\ \\mathbf{b_2^T} \\\\ \\vdots \\\\ \\mathbf{b_m^T} \\end{bmatrix} = \\mathbf{a_1 b_1^T} + \\mathbf{a_2 b_2^T} + \\cdots + \\mathbf{a_m b_m^T}\\]\ncolumn space로 바라보기 \\[A\\mathbf{x} = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} = \\mathbf{a_1} x_1 + \\mathbf{a_2} x_2 + \\cdots + \\mathbf{a_m} x_m \\] (스칼라배의 합)\n\n\\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) 는 2차원 좌표평면의 모든 점을, \\(A=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)은 3차원 좌표평면의 모든 점 표현 가능\n\\(AB = A \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} A \\mathbf{b_1} & A \\mathbf{b_2} & \\cdots & A \\mathbf{b_m} \\end{bmatrix}\\)\ncolumn space: A의 column vector로 만들 수 있는 부분 공간\n\nrow space로 바라보기 \\[\\mathbf{x^T}A = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_m \\end{bmatrix} \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} = x_1 \\mathbf{a_1^T} + x_2 \\mathbf{a_2^T} + \\cdots + x_m \\mathbf{a_m^T} \\]"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#열공간column-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#열공간column-space",
    "title": "Basics (3) - Special Matrices",
    "section": "3 열공간(Column Space)",
    "text": "3 열공간(Column Space)\n\n\ncolumn space: column vector 들이 span 하는 space\n\n\\(A\\)의 column space = \\(C(A)\\) 또는 \\(range(A)\\)\n\nspan: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합\n\nvector에 따라, 점일수도 선일수도 평면일 수도 있음\nvector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space\n\nvector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것\n\n\\(\\mathbf{v_1}\\) 과 \\(\\mathbf{v_2}\\)의 linear combination으로 2차원 좌표평면 나타내기"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#선형-독립linear-independent",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#선형-독립linear-independent",
    "title": "Basics (3) - Special Matrices",
    "section": "4 선형 독립(Linear Independent)",
    "text": "4 선형 독립(Linear Independent)\n\n…and also see\n\n\n선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌\northogonal 하면 independent\n\nbut independent해도 항상 orthogonal하지는 않음 (Independent > Orthogonal)\n\ndefinition: \\(a_1 \\mathbf{v_1} + a_2 \\mathbf{v_2} + a_3 \\mathbf{v_3} \\cdots a_n \\mathbf{v_n} = \\mathbf{0}\\) 를 만족하는 \\(a_1, a_2, a_3, \\cdots a_n\\) 이 \\(a_1 = a_2 = a_3 = \\cdots = a_n = 0\\) 밖에 없을때\n\n\\(\\mathbf{0}\\)는 모든 elements가 \\(0\\)인 벡터\n예: \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\\) 는 \\(-2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) 이 되므로, linearly independent 하지 않음\nindependent한 vector 들의 수 = 표현할 수 있는 차원의 dimension"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#기저basis",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#기저basis",
    "title": "Basics (3) - Special Matrices",
    "section": "5 기저(basis)",
    "text": "5 기저(basis)\n\n주어진 vector space를 span하는 linearly independent한 vectors\n어떤 공간을 이루는 필수적인 구성요소\northogonal 하면 orthogonal basis\n예: 2차원 좌표평면에 대해\n\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) : orthogonal basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) : orthogonal 하지 않은 basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}\\) : linearly independent 하지 않으므로 basis 아님"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "title": "Basics (3) - Special Matrices",
    "section": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)",
    "text": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)\n\n\n6.1 Identity matrix(항등행렬)\n\n항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)\n\n실수에서 곱셈의 항등원은 1\n\n행렬의 항등원: 항등행렬(\\(I\\)) \\[I = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\\]\n\n\\(A_{m \\times n} \\times I_{n \\times n = n} = A_{m \\times n}\\)\n\\(I_{m \\times m = m} \\times A_{m \\times n} = A_{m \\times n}\\)\n\n\n\n\n6.2 Inverse matrix(역행렬)\n\n역원: 연산 결과 항등원이 나오게 하는 연소\n\n실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): \\(a \\times a^{-1} = 1\\)\n\n행렬의 역원: 역행렬(\\(A^{-1}\\)) \\[A \\times A^{-1} = I , A^{-1} \\times A = I\\]\n\n존재하지 않는 경우도 있음\n존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림\n\n존재하지 않으면 singular, degenerate라고 불림\n\nsquare matrix(정사각행렬, \\(m = n\\))은 특수한 경우를 제외하면 역행렬이 항상 존재\n\n역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우\n\n\\(m \\neq n\\)인 행렬의 경우에는 역행렬이 존재하지 않음\n\n다만, 경우에 따라 \\(A \\times A^{-1} = I\\) 를 만족하거나(right inverse), \\(A^{-1} \\times A = I\\)를 만족하는(left inverse)는 \\(A^{-1}\\)이 존재함\n\n연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 \\[A\\mathbf{x} = \\mathbf{b} \\Rightarrow A^{-1}A\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow I\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow \\mathbf{x} = A^{-1}\\mathbf{b}\\]\n\n\n\n\n6.3 Diagonal Matrix(대각행렬)\n\ndiagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix \\[ D = Diag(\\mathbf{a}) = \\begin{bmatrix} a_{1,1} & 0 & \\cdots & 0 \\\\ 0 & a_{2,2} & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & a_{n,n} \\end{bmatrix}\\]\n\nidentity matrix는 diagonal matrix\ndiagnomal matrix는 symmetric matrix 이기도 함\n보통은 square matrix에서 주로 사용됨\n\nsquare matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix\n\n\n\n\n\n6.4 Orthogonal matrix(직교행렬, orthonomal matrix)\n\n행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) \\[A A^T = A^T A = I\\]\nidentity matrix는 orthogonal matrix\nsquare matrix에서만 정의됨\nOrthogonal matrix인 \\(A\\)이면 \\(A^{-1} = A^{T}\\)\n\n각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임\n\ncomplex matrix(복소수 행렬)에서는 unitary matrix라고 부름"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#계수rank",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#계수rank",
    "title": "Basics (3) - Special Matrices",
    "section": "7 계수(Rank)",
    "text": "7 계수(Rank)\n\n\nrank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension\nindependent한 column의 수 = independent한 행의 수: \\(rank(A) = rank(A^T)\\)\n\nproof: Wikipedia\n\n예: \\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix} \\Rightarrow rank=1\\] \\[\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\Rightarrow rank=2\\]\n\\(A_{m \\times n}\\) 의 최대 랭크는 \\(min\\{m,n\\}\\)\n\n\\(rank(A) < min\\{m,n\\}\\) 면 rank-deficient, \\(rank(A) = min\\{m,n\\}\\)면 full (row/column) rank"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#영공간null-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#영공간null-space",
    "title": "Basics (3) - Special Matrices",
    "section": "8 영공간(Null space)",
    "text": "8 영공간(Null space)\n\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) 을 만족하는 \\(\\mathbf{x}\\)의 집합\n\ncolumn space 관점에서 보기: \\(A\\mathbf{x} = x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{0}\\)\nnull space에 항상 들어가는 \\(\\mathbf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\) : trivial solution\n\n모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 \\(\\mathbf{x}=\\mathbf{0}\\)하나 밖에 없음\n\n\n\\(\\mathbf{x}=\\mathbf{0}\\) 가 아닌 vector가 null space에 있으면, 스칼라배(constant \\(c\\)에 대해 \\(c \\mathbf{x}\\)) 역시 null space에 포함됨\n혼동 주의! null space는 column space의 일부가 아님\n\nrow vector의 차원이 null space가 존재하는 공간\n\nrank와 null space의 dimension의 합은 항상 matrix의 column의 수\n\n\\(A_{m \\times n}\\)에 대해, \\(dim(N(A)) = n - r\\)\n\n모든 columns이 다 lienarly independent 하면 null space는 0차원(점)\n\nnull space는 row space와 수직한 space\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0\nrank는 row space의 dimension → row space의 dimension(\\(dim(R(A))\\))과 null space의 dimension(\\(dim(N(A))\\))의 합이 \\(n\\)\n\\(\\mathbb{R^n}\\) 공간에 표현: \n\n겹친 점: 영벡터\n\n\n\nleft null space: \\(\\mathbf{x^T} A = \\mathbf{0^T}\\) 인 \\(\\mathbf{x}\\)\n\n위의 성질을 row에 대해 적용\n\nm 차원에 놓인 벡터\ndimension: \\(dim(N_L(A)) = m - r\\)\ncolumn space와 수직: \\(dim(N_L(A)) +dim(C(A)) = m\\)\n\n\n\\(R(A)\\)에 있는 vector \\(\\mathbf{x_r}\\) 와 \\(N(A)\\)에 있는 vector \\(\\mathbf{x_n}\\)에 대해:\n\n\\(\\mathbf{x_r}\\)에 \\(A\\)를 곱하면 column space로 감\n\\(\\mathbf{x_n}\\)에 \\(A\\)를 곱하면 $\n\\(A(\\mathbf{x_r} +\\mathbf{x_n}) = A\\mathbf{x_r} + A\\mathbf{x_n} = A\\mathbf{x_r} = \\mathbf{b}\\)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#ax-b의-해의-수",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/17.computer_graphics.html#ax-b의-해의-수",
    "title": "Basics (3) - Special Matrices",
    "section": "9 Ax = b의 해의 수",
    "text": "9 Ax = b의 해의 수\n\n\nfull column rank 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 해가 하나\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음 \n\nfull row rank 일때\n\n\\(\\mathbf{b}\\)는 항상 column space 안에 있음: 무한의 해를 가짐\n임의의 특정한 해(particular solution) \\(\\mathbf{x_p}\\)와 null space의 vector \\(\\mathbf{x_n}\\)에 대해, \\(A(\\mathbf{x_p} +\\mathbf{x_n})=\\mathbf{b}\\)\n\n즉, \\(\\mathbf{x_p} +\\mathbf{x_n}\\) 도 해가 됨: complete solution\n\nnull space는 무한하므로, 해도 무한함\n\n\n\nfull rank 일때(square matrix): 해가 하나 존재 (\\(\\mathbf{x} = A^{-1}\\)$)\nrank-deficient 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 무한한 해를 가짐\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html",
    "title": "Basics (3) - Special Matrices",
    "section": "",
    "text": "A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, it can be represented as: \\[\n\\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\] where \\(a_{ij}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column of the matrix \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#basic-matrix-operations",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#basic-matrix-operations",
    "title": "Basics (3) - Special Matrices",
    "section": "2 Basic Matrix Operations",
    "text": "2 Basic Matrix Operations\n\n2.1 Matrix addition\nThe sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.\nGiven two \\(m \\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is defined by:\n\\[\nc_{i,j}=a_{i,j}+b_{i,j}​\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\n\\[\n\\begin{bmatrix}\n  1 & 2 \\\\\n  3 & 4 \\\\\n  5 & 6\n\\end{bmatrix} +\n\\begin{bmatrix}\n  -1 & 0 \\\\\n  2 & -3 \\\\\n  -5 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n  0 & 2 \\\\\n  5 & 1 \\\\\n  0 & 10\n\\end{bmatrix}\n\\]\n\n\n2.2 Scalar multiplication\nThe product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.\nGiven a scalar \\(k\\) and an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), their product \\(k\\mathbf{A}\\) is defined by: \\[\n(k\\mathbf{A})_{i,j} = k(a_{i,j})\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\nExample: \\[\n2\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 & 4 \\\\\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\]\n\n\n2.3 Matrix multiplication\nThe product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is a matrix obtained by multiplying the rows of \\(\\mathbf{A}\\) by the columns of \\(\\mathbf{B}\\).\nGiven two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with dimensions \\(m \\times n\\) and \\(n \\times p\\), respectively, their product \\(\\mathbf{C} = \\mathbf{AB}\\) is an \\(m \\times p\\) matrix defined by: \\[\nc_{i,j} = \\sum_{k=1}^n a_{i,k}b_{k,j}\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq p\\).\nExample: \\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} \\begin{bmatrix}\n-1 & 0 & 2 \\\\\n2 & -3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 & -6 & 4 \\\\\n5 & -12 & 10 \\\\\n7 & -18 & 16\n\\end{bmatrix}\n\\]\n\n\n2.4 Transpose\nThe transpose of an \\(m x n\\) matrix A, denoted by \\(A^T\\), is the \\(n x m\\) matrix obtained by interchanging the rows and columns of A. Formally, if \\(\\mathbf{A} = [a_{ij}]\\) is an m x n matrix, then its transpose \\(\\mathbf{A}^T = [b_{ij}]\\) is an \\(n x m\\) matrix where \\(b_{ij}\\) = \\(a_{ji}\\) for all \\(i\\) and \\(j\\). In other words, the element in the \\(i\\) th row and \\(j\\) th column of \\(A^T\\) is equal to the element in the \\(j\\) th row and ith column of \\(\\mathbf{A}\\).\nGiven an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix defined by: $$ {i,j}^T = {j,i}\n$$\n\nWhen \\(\\mathbf{A}\\) is transposed, diagnoal entries(\\(a_{ii}\\)) do not change but off-diagnoal elements(\\(a_{ij} \\; i \\neq j\\)) change.\nA column vector is tranposed into a row vector, and vice versa.\nsymmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)\n\nExample:\nLet A be the matrix \\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\] The transpose of A, denoted by A^T, is the matrix \\[\n\\mathbf{A}^T = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}\n\\]\n\n\n2.5 Determinant\nLet \\(\\mathbf{A}\\) be an \\(n \\times n\\) square matrix. The determinant of \\(\\mathbf{A}\\), denoted by \\(|\\mathbf{A}|\\) or \\(\\det(\\mathbf{A})\\), is a scalar value calculated as the sum of the products of the elements in any row or column of \\(\\mathbf{A}\\) with their corresponding cofactors, that is,\n\\[\n|\\mathbf{A}|=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n\\]\nwhere \\(a_{ij}\\) is the element of \\(\\mathbf{A}\\) in the \\(i\\)-th row and \\(j\\)-th column, and \\(C_{ij}\\) is the cofactor of \\(a_{ij}\\). The cofactor of \\(a_{ij}\\), denoted by \\(C_{ij}\\), is given by \\((-1)^{i+j}\\) times the determinant of the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)-th row and \\(j\\)-th column of \\(\\mathbf{A}\\).\nThe determinant of an \\(n x n\\) matrix \\(\\mathbf{A}\\) is a scalar value denoted as \\(|\\mathbf{A}|\\). It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a \\(3 x 3\\) matrix \\(\\mathbf{A} =\\) \\[\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\]\nis given by:\n\\[\n|\\mathbf{A}| = a_{11}\n\\begin{vmatrix}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{vmatrix}\n- a_{12}\n\\begin{vmatrix}\na_{21} & a_{23} \\\\\na_{31} & a_{33}\n\\end{vmatrix}\n+ a_{13}\n\\begin{vmatrix}\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{vmatrix}\n\\]\nFor example, consider the \\(3 \\times 3\\) matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\). We can calculate the determinant of \\(\\mathbf{A}\\) using any row or column. Let’s use the first column:\n\\[\n|\\mathbf{A}| = 1\n\\begin{vmatrix}\n5 & 6 \\\\\n8 & 9\n\\end{vmatrix}\n- 4\n\\begin{vmatrix}\n2 & 3\\\\\n8 & 9\n\\end{vmatrix}\n+ 7\n\\begin{vmatrix}\n2 & 5 \\\\\n3 & 6\n\\end{vmatrix} = 0\n\\]\nTherefore, the determinant of \\(\\mathbf{A}\\) is zero.\n\n\n2.6 Inverse\nThe inverse of a square matrix \\(A\\) of size \\(n\\) is a matrix \\(A^{-1}\\) such that the product of \\(A\\) and \\(A^{-1}\\) is the identity matrix \\(I_n\\), i.e. \\(A \\times A^{-1} = I_n\\). If such a matrix exists, then \\(A\\) is said to be invertible or non-singular.\nThe inverse of a square matrix \\(\\mathbf{A}\\) is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the unique matrix that satisfies the following equation: \\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\nFor example, consider the \\(2\\times 2\\) matrix \\[\n\\mathbf{A} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}.\n\\] The inverse of \\(\\mathbf{A}\\) is given by:\n\\[\n\\mathbf{A}^{-1} =\n\\frac{1}{-2}\n\\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\]\nWe can verify that \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\) by computing:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} =\n\\begin{bmatrix} -2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\nLet me give another example and A be a \\(3x3\\) square matrix:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, the inverse of \\(\\mathbf{A}\\), denoted as \\(\\mathbf{A}^{-1}\\), is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\na_{22}a_{33}-a_{23}a_{32} & a_{13}a_{32}-a_{12}a_{33} & a_{12}a_{23}-a_{13}a_{22} \\\\\na_{23}a_{31}-a_{21}a_{33} & a_{11}a_{33}-a_{13}a_{31} & a_{13}a_{21}-a_{11}a_{23} \\\\\na_{21}a_{32}-a_{22}a_{31} & a_{12}a_{31}-a_{11}a_{32} & a_{11}a_{22}-a_{12}a_{21} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nwhere \\(det(\\mathbf{A})\\) is the determinant of \\(\\mathbf{A}\\).\nFor example, let:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, \\(det(A)\\) = -57, and the inverse of \\(\\mathbf{A}\\) is:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{-57}\\begin{bmatrix}\n-24 & 18 & 5 \\\\\n20 & -15 & -4 \\\\\n-3 & 2 & 1 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThere are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.\n\n\n2.7 Rank\nThe rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by \\(\\text{rank}(\\mathbf{A})\\).\nFor example, consider the following matrix:\n\\[\n\\begin{equation*}\n  \\mathbf{A} = \\begin{bmatrix}\n    1 & 2 & 3\\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\\\\\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe columns of \\(\\mathbf{A}\\) are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of \\(\\mathbf{A}\\) is 2.\n\n\n2.8 Trace\nThe trace of a square matrix \\(\\mathbf{A}\\), denoted by \\(\\mathrm{tr}(\\mathbf{A})\\), is defined as the sum of the diagonal elements of \\(\\mathbf{A}\\). In other words, if \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix, then its trace is given by:\n\\[\n\\mathrm{tr}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}\n\\]\nwhere \\(a_{ii}\\) denotes the \\(i\\) th diagonal element of \\(\\mathbf{A}\\).\nFor example, let \\[\n\\begin{bmatrix}\n  2 & 3 & 1 \\\\\n  0 & 5 & 2 \\\\\n  1 & 1 & 4\n\\end{bmatrix}\n\\]\nThen, the trace of \\(\\mathbf{A}\\) is \\(\\mathrm{tr}(\\mathbf{A}) = 2 + 5 + 4 = 11\\)\n\n\n2.9 Eigenvalues and Eigenvectors\nLet A be an \\(n × n\\) square matrix. A scalar \\(\\lambda\\) is called an eigenvalue of \\(\\mathbf A\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that \\[\n\\mathbf{Av}=\\lambda\\mathbf{v}\n\\]\nSuch a vector \\(\\mathbf{v}\\) is called an eigenvector corresponding to \\(\\lambda\\).\nExample:\nLet \\(\\mathbf A\\) be the matrix\nTo find the eigenvalues of \\(\\mathbf A\\), we solve the characteristic equation \\(\\text{det}(\\mathbf A - \\lambda \\mathbf I ) = 0\\), where I is the n × n identity matrix.\n\\[\n\\begin{align*}\n  \\text{det}(\\mathbf A - \\lambda \\mathbf I )\n  &=\n    \\begin{vmatrix}\n    3 - \\lambda & 1 \\\\\n    1 & 3 - \\lambda\n    \\end{vmatrix} \\\\\n  &=\n  (3 - \\lambda)(3 - \\lambda) - 1 \\\\\n  &= \\lambda^2 - 6\\lambda + 8 = 0\n\\end{align*}\n\\]\nSolving this quadratic equation gives us the eigenvalues of \\(\\mathbf A\\): \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 4\\).\nTo find the eigenvectors corresponding to \\(\\lambda_1 = 2\\), we solve the equation \\((\\mathbf A - 2 \\mathbf I)\\mathbf{v} = \\mathbf{0}\\), where \\(\\mathbf I\\) is the \\(2 \\times 2\\) identity matrix.\n\\[\n\\begin{align*}\n  (\\mathbf A - 2 \\mathbf I)\\mathbf{v} =\n    \\begin{bmatrix}\n      1 & 1 \\\\\n      1 & 1\n    \\end{bmatrix}\n  \\begin{bmatrix}\n    x \\\\\n    y\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    0 \\\\\n    0\n  \\end{bmatrix}\n\\end{align*}\n\\]\nSolving this system of equations gives us the eigenvectors corresponding to \\(\\lambda_1 = 2\\): \\(\\mathbf{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\)\nSimilarly, for \\(\\lambda_2 = 4\\), we solve the equation \\((\\mathbf A - 4\\mathbf I)\\mathbf{v}\\) = \\(\\mathbf{0}\\) to get the eigenvectors corresponding to \\(\\lambda_2 = 4\\): \\(\\mathbf{v_2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)\n\n\n2.10 Singular value and Singluar Vectors\nThe singular value decomposition (SVD) of a matrix \\(\\mathbf A\\) is a factorization of \\(\\mathbf A\\) into the product of three matrices as follows:\n\\[\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n\\] where \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) rectangular diagonal matrix with non-negative real numbers on the diagonal, and \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix.\nThe diagonal entries of \\(\\mathbf{\\Sigma}\\) are called the singular values of \\(\\mathbf{A}\\), denoted as \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_r\\) (where \\(r\\) is the rank of \\(\\mathbf{A}\\)), and are arranged in descending order. The columns of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are called the left and right singular vectors of \\(\\mathbf{A}\\), respectively, and are orthonormal vectors.\nFor example, let \\(\\mathbf{A}\\) be a 3 by 2 matrix given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} =\n  \\begin{bmatrix}\n    1 & 2\\\\\n    3 & 4\\\\\n    5 & 6\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe SVD of \\(\\mathbf{A}\\) is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T =\n\\begin{bmatrix}\n  -0.23 & -0.53 & -0.81\\\\\n  -0.53 & -0.72 & 0.45\\\\\n  -0.81 & 0.45 & -0.38\n\\end{bmatrix}\n\\begin{bmatrix}\n  9.53 & 0\\\\\n  0 & 0.90\\\\\n  0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  -0.62 & -0.78\\\\\n  -0.78 & 0.62\n\\end{bmatrix}^T\n\\end{equation*}\n\\]\nwhere the left singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{U}\\), the right singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{V}\\), and the singular values of \\(\\mathbf{A}\\) are the diagonal entries of \\(\\boldsymbol{\\Sigma}\\).\n\n연립 방정식을 행렬의 곱으로 나타내보기 \\[\\begin{matrix}x_1+2y_1=4\\\\2x_1+5y_1=9\\end{matrix} \\quad \\quad \\quad \\begin{matrix}x_2+2y_2=3\\\\2x_2+5y_2=7\\end{matrix}\\] \\[ \\begin{bmatrix} 1 & 2 \\\\ 2 & 5 \\end{bmatrix} \\begin{bmatrix} x_1 & x_2 \\\\ y_1 & y_2 \\end{bmatrix} = \\begin{bmatrix} 4 & 9 \\\\ 3 & 7 \\end{bmatrix}\\]\n중요한 사실(….당연한 사실?)\n\n곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함\n\n\\(A_{m \\times n} \\times B_{o \\times p}\\) 에서 \\(n = o\\) 여야 곱셈 성립\n\n곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 \\(\\times\\) 곱셈의 오른쪽 행렬의 열 수\n\n\\(A_{m \\times n} \\times B_{o \\times p} = C_{m \\times p}\\)\n\n교환법칙(Commutative property)이 성립하지 않음\n\n\\(AB \\neq BA\\)\n\n\n행렬 곱셈의 여러가지 관점\n\n내적으로 바라보기 \\[ A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_1^T b_1} & \\mathbf{a_1^T b_2} & \\cdots & \\mathbf{a_1^T b_m} \\\\ \\mathbf{a_2^T b_1} & \\mathbf{a_2^T b_2} & \\cdots & \\mathbf{a_2^T b_m} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ \\mathbf{a_m^T b_1} & \\mathbf{a_m^T b_2} & \\cdots & \\mathbf{a_m^T b_m} \\end{bmatrix}\\]\nrank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) \\[AB = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1^T} \\\\ \\mathbf{b_2^T} \\\\ \\vdots \\\\ \\mathbf{b_m^T} \\end{bmatrix} = \\mathbf{a_1 b_1^T} + \\mathbf{a_2 b_2^T} + \\cdots + \\mathbf{a_m b_m^T}\\]\ncolumn space로 바라보기 \\[A\\mathbf{x} = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} = \\mathbf{a_1} x_1 + \\mathbf{a_2} x_2 + \\cdots + \\mathbf{a_m} x_m \\] (스칼라배의 합)\n\n\\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) 는 2차원 좌표평면의 모든 점을, \\(A=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)은 3차원 좌표평면의 모든 점 표현 가능\n\\(AB = A \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} A \\mathbf{b_1} & A \\mathbf{b_2} & \\cdots & A \\mathbf{b_m} \\end{bmatrix}\\)\ncolumn space: A의 column vector로 만들 수 있는 부분 공간\n\nrow space로 바라보기 \\[\\mathbf{x^T}A = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_m \\end{bmatrix} \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} = x_1 \\mathbf{a_1^T} + x_2 \\mathbf{a_2^T} + \\cdots + x_m \\mathbf{a_m^T} \\]"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#열공간column-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#열공간column-space",
    "title": "Basics (3) - Special Matrices",
    "section": "3 열공간(Column Space)",
    "text": "3 열공간(Column Space)\n\n\ncolumn space: column vector 들이 span 하는 space\n\n\\(A\\)의 column space = \\(C(A)\\) 또는 \\(range(A)\\)\n\nspan: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합\n\nvector에 따라, 점일수도 선일수도 평면일 수도 있음\nvector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space\n\nvector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것\n\n\\(\\mathbf{v_1}\\) 과 \\(\\mathbf{v_2}\\)의 linear combination으로 2차원 좌표평면 나타내기"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#선형-독립linear-independent",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#선형-독립linear-independent",
    "title": "Basics (3) - Special Matrices",
    "section": "4 선형 독립(Linear Independent)",
    "text": "4 선형 독립(Linear Independent)\n\n…and also see\n\n\n선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌\northogonal 하면 independent\n\nbut independent해도 항상 orthogonal하지는 않음 (Independent > Orthogonal)\n\ndefinition: \\(a_1 \\mathbf{v_1} + a_2 \\mathbf{v_2} + a_3 \\mathbf{v_3} \\cdots a_n \\mathbf{v_n} = \\mathbf{0}\\) 를 만족하는 \\(a_1, a_2, a_3, \\cdots a_n\\) 이 \\(a_1 = a_2 = a_3 = \\cdots = a_n = 0\\) 밖에 없을때\n\n\\(\\mathbf{0}\\)는 모든 elements가 \\(0\\)인 벡터\n예: \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\\) 는 \\(-2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) 이 되므로, linearly independent 하지 않음\nindependent한 vector 들의 수 = 표현할 수 있는 차원의 dimension"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#기저basis",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#기저basis",
    "title": "Basics (3) - Special Matrices",
    "section": "5 기저(basis)",
    "text": "5 기저(basis)\n\n주어진 vector space를 span하는 linearly independent한 vectors\n어떤 공간을 이루는 필수적인 구성요소\northogonal 하면 orthogonal basis\n예: 2차원 좌표평면에 대해\n\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) : orthogonal basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) : orthogonal 하지 않은 basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}\\) : linearly independent 하지 않으므로 basis 아님"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "title": "Basics (3) - Special Matrices",
    "section": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)",
    "text": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)\n\n\n6.1 Identity matrix(항등행렬)\n\n항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)\n\n실수에서 곱셈의 항등원은 1\n\n행렬의 항등원: 항등행렬(\\(I\\)) \\[I = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\\]\n\n\\(A_{m \\times n} \\times I_{n \\times n = n} = A_{m \\times n}\\)\n\\(I_{m \\times m = m} \\times A_{m \\times n} = A_{m \\times n}\\)\n\n\n\n\n6.2 Inverse matrix(역행렬)\n\n역원: 연산 결과 항등원이 나오게 하는 연소\n\n실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): \\(a \\times a^{-1} = 1\\)\n\n행렬의 역원: 역행렬(\\(A^{-1}\\)) \\[A \\times A^{-1} = I , A^{-1} \\times A = I\\]\n\n존재하지 않는 경우도 있음\n존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림\n\n존재하지 않으면 singular, degenerate라고 불림\n\nsquare matrix(정사각행렬, \\(m = n\\))은 특수한 경우를 제외하면 역행렬이 항상 존재\n\n역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우\n\n\\(m \\neq n\\)인 행렬의 경우에는 역행렬이 존재하지 않음\n\n다만, 경우에 따라 \\(A \\times A^{-1} = I\\) 를 만족하거나(right inverse), \\(A^{-1} \\times A = I\\)를 만족하는(left inverse)는 \\(A^{-1}\\)이 존재함\n\n연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 \\[A\\mathbf{x} = \\mathbf{b} \\Rightarrow A^{-1}A\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow I\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow \\mathbf{x} = A^{-1}\\mathbf{b}\\]\n\n\n\n\n6.3 Diagonal Matrix(대각행렬)\n\ndiagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix \\[ D = Diag(\\mathbf{a}) = \\begin{bmatrix} a_{1,1} & 0 & \\cdots & 0 \\\\ 0 & a_{2,2} & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & a_{n,n} \\end{bmatrix}\\]\n\nidentity matrix는 diagonal matrix\ndiagnomal matrix는 symmetric matrix 이기도 함\n보통은 square matrix에서 주로 사용됨\n\nsquare matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix\n\n\n\n\n\n6.4 Orthogonal matrix(직교행렬, orthonomal matrix)\n\n행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) \\[A A^T = A^T A = I\\]\nidentity matrix는 orthogonal matrix\nsquare matrix에서만 정의됨\nOrthogonal matrix인 \\(A\\)이면 \\(A^{-1} = A^{T}\\)\n\n각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임\n\ncomplex matrix(복소수 행렬)에서는 unitary matrix라고 부름"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#계수rank",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#계수rank",
    "title": "Basics (3) - Special Matrices",
    "section": "7 계수(Rank)",
    "text": "7 계수(Rank)\n\n\nrank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension\nindependent한 column의 수 = independent한 행의 수: \\(rank(A) = rank(A^T)\\)\n\nproof: Wikipedia\n\n예: \\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix} \\Rightarrow rank=1\\] \\[\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\Rightarrow rank=2\\]\n\\(A_{m \\times n}\\) 의 최대 랭크는 \\(min\\{m,n\\}\\)\n\n\\(rank(A) < min\\{m,n\\}\\) 면 rank-deficient, \\(rank(A) = min\\{m,n\\}\\)면 full (row/column) rank"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#영공간null-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#영공간null-space",
    "title": "Basics (3) - Special Matrices",
    "section": "8 영공간(Null space)",
    "text": "8 영공간(Null space)\n\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) 을 만족하는 \\(\\mathbf{x}\\)의 집합\n\ncolumn space 관점에서 보기: \\(A\\mathbf{x} = x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{0}\\)\nnull space에 항상 들어가는 \\(\\mathbf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\) : trivial solution\n\n모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 \\(\\mathbf{x}=\\mathbf{0}\\)하나 밖에 없음\n\n\n\\(\\mathbf{x}=\\mathbf{0}\\) 가 아닌 vector가 null space에 있으면, 스칼라배(constant \\(c\\)에 대해 \\(c \\mathbf{x}\\)) 역시 null space에 포함됨\n혼동 주의! null space는 column space의 일부가 아님\n\nrow vector의 차원이 null space가 존재하는 공간\n\nrank와 null space의 dimension의 합은 항상 matrix의 column의 수\n\n\\(A_{m \\times n}\\)에 대해, \\(dim(N(A)) = n - r\\)\n\n모든 columns이 다 lienarly independent 하면 null space는 0차원(점)\n\nnull space는 row space와 수직한 space\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0\nrank는 row space의 dimension → row space의 dimension(\\(dim(R(A))\\))과 null space의 dimension(\\(dim(N(A))\\))의 합이 \\(n\\)\n\\(\\mathbb{R^n}\\) 공간에 표현: \n\n겹친 점: 영벡터\n\n\n\nleft null space: \\(\\mathbf{x^T} A = \\mathbf{0^T}\\) 인 \\(\\mathbf{x}\\)\n\n위의 성질을 row에 대해 적용\n\nm 차원에 놓인 벡터\ndimension: \\(dim(N_L(A)) = m - r\\)\ncolumn space와 수직: \\(dim(N_L(A)) +dim(C(A)) = m\\)\n\n\n\\(R(A)\\)에 있는 vector \\(\\mathbf{x_r}\\) 와 \\(N(A)\\)에 있는 vector \\(\\mathbf{x_n}\\)에 대해:\n\n\\(\\mathbf{x_r}\\)에 \\(A\\)를 곱하면 column space로 감\n\\(\\mathbf{x_n}\\)에 \\(A\\)를 곱하면 $\n\\(A(\\mathbf{x_r} +\\mathbf{x_n}) = A\\mathbf{x_r} + A\\mathbf{x_n} = A\\mathbf{x_r} = \\mathbf{b}\\)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#ax-b의-해의-수",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/18.numerical_linear_algebra.html#ax-b의-해의-수",
    "title": "Basics (3) - Special Matrices",
    "section": "9 Ax = b의 해의 수",
    "text": "9 Ax = b의 해의 수\n\n\nfull column rank 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 해가 하나\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음 \n\nfull row rank 일때\n\n\\(\\mathbf{b}\\)는 항상 column space 안에 있음: 무한의 해를 가짐\n임의의 특정한 해(particular solution) \\(\\mathbf{x_p}\\)와 null space의 vector \\(\\mathbf{x_n}\\)에 대해, \\(A(\\mathbf{x_p} +\\mathbf{x_n})=\\mathbf{b}\\)\n\n즉, \\(\\mathbf{x_p} +\\mathbf{x_n}\\) 도 해가 됨: complete solution\n\nnull space는 무한하므로, 해도 무한함\n\n\n\nfull rank 일때(square matrix): 해가 하나 존재 (\\(\\mathbf{x} = A^{-1}\\)$)\nrank-deficient 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 무한한 해를 가짐\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html",
    "title": "Basics (3) - Special Matrices",
    "section": "",
    "text": "A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, it can be represented as: \\[\n\\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\] where \\(a_{ij}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column of the matrix \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#basic-matrix-operations",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#basic-matrix-operations",
    "title": "Basics (3) - Special Matrices",
    "section": "2 Basic Matrix Operations",
    "text": "2 Basic Matrix Operations\n\n2.1 Matrix addition\nThe sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.\nGiven two \\(m \\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is defined by:\n\\[\nc_{i,j}=a_{i,j}+b_{i,j}​\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\n\\[\n\\begin{bmatrix}\n  1 & 2 \\\\\n  3 & 4 \\\\\n  5 & 6\n\\end{bmatrix} +\n\\begin{bmatrix}\n  -1 & 0 \\\\\n  2 & -3 \\\\\n  -5 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n  0 & 2 \\\\\n  5 & 1 \\\\\n  0 & 10\n\\end{bmatrix}\n\\]\n\n\n2.2 Scalar multiplication\nThe product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.\nGiven a scalar \\(k\\) and an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), their product \\(k\\mathbf{A}\\) is defined by: \\[\n(k\\mathbf{A})_{i,j} = k(a_{i,j})\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\).\nExample: \\[\n2\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 & 4 \\\\\n6 & 8 \\\\\n10 & 12\n\\end{bmatrix}\n\\]\n\n\n2.3 Matrix multiplication\nThe product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is a matrix obtained by multiplying the rows of \\(\\mathbf{A}\\) by the columns of \\(\\mathbf{B}\\).\nGiven two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with dimensions \\(m \\times n\\) and \\(n \\times p\\), respectively, their product \\(\\mathbf{C} = \\mathbf{AB}\\) is an \\(m \\times p\\) matrix defined by: \\[\nc_{i,j} = \\sum_{k=1}^n a_{i,k}b_{k,j}\n\\] for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq p\\).\nExample: \\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix} \\begin{bmatrix}\n-1 & 0 & 2 \\\\\n2 & -3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 & -6 & 4 \\\\\n5 & -12 & 10 \\\\\n7 & -18 & 16\n\\end{bmatrix}\n\\]\n\n\n2.4 Transpose\nThe transpose of an \\(m x n\\) matrix A, denoted by \\(A^T\\), is the \\(n x m\\) matrix obtained by interchanging the rows and columns of A. Formally, if \\(\\mathbf{A} = [a_{ij}]\\) is an m x n matrix, then its transpose \\(\\mathbf{A}^T = [b_{ij}]\\) is an \\(n x m\\) matrix where \\(b_{ij}\\) = \\(a_{ji}\\) for all \\(i\\) and \\(j\\). In other words, the element in the \\(i\\) th row and \\(j\\) th column of \\(A^T\\) is equal to the element in the \\(j\\) th row and ith column of \\(\\mathbf{A}\\).\nGiven an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix defined by: $$ {i,j}^T = {j,i}\n$$\n\nWhen \\(\\mathbf{A}\\) is transposed, diagnoal entries(\\(a_{ii}\\)) do not change but off-diagnoal elements(\\(a_{ij} \\; i \\neq j\\)) change.\nA column vector is tranposed into a row vector, and vice versa.\nsymmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)\n\nExample:\nLet A be the matrix \\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\] The transpose of A, denoted by A^T, is the matrix \\[\n\\mathbf{A}^T = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}\n\\]\n\n\n2.5 Determinant\nLet \\(\\mathbf{A}\\) be an \\(n \\times n\\) square matrix. The determinant of \\(\\mathbf{A}\\), denoted by \\(|\\mathbf{A}|\\) or \\(\\det(\\mathbf{A})\\), is a scalar value calculated as the sum of the products of the elements in any row or column of \\(\\mathbf{A}\\) with their corresponding cofactors, that is,\n\\[\n|\\mathbf{A}|=\\sum_{i=1}^{n}a_{ij}C_{ij}=\\sum_{j=1}^{n}a_{ij}C_{ij}\n\\]\nwhere \\(a_{ij}\\) is the element of \\(\\mathbf{A}\\) in the \\(i\\)-th row and \\(j\\)-th column, and \\(C_{ij}\\) is the cofactor of \\(a_{ij}\\). The cofactor of \\(a_{ij}\\), denoted by \\(C_{ij}\\), is given by \\((-1)^{i+j}\\) times the determinant of the \\((n-1) \\times (n-1)\\) matrix obtained by deleting the \\(i\\)-th row and \\(j\\)-th column of \\(\\mathbf{A}\\).\nThe determinant of an \\(n x n\\) matrix \\(\\mathbf{A}\\) is a scalar value denoted as \\(|\\mathbf{A}|\\). It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a \\(3 x 3\\) matrix \\(\\mathbf{A} =\\) \\[\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\]\nis given by:\n\\[\n|\\mathbf{A}| = a_{11}\n\\begin{vmatrix}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{vmatrix}\n- a_{12}\n\\begin{vmatrix}\na_{21} & a_{23} \\\\\na_{31} & a_{33}\n\\end{vmatrix}\n+ a_{13}\n\\begin{vmatrix}\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{vmatrix}\n\\]\nFor example, consider the \\(3 \\times 3\\) matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\). We can calculate the determinant of \\(\\mathbf{A}\\) using any row or column. Let’s use the first column:\n\\[\n|\\mathbf{A}| = 1\n\\begin{vmatrix}\n5 & 6 \\\\\n8 & 9\n\\end{vmatrix}\n- 4\n\\begin{vmatrix}\n2 & 3\\\\\n8 & 9\n\\end{vmatrix}\n+ 7\n\\begin{vmatrix}\n2 & 5 \\\\\n3 & 6\n\\end{vmatrix} = 0\n\\]\nTherefore, the determinant of \\(\\mathbf{A}\\) is zero.\n\n\n2.6 Inverse\nThe inverse of a square matrix \\(A\\) of size \\(n\\) is a matrix \\(A^{-1}\\) such that the product of \\(A\\) and \\(A^{-1}\\) is the identity matrix \\(I_n\\), i.e. \\(A \\times A^{-1} = I_n\\). If such a matrix exists, then \\(A\\) is said to be invertible or non-singular.\nThe inverse of a square matrix \\(\\mathbf{A}\\) is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the unique matrix that satisfies the following equation: \\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.\nFor example, consider the \\(2\\times 2\\) matrix \\[\n\\mathbf{A} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}.\n\\] The inverse of \\(\\mathbf{A}\\) is given by:\n\\[\n\\mathbf{A}^{-1} =\n\\frac{1}{-2}\n\\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\]\nWe can verify that \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\) by computing:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} =\n\\begin{bmatrix} -2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{I}\n\\]\nLet me give another example and A be a \\(3x3\\) square matrix:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, the inverse of \\(\\mathbf{A}\\), denoted as \\(\\mathbf{A}^{-1}\\), is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})}\\begin{bmatrix}\na_{22}a_{33}-a_{23}a_{32} & a_{13}a_{32}-a_{12}a_{33} & a_{12}a_{23}-a_{13}a_{22} \\\\\na_{23}a_{31}-a_{21}a_{33} & a_{11}a_{33}-a_{13}a_{31} & a_{13}a_{21}-a_{11}a_{23} \\\\\na_{21}a_{32}-a_{22}a_{31} & a_{12}a_{31}-a_{11}a_{32} & a_{11}a_{22}-a_{12}a_{21} \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nwhere \\(det(\\mathbf{A})\\) is the determinant of \\(\\mathbf{A}\\).\nFor example, let:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\] Then, \\(det(A)\\) = -57, and the inverse of \\(\\mathbf{A}\\) is:\n\\[\n\\begin{equation*}\n\\mathbf{A}^{-1} = \\frac{1}{-57}\\begin{bmatrix}\n-24 & 18 & 5 \\\\\n20 & -15 & -4 \\\\\n-3 & 2 & 1 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThere are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.\n\n\n2.7 Rank\nThe rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by \\(\\text{rank}(\\mathbf{A})\\).\nFor example, consider the following matrix:\n\\[\n\\begin{equation*}\n  \\mathbf{A} = \\begin{bmatrix}\n    1 & 2 & 3\\\\\n    4 & 5 & 6 \\\\\n    7 & 8 & 9\\\\\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe columns of \\(\\mathbf{A}\\) are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of \\(\\mathbf{A}\\) is 2.\n\n\n2.8 Trace\nThe trace of a square matrix \\(\\mathbf{A}\\), denoted by \\(\\mathrm{tr}(\\mathbf{A})\\), is defined as the sum of the diagonal elements of \\(\\mathbf{A}\\). In other words, if \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix, then its trace is given by:\n\\[\n\\mathrm{tr}(\\mathbf{A})=\\sum_{i=1}^{n}a_{ij}\n\\]\nwhere \\(a_{ii}\\) denotes the \\(i\\) th diagonal element of \\(\\mathbf{A}\\).\nFor example, let \\[\n\\begin{bmatrix}\n  2 & 3 & 1 \\\\\n  0 & 5 & 2 \\\\\n  1 & 1 & 4\n\\end{bmatrix}\n\\]\nThen, the trace of \\(\\mathbf{A}\\) is \\(\\mathrm{tr}(\\mathbf{A}) = 2 + 5 + 4 = 11\\)\n\n\n2.9 Eigenvalues and Eigenvectors\nLet A be an \\(n × n\\) square matrix. A scalar \\(\\lambda\\) is called an eigenvalue of \\(\\mathbf A\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that \\[\n\\mathbf{Av}=\\lambda\\mathbf{v}\n\\]\nSuch a vector \\(\\mathbf{v}\\) is called an eigenvector corresponding to \\(\\lambda\\).\nExample:\nLet \\(\\mathbf A\\) be the matrix\nTo find the eigenvalues of \\(\\mathbf A\\), we solve the characteristic equation \\(\\text{det}(\\mathbf A - \\lambda \\mathbf I ) = 0\\), where I is the n × n identity matrix.\n\\[\n\\begin{align*}\n  \\text{det}(\\mathbf A - \\lambda \\mathbf I )\n  &=\n    \\begin{vmatrix}\n    3 - \\lambda & 1 \\\\\n    1 & 3 - \\lambda\n    \\end{vmatrix} \\\\\n  &=\n  (3 - \\lambda)(3 - \\lambda) - 1 \\\\\n  &= \\lambda^2 - 6\\lambda + 8 = 0\n\\end{align*}\n\\]\nSolving this quadratic equation gives us the eigenvalues of \\(\\mathbf A\\): \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 4\\).\nTo find the eigenvectors corresponding to \\(\\lambda_1 = 2\\), we solve the equation \\((\\mathbf A - 2 \\mathbf I)\\mathbf{v} = \\mathbf{0}\\), where \\(\\mathbf I\\) is the \\(2 \\times 2\\) identity matrix.\n\\[\n\\begin{align*}\n  (\\mathbf A - 2 \\mathbf I)\\mathbf{v} =\n    \\begin{bmatrix}\n      1 & 1 \\\\\n      1 & 1\n    \\end{bmatrix}\n  \\begin{bmatrix}\n    x \\\\\n    y\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    0 \\\\\n    0\n  \\end{bmatrix}\n\\end{align*}\n\\]\nSolving this system of equations gives us the eigenvectors corresponding to \\(\\lambda_1 = 2\\): \\(\\mathbf{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\)\nSimilarly, for \\(\\lambda_2 = 4\\), we solve the equation \\((\\mathbf A - 4\\mathbf I)\\mathbf{v}\\) = \\(\\mathbf{0}\\) to get the eigenvectors corresponding to \\(\\lambda_2 = 4\\): \\(\\mathbf{v_2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)\n\n\n2.10 Singular value and Singluar Vectors\nThe singular value decomposition (SVD) of a matrix \\(\\mathbf A\\) is a factorization of \\(\\mathbf A\\) into the product of three matrices as follows:\n\\[\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n\\] where \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) rectangular diagonal matrix with non-negative real numbers on the diagonal, and \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix.\nThe diagonal entries of \\(\\mathbf{\\Sigma}\\) are called the singular values of \\(\\mathbf{A}\\), denoted as \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_r\\) (where \\(r\\) is the rank of \\(\\mathbf{A}\\)), and are arranged in descending order. The columns of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are called the left and right singular vectors of \\(\\mathbf{A}\\), respectively, and are orthonormal vectors.\nFor example, let \\(\\mathbf{A}\\) be a 3 by 2 matrix given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} =\n  \\begin{bmatrix}\n    1 & 2\\\\\n    3 & 4\\\\\n    5 & 6\n  \\end{bmatrix}\n\\end{equation*}\n\\]\nThe SVD of \\(\\mathbf{A}\\) is given by:\n\\[\n\\begin{equation*}\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T =\n\\begin{bmatrix}\n  -0.23 & -0.53 & -0.81\\\\\n  -0.53 & -0.72 & 0.45\\\\\n  -0.81 & 0.45 & -0.38\n\\end{bmatrix}\n\\begin{bmatrix}\n  9.53 & 0\\\\\n  0 & 0.90\\\\\n  0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  -0.62 & -0.78\\\\\n  -0.78 & 0.62\n\\end{bmatrix}^T\n\\end{equation*}\n\\]\nwhere the left singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{U}\\), the right singular vectors of \\(\\mathbf{A}\\) are the columns of \\(\\mathbf{V}\\), and the singular values of \\(\\mathbf{A}\\) are the diagonal entries of \\(\\boldsymbol{\\Sigma}\\).\n\n연립 방정식을 행렬의 곱으로 나타내보기 \\[\\begin{matrix}x_1+2y_1=4\\\\2x_1+5y_1=9\\end{matrix} \\quad \\quad \\quad \\begin{matrix}x_2+2y_2=3\\\\2x_2+5y_2=7\\end{matrix}\\] \\[ \\begin{bmatrix} 1 & 2 \\\\ 2 & 5 \\end{bmatrix} \\begin{bmatrix} x_1 & x_2 \\\\ y_1 & y_2 \\end{bmatrix} = \\begin{bmatrix} 4 & 9 \\\\ 3 & 7 \\end{bmatrix}\\]\n중요한 사실(….당연한 사실?)\n\n곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함\n\n\\(A_{m \\times n} \\times B_{o \\times p}\\) 에서 \\(n = o\\) 여야 곱셈 성립\n\n곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 \\(\\times\\) 곱셈의 오른쪽 행렬의 열 수\n\n\\(A_{m \\times n} \\times B_{o \\times p} = C_{m \\times p}\\)\n\n교환법칙(Commutative property)이 성립하지 않음\n\n\\(AB \\neq BA\\)\n\n\n행렬 곱셈의 여러가지 관점\n\n내적으로 바라보기 \\[ A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_1^T b_1} & \\mathbf{a_1^T b_2} & \\cdots & \\mathbf{a_1^T b_m} \\\\ \\mathbf{a_2^T b_1} & \\mathbf{a_2^T b_2} & \\cdots & \\mathbf{a_2^T b_m} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ \\mathbf{a_m^T b_1} & \\mathbf{a_m^T b_2} & \\cdots & \\mathbf{a_m^T b_m} \\end{bmatrix}\\]\nrank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) \\[AB = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b_1^T} \\\\ \\mathbf{b_2^T} \\\\ \\vdots \\\\ \\mathbf{b_m^T} \\end{bmatrix} = \\mathbf{a_1 b_1^T} + \\mathbf{a_2 b_2^T} + \\cdots + \\mathbf{a_m b_m^T}\\]\ncolumn space로 바라보기 \\[A\\mathbf{x} = \\begin{bmatrix} \\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} = \\mathbf{a_1} x_1 + \\mathbf{a_2} x_2 + \\cdots + \\mathbf{a_m} x_m \\] (스칼라배의 합)\n\n\\(A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) 는 2차원 좌표평면의 모든 점을, \\(A=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)은 3차원 좌표평면의 모든 점 표현 가능\n\\(AB = A \\begin{bmatrix} \\mathbf{b_1} & \\mathbf{b_2} & \\cdots & \\mathbf{b_m} \\end{bmatrix} = \\begin{bmatrix} A \\mathbf{b_1} & A \\mathbf{b_2} & \\cdots & A \\mathbf{b_m} \\end{bmatrix}\\)\ncolumn space: A의 column vector로 만들 수 있는 부분 공간\n\nrow space로 바라보기 \\[\\mathbf{x^T}A = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_m \\end{bmatrix} \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \\mathbf{a_2^T} \\\\ \\vdots \\\\ \\mathbf{a_m^T} \\end{bmatrix} = x_1 \\mathbf{a_1^T} + x_2 \\mathbf{a_2^T} + \\cdots + x_m \\mathbf{a_m^T} \\]"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#열공간column-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#열공간column-space",
    "title": "Basics (3) - Special Matrices",
    "section": "3 열공간(Column Space)",
    "text": "3 열공간(Column Space)\n\n\ncolumn space: column vector 들이 span 하는 space\n\n\\(A\\)의 column space = \\(C(A)\\) 또는 \\(range(A)\\)\n\nspan: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합\n\nvector에 따라, 점일수도 선일수도 평면일 수도 있음\nvector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space\n\nvector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것\n\n\\(\\mathbf{v_1}\\) 과 \\(\\mathbf{v_2}\\)의 linear combination으로 2차원 좌표평면 나타내기"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#선형-독립linear-independent",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#선형-독립linear-independent",
    "title": "Basics (3) - Special Matrices",
    "section": "4 선형 독립(Linear Independent)",
    "text": "4 선형 독립(Linear Independent)\n\n…and also see\n\n\n선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌\northogonal 하면 independent\n\nbut independent해도 항상 orthogonal하지는 않음 (Independent > Orthogonal)\n\ndefinition: \\(a_1 \\mathbf{v_1} + a_2 \\mathbf{v_2} + a_3 \\mathbf{v_3} \\cdots a_n \\mathbf{v_n} = \\mathbf{0}\\) 를 만족하는 \\(a_1, a_2, a_3, \\cdots a_n\\) 이 \\(a_1 = a_2 = a_3 = \\cdots = a_n = 0\\) 밖에 없을때\n\n\\(\\mathbf{0}\\)는 모든 elements가 \\(0\\)인 벡터\n예: \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\\) 는 \\(-2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) 이 되므로, linearly independent 하지 않음\nindependent한 vector 들의 수 = 표현할 수 있는 차원의 dimension"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#기저basis",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#기저basis",
    "title": "Basics (3) - Special Matrices",
    "section": "5 기저(basis)",
    "text": "5 기저(basis)\n\n주어진 vector space를 span하는 linearly independent한 vectors\n어떤 공간을 이루는 필수적인 구성요소\northogonal 하면 orthogonal basis\n예: 2차원 좌표평면에 대해\n\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) : orthogonal basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) : orthogonal 하지 않은 basis\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}\\) : linearly independent 하지 않으므로 basis 아님"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix",
    "title": "Basics (3) - Special Matrices",
    "section": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)",
    "text": "6 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)\n\n\n6.1 Identity matrix(항등행렬)\n\n항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)\n\n실수에서 곱셈의 항등원은 1\n\n행렬의 항등원: 항등행렬(\\(I\\)) \\[I = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\\]\n\n\\(A_{m \\times n} \\times I_{n \\times n = n} = A_{m \\times n}\\)\n\\(I_{m \\times m = m} \\times A_{m \\times n} = A_{m \\times n}\\)\n\n\n\n\n6.2 Inverse matrix(역행렬)\n\n역원: 연산 결과 항등원이 나오게 하는 연소\n\n실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): \\(a \\times a^{-1} = 1\\)\n\n행렬의 역원: 역행렬(\\(A^{-1}\\)) \\[A \\times A^{-1} = I , A^{-1} \\times A = I\\]\n\n존재하지 않는 경우도 있음\n존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림\n\n존재하지 않으면 singular, degenerate라고 불림\n\nsquare matrix(정사각행렬, \\(m = n\\))은 특수한 경우를 제외하면 역행렬이 항상 존재\n\n역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우\n\n\\(m \\neq n\\)인 행렬의 경우에는 역행렬이 존재하지 않음\n\n다만, 경우에 따라 \\(A \\times A^{-1} = I\\) 를 만족하거나(right inverse), \\(A^{-1} \\times A = I\\)를 만족하는(left inverse)는 \\(A^{-1}\\)이 존재함\n\n연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 \\[A\\mathbf{x} = \\mathbf{b} \\Rightarrow A^{-1}A\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow I\\mathbf{x} = A^{-1}\\mathbf{b} \\Rightarrow \\mathbf{x} = A^{-1}\\mathbf{b}\\]\n\n\n\n\n6.3 Diagonal Matrix(대각행렬)\n\ndiagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix \\[ D = Diag(\\mathbf{a}) = \\begin{bmatrix} a_{1,1} & 0 & \\cdots & 0 \\\\ 0 & a_{2,2} & \\cdots & 0 \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & a_{n,n} \\end{bmatrix}\\]\n\nidentity matrix는 diagonal matrix\ndiagnomal matrix는 symmetric matrix 이기도 함\n보통은 square matrix에서 주로 사용됨\n\nsquare matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix\n\n\n\n\n\n6.4 Orthogonal matrix(직교행렬, orthonomal matrix)\n\n행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) \\[A A^T = A^T A = I\\]\nidentity matrix는 orthogonal matrix\nsquare matrix에서만 정의됨\nOrthogonal matrix인 \\(A\\)이면 \\(A^{-1} = A^{T}\\)\n\n각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임\n\ncomplex matrix(복소수 행렬)에서는 unitary matrix라고 부름"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#계수rank",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#계수rank",
    "title": "Basics (3) - Special Matrices",
    "section": "7 계수(Rank)",
    "text": "7 계수(Rank)\n\n\nrank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension\nindependent한 column의 수 = independent한 행의 수: \\(rank(A) = rank(A^T)\\)\n\nproof: Wikipedia\n\n예: \\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix} \\Rightarrow rank=1\\] \\[\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\Rightarrow rank=2\\]\n\\(A_{m \\times n}\\) 의 최대 랭크는 \\(min\\{m,n\\}\\)\n\n\\(rank(A) < min\\{m,n\\}\\) 면 rank-deficient, \\(rank(A) = min\\{m,n\\}\\)면 full (row/column) rank"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#영공간null-space",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#영공간null-space",
    "title": "Basics (3) - Special Matrices",
    "section": "8 영공간(Null space)",
    "text": "8 영공간(Null space)\n\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) 을 만족하는 \\(\\mathbf{x}\\)의 집합\n\ncolumn space 관점에서 보기: \\(A\\mathbf{x} = x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{0}\\)\nnull space에 항상 들어가는 \\(\\mathbf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\) : trivial solution\n\n모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 \\(\\mathbf{x}=\\mathbf{0}\\)하나 밖에 없음\n\n\n\\(\\mathbf{x}=\\mathbf{0}\\) 가 아닌 vector가 null space에 있으면, 스칼라배(constant \\(c\\)에 대해 \\(c \\mathbf{x}\\)) 역시 null space에 포함됨\n혼동 주의! null space는 column space의 일부가 아님\n\nrow vector의 차원이 null space가 존재하는 공간\n\nrank와 null space의 dimension의 합은 항상 matrix의 column의 수\n\n\\(A_{m \\times n}\\)에 대해, \\(dim(N(A)) = n - r\\)\n\n모든 columns이 다 lienarly independent 하면 null space는 0차원(점)\n\nnull space는 row space와 수직한 space\n\n\\(A\\mathbf{x}= \\mathbf{0}\\) : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0\nrank는 row space의 dimension → row space의 dimension(\\(dim(R(A))\\))과 null space의 dimension(\\(dim(N(A))\\))의 합이 \\(n\\)\n\\(\\mathbb{R^n}\\) 공간에 표현: \n\n겹친 점: 영벡터\n\n\n\nleft null space: \\(\\mathbf{x^T} A = \\mathbf{0^T}\\) 인 \\(\\mathbf{x}\\)\n\n위의 성질을 row에 대해 적용\n\nm 차원에 놓인 벡터\ndimension: \\(dim(N_L(A)) = m - r\\)\ncolumn space와 수직: \\(dim(N_L(A)) +dim(C(A)) = m\\)\n\n\n\\(R(A)\\)에 있는 vector \\(\\mathbf{x_r}\\) 와 \\(N(A)\\)에 있는 vector \\(\\mathbf{x_n}\\)에 대해:\n\n\\(\\mathbf{x_r}\\)에 \\(A\\)를 곱하면 column space로 감\n\\(\\mathbf{x_n}\\)에 \\(A\\)를 곱하면 $\n\\(A(\\mathbf{x_r} +\\mathbf{x_n}) = A\\mathbf{x_r} + A\\mathbf{x_n} = A\\mathbf{x_r} = \\mathbf{b}\\)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#ax-b의-해의-수",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/19.complex_linear_algebra.html#ax-b의-해의-수",
    "title": "Basics (3) - Special Matrices",
    "section": "9 Ax = b의 해의 수",
    "text": "9 Ax = b의 해의 수\n\n\nfull column rank 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 해가 하나\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음 \n\nfull row rank 일때\n\n\\(\\mathbf{b}\\)는 항상 column space 안에 있음: 무한의 해를 가짐\n임의의 특정한 해(particular solution) \\(\\mathbf{x_p}\\)와 null space의 vector \\(\\mathbf{x_n}\\)에 대해, \\(A(\\mathbf{x_p} +\\mathbf{x_n})=\\mathbf{b}\\)\n\n즉, \\(\\mathbf{x_p} +\\mathbf{x_n}\\) 도 해가 됨: complete solution\n\nnull space는 무한하므로, 해도 무한함\n\n\n\nfull rank 일때(square matrix): 해가 하나 존재 (\\(\\mathbf{x} = A^{-1}\\)$)\nrank-deficient 일때\n\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 있으면 무한한 해를 가짐\n\\(\\mathbf{b}\\)가 column space(\\(C(A)\\))안에 없으면 해가 없음"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/derivative_matrix_vector.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/derivative_matrix_vector.html",
    "title": "Matrix Calculus (1) - Matrix to Vector Derivatives",
    "section": "",
    "text": "Matrix-to-vector derivatives refer to the derivatives of a matrix function with respect to a vector argument. Let \\(\\mathbf{f}(\\mathbf{x})\\) be a matrix-valued function of a vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\). The matrix-to-vector derivative is denoted as follows:\n\\[\n\\frac{\\partial \\mathbf{f}(\\mathbf{x})}{\\partial \\mathbf{x}} =\n\\begin{bmatrix}\n\\frac{\\partial \\mathbf{f}(\\mathbf{x})}{\\partial x_1} &\n\\frac{\\partial \\mathbf{f}(\\mathbf{x})}{\\partial x_2} & \\cdots &\n\\frac{\\partial \\mathbf{f}(\\mathbf{x})}{\\partial x_n}\n\\end{bmatrix}\n\\]\nHere, the matrix-to-vector derivative is a matrix whose \\(i\\)th column is the partial derivative of \\(\\mathbf{f}\\) with respect to the \\(i\\)th component of \\(\\mathbf{x}\\).\nFor example, let \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) be matrices in \\(\\mathbb{R}^{n\\times n}\\) and \\(\\mathbb{R}^{n\\times 1}\\), respectively. Consider the function \\(\\mathbf{f}(\\mathbf{x}) = \\mathbf{Ax}\\), which is a matrix-vector product. The matrix-to-vector derivative of \\(\\mathbf{f}(\\mathbf{x})\\) with respect to \\(\\mathbf{x}\\) is given by: \\[\n\\frac{\\partial \\mathbf{f}(\\mathbf{x})}{\\partial \\mathbf{x}} = \\mathbf{A}\n\\] Here, the derivative is a matrix whose rows are the rows of \\(\\mathbf{A}\\).\n\n\nThis is because the output of this differentiation is a vector (with respect to \\(\\mathbf{x}\\)), rather than a scalar.\nThe differentiation of a quadratic form is the process of finding the gradient of a quadratic form with respect to its input vector.\nGiven a quadratic form \\(f(\\mathbf{x})=\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\), where \\(\\mathbf{x}\\) is an \\(n\\)-dimensional column vector, \\(\\mathbf{b} \\in \\mathbb{R}^n\\) is a vector, and \\(\\mathbf{A}\\) is an \\(n \\times n\\) symmetric matrix, the derivative of \\(f(\\mathbf{x})\\) with respect to \\(\\mathbf{x}\\) is given by:\n\\[\n\\nabla_{\\mathbf x} f(\\mathbf x) = (A + A^T)\\mathbf x + b\n\\]\nIn this expression, \\(\\mathbf{A}^T\\) is the transpose of \\(\\mathbf{A}\\).\n\\(\\mathbf A+\\mathbf A^T\\) is written instead of \\(2\\mathbf A\\) when calculating the gradient of a quadratic form. It is because in general, the matrix \\(\\mathbf A\\) might not be symmetric, so \\(\\mathbf A\\neq \\mathbf A^T\\). However, for any matrix \\(\\mathbf A\\), we have \\(\\mathbf A+\\mathbf A^T = (\\mathbf A+\\mathbf A^T)^T\\), which is a symmetric matrix. Therefore, by writing the gradient as \\(\\nabla_x f(x) = (\\mathbf A+\\mathbf A^T)x\\), we ensure that the gradient is always a symmetric matrix, even if \\(\\mathbf A\\) is not symmetric. This is useful in many applications where symmetric matrices are preferred. But if \\(\\mathbf A\\) is constrained to be a symmetric matrix, \\(2\\mathbf A\\) can be written.\n\n\n\nAs an example, consider the quadratic form \\(f(\\mathbf{x})=x_1^2+2x_1x_2+3x_2^2\\), which can be written in the form \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\), where:\n\\[\n\\mathbf x=\\begin{bmatrix} x_1 \\ x_2 \\end{bmatrix}, \\mathbf A=\\begin{bmatrix} 1 & 1 \\\\ 1 & 3 \\end{bmatrix}\n\\]\nThe derivative of \\(f(\\mathbf{x})\\) with respect to \\(\\mathbf{x}\\) is then:\n\\[\n\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}}=(\\mathbf{A}+\\mathbf{A}^T)\\mathbf{x}=\\begin{bmatrix}2 & 2\\\\2 & 6\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}=\\begin{bmatrix}2x_1+2x_2\\\\2x_1+6x_2\\end{bmatrix}\n\\]\nThis represents the gradient vector of \\(f(\\mathbf{x})\\) at any point \\(\\mathbf{x}\\).\n\n\n\nLet \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\).\nThen, we have \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x} = \\begin{bmatrix} x_1 & x_2 \\end{bmatrix} \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} x_1 & x_2 \\end{bmatrix} \\begin{bmatrix} x_1 + 2x_2 \\\\ 3x_1 + 4x_2 \\end{bmatrix} = x_1^2 + 5x_1x_2 + 4x_2^2\\).\nTo find the gradient of this quadratic form, we can take the partial derivatives of \\(x_1\\) and \\(x_2\\) with respect to each variable:\n\\[\n\\frac{\\partial}{\\partial x_1} (\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = 2x_1 + 5x_2\n\\]\n\\[\n\\frac{\\partial}{\\partial x_2} (\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = 5x_1 + 8x_2\n\\]\n\\[\n\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}}=(\\mathbf{A}+\\mathbf{A}^T)\\mathbf{x}=\\begin{bmatrix}1+1 & 2+3\\\\3+2 & 4+4\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}=\\begin{bmatrix}2 & 5\\\\5 & 8\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix} = \\begin{bmatrix} 2x_1 + 5x_2 \\\\ 5x_1 + 8x_2 \\end{bmatrix}\n\\]\nSo the gradient of \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) is \\(\\nabla_x f(x) = \\begin{bmatrix} 2x_1 + 5x_2 \\\\ 5x_1 + 8x_2 \\end{bmatrix}\\).\n\n\n\nFor the \\(n \\times 1\\) vector \\(\\mathbf{y}\\), the \\(n \\times k\\) matrix \\(\\mathbf{X}\\), the \\(k \\times 1\\) vector \\(\\mathbf{\\beta}\\), when \\(L=(\\mathbf{y}-\\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y}-\\mathbf{X}\\mathbf{\\beta})\\), what is \\(\\frac{\\partial L}{\\partial \\mathbf{\\beta}}\\)?\n\\[\n\\begin{aligned}\nL &= (\\mathbf{y}-\\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y}-\\mathbf{X}\\mathbf{\\beta}) \\\\\n&= \\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{A}\\mathbf{\\beta} - \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} \\\\\n&= \\mathbf{y}^T\\mathbf{y} - 2\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta}\n\\end{aligned}\n\\]\nNow, we can take the derivative of \\(L\\) with respect to \\(\\mathbf{\\beta}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial \\mathbf{\\beta}} &= \\frac{\\partial}{\\partial \\mathbf{\\beta}} (\\mathbf{y}^T\\mathbf{y} - 2\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta}) \\\\\n&= \\frac{\\partial}{\\partial \\mathbf{\\beta}} (- 2\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta}) \\\\\n&= - 2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta}\n\\end{aligned}\n\\]\n\\(\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\) is the solution to the optimization problem by taking its derivative with respect to and setting it equal to zero.\nStarting with the expression for \\(L\\):\n\\[\nL=(\\mathbf{y}-\\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y}-\\mathbf{X}\\mathbf{\\beta})\n\\]\nExpanding the quadratic term gives:\n\\[\nL=\\mathbf{y}^T\\mathbf{y}-\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y}-\\mathbf{y}^T\\mathbf{X}\\mathbf{\\beta}+\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta}\n\\]\nTaking the derivative of \\(L\\) with respect to \\(\\mathbf{\\beta}\\) and setting it to zero gives:\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} = 0\n\\]\nSolving for \\(\\mathbf{\\beta}\\) gives:\n\\[\n\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta}=\\mathbf{X}^T\\mathbf{y}\n\\]\n\\[\n\\mathbf{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n\\] Multiplying both sides of the equation by \\((\\mathbf{X}^T\\mathbf{X})\\) gives:\n\\[\n(\\mathbf{X}^T\\mathbf{X})\\mathbf{\\beta}=\\mathbf{X}^T\\mathbf{y}\n\\]\nTherefore, we have verified that \\[\n\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n\\] is the solution to the optimization problem of OLS (Ordinary Least Square)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/derivative_vector_matrix.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/derivative_vector_matrix.html",
    "title": "Matrix Calculus (1) - Matrix to Vector Derivatives",
    "section": "",
    "text": "The matrix-to-vector derivative is a derivative where a function \\(f(\\mathbf{X})\\) maps an \\(m \\times n\\) matrix \\(\\mathbf{X}\\) to a \\(p\\)-dimensional vector \\(\\mathbf{y}\\), and we want to find the derivative of \\(\\mathbf{y}\\) with respect to \\(\\mathbf{X}\\). It is denoted by \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{X}}\\).\nFormally, let \\(\\mathbf{X} \\in \\mathbb{R}^{m \\times n}\\) and \\(\\mathbf{y} = f(\\mathbf{X}) \\in \\mathbb{R}^p\\) be a function that maps an \\(m \\times n\\) matrix to a \\(p\\)-dimensional vector. Then, the matrix-to-vector derivative is defined as: \\[\n\\begin{bmatrix}\n\\frac{\\partial \\mathbf{y}}{\\partial x_{11}} & \\frac{\\partial \\mathbf{y}}{\\partial x_{12}} & \\cdots & \\frac{\\partial \\mathbf{y}}{\\partial x_{1n}} \\\\\n\\frac{\\partial \\mathbf{y}}{\\partial x_{21}} & \\frac{\\partial \\mathbf{y}}{\\partial x_{22}} & \\cdots & \\frac{\\partial \\mathbf{y}}{\\partial x_{2n}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial \\mathbf{y}}{\\partial x_{m1}} & \\frac{\\partial \\mathbf{y}}{\\partial x_{m2}} & \\cdots & \\frac{\\partial \\mathbf{y}}{\\partial x_{mn}} \\\\\n\\end{bmatrix}\n\\]\nwhere each element of the matrix is the derivative of the corresponding element of \\(\\mathbf{y}\\) with respect to the corresponding element of \\(\\mathbf{X}\\).\nFor example, let \\(f(\\mathbf{X}) = \\mathbf{A}\\mathbf{X}+\\mathbf{b}\\), where \\(\\mathbf{A} \\in \\mathbb{R}^{p \\times m}\\), \\(\\mathbf{X} \\in \\mathbb{R}^{m \\times n}\\), and \\(\\mathbf{b} \\in \\mathbb{R}^p\\). Then, the matrix-to-vector derivative of \\(\\mathbf{y} = f(\\mathbf{X})\\) with respect to \\(\\mathbf{X}\\) is: \\[\n\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{X}} = \\mathbf{A}\n\\]\n\n\nThis is because the output of this differentiation is a vector (with respect to \\(\\mathbf{x}\\)), rather than a scalar.\nThe differentiation of a quadratic form is the process of finding the gradient of a quadratic form with respect to its input vector.\nGiven a quadratic form \\(f(\\mathbf{x})=\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\), where \\(\\mathbf{x}\\) is an \\(n\\)-dimensional column vector, \\(\\mathbf{b} \\in \\mathbb{R}^n\\) is a vector, and \\(\\mathbf{A}\\) is an \\(n \\times n\\) symmetric matrix, the derivative of \\(f(\\mathbf{x})\\) with respect to \\(\\mathbf{x}\\) is given by:\n\\[\n\\nabla_{\\mathbf x} f(\\mathbf x) = (A + A^T)\\mathbf x + b\n\\]\nIn this expression, \\(\\mathbf{A}^T\\) is the transpose of \\(\\mathbf{A}\\).\n\\(\\mathbf A+\\mathbf A^T\\) is written instead of \\(2\\mathbf A\\) when calculating the gradient of a quadratic form. It is because in general, the matrix \\(\\mathbf A\\) might not be symmetric, so \\(\\mathbf A\\neq \\mathbf A^T\\). However, for any matrix \\(\\mathbf A\\), we have \\(\\mathbf A+\\mathbf A^T = (\\mathbf A+\\mathbf A^T)^T\\), which is a symmetric matrix. Therefore, by writing the gradient as \\(\\nabla_x f(x) = (\\mathbf A+\\mathbf A^T)x\\), we ensure that the gradient is always a symmetric matrix, even if \\(\\mathbf A\\) is not symmetric. This is useful in many applications where symmetric matrices are preferred. But if \\(\\mathbf A\\) is constrained to be a symmetric matrix, \\(2\\mathbf A\\) can be written.\n\n\n\nAs an example, consider the quadratic form \\(f(\\mathbf{x})=x_1^2+2x_1x_2+3x_2^2\\), which can be written in the form \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\), where:\n\\[\n\\mathbf x=\\begin{bmatrix} x_1 \\ x_2 \\end{bmatrix}, \\mathbf A=\\begin{bmatrix} 1 & 1 \\\\ 1 & 3 \\end{bmatrix}\n\\]\nThe derivative of \\(f(\\mathbf{x})\\) with respect to \\(\\mathbf{x}\\) is then:\n\\[\n\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}}=(\\mathbf{A}+\\mathbf{A}^T)\\mathbf{x}=\\begin{bmatrix}2 & 2\\\\2 & 6\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}=\\begin{bmatrix}2x_1+2x_2\\\\2x_1+6x_2\\end{bmatrix}\n\\]\nThis represents the gradient vector of \\(f(\\mathbf{x})\\) at any point \\(\\mathbf{x}\\).\n\n\n\nLet \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\).\nThen, we have \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x} = \\begin{bmatrix} x_1 & x_2 \\end{bmatrix} \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} x_1 & x_2 \\end{bmatrix} \\begin{bmatrix} x_1 + 2x_2 \\\\ 3x_1 + 4x_2 \\end{bmatrix} = x_1^2 + 5x_1x_2 + 4x_2^2\\).\nTo find the gradient of this quadratic form, we can take the partial derivatives of \\(x_1\\) and \\(x_2\\) with respect to each variable:\n\\[\n\\frac{\\partial}{\\partial x_1} (\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = 2x_1 + 5x_2\n\\]\n\\[\n\\frac{\\partial}{\\partial x_2} (\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = 5x_1 + 8x_2\n\\]\n\\[\n\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}}=(\\mathbf{A}+\\mathbf{A}^T)\\mathbf{x}=\\begin{bmatrix}1+1 & 2+3\\\\3+2 & 4+4\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}=\\begin{bmatrix}2 & 5\\\\5 & 8\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix} = \\begin{bmatrix} 2x_1 + 5x_2 \\\\ 5x_1 + 8x_2 \\end{bmatrix}\n\\]\nSo the gradient of \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) is \\(\\nabla_x f(x) = \\begin{bmatrix} 2x_1 + 5x_2 \\\\ 5x_1 + 8x_2 \\end{bmatrix}\\)."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/quadratic_form.html",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/quadratic_form.html",
    "title": "Matrix Transformation (5) - Quadratic Form",
    "section": "",
    "text": "For a vector \\(\\mathbf{x} = [x_1,x_2,\\ldots,x_n]^T\\), the quadratic form is defined as\n\\(Q(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\)\nwhere \\(\\mathbf{A}\\) is an \\(n \\times n\\) symmetric matrix.\nHere, \\(\\mathbf{x}^T\\) represents the transpose of the vector \\(\\mathbf{x}\\) and \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) represents the dot product of the vector \\(\\mathbf{x}\\) with itself after the transformation by the matrix \\(\\mathbf{A}\\).\nFor example, let \\(\\mathbf{x} = [x_1,x_2]^T\\) and \\(\\mathbf{A}\\) be a \\(2 \\times 2\\) symmetric matrix given by:\n\\[\n\\mathbf A = \\begin{bmatrix} 2&1 \\\\ 1&3 \\end{bmatrix}\n\\]\nThen, the quadratic form \\(Q(\\mathbf{x})\\) can be written as:\n\\[\n\\mathbf Q(\\mathbf x) = \\begin{bmatrix} x_1 & x_2 \\end{bmatrix}\\begin{bmatrix} 2&1 \\\\ 1&3 \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}=2x_1^2+4x_1x_2+3x_2^2\n\\]\nHere, we can see that the quadratic form can be represented as a polynomial function of degree 2 in the variables \\(x_1\\) and \\(x_2\\) with the coefficients given by the entries of the symmetric matrix \\(\\mathbf{A}\\).\na quadratic form can be expressed as a bilinear form. In other words, a quadratic form can be written in terms of a bilinear form by defining a new matrix that is the sum of the matrix representing the quadratic form and its transpose.\nMore formally, suppose we have a quadratic form defined as:\n\\(q(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\)\nwhere \\(\\mathbf{A}\\) is a symmetric matrix. Then, we can define a bilinear form as:\n\\(b(\\mathbf{x}, \\mathbf{y}) = \\frac{1}{2}(\\mathbf{x}^T \\mathbf{A} \\mathbf{y} + \\mathbf{y}^T \\mathbf{A} \\mathbf{x})\\)\nNote that the factor of \\(\\frac{1}{2}\\) is introduced to avoid double-counting. It can be shown that the two forms are equivalent, in the sense that for any \\(\\mathbf{x}\\), \\(q(\\mathbf{x}) = b(\\mathbf{x}, \\mathbf{x})\\).\nIn other words, every quadratic form can be expressed as a bilinear form, and every symmetric bilinear form can be expressed as a quadratic form."
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/linear_algebra/quadratic_form.html#examples",
    "href": "docs/blog/posts/Mathmatics/linear_algebra/quadratic_form.html#examples",
    "title": "Matrix Transformation (5) - Quadratic Form",
    "section": "2 Examples",
    "text": "2 Examples\n\n2.1 Sum of Squares\nThe sum of squares of a vector \\(\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^T\\) can be represented as a quadratic form \\(\\mathbf{x}^T\\mathbf{x}\\). To see this, consider the sum of squares:\n\\[\n\\sum_{i=1}^{n} x_i^2 = x_1^2 + x_2^2 + \\dots +x_n^2\n\\]\nNow, we can write this in vector form as:\n\\[\n\\mathbf x^T \\mathbf I \\mathbf x =  \\mathbf x^T \\mathbf x = \\begin{bmatrix} x_1 & x_2 & \\dots & x_n \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\n\\]\nTherefore, the sum of squares can be represented as a quadratic form \\(\\mathbf{x}^T\\mathbf{x}\\).\n\n\n2.2 Variability of Vector, x\nThe covariance matrix of a random vector \\(\\mathbf{x}\\) can be represented as a quadratic form in terms of the vector \\(\\mathbf{x}\\) and the matrix \\(\\mathbf{C}\\) as follows:\n\\[\n\\mathbf x^T \\mathbf C \\mathbf x\n\\]\nwhere \\(\\mathbf{C}\\) is the covariance matrix. This expression is a quadratic form because it involves a quadratic polynomial in the elements of \\(\\mathbf{x}\\).\nIn this representation, the diagonal elements of \\(\\mathbf{C}\\) correspond to the variances of the individual components of \\(\\mathbf{x}\\), and the off-diagonal elements correspond to the covariances between the components. The expression \\(\\mathbf{x}^T \\mathbf{C} \\mathbf{x}\\) measures the variability of the random vector \\(\\mathbf{x}\\) in all possible directions, weighted by the covariances between the components.\n\n\n\n\n\n\nNote\n\n\n\nThe covariance matrix \\(\\mathbf{C}\\) measures the covariance between each pair of components of the random vector \\(\\mathbf{x}\\). It is a matrix that summarizes the pairwise covariances between the components of \\(\\mathbf{x}\\).\nOn the other hand, the quadratic form \\(\\mathbf{x}^T \\mathbf{C} \\mathbf{x}\\) measures the total variability of \\(\\mathbf{x}\\), taking into account the covariances between all possible pairs of components.\nIt does this by weighting the contribution of each component to the overall variability by its covariance with every other component. So, while the covariance matrix \\(\\mathbf{C}\\) captures the pairwise covariances between the components of \\(\\mathbf{x}\\), the quadratic form \\(\\mathbf{x}^T \\mathbf{C} \\mathbf{x}\\) captures the total variability of \\(\\mathbf{x}\\) in all directions.\n\n\nLet’s take a simple example with a 2-dimensional random vector \\(\\mathbf{x}=[x_1, x_2]^T\\). We can think of this random vector as representing data points in a 2D space. The covariance matrix \\(\\mathbf{C}\\) will capture the covariances between \\(x_1\\) and \\(x_2\\). Let’s say that the covariance matrix is given by:\n\\[\n\\mathbf C =\\begin{bmatrix} \\sigma_{x_1} & \\operatorname{Cov}(x_1,x_2) \\\\ \\operatorname{Cov}(x_2,x_1) & \\sigma_{x_2} \\end{bmatrix}\n\\]\nwhere \\(\\sigma_{x_1}^2\\) and \\(\\sigma_{x_2}^2\\) are the variances of \\(x_1\\) and \\(x_2\\), respectively, and \\(\\text{Cov}(x_1,x_2)\\) is their covariance.\nNow, let’s consider the quadratic form \\(\\mathbf{x}^T \\mathbf{C} \\mathbf{x}\\). This expression gives us a scalar value that measures the variability of the random vector \\(\\mathbf{x}\\) in all possible directions, weighted by the covariances between the components. We can see this geometrically by plotting the data points in the 2D space and drawing an ellipse that captures the variability of the data. The shape of the ellipse is determined by the eigenvalues and eigenvectors of the covariance matrix \\(\\mathbf{C}\\).\nTo see this, let’s first rewrite the quadratic form as: \\[\n\\mathbf x^T \\mathbf C \\mathbf x = \\sigma_{x_1}^2x_1^2 +2\\operatorname{Cov}(x_1,x_2)x_1x_2+\\sigma_{x_2}^2\n\\]\nThis is a quadratic equation in \\(x_1\\) and \\(x_2\\) and can be thought of as the equation of an ellipse centered at the origin by the determinant of the conic equation. The shape of the ellipse is determined by the coefficients of the quadratic terms, which are the variances and covariances in the covariance matrix \\(\\mathbf{C}\\).\nNow, let’s find the eigenvectors and eigenvalues of the covariance matrix \\(\\mathbf{C}\\). The eigenvectors are the directions along which the data has the most variance, and the corresponding eigenvalues are the variances of the data along those directions.\nLet’s assume that the eigenvalues of \\(\\mathbf{C}\\) are ordered such that \\(\\lambda_1 \\geq \\lambda_2\\). Then, the eigenvectors \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) satisfy:\n\\[\n\\mathbf C \\mathbf v_1 =\\lambda_1\\mathbf v_1 \\text{  }\n\\mathbf C \\mathbf v_2 =\\lambda_2\\mathbf v_2\n\\]\nThese equations can be rewritten as:\n\\[\n\\begin{equation}\n\\begin{bmatrix}\n  \\sigma_{x_1}^2 & \\text{Cov}(x_1,x_2)\\\\\n  \\text{Cov}(x_1,x_2) & \\sigma_{x_2}^2\n\\end{bmatrix}\n\\begin{bmatrix}\n  v_{11}\\\\\n  v_{21}\n\\end{bmatrix}\n= \\lambda_1\n\\begin{bmatrix}\nv_{11}\\\\\nv_{21}\n\\end{bmatrix}\n\\end{equation}\n\\]\nThis equation can be expanded as:\n\\(\\sigma_{x_1}^2 v_{11} + \\text{Cov}(x_1,x_2) v_{21} = \\lambda_1 v_{11}\\)\n\\(\\text{Cov}(x_1,x_2) v_{11} + \\sigma_{x_2}^2 v_{21} = \\lambda_1 v_{21}\\)\nNow, let’s multiply the first equation by \\(v_{11}\\) and the second equation by \\(v_{21}\\), and then subtract the second equation from the first:\n\\((\\sigma_{x_1}^2 - \\lambda_1)v_{11}v_{21} + \\text{Cov}(x_1,x_2)(v_{21}^2 - v_{11}^2) = 0\\)\nThis can be rewritten as:\n\\(\\frac{v_{21}}{v_{11}} = \\frac{\\sigma_{x_1}^2 - \\lambda_1}{\\text{Cov}(x_1,x_2)} - \\frac{v_{11}}{v_{21}}\\)\nLet \\(t = \\frac{v_{21}}{v_{11}}\\). Then, we have:\n\\(t^2 - \\left(\\frac{\\sigma_{x_1}^2 + \\sigma_{x_2}^2}{\\text{Cov}(x_1,x_2)}\\right)t + \\frac{\\lambda_1}{\\text{Cov}(x_1,x_2)} = 0\\)\nThis is a quadratic equation in \\(t\\), and its roots can be solved using the quadratic formula. The roots are:\n\\(t_1 = \\frac{\\sigma_{x_1}^2 - \\sigma_{x_2}^2 + \\sqrt{(\\sigma_{x_1}^2 - \\sigma_{x_2}^2)^2 + 4\\text{Cov}(x_1,x_2)^2}}{2\\text{Cov}(x_1,x_2)}\\)\n\\(t_2 = \\frac{\\sigma_{x_1}^2 - \\sigma_{x_2}^2 - \\sqrt{(\\sigma_{x_1}^2 - \\sigma_{x_2}^2)^2 + 4\\text{Cov}(x_1,x_2)^2}}{2\\text{Cov}(x_1,x_2)}\\)\nFinally, the eigenvectors \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) are given by:\n\\[\n\\mathbf{v}_1 = \\begin{bmatrix}1 \\\\ t_1 \\end{bmatrix} \\text{  }\n\\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ t_2 \\end{bmatrix}\n\\]\nThese eigenvectors define the principal components of the data, which are the orthogonal directions in the feature space along which the data varies the most.\nLet’s apply this difference between \\(\\mathbf C= \\operatorname{Cov(\\mathbf X)}\\) and \\(\\mathbf x^T \\mathbf{C} \\mathbf x\\) or PCA to Iris dataset:\n\n\nC=Cov(X) =\n[[ 1.00671141 -0.11835884  0.87760447  0.82343066]\n [-0.11835884  1.00671141 -0.43131554 -0.36858315]\n [ 0.87760447 -0.43131554  1.00671141  0.96932762]\n [ 0.82343066 -0.36858315  0.96932762  1.00671141]]\n\n\n\n\n\nThis representation is useful in many statistical and machine learning applications, where the covariance matrix provides information about the variability and dependencies between different features or variables. For example, in principal component analysis (PCA), the covariance matrix is used to identify the directions of maximum variability in a dataset, which can be used to reduce the dimensionality of the data while retaining as much information as possible.\n\n\n2.3 PCA\nThe principal components of a dataset can be obtained by finding the eigenvectors of the covariance matrix. In other words, we can express the covariance matrix as a quadratic form:\n\\[\n\\mathbf C = \\mathbf x^T \\mathbf A \\mathbf x\n\\]\nwhere \\(\\mathbf{x}\\) is a column vector of centered data, and \\(\\mathbf{A}\\) is a symmetric positive semi-definite matrix (the covariance matrix). Diagonalizing \\(\\mathbf{A}\\) gives us the eigenvalues and eigenvectors, which are used to transform the original data into a new coordinate system, where the first axis (the first principal component) corresponds to the direction of greatest variance, the second axis (the second principal component) corresponds to the direction of second greatest variance, and so on. This new coordinate system is called the principal component space.\nIn summary, PCA can be seen as a method for finding the principal components of a dataset by diagonalizing the covariance matrix, which can be expressed as a quadratic form.\n\n\n2.4 Positive Definit Matrix\na symmetric matrix \\(A\\) is positive definite if and only if the quadratic form \\(f(\\mathbf{x})=\\mathbf{x}^T A \\mathbf{x}\\) is positive for all nonzero vectors \\(\\mathbf{x}\\).\nTo see why this is true, consider the eigenvalue decomposition of \\(A\\), which can be written as \\(A = Q \\Lambda Q^T\\), where \\(Q\\) is an orthogonal matrix and \\(\\Lambda\\) is a diagonal matrix containing the eigenvalues of \\(A\\). Then, for any nonzero vector \\(\\mathbf{x}\\),\n\n\n\n\n\n\nDiagonalization\n\n\n\nDiagonalization is a process of finding a diagonal matrix \\(\\mathbf D\\) and an invertible matrix \\(\\mathbf P\\) such that \\(P^{-1}AP = D\\), where \\(\\mathbf A\\) is a square matrix. In other words, diagonalization is a way of representing a matrix as a diagonal matrix, which is a matrix with non-zero values only on its main diagonal.\n\n\nwe have\n\\[\n\\begin{aligned}\n\\mathbf x^T \\mathbf A \\mathbf x&=\\mathbf x^T \\mathbf Q \\mathbf \\Lambda \\mathbf Q^T \\mathbf x\\\\\n&=(\\mathbf x^T \\mathbf Q)\\mathbf \\Lambda ( \\mathbf Q^T\\mathbf x)\\\\\n&=\\sum_{i=1}^{n} \\lambda_iy_i^2\n\\end{aligned}\n\\]\nwhere \\(y_i = (\\mathbf{x}^T Q)_i\\) is the \\(i\\)th coordinate (i.e., a scalar value that represents the position of a point or a vector relative to a chosen basis) of \\(\\mathbf{x}^T Q\\) and \\(n\\) is the dimension of \\(\\mathbf{x}\\) and \\(A\\). Note that since \\(Q\\) is orthogonal, we have \\(Q^T Q = I\\), so \\(y_i = \\mathbf{q}_i^T \\mathbf{x}\\), where \\(\\mathbf{q}_i\\) is the \\(i\\)th column of \\(Q\\). Therefore, the quadratic form \\(f(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}\\) can be written in terms of the eigenvalues of \\(A\\) and the coordinates of \\(\\mathbf{x}\\) with respect to the eigenvectors of \\(A\\).\nSince \\(A\\) is positive definite, we have \\(\\lambda_i > 0\\) for all \\(i\\), and so \\(\\sum_{i=1}^n \\lambda_i y_i^2 > 0\\) for all nonzero vectors \\(\\mathbf{x}\\). Therefore, the quadratic form \\(f(\\mathbf{x})=\\mathbf{x}^T A \\mathbf{x}\\) is positive for all nonzero vectors \\(\\mathbf{x}\\), which implies that \\(A\\) is positive definite.\nIn other words, the positive definiteness of a symmetric matrix \\(A\\) is equivalent to the positivity of the associated quadratic form \\(f(\\mathbf{x})=\\mathbf{x}^T A \\mathbf{x}\\) for all nonzero vectors \\(\\mathbf{x}\\).\nTherefore, a symmetric matrix, \\(\\mathbf A\\) is said to be positive definite if all of its eigenvalues are positive or equivalently, a symmetric matrix, \\(\\mathbf A\\) is positive definite if left-multiplying and right-multiplying it by the same vector, \\(\\mathbf x\\) always gives a positive number if \\(\\mathbf x^T \\mathbf A \\mathbf x\\)"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/optimization/minimizer.html",
    "href": "docs/blog/posts/Mathmatics/optimization/minimizer.html",
    "title": "Minimizer & Maximizer",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\nA minimizer or a maximizer refer to a point in the domain of a function where the function achieves its minimum or maximum value. More formally, let \\(f: X → \\mathbb R\\) be a real-valued function defined on a set \\(X \\subset \\mathbb R\\).\n\nDefinition 1 \\(f(x^*)\\) with a point \\(x^* \\in X\\) is called a minimum if \\(f(x^*) \\le f(x)\\) for all \\(x \\in X\\). \\[\n\\min_{x\\in X}{f(x)}\n\\] , which means a minimum, \\(f(x)\\).\n\n\nDefinition 2 \\(f(x^*)\\) with a point \\(x^* \\in X\\) is called a maximum if \\(f(x^*) \\ge f(x)\\) for all \\(x \\in X\\). \\[\n\\max_{x\\in X}{f(x)}\n\\] , which means a maximum, \\(f(x)\\).\n\n\nDefinition 3 A point \\(x^* \\in X\\) is called a minimizer of \\(f\\) if \\(f(x^*) \\le f(x)\\) for all \\(x \\in X\\). \\[\n\\arg\\min_{x\\in X}{f(x)}\n\\] , which means \\(x\\) that mainimizes \\(f(x)\\).\n\n\nDefinition 4 A point \\(x^* \\in X\\) is called a maximizer of \\(f\\) if \\(f(x^*) \\ge f(x)\\) for all \\(x \\in X\\). \\[\n\\arg\\max_{x\\in X}{f(x)}\n\\] , which means \\(x\\) that maximizes \\(f(x)\\).\n\n\nDefinition 5 A point \\(x^* \\in X\\) is called a global minimizer of \\(f\\) if \\(f(x^*) \\le f(x)\\) for all \\(x \\in \\mathbb R\\). \\[\n\\arg\\min_{x\\in \\mathbb R}{f(x)}\n\\] , which means \\(x\\) that minimizes \\(f(x)\\).\n\n\nDefinition 6 A point \\(x^* \\in X\\) is called a global maximizer of \\(f\\) if \\(f(x^*) \\ge f(x)\\) for all \\(x \\in \\mathbb R\\). \\[\n\\arg\\max_{x\\in \\mathbb R}{f(x)}\n\\] , which means \\(x\\) that maximizes \\(f(x)\\).\n\n\nDefinition 7 A point \\(x^* \\in X\\) is called a strict global maximizer of \\(f\\)\nif \\(f(x^*) > f(x)\\) for all \\(x \\in \\mathbb R\\).\n\n\nDefinition 8 A point \\(x^* \\in X\\) is called a strict global miniimizer of \\(f\\)\nif \\(f(x^*) < f(x)\\) for all \\(x \\in \\mathbb R\\).\n\n\nDefinition 9 Let \\(f: X \\to \\mathbb{R}\\) be a real-valued function defined on a domain \\(X \\subseteq \\mathbb{R}\\), and let \\(x^* \\in X\\) be a point in \\(X\\). We say that \\(x^*\\) is a local minimizer of \\(f(x)\\) if there exists a radius \\(\\delta>0\\) of a neighborhood \\(N(x^*)\\) of \\(x^*\\) or \\(|x-x^*|<\\delta\\) such that \\(f(x^*) \\leq f(x)\\) for all \\(x \\in N(x^*) \\cap X\\).\n\n\nDefinition 10 Let \\(f: X \\to \\mathbb{R}\\) be a real-valued function defined on a domain \\(X \\subseteq \\mathbb{R}\\), and let \\(x^* \\in X\\) be a point in \\(X\\). We say that \\(x^*\\) is a local maximizer of \\(f(x)\\) if there exists a radius \\(\\delta>0\\) of a neighborhood \\(N(x^*)\\) of \\(x^*\\) or \\(|x-x^*|<\\delta\\) such that \\(f(x^*) \\geq f(x)\\) for all \\(x \\in N(x^*) \\cap X\\).\n\n\nDefinition 11 Let \\(f: X \\to \\mathbb{R}\\) be a real-valued function defined on a domain \\(X \\subseteq \\mathbb{R}\\), and let \\(x^* \\in X\\) be a point in \\(X\\). We say that \\(x^*\\) is a strict local minimizer of \\(f(x)\\) if there exists a radius \\(\\delta>0\\) of a neighborhood \\(N(x^*)\\) of \\(x^*\\) or \\(|x-x^*|<\\delta\\) such that \\(f(x^*) < f(x)\\) for all \\(x \\in N(x^*) \\cap X\\).\n\n\nDefinition 12 Let \\(f: X \\to \\mathbb{R}\\) be a real-valued function defined on a domain \\(X \\subseteq \\mathbb{R}\\), and let \\(x^* \\in X\\) be a point in \\(X\\). We say that \\(x^*\\) is a strict local maximizer of \\(f(x)\\) if there exists a radius \\(\\delta>0\\) of a neighborhood \\(N(x^*)\\) of \\(x^*\\) or \\(|x-x^*|<\\delta\\) such that \\(f(x^*) > f(x)\\) for all \\(x \\in N(x^*) \\cap X\\).\n\n\nDefinition 13 It is said to be a critical point if \\(f'(x)\\) exists and \\(f'(x^*)=0\\) for \\(x^* \\in X\\).\n\nNote that a function may have multiple minimizers, and some functions may not have a minimizer at all.\nExample\n\nIf \\(f(x)=2(x-3)^2+8\\), then\nthe vertex is \\((3,8)\\), the global minimizer is \\(3\\), the global minimum is \\(8\\).\nIf \\(f(x)=x^3-3x^2+4\\), then\n\\(f'(x)=3x^2-6x=3x(x-2)\\) and the critical points are \\((0,4), (2,0)\\).\nIf \\(f(x)=7x^5-35x+4\\), then\n\\(f'(x)=35x^4-35=35(x^2+1)(x-1)(x+1)\\) critical points are \\((-1,0), (1,32)\\).\nIf \\(f(x)=\\frac{2x}{x^2+1}\\), then\n\\(f'(x)=\\frac{2(1-x)(1+x)}{(1+x^2)^2}\\) the critical points are \\((-1,-1), (1,1)\\).\nIf \\(f(x)=4x^5-\\frac{20}{3}x^3+4\\), then\n\\(f'(x)=20x^4-20x^2=20(x^2)(x-1)(x+1)\\) critical points are \\((-1,\\frac{20}{3}), (1,\\frac{4}{3})\\).\nIf \\(f(x)=\\frac{(x^2-1)}{(x-2)}\\), then\n\\(f'(x)=\\frac{(x^2-4x+1)}{(x-2)^2}\\), critical points are \\((-1,\\frac{20}{3}), (1,\\frac{4}{3})\\).\nIf \\(f(x)=\\mathrm{e}^{\\sin\\left(x^2+1\\right)}\\), then\n\\(f'(x)=2x\\mathrm{e}^{\\sin\\left(x^2+1\\right)}\\cos\\left(x^2+1\\right)\\) , \\(f''(x)=-2\\mathrm{e}^{\\sin\\left(x^2+1\\right)}\\cdot\\left(2x^2\\sin\\left(x^2+1\\right)-2x^2\\cos^2\\left(x^2+1\\right)-\\cos\\left(x^2+1\\right)\\right)\\), critical points are \\((0,e^{\\sin(1)}), (\\pm\\sqrt{(2n-1)\\frac{\\pi}{2}-1} )\\) where \\(n=1,2,\\dots\\).\nIf \\(f(x)=3x^4-4x^3+1\\), then\n\\(f'(x)=12x^3-12x^2=12x^2(x-1)\\) , \\(f''(x)=36x^2-24x=12x(3x-2)\\), critical points are \\((0,1), (1,0)\\).\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    return 2*(x-3)**2+8\ndef f2(x):\n    return x**3-3*x**2+4\ndef f3(x):\n    return 7*x**5-35*x+4\ndef f4(x):\n    return 2*x/(x**2+1)\ndef f5(x):\n    return 4*x**5-(20/3)*x**3+4\ndef f6(x):\n    return 3*x**4-4*x**3+1\n\ndef df(x):\n    return 4*(x-3)\ndef df2(x):\n    return 3*x**2-6*x\ndef df3(x):\n    return 35*x**4-35\ndef df4(x):\n    return 2*(1-x)*(1+x)/(1+x**2)**2\ndef df5(x):\n    return 20*x**4-20*x**2\ndef df6(x):\n    return 12*x**2*(x-1)\n\ndef ddf(n):\n    return np.repeat(4,n)\ndef ddf2(x):\n    return 6*x-6\ndef ddf3(x):\n    return 140*x**3\ndef ddf4(x):\n    return (4*x*(x**2-3))/(x**2+1)**3\ndef ddf5(x):\n    return 80*x**3-40*x\ndef ddf6(x):\n    return 12*x*(3*x-2)\n\n\n\n\nCode\n# Create a range of x values\nx = np.linspace(-2, 8, 1000)\n\n# Plot the function\nplt.plot(x, f(x), label=r'$f(x)=2(x-3)^2+8$')\nplt.plot(x, df(x), label=r'$df(x)=4(x-3)$')\nplt.plot(x, ddf(len(x)), label=r'$d^2f(x)=4$')\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nx = np.linspace(-1, 4, 1000)\n\nplt.plot(x, f2(x), label=r'$f(x)=x^3-3x^2+4$')\nplt.plot(x, df2(x), label=r'$df(x)=3x^2-6x=3x(x-2)$')\nplt.plot(x, ddf2(x), label=r'$d^2f(x)=6x-6=6(x-1)$')\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\n# Create a range of x values\nx = np.linspace(-1.2, 1.2, 1000)\n\n# Plot the function\nplt.plot(x, f3(x), label=r'$f(x)=7x^5-35x+4$')\nplt.plot(x, df3(x), label=r'$df(x)=35x^4-35=35(x^2+1)(x-1)(x+1)$')\nplt.plot(x, ddf3(x), label=r'$d^2f(x)=120x^3$')\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nx = np.linspace(-2, 4, 1000)\n\nplt.plot(x, f4(x), label=r'$\\frac{2x}{x^2+1}$')\nplt.plot(x, df4(x), label=r'$df(x)=\\frac{2(1-x)(1+x)}{(1+x^2)^2}$')\nplt.plot(x, ddf4(x), label=r'$d^2f(x)=\\frac{4x(x^2-3)}{(1+x^2)^3}$')\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nx = np.linspace(-2, 2, 1000)\n\n# Plot the function\nplt.plot(x, f5(x), label=r'$f(x)=4x^5-\\frac{20}{3}x^3+4$')\nplt.plot(x, df5(x), label=r'$df(x)=20x^4-20x^2=20(x^2)(x-1)(x+1)$')\nplt.plot(x, ddf5(x), label=r'$d^2f(x)=80x^3-40x=40x(x-1)(x+1)$')\n\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\nplt.axis([-2, 2, -20, 20])\nplt.legend()\nplt.show()\n\n\n\n\n\n\nI felt too annoyed to type this latex and make the function…\n\nTheorem 1 For \\(f:X \\rightarrow \\mathbb R\\), let \\(f(x)\\), \\(f'(x)\\), and \\(f''(x)\\) be all continuous. For \\(x^* \\in X\\), \\(f'(x^*)=0\\)\n\nIf \\(f''(x)\\ge 0\\) for all \\(x \\in X\\), \\(x^*\\) is a global minimizer\nIf \\(f''(x)> 0\\) for all \\(x \\in X\\), \\(x^*\\) is a strict global minimizer\nIf \\(f''(x^*)> 0\\), \\(x^*\\) is a strict local minimizer\n\n\nthe reverse of the stament 1 in Theorem 1 is not true.\nCounter Example\nIf \\(f(x)=3x^4-4x^3+1\\), then\n\\(f'(x)=12x^3-12x^2=12x^2(x-1)\\) , \\(f''(x)=36x^2-24x=12x(3x-2)\\), critical points are \\((0,1), (1,0)\\).\n\\(x^*=1\\) is a global minimizer. For \\(x\\in \\mathbb R\\), \\(f''(x)\\ge 0\\) is not true.\n\n\nCode\nx = np.linspace(-1, 1.5, 1000)\n\n# Plot the function\nplt.plot(x, f6(x), label=r'$f(x)=3x^4-4x^3+1$')\nplt.plot(x, df6(x), label=r'$df(x)=12x^3-12x^2=12x^2(x-1)$')\nplt.plot(x, ddf6(x), label=r'$d^2f(x)=36x^2-24x=12x(3x-2)$')\n\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\nplt.axis([-1, 1.5, -5, 5])\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 Go to Blog Content List\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/taylor_series/index.html",
    "href": "docs/blog/posts/Mathmatics/taylor_series/index.html",
    "title": "Taylor’s Series",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 A sequence is a list of numbers written in a definite order: \\[\na_1, a_2, a_3, \\dots, a_n, a_{n+1} \\dots  \n\\]\nthe number \\(a_1\\), \\(a_2\\), and \\(a_n\\) are called the first term, the second term, the nth term. Since for every positive inter \\(n\\) there is a corresponding number \\(a_n\\), a sequence can be defined as a funtion with domain of the set of positive integers. The notation of the sequence function is \\(a_n\\) instead of \\(f(n)\\) in convention: \\[\n\\text{The sequence } \\{a_1,a_2,a_3,\\dots\\} \\text{ is also denoted by } \\{a_n\\} \\text{ or } \\{a_n\\}_{n=1}^{\\infty}\n\\] .\n\n\nDefinition 2 A series or infinite series is defined as a sum of the terms of an infinite sequence \\(\\{a_n\\}_{n=1}^{\\infty}\\): \\[\na_1+ a_2+ a_3+ \\dots+ a_n+ a_{n+1} \\dots  \n\\]\nThe notation of a series is: \\(\\sum_{n=1}^{\\infty} a_n\\) or \\(\\sum a_n\\).\n\n\nDefinition 3 A power series is a series of the following form: \\[\n\\sum_{n=0}^{\\infty} c_nx^n = c_0x^0+c_1x^1+c_2x^2+ \\dots+ c_nx^n+c_{n+1}x^{n+1}+\\dots  \n\\]\nwhere \\(x\\) is a variable and the \\(c_n\\)’s are constants called the coefficients of the series.\n\n\nDefinition 4 A power series centered at a is a series of the following form: \\[\n\\begin{aligned}\n    \\sum_{n=0}^{\\infty} c_n(x-a)^n &= c_0(x-a)^0+c_1(x-a)^1+c_2(x-a)^2+ \\dots+ c_n(x-a)^n+c_{n+1}(x-a)^{n+1}+\\dots  \\\\\n                                    &= c_0+c_1(x-a)^1+c_2(x-a)^2+ \\dots+ c_n(x-a)^n+c_{n+1}(x-a)^{n+1}+\\dots  \n\\end{aligned}\n\\]\nwhere \\(x\\) is a variable and the \\(c_n\\)’s are constants called the coefficients of the series.\n\n\nTheorem 1 \\(f\\) is said to be a expanded power series centered at a : \\[\n\\text{if }f(x)=\\sum_{n=0}^{\\infty} c_n(x-a)^n |x-a|<R, \\text{ then, its coefficients are given by the formula } c_n=\\frac{f^{(n)}(a)}{n!}\n\\]\nwhere \\(x\\) is a variable and the \\(c_n\\)’s are constants called the coefficients of the series.\n\nProof)\nLet \\(f\\) is any function that can be represented by a powerseries.\n$$\n\\[\\begin{aligned}\n    \n    f(x) &= c_0+c_1(x-a)^1+c_2(x-a)^2+ \\dots+ c_n(x-a)^n+c_{n+1}(x-a)^{n+1}+\\dots \\text{  }|x-a|<R\\\\\n    f(a) &= 0\\\\\n    f'(x) &= c_1+2c_2(x-a)+ 3c_3(x-a)^2+ \\dots \\text{  }|x-a|<R\\\\ \\\\\n    f'(a) &= c_1 \\\\\n    f''(x) &= 2c_2(x-a)+ 2\\times 3c_3(x-a)+ 3\\times 4c_4(x-a)^2+ \\dots \\text{  }|x-a|<R\\\\\n    f''(a) &= 2c_2 \\\\\n    f'''(x) &= 3!c_3 + 4! c_4(x-a)+3\\times 4 \\times 5 c_4(x-a)^2 \\dots \\text{  }|x-a|<R\\\\\n    f'''(a) &= 3!c_3 \\\\\n    \\vdots \\\\\n    f^{(n)}(a) &= n!c_n \\\\\n    c_n&=\\frac{f^{(n)}}{n!}\n\\end{aligned}\\]\n$$\n\nDefinition 5 \\(f\\) is said to be a Taylor’s series if f has a expanded power series at a with the following form: \\[\n\\begin{aligned}\n    f(x)&=\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n!}(x-a)^n \\\\\n        &= \\frac{f^{(0)}(a)}{0!}(x-a)^0+\\frac{f^{(1)}(a)}{1!}(x-a)^1+\\frac{f^{(2)}(a)}{2!}(x-a)^2+\\frac{f^{(0)}(a)}{0!}(x-a)^3 + \\dots \\\\\n        &= f(a)+\\frac{f^{(1)}(a)}{1!}(x-a)^1+\\frac{f^{(2)}(a)}{2!}(x-a)^2+\\frac{f^{(3)}(a)}{3!}(x-a)^3 + \\dots\n\\end{aligned}\n\\]\n\n\nDefinition 6 \\(f\\) is said to be a Maclaurin series if f has a Taylor’s series with the special case \\(a=0\\): \\[\n\\begin{aligned}\n    f(x)&=\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(0)}{n!}(x)^n \\\\\n        &= \\frac{f^{(0)}(0)}{0!}x^0+\\frac{f^{(1)}(0)}{1!}x^1+\\frac{f^{(2)}(0)}{2!}x^2+\\frac{f^{(3)}(0)}{3!}x^3 + \\dots \\\\\n        &= f(0)+\\frac{f^{(1)}(0)}{1!}x^1+\\frac{f^{(2)}(0)}{2!}x^2+\\frac{f^{(3)}(0)}{3!}x^3 + \\dots\n\\end{aligned}\n\\]\n\n\n\nCode\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndegrees = (1, 3, 5, 7)\nls = ('-', '--', '-.', ':')\n\ndef  taylor_e(x, a, n) :\n    \"\"\"\n    x* = a 에서 전개\n    f(x) = f(a) + f'(a)*(x-a) + (1/2!)f''(a)(x-a)^2 + ... + (1/k!)f^(k)(a)(x-a)^k + R_k\n    \"\"\"\n    signs  = (1, -1, -1, 1)\n    derivs = (np.cos, np.sin, np.cos, np.sin)\n\n    fx =  np.sin(a) \n    \n    for i in range(1, n+1) : \n        fx += (signs[(i%4)-1]*derivs[(i%4)-1](a)) / math.factorial(i)*(x-a)**i\n    \n    return fx\n    \nx = np.linspace(-10, 10, 100)\ny = np.sin(x)\n\nfig = plt.figure(figsize=(12,5))\nax = fig.add_subplot(1, 1, 1)\n\nax.xaxis.set_tick_params(labelsize=12)\nax.yaxis.set_tick_params(labelsize=12)\nax.set_xlabel(r'$x$', fontsize=15)\nax.set_ylabel(r'$x$', fontsize=15)\nax.grid(False)\n\ntaylors = (taylor_e(x, 0, i) for i in degrees)\nax.plot(x, y , lw=3, color='gray', \n        label=r\"sin(x)\")\n\nfor i, taylor in enumerate(taylors) :\n    ax.plot(x, taylor, lw=2, ls=ls[i], color='k', \n            label=\"degree {}\".format(degrees[i]))\n\n\nax.legend(fontsize=11)\nax.set_ylim([-5, 5])\n\n# plt.suptitle(\"Taylor series, order=1,2,3\", fontsize=15)\n\n\nplt.show()\n\n\n\n\n\n\nTheorem 2 If \\(f(x)\\) is differentiable (and therefore continuous) \\(n\\) times on \\([a,b]\\) , then there exists \\(c\\) such that \\[\n\\begin{aligned}\n    f(b)&=f(a)+f'(a)(b-a)+\\frac{f^{(2)}(a)^2}{2!}(b-a)^2+\\dots+\\\\\n    &\\frac{f^{(n-1)}(a)}{(n-1)!}(b-a)^{n-1} + \\frac{f^{(n)}(c)}{n!}(b-a)^{n}, a<c<b\n\\end{aligned}\n\\] .\n\nProof) find \\(k\\) such that \\(f(b)-(f(a)+f'(a)(b-a)+\\frac{f^{(2)}(a)^2}{2!}(b-a)^2+\\dots+\\frac{f^{(n-1)}(a)}{(n-1)!}(b-a)^{n-1} + k(b-a)^{n})=0\\).\n\\[\n\\begin{aligned}\n    \\text{Let }F(x)&=f(b)-(f(x)+f'(x)(b-x)+\\frac{f^{(2)}(a)^2}{2!}(b-x)^2+\\dots+\\\\\n    &\\frac{f^{(n-1)}(x)}{(n-1)!}(b-x)^{n-1} + k(b-x)^{n})\n\\end{aligned}\n\\]\nThen, \\(F(x)\\) is differentiable on \\([a,b]\\). In addition, since \\(F(a)=F(b)=0\\), there exists \\(c\\) such that \\(F'(c)=0\\), \\(a<c<b\\) by the Mean Value Theorem. Thus,\n\\[\n\\begin{aligned}\n  F'(x)&=-\\frac{f^{(n)}(x)}{(n-1)!}(b-x)^{n-1}+kn(b-x)^{n-1}\\\\\n  F'(c)&=-\\frac{f^{(n)}(c)}{(n-1)!}(b-c)^{n-1}+kn(b-c)^{n-1}=0\\\\\n  kn&=\\frac{f^{(n)}(c)}{(n-1)!}\\\\\n  \\therefore k&=\\frac{f^{(n)}(c)}{n!}\n\\end{aligned}\n\\]\n\nTheorem 3 If \\(f\\) is differentiable (and therefore continuous) \\(n\\) times on \\([a,b]\\) , \\(x^*,x \\in [a,b]\\), and \\(x\\ne x^*\\), there exists \\(\\theta\\) such that \\(0<\\theta<1\\) and\n\\[\n\\begin{aligned}\n    f(x)&=f(x^*)+f'(x^*)(x-x^*)+\\frac{f^{(2)}(x^*)^2}{2!}(x-x^*)^2+\\dots+\\\\\n    &\\frac{f^{(n-1)}(x^*)}{(n-1)!}(x-x^*)^{n-1} + \\frac{f^{(n)}(x^*+\\theta(x-x^*))}{n!}(x-x^*)^{n}\n\\end{aligned}\n\\] .\n\nIn the above expression (Theorem 3), \\(f(x)\\) can be expressed with finite terms because it is differentiable \\(n\\) times. The symbol \\(\\theta\\) represents a value between 0 and 1, and it is used in the context of Taylor’s theorem with remainder (see Definition 5). The general form of Taylor’s theorem with remainder is:\n\\[\nf(x) = f(x^*) + \\frac{f'(x^*)}{1!}(x-x^*) + \\frac{f''(x^*)}{2!}(x-x^*)^2 + ... + \\frac{f^{(n)}(x^*)}{n!}(x-x^*)^n + R_n(x)\n\\]\nwhere \\(R_n(x)\\) is the remainder term that involves the \\(n+1\\) th derivative of f evaluated at some point \\(c\\) between \\(x\\) and \\(x^*\\):\n\\[\n\\begin{aligned}\n  R_n(x)&=f(x)- (f(x^*) + \\frac{f'(x^*)}{1!}(x-x^*) + \\frac{f''(x^*)}{2!}(x-a)^2 + ... + \\frac{f^{(n)}(x^*)}{n!}(x-x^*)^n)\\\\\n  R_n(x)&= \\frac{f^{(n+1)}(c)}{(n+1)!}(x-x^*)^{n+1}\n\\end{aligned}\n\\]\nIn the given expression, \\(x^*+\\theta(x-x^*)\\) is the value of \\(c\\) that lies between \\(x\\) and \\(x^*\\), where \\(\\theta\\) is a scalar value between \\(0\\) and \\(1\\). In other words, \\(c\\) is an internally dividing point, \\(i\\) that divides the segment \\(\\overline{xx^*}\\) in the ratio \\(\\overline{x^*i}:\\overline{ix}=\\theta:(1-\\theta)\\) because \\(x^*+\\theta(x-x^*)=x^*(1-\\theta)+\\theta x\\) .Therefore, \\(x^*+\\theta(x-x^*)\\) of the remainder term in the expression is somewhere between \\(x and x^*\\):\n\\[\n\\frac{f^{(n)}(x^*+\\theta(x-x^*))}{(n)!}(x-x^*)^{n}\n\\]\n\n\n\\(f(x)=x^3-3x^2+4\\)\n\n\nCode\ndef f(x):\n    return x**3-3*x**2+4\ndef df(x):\n    return 3*x**2-6*x\ndef d2f(x):\n    return 6*x-6\ndef d3f(n):\n    return np.repeat(6,n)\n\n\n\n# Define the Taylor series expansion up to the 3rd order\ndef taylor(x):\n    return f(0) + df(0)*x + d2f(0)*(x**2)/2 + d3f(len(x))*(x**3)/6 \n\n# Create a range of x values\nx = np.linspace(-np.pi, np.pi, 100)\n\n# Calculate the function and its approximation using the Taylor series expansion\ny = f(x)\ny2 = df(x)\ny3 = d2f(x)\ny_approx = taylor(x)\n\n# Plot the function and its approximation\n\nplt.plot(x, y, '--',lw=5, label=r'$f(x)=x^3-3x^2+4$')\nplt.plot(x, df(x), label=r'$f(x)=3x^2-6x$')\nplt.plot(x, d2f(x), label=r'$f(x)=6x-6$')\nplt.plot(x, d3f(len(x)), label=r'$f(x)=6$')\nplt.plot(x, y_approx, label='Taylor Approximation')\nplt.legend()\nplt.show()\n\n\n\n\n\nIf \\(f(x)=x^3-3x^2+4\\), then \\(\\frac{df(x)}{dx}=3x^2-6x\\), \\(\\frac{d^2f(x)}{dx^2}=6x-6\\), \\(\\frac{d^3f(x)}{dx^3}=6\\), \\(\\frac{d^nf(x)}{dx^n}=0, n\\ge 4\\).\nFor \\(x\\ne x^*\\),\n\\[\n\\begin{aligned}\n  f(x)&=f(x^*)+f'(x^*)(x-x^*)+\\frac{f^{(2)}(x^*)^2}{2!}(x-x^*)^2+\\frac{f^{(3)}(x^*+\\theta(x-x^*))^3}{3!}(x-x^*)^3\\\\\n  f(x)&=f(x^*)+(3x^2-6x)(x-x^*)+\\frac{(6x-6)}{2!}(x-x^*)^2+\\frac{6}{3!}(x-x^*)^3\\\\\n  f(x)&=f(0)+(3x^2-6x)(x)+\\frac{(6x-6)}{2!}(x)^2+\\frac{6}{3!}(x)^3 \\text{ } (x^*=0)\\\\\n  f(x)&=x^3-3x^2+4\n\\end{aligned}     \n\\]\nIf f is differentated 3 times, f is expressed with \\(\\theta\\). But, if differentiated more than 4 times, the \\(\\theta\\) disappears.\n\n\n\nThe second derivative test is a method used to determine whether a critical point of a function is a local maximum, local minimum, or a saddle point. The test uses the sign of the second derivative of the function at the critical point to determine its nature. In other words, the second derivative test uses the sign of the second derivative to determine the concavity of the function at a critical point, which in turn determines whether the critical point is a local maximum, local minimum, or a saddle point.\n\nDefinition 7 Let \\(f\\) be a function with a critical point at \\(x^*\\). If \\(f\\) is twice differentiable at \\(x^*\\), then:\n\nIf \\(f''(x^*) > 0\\), then \\(f\\) has a local minimum at \\(x^*\\).\nIf \\(f''(x^*) < 0\\), then \\(f\\) has a local maximum at \\(x^*\\).\nIf \\(f''(x^*) = 0\\) and there exist values of \\(x\\) close to \\(x^*\\) such that \\(f''(x) < 0\\) and \\(f''(x) > 0\\), then \\(f\\) has a saddle point at \\(x^*\\).\nIf \\(f''(x^*) = 0\\) and there do not exist values of \\(x\\) close to \\(x^*\\) such that \\(f''(x) < 0\\) and \\(f''(x) > 0\\), then the test is inconclusive and we may need to use other methods to determine the nature of the critical point.\n\n\nThe second derivative test is a method used to determine the nature of a critical point of a function by examining the concavity of the function at that point.\nIf the second derivative of the function is positive at a critical point, then the function is concave up at that point, and the critical point is a local minimum. If the second derivative is negative, then the function is concave down, and the critical point is a local maximum. If the second derivative is zero, the test is inconclusive, and the other methods should be tried to determine the nature of the critical point. If there exist values of \\(x\\) close to \\(x^*\\) such that \\(f''(x) < 0\\) and \\(f''(x) > 0\\), then \\(f\\) has a saddle point at \\(x^*\\) (see Figure 1).\n\n\n\n\n\n\nWhat Is a Saddle Point?\n\n\n\nA saddle point is a type of critical point of a function where the first-order partial derivatives of the function are zero, but the behavior of the function around the point is neither a local maximum nor a local minimum. Instead, the behavior is like a saddle shape, hence the name “saddle point” (see Figure 1).\nAt a saddle point, the function changes concavity in different directions, meaning that the function is concave up in some directions and concave down in other directions. In other words, the Hessian matrix of the function (the matrix of second-order partial derivatives) evaluated at the saddle point has both positive and negative eigenvalues, indicating that the curvature of the function changes in different directions.\nSaddle points are important in optimization and machine learning because they can cause difficulties in finding the global minimum of a function. At a saddle point, gradient-based optimization algorithms can get stuck because the gradient is zero but the curvature of the function prevents the algorithm from moving in a direction that decreases the function value. This can result in slow convergence or even convergence to a suboptimal solution.\n\n\n\n\n\nFigure 1: Saddle Point Example\n\n\nSourced from Wiki By Nicoguaro - Own work, CC BY 3.0\nThus, for the problem of finding the maximum and minimum, it is usually sufficient for \\(f\\) to be differentiable twice in a taylor series.\n\\[\n\\begin{aligned}\n    f(x)&=f(x^*)+f'(x^*)(x-x^*)+\\frac{f^{(2)}(x^*+\\theta(x-x^*))}{2!}(x-x^*)^{2} ,\\text{ } 0<\\theta<1\n\\end{aligned}\n\\tag{1}\\]\nIt is the case that \\(f'(x^*)=0\\) to find \\(x^*\\) that makes the extrema (minimum or maximum) of \\(f\\). So, we can set \\(f'(x^*)\\) in Equation 1 as \\(0\\). Then,\n\\[\n\\begin{aligned}\n    f(x)&=f(x^*)+\\frac{f^{(2)}(x^*+\\theta(x-x^*))}{2!}(x-x^*)^{2} ,\\text{ } 0<\\theta<1\n\\end{aligned}\n\\tag{2}\\]\nIn Equation 2, we can make a certain conclusion on the second derivative test depending on the sign of the variable (not a constant because of \\(\\theta\\)), \\(f^{(2)}(x^*+\\theta(x-x^*))\\) because \\((x-x^*)^{2}>0\\):\n\nIf \\(f'(x^*)=0\\), \\(x \\in [a,b]\\), and \\(f''(x)>0\\), then \\(f(x)=f(x^*)+d, \\text{ } (d>0)\\)\n\\(\\therefore f(x)>f(x^*)\\), which means that \\(f(x)\\) has a minimum at \\(x^*\\).\nIf \\(f'(x^*)=0\\), \\(x \\in [a,b]\\), and \\(f''(x)<0\\), then \\(f(x)=f(x^*)-d, \\text{ } (d>0)\\)\n\\(\\therefore f(x)<f(x^*)\\), which means that \\(f(x)\\) has a maximum at \\(x^*\\).\n\n\n\nIf \\(f(x)=e^{x^2}, f'(x)=2xe^{x^2}, \\text{ and } f^{''}(x)=(2+4x^2)e^{x^2}\\), the case \\(f'(x^*)=0\\) is when \\(x^*=0\\). Since \\(f^{''}(x)>0\\) for all \\(x \\in \\mathbb{R}\\), \\(f(0)=1\\) is a minimum at \\(x^*\\).\n\\(f(x)=e^{x^2}\\)\n\n\nCode\ndef f(x):\n    return np.exp(x**2)\ndef df(x):\n    return 2*x*np.exp(x**2)\ndef d2f(x):\n    return (2+4*x**2)*np.exp(x**2)\n\n# Create a range of x values\nx = np.linspace(-1, 1, 100)\n\n# Calculate the function and its approximation using the Taylor series expansion\ny = f(x)\ny2 = df(x)\ny3 = d2f(x)\ny_approx = taylor(x)\n\n# Plot the function and its approximation\n\nplt.plot(x, y, label=r\"$f(x)=e^{x^2}$\")\nplt.plot(x, df(x), label=r\"$f'(x)=2xe^{x^2}$\")\nplt.plot(x, d2f(x), label=r\"$f^{''}(x)=(2+4x^2)e^{x^2}$\")\nplt.axhline(y=0, color='gray')\nplt.axvline(x=0, color='gray')\n\nplt.plot(x, y_approx, label='Taylor Approximation')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/transformation/index.html",
    "href": "docs/blog/posts/Mathmatics/transformation/index.html",
    "title": "Transofrmations of Functions",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nThanslations are about vertical and horizontal sifts. To be more sepecific, if \\(c\\) is a positive number, then\n\n\\(y=f(x)+c\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units upward\n\\(y=f(x)-c\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units downward\n\\(y=f(x-c)\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units to the right\n\\(y=f(x+c)\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units to the left\n\n\n\ndraw \\(y=x\\), \\(y=(x-3)\\), \\(y=x-3\\), \\(y=(x+3)\\), \\(y=x+3\\)\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = np.linspace(-10, 10, 1000)\ny = x\ny2 = x-3\ny3 = x+3\n\n\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.grid(True, which='both')\n\nplt.plot(x,y,color='black',label='y=x')\nplt.plot(x,y2,color='red',label='y=(x-3) or (y+3)=x')\nplt.plot(x,y3,color='blue',label='y=(x+3) or (y-3)=x')\n\nplt.title('Traslation of Functions')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThere are largely two types of transofrmations of functions: stretching and reflecting. To be more sepecific, if \\(c\\) is larger than 1, then\n\n\\(y=cf(x)\\), stretch the graph of \\(y=f(x)\\) vertically by a factor of \\(c\\)\n\\(y=\\frac{1}{c}f(x)\\), shrink the graph of \\(y=f(x)\\) vertically by a factor of \\(c\\)\n\\(y=f(cx)\\), shrink the graph of \\(y=f(x)\\) horizontally by a factor of \\(c\\)\n\\(y=f(\\frac{x}{c})\\), stretch the graph of \\(y=f(x)\\) horizontally by a factor of \\(c\\)\n\n\n\ndraw \\(y=\\sin x\\), \\(y=\\sin 2x\\), \\(y=\\frac{1}{2} \\sin x\\)\n\n\nCode\nx = np.linspace(-10, 10, 1000)\ny = np.sin(x)\ny2 = np.sin(2*x)\ny3 = np.sin(x/2)\ny4 = 2*np.sin(x)\ny5 = np.sin(x)/2\n\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.grid(True, which='both')\n\nplt.plot(x,y,color='black',label='y=sin(x)')\nplt.plot(x,y2,color='red',label='y=sin(2x)')\nplt.plot(x,y3,color='blue',label=r'y=sin($\\frac{x}{2}$)')\nplt.plot(x,y4,color='green',label=r'y=2sin(x)')\nplt.plot(x,y5,color='orange',label=r'y=$\\frac{1}{2}$sin(x) or (2y)=sin(x)')\n\nplt.title('Trasformation of Functions')\nplt.legend(shadow=True, loc=(-0.2, 1.05), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\n\n\n\ndraw \\(x^2+y^2=1\\), \\(\\frac{x^2}{4}+\\frac{y^2}{9}=1\\), \\((x-3)^2+(y-3)^2=1\\), \\(\\frac{(x-3)^2}{4}+\\frac{(y-3)^2}{9}=1\\)\n\n\n\nCode\n# initialize x and y using radian\n# theta = np.linspace(0, 2*np.pi, 1000)\n# x = np.cos(theta)\n# y = np.sin(theta)\n\n# initialize x and y without using radian\nx = np.linspace(-1, 1, 1000)\ny1 = np.sqrt(1 - x*x)\ny2 = -np.sqrt(1 - x*x)\n\nfig, ax = plt.subplots()\n\n# unit circle\nax.plot(x, y1, color='black', label=r'$x^2+y^2=1$')\nax.plot(x, y2, color='black')\nax.plot(0,0,'o',color='black', label='(0,0)')\n\n# unit circle translated by 2\nax.plot(x+3, y1+3, color='red', label=r'$(x-3)^2+(y-3)^2=1$')\nax.plot(x+3, y2+3, color='red')\nax.plot(3,3,'o',color='red', label='(3,3)')\n\n# eplipse: a unit circle transformed by 2,3 in x, y\nax.plot(2*x, 3*y1, color='blue', label=r'$\\frac{x^2}{4}+\\frac{y^2}{9}=1$')\nax.plot(2*x, 3*y2, color='blue')\nax.plot(0,-np.sqrt(9-4), 'o', color='blue', label=r'$F_1=(0,\\sqrt{5}),F_2=(0,-\\sqrt{5})$')\nax.plot(0,np.sqrt(9-4), 'o', color='blue')\n\n# translated eplipse: a unit circle translated by 2 and transformed by 2,3 in x, y\nax.plot(2*x+3, 3*y1+3, color='green', label=r'$\\frac{(x-3)^2}{4}+\\frac{(y-3)^2}{9}=1$')\nax.plot(2*x+3, 3*y2+3, color='green')\nax.plot(0+3,-np.sqrt(9-4)+3, 'o', color='green', label=r'$F_1=(3,\\sqrt{5}+3),F_2=(3,-\\sqrt{5}+3)$')\nax.plot(0+3,np.sqrt(9-4)+3, 'o', color='green')\n\n# height/width=1\nax.set_aspect(1)\n\nax.set_xlim([-4, 7])\nax.set_ylim([-4, 7])\n\nax.grid(True)\nax.set_title(\"Transformation of Functions\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\n\nplt.legend(shadow=True, loc=(-0.5, 1.05), handlelength=1.5, fontsize=8)\n\n# show the plot\nplt.show()\n\n\n\n\n\n\nTry using \\(y=\\sin (\\theta)\\), \\(x=\\cos (\\theta)\\), by yourself, to draw \\(x^2+y^2=1\\), \\(\\frac{x^2}{4}+\\frac{y^2}{9}=1\\), \\((x-3)^2+(y-3)^2=1\\), \\(\\frac{(x-3)^2}{4}+\\frac{(y-3)^2}{9}=1\\)\n\n\n\n\n\n\n\\(y=-f(x)\\), reflect the graph of \\(y=f(x)\\) about the x-axis\n\\(y=f(-x)\\), reflect the graph of \\(y=f(x)\\) about the y-axis\n\\(-y=f(-x)\\), reflect the graph of \\(y=f(x)\\) about the origin on the 2D plain\n\\(x=f(y)\\), reflect the graph of \\(y=f(x)\\) about the \\(y=x\\)\n\n\n\n\ndraw \\(y=\\sin x\\), \\(y=\\sin (-x)\\), \\(y=-\\sin x\\), \\(-y=-\\sin x\\), \\(x=-\\sin y\\)\n\n\n\nCode\nx = np.linspace(0, 2*np.pi, 1000)\nx1 = np.linspace(-2*np.pi, 2*np.pi, 1000)\ny = np.sin(x)\n\nfig, ax = plt.subplots()\n\n# unit circle\nax.plot(x, y, color='black', label='$y=\\sin (x)$')\nax.plot(-x, y, color='blue', label='$y=\\sin (-x)$')\nax.plot(x, -y, color='green', label='$y=-\\sin (x)$')\nax.plot(-x, -y, color='orange', label='$-y=-\\sin (x)$')\nax.plot(x1,x1, color='red', label='$y=x$')\nax.plot(np.sin(y),x, color='black', label='$y=sin^{-1}(x)$',linestyle='dashed')\n\n# height/width=1\nax.set_aspect(1)\n\nax.set_xlim([-7, 7])\nax.set_ylim([-7, 7])\n\nax.grid(True)\nax.set_title(\"Reflection of Functions\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\n# show the plot\nplt.show()\n\n\n\n\n\n\nDraw \\(S(x)=\\frac{1}{1+e^{-x}}\\)\n\n\n\nCode\nx = np.linspace(-10, 10, 1000)\ny = 1/(1+np.exp(-x))\n\nfig, ax = plt.subplots()\n\n# unit circle\nax.plot(x, y, color='black', label='$y=S(x)$')\nax.plot(-x, y, color='blue', label='$y=S(-x)$')\nax.plot(x, -y, color='green', label='$y=-S(x)$')\nax.plot(-x, -y, color='orange', label='$-y=-S(x)$')\nax.plot(x,x, color='red', label='$y=x$')\nax.plot(np.sin(y),x, color='black', label='$y=S^{-1}(x)$',linestyle='dashed')\n\n# height/width=1\nax.set_aspect(1)\n\nax.set_xlim([-3, 3])\nax.set_ylim([-3, 3])\n\nax.grid(True)\nax.set_title(\"Reflection of Functions\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nDistribution of Raw Data\n\n\n\nCode\ndata1 = np.random.normal(loc=5,scale=6.0,size=1000) #loc=mean, scale=sd\ndata2 = np.random.normal(loc=-4,scale=2.0,size=1000)\ndata3 = np.random.normal(loc=-7,scale=2.5,size=1000)\ndata4 = np.random.normal(loc=0,scale=1.0,size=1000)\n\n# the range of values to evaluate the PDF\nx = np.linspace(data2.min(), data1.max(), 10000)\n\nbins_number=100\n\n# Plot the data and PDF\nplt.hist(data1, density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(5,6^2)$')\nplt.hist(data2, density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-4,2^2)$')\nplt.hist(data3, density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-7,2.5^2)$')\nplt.hist(data4, density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Raw Data\")\nplt.show()\n\n\n\n\n\n\nDistribution of Translated Raw Data\n\n\n\nCode\n# Plot the data and PDF\nplt.hist(data1-data1.mean(), density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(5-5,6^2)$')\nplt.hist(data2-data2.mean(), density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-4-(-4),2^2)$')\nplt.hist(data3-data3.mean(), density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-7-(-7),2.5^2)$')\nplt.hist(data4-data4.mean(), density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0-0,1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Translated Raw Data\")\nplt.show()\n\n\n\n\n\n\nDistribution of Transformed Raw Data\n\n\n\nCode\n# Plot the data and PDF\nplt.hist(data1/data1.std(), density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(5,6^2/6^2)$')\nplt.hist(data2/data2.std(), density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-4,2^2/2^2)$')\nplt.hist(data3/data3.std(), density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-7,2.5^2/5^2)$')\nplt.hist(data4/data4.std(), density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1/1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Transformed Raw Data\")\nplt.show()\n\n\n\n\n\n\nDistribution of Standardized Raw Data\n\n\n\nCode\n# Plot the data and PDF\nplt.hist((data1-data1.mean())/data1.std(), density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\nplt.hist((data2-data2.mean())/data2.std(), density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\nplt.hist((data3-data3.mean())/data3.std(), density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\nplt.hist((data4-data4.mean())/data4.std(), density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Standardized Raw Data\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTransformation in Statistics? ex) transformation of random variable\nTransformation in Linear Algebra? ex) linear transformation or linear mapping (transformation matrix)\nTransformation in Machine Learning? ex) scaling (min-max normalization or standardization)\n\n\n\n\n\n\n\n\nThanslations are about vertical and horizontal sifts. To be more sepecific, if \\(c\\) is a positive number, then\n\n\\(y=f(x)+c\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units upward\n\\(y=f(x)-c\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units downward\n\\(y=f(x-c)\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units to the right\n\\(y=f(x+c)\\), shift the graph of \\(y=f(x)\\) a distance \\(c\\) units to the left\n\n\n\ndraw \\(y=x\\), \\(y=(x-3)\\), \\(y=x-3\\), \\(y=(x+3)\\), \\(y=x+3\\)\n\n\nCode\nx = np.linspace(-10, 10, 1000)\ny = x\ny2 = x-3\ny3 = x+3\n\n\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.grid(True, which='both')\n\nplt.plot(x,y,color='black',label='y=x')\nplt.plot(x,y2,color='red',label='y=(x-3) or (y+3)=x')\nplt.plot(x,y3,color='blue',label='y=(x+3) or (y-3)=x')\n\nplt.title('Traslation of Functions')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThere are largely two types of transofrmations of functions: stretching and reflecting. To be more sepecific, if \\(c\\) is larger than 1, then\n\n\\(y=cf(x)\\), stretch the graph of \\(y=f(x)\\) vertically by a factor of \\(c\\)\n\\(y=\\frac{1}{c}f(x)\\), shrink the graph of \\(y=f(x)\\) vertically by a factor of \\(c\\)\n\\(y=f(cx)\\), shrink the graph of \\(y=f(x)\\) horizontally by a factor of \\(c\\)\n\\(y=f(\\frac{x}{c})\\), stretch the graph of \\(y=f(x)\\) horizontally by a factor of \\(c\\)\n\n\n\ndraw \\(y=\\sin x\\), \\(y=\\sin 2x\\), \\(y=\\frac{1}{2} \\sin x\\)\n\n\nCode\nx = np.linspace(-10, 10, 1000)\ny = np.sin(x)\ny2 = np.sin(2*x)\ny3 = np.sin(x/2)\ny4 = 2*np.sin(x)\ny5 = np.sin(x)/2\n\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.grid(True, which='both')\n\nplt.plot(x,y,color='black',label='y=sin(x)')\nplt.plot(x,y2,color='red',label='y=sin(2x)')\nplt.plot(x,y3,color='blue',label=r'y=sin($\\frac{x}{2}$)')\nplt.plot(x,y4,color='green',label=r'y=2sin(x)')\nplt.plot(x,y5,color='orange',label=r'y=$\\frac{1}{2}$sin(x) or (2y)=sin(x)')\n\nplt.title('Trasformation of Functions')\nplt.legend(shadow=True, loc=(-0.2, 1.05), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\n\n\n\ndraw \\(x^2+y^2=1\\), \\(\\frac{x^2}{4}+\\frac{y^2}{9}=1\\), \\((x-3)^2+(y-3)^2=1\\), \\(\\frac{(x-3)^2}{4}+\\frac{(y-3)^2}{9}=1\\)\n\n\n\nCode\n# initialize x and y using radian\n# theta = np.linspace(0, 2*np.pi, 1000)\n# x = np.cos(theta)\n# y = np.sin(theta)\n\n# initialize x and y without using radian\nx = np.linspace(-1, 1, 1000)\ny1 = np.sqrt(1 - x*x)\ny2 = -np.sqrt(1 - x*x)\n\nfig, ax = plt.subplots()\n\n# unit circle\nax.plot(x, y1, color='black', label=r'$x^2+y^2=1$')\nax.plot(x, y2, color='black')\nax.plot(0,0,'o',color='black', label='(0,0)')\n\n# unit circle translated by 2\nax.plot(x+3, y1+3, color='red', label=r'$(x-3)^2+(y-3)^2=1$')\nax.plot(x+3, y2+3, color='red')\nax.plot(3,3,'o',color='red', label='(3,3)')\n\n# eplipse: a unit circle transformed by 2,3 in x, y\nax.plot(2*x, 3*y1, color='blue', label=r'$\\frac{x^2}{4}+\\frac{y^2}{9}=1$')\nax.plot(2*x, 3*y2, color='blue')\nax.plot(0,-np.sqrt(9-4), 'o', color='blue', label=r'$F_1=(0,\\sqrt{5}),F_2=(0,-\\sqrt{5})$')\nax.plot(0,np.sqrt(9-4), 'o', color='blue')\n\n# translated eplipse: a unit circle translated by 2 and transformed by 2,3 in x, y\nax.plot(2*x+3, 3*y1+3, color='green', label=r'$\\frac{(x-3)^2}{4}+\\frac{(y-3)^2}{9}=1$')\nax.plot(2*x+3, 3*y2+3, color='green')\nax.plot(0+3,-np.sqrt(9-4)+3, 'o', color='green', label=r'$F_1=(3,\\sqrt{5}+3),F_2=(3,-\\sqrt{5}+3)$')\nax.plot(0+3,np.sqrt(9-4)+3, 'o', color='green')\n\n# height/width=1\nax.set_aspect(1)\n\nax.set_xlim([-4, 7])\nax.set_ylim([-4, 7])\n\nax.grid(True)\nax.set_title(\"Transformation of Functions\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\n\nplt.legend(shadow=True, loc=(-0.5, 1.05), handlelength=1.5, fontsize=8)\n\n# show the plot\nplt.show()\n\n\n\n\n\n\nTry using \\(y=\\sin (\\theta)\\), \\(x=\\cos (\\theta)\\), by yourself, to draw \\(x^2+y^2=1\\), \\(\\frac{x^2}{4}+\\frac{y^2}{9}=1\\), \\((x-3)^2+(y-3)^2=1\\), \\(\\frac{(x-3)^2}{4}+\\frac{(y-3)^2}{9}=1\\)\n\n\n\n\n\n\n\\(y=-f(x)\\), reflect the graph of \\(y=f(x)\\) about the x-axis\n\\(y=f(-x)\\), reflect the graph of \\(y=f(x)\\) about the y-axis\n\\(-y=f(-x)\\), reflect the graph of \\(y=f(x)\\) about the origin on the 2D plain\n\\(x=f(y)\\), reflect the graph of \\(y=f(x)\\) about the \\(y=x\\)\n\n\n\n\ndraw \\(y=\\sin x\\), \\(y=\\sin (-x)\\), \\(y=-\\sin x\\), \\(-y=-\\sin x\\), \\(x=-\\sin y\\)\n\n\n\nCode\nx = np.linspace(0, 2*np.pi, 1000)\nx1 = np.linspace(-2*np.pi, 2*np.pi, 1000)\ny = np.sin(x)\n\nfig, ax = plt.subplots()\n\n# unit circle\nax.plot(x, y, color='black', label='$y=\\sin (x)$')\nax.plot(-x, y, color='blue', label='$y=\\sin (-x)$')\nax.plot(x, -y, color='green', label='$y=-\\sin (x)$')\nax.plot(-x, -y, color='orange', label='$-y=-\\sin (x)$')\nax.plot(x1,x1, color='red', label='$y=x$')\nax.plot(np.sin(y),x, color='black', label='$y=sin^{-1}(x)$',linestyle='dashed')\n\n# height/width=1\nax.set_aspect(1)\n\nax.set_xlim([-7, 7])\nax.set_ylim([-7, 7])\n\nax.grid(True)\nax.set_title(\"Reflection of Functions\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\n# show the plot\nplt.show()\n\n\n\n\n\n\nDraw \\(S(x)=\\frac{1}{1+e^{-x}}\\)\n\n\n\nCode\nx = np.linspace(-10, 10, 1000)\ny = 1/(1+np.exp(-x))\n\nfig, ax = plt.subplots()\n\n# unit circle\nax.plot(x, y, color='black', label='$y=S(x)$')\nax.plot(-x, y, color='blue', label='$y=S(-x)$')\nax.plot(x, -y, color='green', label='$y=-S(x)$')\nax.plot(-x, -y, color='orange', label='$-y=-S(x)$')\nax.plot(x,x, color='red', label='$y=x$')\nax.plot(np.sin(y),x, color='black', label='$y=S^{-1}(x)$',linestyle='dashed')\n\n# height/width=1\nax.set_aspect(1)\n\nax.set_xlim([-3, 3])\nax.set_ylim([-3, 3])\n\nax.grid(True)\nax.set_title(\"Reflection of Functions\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nDistribution of Raw Data\n\n\n\nCode\ndata1 = np.random.normal(loc=5,scale=6.0,size=1000) #loc=mean, scale=sd\ndata2 = np.random.normal(loc=-4,scale=2.0,size=1000)\ndata3 = np.random.normal(loc=-7,scale=2.5,size=1000)\ndata4 = np.random.normal(loc=0,scale=1.0,size=1000)\n\n# the range of values to evaluate the PDF\nx = np.linspace(data2.min(), data1.max(), 10000)\n\nbins_number=100\n\n# Plot the data and PDF\nplt.hist(data1, density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(5,6^2)$')\nplt.hist(data2, density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-4,2^2)$')\nplt.hist(data3, density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-7,2.5^2)$')\nplt.hist(data4, density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Raw Data\")\nplt.show()\n\n\n\n\n\n\nDistribution of Translated Raw Data\n\n\n\nCode\n# Plot the data and PDF\nplt.hist(data1-data1.mean(), density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(5-5,6^2)$')\nplt.hist(data2-data2.mean(), density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-4-(-4),2^2)$')\nplt.hist(data3-data3.mean(), density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-7-(-7),2.5^2)$')\nplt.hist(data4-data4.mean(), density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0-0,1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Translated Raw Data\")\nplt.show()\n\n\n\n\n\n\nDistribution of Transformed Raw Data\n\n\n\nCode\n# Plot the data and PDF\nplt.hist(data1/data1.std(), density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(5,6^2/6^2)$')\nplt.hist(data2/data2.std(), density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-4,2^2/2^2)$')\nplt.hist(data3/data3.std(), density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(-7,2.5^2/5^2)$')\nplt.hist(data4/data4.std(), density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1/1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Transformed Raw Data\")\nplt.show()\n\n\n\n\n\n\nDistribution of Standardized Raw Data\n\n\n\nCode\n# Plot the data and PDF\nplt.hist((data1-data1.mean())/data1.std(), density=True, color=\"red\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\nplt.hist((data2-data2.mean())/data2.std(), density=True, color=\"green\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\nplt.hist((data3-data3.mean())/data3.std(), density=True, color=\"blue\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\nplt.hist((data4-data4.mean())/data4.std(), density=True, color=\"black\", alpha=0.5, bins=bins_number,label=r'$X \\sim N(0,1)$')\n\nplt.legend(shadow=True, loc=(-0.25, 1.05), handlelength=1.5, fontsize=8)\nplt.title(\"Distribution of Standardized Raw Data\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTransformation in Statistics? ex) transformation of random variable\nTransformation in Linear Algebra? ex) linear transformation or linear mapping (transformation matrix)\nTransformation in Machine Learning? ex) scaling (min-max normalization or standardization)\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/Mathmatics/variables/index.html",
    "href": "docs/blog/posts/Mathmatics/variables/index.html",
    "title": "Variable Types",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nVariable types can be classified with various perspectives depending on research purpose:\n\nFrom the perspective of a data type, variable types are largely divided into two categories:\n\nquantitative variable: a variable containing quantitative data that represents quantity\ncategorical variable: a variable containing qualitative data that represents groups\n\nFrom the standpoint of modeling or experiment designs, variable types are largely divided into 3 categories:\n\nindependent variable: a variable (cause) that might have an effect on a dependent variabe (result).\ndependent variable: a variable (result) that might be influenced by independent variables (cause).\ncontrol variable: a variable that is fixed to look into a relation between an independent variable in your interest and dependent variable.\n\nFrom the point of mathmatical view, variable types are categorized largely into 4 categories:\n\nunivariable: each subject gives rise to a single measurement of independent variable termed exploratory variable.\nunivariate: each subject gives rise to a single measurement of dependent variables termed response.\nmultivariate: each subject gives rise to a vector of measurements of independent variables termed exploratory variables.\nmultivariable: each subject gives rise to a vector of measurements of dependent variables termed responses.\n\n\n\n\n\n\nthe values of the quantitative variables with which you can conduct arithematic operations. There are two types of quantitative variables: discrete and continuous.\n\n\n\nAs integer, count valuess of individual items.\nex: number of people, number of different events, etc.\n\n\n\n\n\nAs real number, measurement values of continuous or uncountable values.\nex: height, weight, distance, volume, age, etc.\n\n\n\n\n\nCategorical variables contain grouping values representing categories rather than quantity. There are three types of categorical variables: binary, nominal, and ordinal variables.\n\n\n\nBinary variables a.k.a dichotomous variables contain two types of values, true or false, 1 or 0\n\nex: disease/non-disease, heads/tails in flipping a coin, win/lose in a game\n\n\n\n\n\ncatogories with no rank or order among them.\nex: gender, races, colors, brands, company names\n\n\n\n\n\ncatogories ranked in a specific order\nex: ranks in a game, places in a line, rating scale responses in a movie review\n\n\n\n\n\n\nExperiments or models are usually designed or built to discover what effect one variable has on another.\n\n\n\nIndependent variable is a variable you can set to observe an effect on the outcome of an experiment.\n\nBy many people, independent Variables are also commonly called predictors, explanatory variables, treatment variables, features, etc.\n\n\n\n\n\nDependent variable is a variable that represents the outcome of the experiment.\nBy many people, dependent variables are also commonly called outcome variables, response variables, targets, etc.\n\n\n\n\n\nControl variable is a variable that is held fixed throughout the experiment.\nPositive control: a variable that is set for showing effect on the dependent variable.\nNegative control: a variable that is set for showing no effect on the dependent variable.\nInternal control: a variable that is set for showing effect on the dependent variable with a researcher’s certain intention.\n\n\n\n\n\n\n\nFYI\n\n\n\nStrictly speaking, the synonyms of independent and dependent variables are all slightly different for the different purpose.\n\nIn association research, the use of the terms “dependent” and “independent” should be avoided because the research does not focus on causality between one another.\n\nWhen the before-and-after relationship is clear, there might be cases where one variable clearly precedes the other\n\nfor example, rainfall leads to mud, rather than the other way around.\nIn these cases, you may call the rainfall a predictor and the mud an outcome variable.\n\n\n\n\n\n\n\n\nThere are largely 3 types of variables: confounders, latent variables, and composite variables\n\n\n\nConfounding variables or confounders\n\nConfounder is a variable that hides the true effect of another variable in an experiment by confounding the association between independent and dependent variables. This can happen when the 3rd variable has effect on both independent variable and dependent variable but the 3rd variable has not been controlled in your experiment. Confounders run a high risk of introducing a variety of research biases to your analysis result, particularly omitted variable bias.\n\nex: When conducting a study on muscle mass increase for dumbbells in a gym, if gender is not included in the research model, gender is a confounder. This is because men and women have different innate muscle mass and baseline for lifting dumbbells.\n\n\n\n\n\nLatent variable is a variable that can’t be measured directly but indirectly via a proxy.\nex: lactose tolerance of a person cannot be measured directly but indirectly inferred from measurements of a person’s can be inferred from measurements of digestion ability with biochemical metrics in a certain designed experiment.\n\n\n\n\n\nComposite variable is a variable made by combining multiple variables of your data. These variables are created not when you measure it but when you analyze data,\nex: When your academic performance is measured with math, physics, literature, and writing composition, your numerical academic performance can be measured by combining math with physics, and your language academic performance by combining literature with writing composition.\n\n\n\n\n\n\nunivariable: each subject gives rise to a single measurement of independent variable termed exploratory variable.\nunivariate: each subject gives rise to a single measurement of dependent variables termed response.\nmultivariate: each subject gives rise to a vector of measurements of independent variables termed exploratory variables.\nmultivariable: each subject gives rise to a vector of measurements of dependent variables termed responses.\n\n\n\n\n\nData types can also be classified with various perspectives depending on research purpose:\n\nFrom the perspective of programming or computer science data type\n\n\n\n\n\n\n\n\n\nData Type\nDefinition\nExamples\n\n\n\n\nInteger (int)\nNumeric data type for numbers without fractions\n-707, 0, 707\n\n\nFloating Point (float)\nNumeric data type for numbers with fractions\n707.07, 0.7, 707.00\n\n\nCharacter (char)\nSingle letter, digit, punctuation mark, symbol, or blank space\na, 1, !\n\n\nString (str or text)\nSequence of characters, digits, or symbols—always treated as text\nhello, +1-999-666-3333\n\n\nBoolean (bool)\nTrue or false values\n0 (false), 1 (true)\n\n\nEnumerated type (enum)\nSmall set of predefined unique values (elements or enumerators) that can be text-based or numerical\nrock (0), jazz (1)\n\n\nArray\nList with a number of elements in a specific order—typically of the same type\nrock (0), jazz (1), blues (2), pop (3)\n\n\nDate\nDate in the YYYY-MM-DD format (ISO 8601 syntax)\n2021-09-28\n\n\nTime\nTime in the hh:mm:ss format for the time of day, time since an event, or time interval between events\n12:00:59\n\n\nDatetime\nDate and time together in the YYYY-MM-DD hh:mm:ss format\n2021-09-28 12:00:59\n\n\nTimestamp\nNumber of seconds that have elapsed since midnight (00:00:00 UTC), 1st January 1970 (Unix time)\n1632855600\n\n\n\n\nFrom the perspective of data measurement\n\nlongitudinal (or repeated) data: Each subject gives rise to a vector of measurements, but these represent the same response measured at a sequence of observation times\ncross-sectional data : Outcome variable(s) and covariates that are measured at a single time point\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScribbr\n\n\n\n\n\n\n\n\n\n\n\n\nProject Content List\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/ML/2023-02-03_pytorch_introduction/index.html",
    "href": "docs/blog/posts/ML/2023-02-03_pytorch_introduction/index.html",
    "title": "Pytorch Introduction",
    "section": "",
    "text": "아직 GPU를 못잡았음 -> Google Colab에서만 돌아감"
  },
  {
    "objectID": "docs/blog/posts/ML/2023-02-03_pytorch_introduction/index.html#pytorch-overview",
    "href": "docs/blog/posts/ML/2023-02-03_pytorch_introduction/index.html#pytorch-overview",
    "title": "Pytorch Introduction",
    "section": "Pytorch Overview",
    "text": "Pytorch Overview\n\nPyTorch는 기계 학습 프레임워크(framework) 중 하나다.\n\nPyTorch의 텐서(tensor)는 NumPy 배열과 매우 유사하다.\nTensor flow 보다 사용 비중이 늘어나고 있다.\nTensor: 고차원 데이터 (배열)를 의미, 3차원 배열 이상\n\nPyTorch를 사용하면, GPU 연동을 통해 효율적으로 딥러닝 모델을 학습할 수 있다.\nGoogle Colab을 이용하면, 손쉽게 PyTorch를 시작할 수 있다.\nGoogle Colab에서는 [런타임] - [런타임 유형 변경]에서 GPU를 선택할 수 있다.\nGoogle Colab에선 pytoch가 내장되어 있기 때문에 따로 설치할 필요 없음\n\n\nGPU 사용 여부 체크하기\n\n텐서간의 연산을 수행할 때, 기본적으로 두 텐서가 같은 장치에 있어야 한다.\n연산을 수행하는 텐서들을 모두 GPU에 올린 뒤에 연산을 수행한다.\nGPU는 고차원 행렬곱같은 병렬 처리 연산에 최적화 되어 있다.\n\ntensor 자체가 고차원 배열이기 때문에 데이터를 불러오면 tensor 형태로 바꿀 수 있다.\n\n\n텐서(tensor) 객체 생성 (기본 python 데이터 유형)\n\n\nCode\nimport torch\n\n# data initialization: 초기화된 데이터는 gpu에 있음\ndata = [\n  [1, 2],\n  [3, 4]\n]\n\nx = torch.tensor(data) # list를 tensor 형태로 바꾸기. \nprint(x.is_cuda)\n\nx = x.cuda() # CPU -> GPU로 옮기기\nprint(x.is_cuda)\n\nx = x.cpu() # GPU -> CPU로 옮기기\nprint(x.is_cuda)\n\n\n\n서로 다른 장치(device)에 있는 텐서끼리 연산을 수행하면 오류가 발생한다.\n\n\n\nCode\n# GPU 장치의 텐서\na = torch.tensor([\n    [1, 1],\n    [2, 2]\n]).cuda()\n\n# CPU 장치의 텐서\nb = torch.tensor([\n    [5, 6],\n    [7, 8]\n])\n\n# print(torch.matmul(a, b)) # 오류 발생\nprint(torch.matmul(a.cpu(), b))\n\n\n\n\n2. 텐서 소개 및 생성 방법\n\n1) 텐서의 속성\n\n텐서의 기본 속성으로는 다음과 같은 것들이 있다.\n\n모양(shape): 텐서 객체의 차원을 확인할 수 있다. (tensor_var.shape)\n자료형(data type) : 텐서의 기본 자료형은 float type (tensor_var.dtype)\n저장된 장치: CPU인지 GPU인지 확인 (tensor_var.device)\n\n\n\n\nCode\ntensor = torch.rand(3, 4)\n\nprint(tensor)\nprint(f\"Shape: {tensor.shape}\")\nprint(f\"Data type: {tensor.dtype}\")\nprint(f\"Device: {tensor.device}\")\n\n\n\n\n2) 텐서 초기화\n\n리스트 데이터에서 직접 텐서를 초기화할 수 있다.\n\n\n\nCode\ndata = [\n  [1, 2],\n  [3, 4]\n]\nx = torch.tensor(data)\n\nprint(x)\n\n\n\nNumPy 배열에서 텐서를 초기화할 수 있다.\n\n\n\nCode\na = torch.tensor([5])\nb = torch.tensor([7])\n\nc = (a + b).numpy() # tensor -> numpy array\nprint(c)\nprint(type(c)) # ndarray: numpy array \n\nresult = c * 10\ntensor = torch.from_numpy(result) # numpy array -> tensor \nprint(tensor)\nprint(type(tensor))\n\n\n\n\n3) 다른 텐서로부터 data를 가져와 텐서 초기화하기\n\n다른 텐서의 정보를 토대로 텐서를 초기화할 수 있다.\n텐서의 속성은 모양(shape)과 자료형(data type)이 있다\n\n\n\nCode\nx = torch.tensor([\n    [5, 7],\n    [1, 2]\n])\n\n# x와 같은 shape와 data type을 가지지만, 값이 1인 텐서 생성\nx_ones = torch.ones_like(x)\nprint(x_ones)\n# x와 같은 shape를 가지되, 자료형은 float으로 덮어쓰고, 값은 랜덤으로 채우기\nx_rand = torch.rand_like(x, dtype=torch.float32) # uniform distribution [0, 1)\nprint(x_rand)\n\n\n\n\n\n3. 텐서의 형변환 및 차원 조작\n\n텐서는 넘파이(NumPy) 배열처럼 조작할 수 있다.\n\n\n1) 텐서의 특정 차원 접근하기\n\n텐서의 원하는 차원에 접근할 수 있다.\n\n\n\nCode\ntensor = torch.tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12]\n])\n\nprint(tensor[0]) # the first row\nprint(tensor[:, 0]) # indexing the first column with all the rows\n# whatever the previous columns are, indexing the last column with all the rows\nprint(tensor[..., -1]) \n\n\n\n\n2) 텐서 이어붙이기(Concatenate)\n\n두 텐서를 이어 붙여 연결하여 새로운 텐서를 만들 수 있다.\n\n\n\nCode\ntensor = torch.tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12]\n])\n\n# dim: 텐서를 이어 붙이기 위한 축\n# 0번 축(행)을 기준으로 이어 붙이기: 즉, row bind로 연결\nresult = torch.cat([tensor, tensor, tensor], dim=0) \nprint(result)\nprint(result.shape) # 9x4\n\n# 1번 축(열)을 기준으로 이어 붙이기: 즉, column bind로 연결\nresult = torch.cat([tensor, tensor, tensor], dim=1)  \nprint(result)\nprint(result.shape) # 3x12\n\n\n\n\n3) 텐서 형변환(Type Casting)\n\n텐서의 자료형(정수, 실수 등)을 변환할 수 있다.\n\n\n\nCode\na = torch.tensor([2], dtype=torch.int) # integers\nb = torch.tensor([5.0]) # real numbers\n\nprint(a.dtype)\nprint(b.dtype)\n\n# 텐서 a는 자동으로 float32형으로 형변환 처리\nprint(a + b)\n# 텐서 b를 int32형으로 형변환하여 덧셈 수행\nprint(a + b.type(torch.int32))\n\n\n\n\n4) 텐서의 모양 변경\n\nview()는 텐서의 shape를 변경할 때 사용한다.\n이때, 텐서(tensor)의 순서는 변경되지 않는다.\n\n\n\nCode\n# view()는 텐서의 모양을 변경할 때 사용한다.\n# 이때, 텐서(tensor)의 순서는 변경되지 않는다.\na = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8])\nb = a.view(4, 2) # 4*2=8 개, # call by reference\nprint(b)\n\n# a의 값을 변경하면 b도 변경: 메모리 주소값을 공유\na[0] = 7\nprint(b)\n\n# a의 값을 복사(copy)한 뒤에 변경, 새로운 메모리값 할당\nc = a.clone().view(4, 2) # call by value, 아예 다른 객체\na[0] = 9\nprint(c)\n\n\n\n\n5) 텐서의 차원 교환\n\n하나의 텐서에서 특정한 차원끼리 순서를 교체할 수 있다.\n\n\n\nCode\na = torch.rand((64, 32, 3))\nprint(a.shape)\n\nb = a.permute(2, 1, 0) # 차원을 바꿔줌\n# (2번째 축, 1번째 축, 0번째 축)의 형태가 되도록 한다.\nprint(b.shape)\n\n\n\n\n\n4. 텐서의 연산과 함수\n\n1) 텐서의 연산\n\n텐서에 대하여 사칙연산 등 기본적인 연산을 수행할 수 있다.\n\n\n\nCode\n# 같은 크기를 가진 두 개의 텐서에 대하여 사칙연산 가능\n# 기본적으로 요소별(element-wise) 연산, 행렬의 연산과 다름\na = torch.tensor([\n    [1, 2],\n    [3, 4]\n])\nb = torch.tensor([\n    [5, 6],\n    [7, 8]\n])\nprint(a + b)\nprint(a - b)\nprint(a * b)\nprint(a / b)\n\n\n\n행렬 곱을 수행할 수 있다.\n\n\n\nCode\na = torch.tensor([\n    [1, 2],\n    [3, 4]\n])\nb = torch.tensor([\n    [5, 6],\n    [7, 8]\n])\n# 행렬 곱(matrix multiplication) 수행\nprint(a.matmul(b))\nprint(torch.matmul(a, b))\n\n\n\n\n2) 텐서의 평균 함수\n\n텐서의 평균(mean)을 계산할 수 있다.\n\n\n\nCode\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.mean()) # 전체 원소에 대한 평균\nprint(a.mean(dim=0)) # 각 열에 대하여 평균 계산\nprint(a.mean(dim=1)) # 각 행에 대하여 평균 계산\n\n\n\n\n3) 텐서의 합계 함수\n\n텐서의 합계(sum)를 계산할 수 있다.\n\n\n\nCode\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.sum()) # 전체 원소에 대한 합계\nprint(a.sum(dim=0)) # 각 열에 대하여 합계 계산\nprint(a.sum(dim=1)) # 각 행에 대하여 합계 계산\n\n\n\n\n4) 텐서의 최대 함수\n\nmax() 함수는 원소의 최댓값을 반환한다.\nargmax() 함수는 가장 큰 원소(최댓값)의 인덱스를 반환한다.\n\n\n\nCode\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.max()) # 전체 원소에 대한 최댓값\nprint(a.max(dim=0)) # 각 열에 대하여 최댓값 계산\nprint(a.max(dim=1)) # 각 행에 대하여 최댓값 계산\n\n\n\n\nCode\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a)\nprint(a.argmax()) # 전체 원소에 대한 최댓값의 인덱스\nprint(a.argmax(dim=0)) # 각 열에 대하여 최댓값의 인덱스 계산\nprint(a.argmax(dim=1)) # 각 행에 대하여 최댓값의 인덱스 계산\n\n\n\n\n5) 텐서의 차원 줄이기 혹은 늘리기\n\nunsqueeze() 함수는 크기가 1인 차원을 추가한다.\n\n배치(batch) 차원을 추가하기 위한 목적으로 흔히 사용된다.\n\nsqueeze() 함수는 크기가 1인 차원을 제거한다.\n\n\n\nCode\na = torch.Tensor([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8]\n])\nprint(a.shape)\n\n# 첫 번째 축에 차원 추가\na = a.unsqueeze(0)\nprint(a)\nprint(a.shape)\n\n# 네 번째 축에 차원 추가\na = a.unsqueeze(3)\nprint(a)\nprint(a.shape)\n\n\n\n\nCode\n# 크기가 1인 차원 제거\na = a.squeeze()\nprint(a)\nprint(a.shape)\n\n\n\n\n\n5. 자동 미분과 기울기(Gradient)\n\nPyTorch에서는 연산에 대하여 자동 미분을 수행할 수 있다.\n각 텐서 변수에 대해 gradient추적이 가능하여 텐서 연산에 각 텐서 변수의 기울기(민감도)를 추적할 수 있다.\n\n\n\nCode\nimport torch\n\n# requires_grad를 설정할 때만 기울기 추적\nx = torch.tensor([3.0, 4.0], requires_grad=True)\ny = torch.tensor([1.0, 2.0], requires_grad=True)\nz = x + y #z를 연산하는데 x와 y의 민감도를 추적할 수 있다.\n# x or y의 민감도 즉 gradient가 크다는 것은 변수의 값이 조금만 바뀌어도 z값에 큰 영향을 미친다는것을 의미 \n\nprint(z) # [4.0, 6.0]\nprint(z.grad_fn) # 더하기(add), \n# AddBackward: 기울기를 구하는 과정에서 Add를 사용한다. 뭔뜻인지? ㅋ\n# Add를 연산하는 과정에서 기울기를 구하는거 아님?\n\nout = z.mean()\nprint(out) # 5.0\nprint(out.grad_fn) # 평균(mean)\n\nout.backward() # scalar에 대하여 모든 연산의 기울기를 추적 가능\nprint(x.grad) # tensor([0.5000, 0.5000]), 0.5: x의 값이 1만큼 바뀔 때 output값이 0.5만큼 바뀐다는것을 의미\nprint(y.grad) # tensor([0.5000, 0.5000]),\nprint(z.grad) # leaf variable에 대해서만 gradient 추적이 가능하다. 따라서 None.\n\n\n\n일반적으로 모델을 학습할 때는 기울기(gradient)를 추적한다.\n\n왜냐면, 가중치를 기울기에 따라 업데이트 해야하기 때문.\n\n하지만, 학습된 모델을 사용할 때는 파라미터를 업데이트하지 않으므로, 기울기를 추적하지 않는 것이 일반적이다.\n\n\n\nCode\ntemp = torch.tensor([3.0, 4.0], requires_grad=True)# tape,라 부름. 왜?\nprint(temp.requires_grad)\nprint((temp ** 2).requires_grad)\n\n# 기울기 추적을 하지 않기 때문에 계산 속도가 더 빠르다.\nwith torch.no_grad():\n    temp = torch.tensor([3.0, 4.0], requires_grad=True)\n    print(temp.requires_grad)\n    print((temp ** 2).requires_grad)"
  },
  {
    "objectID": "docs/blog/posts/ML/2023-02-03_tf_introduction/index.html",
    "href": "docs/blog/posts/ML/2023-02-03_tf_introduction/index.html",
    "title": "Tensor Introduction",
    "section": "",
    "text": "pytorch 이전 까지 deep learning을 위해 가장 많이 사용되었던 Framework\n2020년 이후로 pytorch를 더 많이 사용하지만 여전히 많은 사람들이 Tensor Flow 사용\n데이터 자료형으로 텐서(tensor) 객체를 사용\nTensorflow에서는 텐서(tensor)를 NumPy 배열처럼 사용할 수 있다.\nGPU 사용 지원\n\n\n\n\nGPU를 사용하면 TensorFlow나 pytorch에서 딥러닝 모델을 효과적 구현 가능\n각 텐서(tensor)와 연산이 어떠한 장치에 할당되었는지 출력할 수 있다.\n\n\n\nCode\nimport tensorflow as tf\n# placement 함수: 각 텐서와 연산이 어떠한 장치에 할당되었는지 출력하기\n#tf.debugging.set_log_device_placement(True)\n\n# 텐서 생성\na = tf.constant([\n    [1, 1],\n    [2, 2]\n])\nb = tf.constant([\n    [5, 6],\n    [7, 8]\n])\n\nc = tf.matmul(a, b)\nprint(\"matrix multiplication: \", c)\n\n#tf.debugging.set_log_device_placement(False)\n\n\nmatrix multiplication:  tf.Tensor(\n[[12 14]\n [24 28]], shape=(2, 2), dtype=int32)\n\n\n\n\nCode\nfrom tensorflow.python.client import device_lib\n# 구체적으로 사용 중인 장치(device) 정보 출력\ndevice_lib.list_local_devices()\n\n\n[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 7396388988253335861\n xla_global_id: -1]\n\n\n\n\n\n\nTensorFlow에서의 텐서(tensor)는 기능적으로 넘파이(NumPy)와 매우 유사하다.\n기본적으로 다차원 배열을 처리하기에 적합한 자료구조로 이해할 수 있다.\nTensorFlow의 텐서는 “자동 미분” 기능을 제공한다.\nTensorFlow는 기능적으로 Pytorch와 거의 같음, 하지만 문법이 불편함\nTensorFlow 2.0부터는 pytorch와 문법적으로 유사\n\n\n\n\n\n특징\n\n기본적으로 다차원 배열을 처리하기에 적합한 자료구조로 이해할 수 있다\nTensorFlow에서의 텐서(tensor)는 기능적으로 넘파이(NumPy)의 ndarray 객체와 유사\n기본 python 데이터 유형을 자동 변환 (e.g., list)\nTensorFlow의 텐서는 “자동 미분” 기능을 제공한다.\n\n속성\n\n크기 (shape)\n자료형 (data type)\n저장된 장치, 가속기 메모리에 상주 가능 (e.g., GPU )\n\nNumpy 배열과 tf.Tensor의 차이점\n\n텐서는 가속기 메모리(GPU, TPU)에서 사용 가능\n텐서는 불변성(immutable)\n\n\n\n\n\n\n\nCode\n# 기본적인 모양(shape), 자료형(data type) 출력\n\ndata = [\n    [1, 2],\n    [3, 4]\n]\nx = tf.constant(data) # list -> tensor object로 변환\nprint(x)\nprint(tf.rank(x)) # 축(axis)의 개수 출력 = 차원의 개수 출력\n\ndata = tf.constant(\"String\") # 문자열 (string)도 int형 tensor로 변환 가능\nprint(data)\n\n# NumPy 배열에서 텐서를 초기화할 수 있다.\n\n## 파이썬의 리스트 넘파이는 compatible하다. 상호보완적으로 교체가 가능\n\na = tf.constant([5])\nb = tf.constant([7])\n\nc = (a + b).numpy()\nprint(c)\nprint(type(c))\n\nresult = c * 10\ntensor = tf.convert_to_tensor(result) # numpy -> tensor\nprint(tensor)\nprint(type(tensor))\n\n\ntf.Tensor(\n[[1 2]\n [3 4]], shape=(2, 2), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(b'String', shape=(), dtype=string)\n[12]\n<class 'numpy.ndarray'>\ntf.Tensor([120], shape=(1,), dtype=int32)\n<class 'tensorflow.python.framework.ops.EagerTensor'>\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\nprint(tf.math.add(1, 2))\nprint(tf.math.add([1, 2], [3, 4]))\nprint(tf.math.square(5))\nprint(tf.math.reduce_sum([1, 2, 3]))\n\n# Operator overloading is also supported\nprint(tf.math.square(2) + tf.math.square(3))\n\ndata = [\n    [1,2],\n    [3,4]\n]\nx = tf.constant(data)\nprint(x)\nprint(x.shape)\nprint(x.dtype)\nprint(tf.rank(x)) # tf.rank() : 축(axis)의 개수 출력 (차원의 개수)\n\n\ntf.Tensor(3, shape=(), dtype=int32)\ntf.Tensor([4 6], shape=(2,), dtype=int32)\ntf.Tensor(25, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(13, shape=(), dtype=int32)\ntf.Tensor(\n[[1 2]\n [3 4]], shape=(2, 2), dtype=int32)\n(2, 2)\n<dtype: 'int32'>\ntf.Tensor(2, shape=(), dtype=int32)\n\n\n\n\n\n\nTensorFlow 연산은 자동으로 NumPy 배열을 텐서(tensor)로 변환\nNumPy 연산은 자동으로 텐서(tensor)를 NumPy 배열로 변환\n\n\n\nCode\nimport numpy as np\nndarray = np.ones([3, 3])\nndarray\n\ntensor = tf.math.multiply(ndarray, 42)\ntensor\nnp.add(tensor, 1)\ntensor.numpy() # numpy.ndarray\ntype(tensor.numpy())\nctensor = tf.constant(ndarray)\nctensor\n\n\n<tf.Tensor: shape=(3, 3), dtype=float64, numpy=\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])>\n\n\n\n\n\n\n텐서(tensor) 객체 생성 (tf.Tensor)\ntf.ones_like(x) : 값이 1이고 x와 shape & data type이 동일한 텐서 생성\n\n\n\nCode\nx = tf.constant([\n    [5, 7],\n    [3, 2]\n])\n\nx_ones = tf.ones_like(x)\nx_ones\n     \n\nx = tf.constant([\n    [5.1, 7.0],\n    [3.4, 2.1]\n])\n\nx_ones = tf.ones_like(x)\nx_ones\n\n# tf.random.uniform(shape, dtype) : 랜덤 값으로 원하는 shape과 dtype을 갖는 텐서 생성\nx_rand = tf.random.uniform(shape=x.shape, dtype=tf.float32)\nx_rand\n\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[0.4367851 , 0.24832237],\n       [0.23000729, 0.55093694]], dtype=float32)>\n\n\n\n\n\n특정 차원 접근\n\n\nCode\ntensor = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8],\n    [9,10,11,12]\n])\n\nprint(tensor[0])       # first row\nprint(tensor[:, 0])    # first column\nprint(tensor[..., -1]) # last column\n\n\ntf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\ntf.Tensor([1 5 9], shape=(3,), dtype=int32)\ntf.Tensor([ 4  8 12], shape=(3,), dtype=int32)\n\n\n텐서 Concatenate\naxis : 어느 축을 기준으로 객체를 이어붙일지 결정\naxis=0 : 0번째 축 (=row) axis=1 : 1번째 축 (=column)\n\n\nCode\ntensor = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8],\n    [9,10,11,12]\n])\n\ntensor_concat = tf.concat([tensor, tensor, tensor], axis=0) # row\ntensor_concat\n\ntensor_concat = tf.concat([tensor, tensor, tensor], axis=1) # column\ntensor_concat\n\n\n<tf.Tensor: shape=(3, 12), dtype=int32, numpy=\narray([[ 1,  2,  3,  4,  1,  2,  3,  4,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  5,  6,  7,  8,  5,  6,  7,  8],\n       [ 9, 10, 11, 12,  9, 10, 11, 12,  9, 10, 11, 12]])>\n\n\n\n\n\n\nCode\na = tf.constant([2])   # dtype: int32\nb = tf.constant([5.0]) # dtype: float32\n\nprint('a dtype: ', a.dtype, '\\nb dtype: ', b.dtype)\n\n\na dtype:  <dtype: 'int32'> \nb dtype:  <dtype: 'float32'>\n\n\n\n\nCode\na + b # dtype 불일치 -> InvalidArgumentError 발생\n\n\n\n\nCode\ntf.cast(a, tf.float32) + b # a의 dtype을 b의 dtype으로 변환 후 계산\n\n\n<tf.Tensor: shape=(1,), dtype=float32, numpy=array([7.], dtype=float32)>\n\n\n\n\n\n\n\nCode\nx = tf.Variable([1,2,3,4,5,6,7,8])\ny = tf.reshape(x, (4,2))           # row=4, col=2\ny\n\n\n<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4],\n       [5, 6],\n       [7, 8]])>\n\n\n\n\n\n\n\nCode\nx.assign_add([1,1,1,1,1,1,1,1])\nprint(x) # 1씩 더해짐\nprint(y) # 값 변화 X\n\n\n<tf.Variable 'Variable:0' shape=(8,) dtype=int32, numpy=array([2, 3, 4, 5, 6, 7, 8, 9])>\ntf.Tensor(\n[[1 2]\n [3 4]\n [5 6]\n [7 8]], shape=(4, 2), dtype=int32)\n\n\n\n\n\ntf.transpose(a, perm=[], ...) a의 차원 순서를 바꾼다. perm=[2, 1, 0]일 경우, a의 2번째 축을 첫번째로, 1번째 축을 두번째로, 0번째 축을 세번째로 교환하겠다는 의미\n\n\nCode\na = tf.random.uniform((64, 32, 3))\nprint(a.shape)\n\nb = tf.transpose(a, perm=[2, 1, 0]) # 차원 자체를 교환\nprint(b.shape)\n\n\n(64, 32, 3)\n(3, 32, 64)\n\n\n\n\n\nelement끼리 연산한다\n\n\nCode\na = tf.constant([\n    [1,2],\n    [3,4]\n])\nb = tf.constant([\n    [1,2],\n    [3,4]\n])\n\nprint(a + b)\nprint(a - b)\nprint(a * b)\nprint(a / b)\n\n\ntf.Tensor(\n[[2 4]\n [6 8]], shape=(2, 2), dtype=int32)\ntf.Tensor(\n[[0 0]\n [0 0]], shape=(2, 2), dtype=int32)\ntf.Tensor(\n[[ 1  4]\n [ 9 16]], shape=(2, 2), dtype=int32)\ntf.Tensor(\n[[1. 1.]\n [1. 1.]], shape=(2, 2), dtype=float64)\n\n\n\n\n\n\n\nCode\na = tf.constant([\n    [1,2],\n    [3,4]\n])\nb = tf.constant([\n    [1,2],\n    [3,4]\n])\ntf.matmul(a, b)\n\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 7, 10],\n       [15, 22]])>\n\n\n\n\n\n차원을 축소하며 평균을 계산\n\ntf.reduce_mean(a, axis=0) : 0차원(행)을 축소하여 평균 계산 -> 각 열에 대한 평균\ntf.reduce_mean(a, axis=1) : 1차원(열)을 축소하여 평균 계산 -> 각 행에 대한 평균\n\n\n\nCode\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\n\nprint(tf.reduce_mean(a))         # a 전체 평균\nprint(tf.reduce_mean(a, axis=0)) # 각 column에 대한 평균\nprint(tf.reduce_mean(a, axis=1)) # 각 row에 대한 평균\n\n\ntf.Tensor(4, shape=(), dtype=int32)\ntf.Tensor([3 4 5 6], shape=(4,), dtype=int32)\ntf.Tensor([2 6], shape=(2,), dtype=int32)\n\n\n\n\n\n차원을 축소하며 합계를 계산 (평균과 동일하게 동작)\n\n\nCode\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\n\nprint(tf.reduce_sum(a))         # a 전체 합계\nprint(tf.reduce_sum(a, axis=0)) # 각 column에 대한 합계\nprint(tf.reduce_sum(a, axis=1)) # 각 row에 대한 합계\n\n\ntf.Tensor(36, shape=(), dtype=int32)\ntf.Tensor([ 6  8 10 12], shape=(4,), dtype=int32)\ntf.Tensor([10 26], shape=(2,), dtype=int32)\n\n\n\n\n\n\nmax() : 원소의 최댓값 반환\nargmax() : 최댓값의 index를 반환\n\n\n\nCode\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\n\nprint(tf.reduce_max(a))         # a 전체 원소의 최댓값\nprint(tf.reduce_max(a, axis=0)) # 각 column에 대한 최댓값\nprint(tf.reduce_max(a, axis=1)) # 각 row에 대한 최댓값\nprint(tf.argmax(a, axis=0)) # 각 column에 대한 최댓값의 index\nprint(tf.argmax(a, axis=1)) # 각 row에 대한 최댓값의 index\n\n\ntf.Tensor(8, shape=(), dtype=int32)\ntf.Tensor([5 6 7 8], shape=(4,), dtype=int32)\ntf.Tensor([4 8], shape=(2,), dtype=int32)\ntf.Tensor([1 1 1 1], shape=(4,), dtype=int64)\ntf.Tensor([3 3], shape=(2,), dtype=int64)\n\n\n\n차원 축소\n\nsqueeze() : 크기가 1인 차원을 제거\n\n차원 확장\n\nexpand_dims() : 크기가 1인 차원을 추가\n흔히 배치(batch) 차원을 추가하기 위한 목적으로 사용됨\npytorch에서는 차원 축소 시, unsqueeze() 사용\n\n\n\n\nCode\na = tf.constant([\n    [1,2,3,4],\n    [5,6,7,8]\n])\nprint('original a shape: ', a.shape)\n\na = tf.expand_dims(a, 0) # 첫번째 축에 차원 추가\nprint('add 0th dims: ', a.shape)\n\na = tf.expand_dims(a, 3) # 세번째 축에 차원 추가\nprint('add 3rd dims: ', a.shape)\n\n\nprint(tf.squeeze(a).shape)         # 크기가 1인 차원을 모두 제거 \nprint(tf.squeeze(a, axis=3).shape) # 세번째 차원을 제거\n\n\n#tf.squeeze(a, axis=1) # 제거하려는 차원의 크기가 1이 아닐 경우 오류 발생\n\n\noriginal a shape:  (2, 4)\nadd 0th dims:  (1, 2, 4)\nadd 3rd dims:  (1, 2, 4, 1)\n(2, 4)\n(1, 2, 4)\n\n\n\n\n\n\n\n기울기 테이프 (Gradient Tape)\n중간 연산들을 테이프에 기록하고 역전파(back propagation)를 수행했을 때 기울기가 계산됨\nTensorFlow에서는 변수가 아닌 상수에 대해 기본적으로 기울기를 측정하지 않음 (not watched). 또한 변수여도 학습 가능하지 않으면(not trainable) 자동 미분을 사용하지 않음\n\n\n\nCode\nx = tf.Variable([3.0, 4.0])\ny = tf.Variable([3.0, 4.0])\n\n# with 구문 안에서 진행되는 모든 연산들을 기록\nwith tf.GradientTape() as tape:\n  z = x + y\n  loss = tf.math.reduce_mean(z)\n\ndx = tape.gradient(loss, x) # loss가 scalar이므로 계산 가능\nprint(dx)\n\n\ntf.Tensor([0.5 0.5], shape=(2,), dtype=float32)\n\n\nTensorFlow에서는 변수가 아닌 상수에 대해 기본적으로 기울기를 측정하지 않음 (not watched). 또한 변수여도 학습 가능하지 않으면(not trainable) 자동 미분을 사용하지 않음\n\n\nCode\nx = tf.linspace(-10, 10, 100) # -10 ~ 10까지 100r개의 데이터 생성\n\nwith tf.GradientTape() as tape:\n  tape.watch(x) # x에 대해 기울기를 측정할거니까 기록해줘\n  y = tf.nn.sigmoid(x)\n\ndx = tape.gradient(y, x)\nprint(dx)\n\n\ntf.Tensor(\n[4.53958077e-05 5.55575620e-05 6.79936937e-05 8.32130942e-05\n 1.01838442e-04 1.24631609e-04 1.52524715e-04 1.86658091e-04\n 2.28426653e-04 2.79536554e-04 3.42074339e-04 4.18591319e-04\n 5.12206458e-04 6.26731702e-04 7.66824507e-04 9.38173215e-04\n 1.14772200e-03 1.40394326e-03 1.71716676e-03 2.09997591e-03\n 2.56768332e-03 3.13889855e-03 3.83620191e-03 4.68693782e-03\n 5.72413978e-03 6.98759437e-03 8.52504404e-03 1.03935138e-02\n 1.26607241e-02 1.54065171e-02 1.87241696e-02 2.27213903e-02\n 2.75206964e-02 3.32587242e-02 4.00838615e-02 4.81513998e-02\n 5.76152215e-02 6.86149280e-02 8.12573764e-02 9.55919842e-02\n 1.11580066e-01 1.29060077e-01 1.47712989e-01 1.67034879e-01\n 1.86326443e-01 2.04710159e-01 2.21183725e-01 2.34711795e-01\n 2.44347497e-01 2.49363393e-01 2.49363393e-01 2.44347497e-01\n 2.34711795e-01 2.21183725e-01 2.04710159e-01 1.86326443e-01\n 1.67034879e-01 1.47712989e-01 1.29060077e-01 1.11580066e-01\n 9.55919842e-02 8.12573764e-02 6.86149280e-02 5.76152215e-02\n 4.81513998e-02 4.00838615e-02 3.32587242e-02 2.75206964e-02\n 2.27213903e-02 1.87241696e-02 1.54065171e-02 1.26607241e-02\n 1.03935138e-02 8.52504404e-03 6.98759437e-03 5.72413978e-03\n 4.68693782e-03 3.83620191e-03 3.13889855e-03 2.56768332e-03\n 2.09997591e-03 1.71716676e-03 1.40394326e-03 1.14772200e-03\n 9.38173215e-04 7.66824507e-04 6.26731702e-04 5.12206458e-04\n 4.18591319e-04 3.42074339e-04 2.79536554e-04 2.28426653e-04\n 1.86658091e-04 1.52524715e-04 1.24631609e-04 1.01838442e-04\n 8.32130942e-05 6.79936937e-05 5.55575620e-05 4.53958077e-05], shape=(100,), dtype=float64)\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.plot(x, y, 'r', label=\"y\")\nplt.plot(x, dx, 'b--', label=\"dy/dx\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "docs/blog/posts/ML/2023-02-06_naive_bayes/index.html",
    "href": "docs/blog/posts/ML/2023-02-06_naive_bayes/index.html",
    "title": "Naive Bayes Classification",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n얼굴 인식: 분류기로 얼굴, 코, 입, 눈 등과 같은 여러 특징을 식별\n날씨 예측: 날씨가 좋을지 나쁠지 예측\n의료 진단: 의료 전문가는 나이브 베이즈를 사용하여 환자가 심장병, 암 및 기타 질병과 같은 특정 질병 및 상태에 대한 고위험군인지 여부를 확인\n뉴스 분류: google 뉴스는 뉴스 유형을 분류\n쇼핑: 한 사람이 제품을 구매할지 여부를 예측하기 위해 요일, 할인 및 무료 배송의 특정 조합으로 나이브 베이즈 분류기 사용. 쇼핑한 날이 주중인지 주말인지 공휴일인지 기록하고 지정된 날짜에 대해 할인 및 무료 배송이 있는지 여부를 확인.\n\n\n\n\n\n간단하고 구현하기 쉬움\n훈련 데이터가 많이 필요하지 않음\n연속 데이터와 이산 데이터를 모두 처리\n예측 변수와 데이터 포인트의 수로 확장성이 뛰어남\n빠르고 실시간 예측에 사용\n관련 없는 특성에 민감하지 않음\n텍스트 분류는 나이브 베이즈 분류기의 가장 인기있는 응용 프로그램\n\n\n\n\n나이브 베이즈를 이용하면 조건이 주어질 때의 사건 발생 여부를 에측한다. 즉, 주어진 데이터를 이용해 사건 발생 확률 모형을 생성하고 새로운 데이터가 들어왔을 때 예측을 한다. 예측 결과는 사건이 발생할 확률과 사건이 발생하지 않을 확률이 출력되며 확률 높은 쪽을 선택하여 결과를 출력한다. 주어진 데이터는 조건이라고 가정하고 사건 조건부 발생 활률을 추정하는 것이다.\n조건이 주어졌을 때 조건부 확률을 계산하는 방식은 베이즈 정리를 이용한다.\n\n\n\n\n\n\n\n\n\n\nThe radius of the circle is 10."
  },
  {
    "objectID": "docs/blog/posts/ML/2023-02-06_naive_bayes/text_classification.html",
    "href": "docs/blog/posts/ML/2023-02-06_naive_bayes/text_classification.html",
    "title": "Kwangmin Kim",
    "section": "",
    "text": "# 필요 라이브러리 로딩\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nfrom sklearn.datasets import fetch_20newsgroups\n\ndata = fetch_20newsgroups()\nprint(data.target_names)\n\nModuleNotFoundError: No module named 'seaborn'\n\n\n\n# 모든 카테고리 정의\ncategories =['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', \n             'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', \n             'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', \n             'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', \n             'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', \n             'talk.politics.misc', 'talk.religion.misc']\n# 모든 카테고리 데이터 훈련하기\ntrain = fetch_20newsgroups(subset='train', categories=categories)\n# 모든 카테고리 데이터 테스트하기\ntest = fetch_20newsgroups(subset='test', categories=categories)\n\n\n# 훈련 데이터 보기\nprint(test.data[5])\n# print(train.data[5])\n#print(len(train.data))\n\nFrom: banschbach@vms.ocom.okstate.edu\nSubject: Re: Candida(yeast) Bloom, Fact or Fiction\nOrganization: OSU College of Osteopathic Medicine\nLines: 91\nNntp-Posting-Host: vms.ocom.okstate.edu\n\nIn article <1rp8p1$2d3@usenet.INS.CWRU.Edu>, esd3@po.CWRU.Edu (Elisabeth S. Davidson) writes:\n> \n> In a previous article, banschbach@vms.ocom.okstate.edu () says:\n>>least a few \"enlightened\" physicians practicing in the U.S.  It's really \n>>too bad that most U.S. medical schools don't cover nutrition because if \n>>they did, candida would not be viewed as a non-disease by so many in the \n>>medical profession.\n> \n> Case Western Reserve Med School teaches nutrition in its own section as\n> well as covering it in other sections as they apply (i.e. B12\n> deficiency in neuro as a cause of neuropathy, B12 deficiency in\n> hematology as a cause of megaloblastic anemia), yet I sill\n> hold the viewpoint of mainstream medicine:  candida can cause\n> mucocutaneous candidiasis, and, in already very sick patients\n> with damaged immune systems like AIDS and cancer patients,\n> systemic candida infection.  I think \"The Yeast Connection\" is\n> a bunch of hooey.  What does this have to do with how well\n> nutrition is taught, anyway?\n\nElisabeth, let's set the record straight for the nth time, I have not read \n\"The Yeast Connection\".  So anything that I say is not due to brainwashing \nby this \"hated\" book.  It's okay I guess to hate the book, by why hate me?\nElisabeth, I'm going to quote from Zinsser's Microbiology, 20th Edition.\nA book that you should be familiar with and not \"hate\". \"Candida species \ncolonize the mucosal surfaces of all humans during birth or shortly \nthereafter.  The risk of endogenous infection is clearly ever present.  \nIndeed, candidiasis occurs worldwide and is the most common systemic \nmycosis.\"  Neutrophils play the main role in preventing a systemic \ninfection(candidiasis) so you would have to have a low neutrophil count or \n\"sick\" neutrophils to see a systemic infection.  Poor diet and persistent \nparasitic infestation set many third world residents up for candidiasis.\nYour assessment of candidiasis in the U.S. is correct and I do not dispute \nit.\n\nWhat I posted was a discussion of candida blooms, without systemic \ninfection.  These blooms would be responsible for local sites of irritation\n(GI tract, mouth, vagina and sinus cavity).  Knocking down the bacterial \ncompetition for candida was proposed as a possible trigger for candida \nblooms.  Let me quote from Zinsser's again: \"However, some factors, such as \nthe use of a broad-spectrum antibacterial antibiotic, may predispose to \nboth mucosal and systemic infections\".  I was addressing mucosal infections\n(I like the term blooms better).  The nutrition course that I teach covers \nthis effect of antibiotic treatment as well as the \"cure\".  I guess that \nyour nutrition course does not, too bad.  \n\n\n>>Here is a brief primer on yeast.  Yeast infections, as they are commonly \n>>called, are not truely caused by yeasts.  The most common organism responsible\n>>for this type of infection is Candida albicans or Monilia which is actually a \n>>yeast-like fungus.  \n> \n> Well, maybe I'm getting picky, but I always thought that a yeast\n> was one form that a fungus could exist in, the other being the\n> mold form.  Many fungi can occur as either yeasts or molds, \n> depending on environment.  Candida exibits what is known as\n> reverse dimorphism - it exists as a mold in the tissues\n> but exists as a yeast in the environment.  Should we maybe\n> call it a mold infection?  a fungus infection?  Maybe we\n> should say it is caused by a mold-like fungus.\n>  \n>> \n>>Martin Banschbach, Ph.D.\n>>Professor of Biochemistry and Chairman\n>>Department of Biochemistry and Microbiology\n>>OSU College of Osteopathic Medicine\n>>1111 West 17th St.\n>>Tulsa, Ok. 74107\n>>\n> \n> You're the chairman of Biochem and Micro and you didn't know \n> that a yeast is a form of a fungus?  (shudder)\n> Or maybe you did know, and were oversimplifying?\n\nMy, my Elisabeth, do I detect a little of Steve Dyer in you?  If you \nnoticed my faculty rank, I'm a biochemist, not a microbiologist.\nCandida is classifed as a fungus(according to Zinsser's).  But, as you point \nout, it displays dimorphism.  It is capable of producing yeast cells, \npseudohyphae and true hyphae.  Elisabeth, you are probably a microbiologist \nand that makes a lot of sense to you.  To a biochemist, it's a lot of \nGreek.  So I called it a yeast-like fungus, go ahead and crucify me.\n\nYou know Elisabeth, I still haven't been able to figure out why such a small \nlittle organism like Candida can bring out so much hostility in people in \nSci. Med.  And I must admitt that I got sucked into the mud slinging too.\nI keep hoping that if people will just take the time to think about what \nI've said, that it will make sense.  I'm not asking anyone here to buy into \n\"The Yeast Connection\" book because I don't know what's in that book, plain \nand simple. And to be honest with you, I'm beginning to wish that it was never \nwritten.\n\nMarty B.\n\n\n\n\n# 필수 라이브러리 임포트\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline\n\n# 다항식 나이브베이즈(Multinomial Navie Bayes) 기반 모델 생성\nmodel = make_pipeline(TfidfVectorizer(), MultinomialNB())\n# 훈련데이터로 모델 훈련하기\nmodel.fit(train.data, train.target)\n\n# 테스트 데이터를 위한 레이블 생성하기\nlabels = model.predict(test.data)\n\n\n# 혼동 행렬과 히트 맵 생성하기\nfrom sklearn.metrics import confusion_matrix\nmat = confusion_matrix(test.target, labels)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, \n            xticklabels=train.target_names, yticklabels=train.target_names)\n\n# 혼동 행렬의 히트 맵 플로팅하기\nplt.xlabel('true label')\nplt.xlabel('predicted label')\n\nText(0.5, 12.453125, 'predicted label')\n\n\n\n\n\n\n# 훈련 모델 기반의 새로운 데이터 상 카테고리 예측하기\ndef predict_category(s, train=train, model=model):\n    pred = model.predict([s])\n    return train.target_names[pred[0]]\n\n\npredict_category('Jesus Christ')\n\n'soc.religion.christian'\n\n\n\npredict_category('Sending load to International Space Station')\n\n'sci.space'\n\n\n\npredict_category('Audio is better than BMW')\n\n'sci.electronics'\n\n\n\npredict_category('Prsident of India')\n\n'soc.religion.christian'"
  },
  {
    "objectID": "docs/blog/posts/ML/guide_map/index.html",
    "href": "docs/blog/posts/ML/guide_map/index.html",
    "title": "Content List, Machine Learning",
    "section": "",
    "text": "(Draft)"
  },
  {
    "objectID": "docs/blog/posts/ML/guide_map/index.html#basic",
    "href": "docs/blog/posts/ML/guide_map/index.html#basic",
    "title": "Content List, Machine Learning",
    "section": "Basic",
    "text": "Basic\n\nR\n\n\nPython\n\nTensor Flow Framework\n\n2023-02-03, Tensor Flow Introduction\n\n\n\nPytorch Framework\n\n2023-02-03, Pytorch Introduction"
  },
  {
    "objectID": "docs/blog/posts/ML/guide_map/index.html#machine-learning-methods",
    "href": "docs/blog/posts/ML/guide_map/index.html#machine-learning-methods",
    "title": "Content List, Machine Learning",
    "section": "Machine Learning Methods",
    "text": "Machine Learning Methods\n\nSupervised Learning\n\n0000-00=00, [Linear Regression]\n0000-00=00, [Logistic Regression]\n0000-00=00, [Generative Models]\n\n0000-00=00, [Linear Discriminant Analysis]\n0000-00=00, [Quadratic Discriminant Analysis]\n0000-00=00, [Naive Bayes]\n\n0000-00=00, [Resampling Methods]\n0000-00=00, [Regularization]\n0000-00=00, [Smoothing]\n0000-00=00, [Tree Based Methods]\n0000-00=00, [Support Vector Machine]\n0000-00=00, [PCR]\n0000-00=00, [PLS]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n\n\n\nUnupervised Learning\n\n0000-00=00, [PCA]\n0000-00=00, [K means clustering]\n0000-00=00, [Hierarchical Clustering]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]\n0000-00=00, [Linear Regression]"
  },
  {
    "objectID": "docs/blog/posts/Patent/guide_map/index.html",
    "href": "docs/blog/posts/Patent/guide_map/index.html",
    "title": "Content List, Patent",
    "section": "",
    "text": "(Draft)"
  },
  {
    "objectID": "docs/blog/posts/Patent/guide_map/index.html#basic",
    "href": "docs/blog/posts/Patent/guide_map/index.html#basic",
    "title": "Content List, Patent",
    "section": "Basic",
    "text": "Basic"
  },
  {
    "objectID": "docs/blog/posts/statistics/2022-12-08-P-value/index.html",
    "href": "docs/blog/posts/statistics/2022-12-08-P-value/index.html",
    "title": "p-values",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n실험 결과가 우연히 생성된 것보다 더 극단적인 경우를 통계적으로 유의하다고 한다. (결과가 귀무 가설 하의 분포와 다른 분포에서 나올 수 있다고 생각해 보십시오.)\n\n\n\n’p-값’의 p는 ’확률’을 나타냅니다. p-값은 실험에서 관찰된 결과가 귀무 가설 하에서 발생할 수 있는 극단적인 결과를 얻을 확률의 합계입니다. 즉, p-값은 실험 결과가 우연히 얻어질 확률입니다.\n\n\n\n우연한 결과가 통계적으로 유의미하다고 하기 위해 실험의 실제 결과를 넘어서야 하는 극단적이거나 드문 결과의 확률 임계값입니다.\n\n\n\n귀무가설이 참인데 실수로 귀무가설을 기각하는 오류\n\n\n\n대립가설이 참인데 실수로 귀무가설을 기각하지 못하는 오류\n\n\n\n\n\np-값은 테스트 결과의 유의성을 측정할 때 효율적이고 효과적인 통계 지표입니다. 회귀 분석을 수행했다고 가정해 봅시다. 그런 다음 회귀 모델의 결과로 베타 계수와 표준 오차를 얻을 수 있습니다.\n\nNumber of Cases of How You Interpret Regresssion Result\n\n\n\nhigh Standard Error\nlow Standard Error\n\n\n\n\nhigh \\(\\beta\\)\nUnclear Interpretation\nOK\n\n\nlow \\(\\beta\\)\nOK\nUnclear Interpretation\n\n\n\n위의 표는 회귀 모델의 결과를 해석할 수 있는 경우의 수를 보여줍니다. 각 계수 \\(\\beta\\) 에 대해 4개의 경우가 있습니다.\n\nhigh \\(\\beta\\) and high Standard Error mean that 해당 변수가 강한 영향을 미치나 그 영향이 변동될 수 있음을 의미하므로 회귀 모델에서 도출된 \\(\\beta\\) 계수는 유의하지 않을 가능성이 높습니다. 그 효과가 통계적으로 유의미한지 확신할 수 없습니다.\nhigh \\(\\beta\\) and low Standard Error mean that the corresponding variable has a strong effect, and its variation is small, so the \\(\\beta\\) coefficient resulted from your regression model is likely to be significant.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, its effect has a high variation. So, we can clearly interprete the variable with the \\(\\beta\\) as a variable that is not significantly associated with your response variable.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, but its effect has a low variation. So, it is difficult to conclude that the variable is significant.\n\nThe p-value could be used to provide a clearer interpretation of the unclear situation (i.e. (high \\(\\beta\\), high Standard Error), (low \\(\\beta\\), high Standard Error) ) by looking at the ratio of the estimated value of a parameter(= \\(\\beta\\)) to its standard error on the distribution under the null hypothesis. By general convention, the cut-off of p-value indicating statistical signficance is 0.05.\n\n\n\nDespite the goodness of p-value, it is controversial to make a decision based solely on the p-value. As mentioned above, p-value is the probability that the result of your experiment is due to chance. In addition, looking into \\(\\frac{\\beta}{\\frac{s.e}{\\sqrt{n}}}\\), the p-value gets smaller as the sample size becomes larger and larger. It should be avoided that something is proved just because a low p-value is calucated.\nEven if a result is statistically significant, that does not necessarily mean it has real significance. A small difference that has no practical meaning can be statistically significant if the sample size is large enough. It is because large samples ensure that meaningless effects can become big enough to possibly exclude chance due to simple math.\nThe American Statistical Association (ASA) has released a statement of six principles for researchers and journal editors on p-values:\nSource: ASA Statement on Statistical Significance and p-values\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nProper inference requires full reporting and transparency.\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n\n\n\n\n\nPersonally, I make use of p-values as a tool in data science to just check whether a model result or a set of variables that appears interesting and useful is in the range of normal variability by chance in the exploratory data analysis(EDA) or data mining step.\nIf you want to get a statistical significance level through p-values, other methodologies could help increase the accuracy of real significance such as permuted p-values, q-values, and penalization on multiple comparison tests\n\n\n\n\n\nIt is said to be statistically significant if a result of your experiment is more extreme than one that is produced by chance. (Try thinking that your result could have come from a different distribution from the one under the null hypothesis.)\n\n\n\np of ‘p-value’ stands for ‘probability’. The p-value is the summation of the probabilities of obtaining results as extreme as the observed results from your experiments could occur under the null hypothesis. In other words, p-value is the probability that the result of your experiment is obtained by chance.\n\n\n\nThe probability threshold of the extreme or rarer results that chance results must be beyond actual results of your experiments in order to be said to be statistically significant.\n\n\n\nconcluding \\(H_o\\) or the null hypothesis is true by mistake.\n\n\n\nconcluding \\(H_a\\) or the alternative hypothesis is true by mistake.\n\n\n\n\n\np-value is an efficient and effective statistical index when to measure the significance of your test result. Let’s make an assumption that you have conducted a regression analysis. Then, you can get beta coefficients and their standard errors as results of your regression model.\n\nNumber of Cases of How You Interpret Regresssion Result\n\n\n\nhigh Standard Error\nlow Standard Error\n\n\n\n\nhigh \\(\\beta\\)\nUnclear Interpretation\nOK\n\n\nlow \\(\\beta\\)\nOK\nUnclear Interpretation\n\n\n\nThe above table shows the number of cases you can interprete the results of your regression model. There are 4 cases for each coefficient \\(\\beta\\).\n\nhigh \\(\\beta\\) and high Standard Error mean that the corresponding variable has a strong effect but its effect may be fluctuated, so the \\(\\beta\\) coefficient resulted from your regression model is likely to be not significant. We are not sure that its effect is statistically significant.\nhigh \\(\\beta\\) and low Standard Error mean that the corresponding variable has a strong effect, and its variation is small, so the \\(\\beta\\) coefficient resulted from your regression model is likely to be significant.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, its effect has a high variation. So, we can clearly interprete the variable with the \\(\\beta\\) as a variable that is not significantly associated with your response variable.\nlow \\(\\beta\\) and high Standard Error mean that the corresponding variable has a weak effect on your reponse variable, but its effect has a low variation. So, it is difficult to conclude that the variable is significant.\n\nThe p-value could be used to provide a clearer interpretation of the unclear situation (i.e. (high \\(\\beta\\), high Standard Error), (low \\(\\beta\\), high Standard Error) ) by looking at the ratio of the estimated value of a parameter(= \\(\\beta\\)) to its standard error on the distribution under the null hypothesis. By general convention, the cut-off of p-value indicating statistical signficance is 0.05.\n\n\n\nDespite the goodness of p-value, it is controversial to make a decision based solely on the p-value. As mentioned above, p-value is the probability that the result of your experiment is due to chance. In addition, looking into \\(\\frac{\\beta}{\\frac{s.e}{\\sqrt{n}}}\\), the p-value gets smaller as the sample size becomes larger and larger. It should be avoided that something is proved just because a low p-value is calucated.\nEven if a result is statistically significant, that does not necessarily mean it has real significance. A small difference that has no practical meaning can be statistically significant if the sample size is large enough. It is because large samples ensure that meaningless effects can become big enough to possibly exclude chance due to simple math.\nThe American Statistical Association (ASA) has released a statement of six principles for researchers and journal editors on p-values:\nSource: ASA Statement on Statistical Significance and p-values\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nProper inference requires full reporting and transparency.\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n\n\n\n\n\nPersonally, I make use of p-values as a tool in data science to just check whether a model result or a set of variables that appears interesting and useful is in the range of normal variability by chance in the exploratory data analysis(EDA) or data mining step.\nIf you want to get a statistical significance level through p-values, other methodologies could help increase the accuracy of real significance such as permuted p-values, q-values, and penalization on multiple comparison tests\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-07-anova/index.html",
    "href": "docs/blog/posts/statistics/2023-01-07-anova/index.html",
    "title": "ANOVA",
    "section": "",
    "text": "(draft)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-07-anova/index.html#statistical-methods-similar-to-anova",
    "href": "docs/blog/posts/statistics/2023-01-07-anova/index.html#statistical-methods-similar-to-anova",
    "title": "ANOVA",
    "section": "1 Statistical Methods Similar to ANOVA",
    "text": "1 Statistical Methods Similar to ANOVA\n\n2023-01-27, repeated measures ANOVA\n2023-01-27, ANCOVA\n2023-01-28, MANOVA\n\n\n\n\nKorean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n1.1 Description\nANOVA는 3개 이상의 모집단 사이의 평균의 동일성을 검정하는 통계 분석 방법이다.\n\n일원 분산 분석 (One-way ANOVA)\n\n그룹을 구분하는 변수가 1개\nBetween-Groups one-way ANOVA(집단간 일원분산분석): 관측치를 grouping하는 범주형 변수가 1개이며 각 관측치는 범주형 변수에 의해 구분되는 그룹들 가운데 반드시 하나에만 할당되어야한다. 즉, 어떠한 경우에도 하나의 관측치 또는 샘플이 여러 groups에 동시에 들어가면 안된다. 이 때 이렇게 그룹을 나누는 범주형 변수를 집단간 요인이라고 한다.\nWithin-groups one-way ANOVA (집단 내 일원분산분석) or repeated measures ANOVA: 시간과 같은 하나의 범주형 변수로 샘플들을 측정한다. 시간의 경과에 따라 측정된 샘플들을 범주형 변수의 여러 기간에 걸쳐 모두 할당시킨다. 즉, 하나의 샘플이 여러 그룹에 다른 측정치로 관찰될 수 있다. 예를들어, sample A가 4주, 8주, 12주, 16주 그룹에 모두 측정 된다. 이때 기간변수는 집단 내 요인이라고 부른다.\n\n이원 분산 분석 (Two-way ANOVA)\n\n집단을 구분하는 변수가 2개이며 각 집단 간 요인과 집단 내 요인을 나타낸다.\n이원 분석 부터는 main effect와 interaction effect가 존재한다.\n범주형 변수 A와 범주형 변수 B의 Main effect 계산\n범주형 변수 A와 범주형 변수 B의 상호 작용 효과 or 교호 작용 효과 (Interaction effect) 계산\ngroup을 구분하는 독립변수가 2개 일때 모집단 간 평균의 동일성 검정\n\n2개의 주효과(main effect) 검정: 각 독립 변수에 의해 만들어지는 집단 간 평균의 차이에 대한 검정\n\n먼저, 두 독립변수가 종속변수에 개별적으로 영향을 미치는지 검정\n\n\n1개의 상호작용효과(interaction effect) 검정: 두 독립 변수의 조합에 의해 만들어지는 집단 간 평균의 차이에 대한 검정\n\n두 독립변수의 조합이 종속변수와 유의한 영향관계를 갖는지 검정\n\n만약에 유의하다면 2개의 독립변수가 합쳐져서 나온 파생효과이기 때문에 1개만 골라서 분석해서 해석 할 수 없음\n\n\n\n\n\n1.2 How to conduct ANOVA?\n\n분산 분석은 F검정(F test)을 통해 수행한다.\nF 검정은 집단 간 분산 (between-groups variability)과 집단 내 분산 (within-groups variability)의 ratio로 계산된 F값 (F value or F statistic)을 토대로 가설검정을 수행한다. 이때 F value or F statistic을 통계 검정을 위한 검정통계량 (test statistic) 라고 부른다.\nF 검정 결과가 통계적으로 유의하면 집단 간 평균의 차이가 존재한다. (즉, 독립 변수가 종속변수에 영향을 미침)\nF 분포 2개의 자유도에 의해 분포의 모양이 결정되며 대체로 오른쪽으로 긴 꼬리를 갖는다\n\n첫 번째 자유도: 집단 간(between-group)의 자유도\n두 번째 자유도: 집단 내(within-group)의 자유도\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(gplots)\nlibrary(rmarkdown)\nknitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)\noptions(digits = 5)\nset.seed(20230109)\n\n\n\n\n\n\n\n종속 변수의 변동성은 다음과 같이 설명되기 때문에 아래의 식을 만족한다.\n\\[SS_{total}=SS_A+SS_B+SS_{AB}+SS_{error}\\]\n\\(SS_{total}\\)은 쉽게 구할 수 있고 \\(SS_A\\), \\(SS_B\\), \\(SS_{error}\\)를 계산하여 빼준다.\nTwo Way Anova SS 계산 공식 링크\n\\(SS_{AB}\\) 즉,\n\\[ SS_{AB}=SS_{total}-SS_A-SS_B-SS_{error} \\]\n\n\n1.3 Meaning\nANOVA는 집단 간 분산과 집단 내 분산의 비교하는 방식으로, 좀 더 구체적으로는 집단 간 분산과 집단 내 분산의 비를 계산하여, 집단 간 분산이 클수록 그리고 집단 분산이 작을 수록 집단 평균이 다를 가능성이 증가한다는 알고리즘에 기초한다.\n\n\n2 Application to Example\n\n2.1 Data Description\n\n2.1.1 Raw Data\n(…민감 정보 제거 및 데이터 변환 후 컨설팅 내용 일부 발췌…)\n\n\n\nexample data는 Day, Run, response의 변수들을 포함하고 있습니다. 공유해주신 정보에 따르면 아마도 Run은 오전과 오후를 나누는 변수인 것으로 생각 됩니다. 이 data만 보면 아마도 같은 샘플에 대해서 시약 제품이 시간에 따라 얼마나 안정적인 performance를 보여주는지 검사하는 실험으로 추측됩니다. 좀 더 분석하기 용이한 형태로 data structure를 바꾸겠습니다.\n\n\n2.1.2 Processed Data\n\n\n\n\n  \n\n\n\n재가공된 data는 120개의 샘플과 5개의 변수를 갖고있습니다. 변수 목록은 다음과 같습니다.\n\nid: 열번호, 총 20일간 하루 2회 구동(AM, PM) 구동, 오전 오후 각 각 3번씩 구동 총 120 \\((=20 \\times 3 \\times 2)\\) 샘플\n\nDay: Day1~20\n\nnoon: AM= before noon, PM= after noon\nRun: 1회 구동당 3번 반복씩1, 2, 3\n\nresponse: response variable, 낮을 수록 좋음\n\nANOVA의 Assumption\n\nresponse variable should follow normal distribution.\n\nhomoscedasticity, equality of variance: 각 집단의 분포는 모두 동일한 분산을 가짐\nANOVA의 가정들을 반드시 충족하지 않아도 되지만 충족하면 Power 가 올라감\n\n\n\n\n2.2 EDA (Explorator Data Analysis)\n이 data는 아래 처럼 1의 결측치를 갖고 있습니다.\n\n\n\n\n\nid\nDay\nnoon\nRun\nresponse\n\n\n\n\n117\n20\nAM\n3\nNA\n\n\n\n\n\nCt에 대한 Global Statistics는 다음과 같습니다.\n\n\n\n\n\ncount\nglobal_response_mean\nglobal_response_sd\nglobal_response_CV\n\n\n\n\n119\n38.727\n18.47\n47.694 %\n\n\n\n\n\nDay groups의 Statistics은 다음과 같습니다.\n\n\n\n\n  \n\n\n\nAM/PM groups의 Statistics은 다음과 같습니다.\n\n\n\n\n  \n\n\n\nDays와 AM/PM 조합 groups의 Statistics은 다음과 같습니다.\n\n\n\n\n  \n\n\n\n이제 ANOVA를 수행하기 위한 basic statistics는 모두 구했습니다. ANOVA를 수행하기 위해 집단 간 분산과 집단 내 분산을 계산하도록 하겠습니다.\n\n\n2.3 집단 간 분산\n앞에서 설명 드린바로 유추해보면 예시 data의 집단 간 분산의 범주형 변수는 Day로 설정하는 것이 합리적인 것으로 보입니다.\n\n\\(g=g\\) Day의 sample size = 20, 자유도 = 20-1 = 19 입니다.\n\\(n_g=g\\) group의 sample size, \\(\\overline{X}_g=g\\) 의 sample mean은 다음과 같습니다.\n\\(\\overline{X}\\) = global sample mean = 38.72681\n집단 간 분산: \\(\\frac{집단 간 제곱합}{자유도}=\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n2.3.1 SS_Day (집단간 분산 Day)\nDay sq = \\([(\\overline{X}_g-\\overline{X})^2n_g]\\) 집단(Day)간 분산 계산, 집단(Day)간 분산 = \\(\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n\n\n  \n\n\n\nAnalysis-In program의 ANOVA결과값과 일치하는 것을 볼 수 있습니다. $SS_{day} $= 7025.97838 with \\(df=19\\).\n\n\n2.3.2 SS_noon (집단간 분산 noon)\nnoon sq = \\([(\\overline{X}_g-\\overline{X})^2n_g]\\) 집단(noon)간 분산 계산, 집단(noon)간 분산 = \\(\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n\n\n  \n\n\n\nAnalysis-In program의 결과에서 찾아 볼 수 없죠? 이 결과는 숨어 있습니다. 상호 작용에 대한 분산값을 구하고 나면 정체를 알 수 있습니다.\n\\(SS_{noon}\\) = 319.76458 with \\(df=1\\).\n\n\n2.3.3 SS_error (집단내 분산)\n\n집단 내 분산 (within-groups variability)\n\n\n\n\n\n  \n\n\n\n\\(SS_{error}\\) = 2.47041^{4}\nAnalysis-In program의 결과와 일치하는 것을 확인할 수 있습니다.\n\n\n2.3.4 SS_total\n\n\n\n\n  \n\n\n\n\\(SS_{total}\\) = 4.02557^{4}\nAnalysis-In program의 ANOVA 결과 table에 있는 SS들의 합과 일치하는 것을 확인할 수 있습니다.\n\n\n2.3.5 상호 작용 분산\n\n\n\n\\(SS_{interaction}=SS_{DayNoon}= SS_{total}-SS_{Day}-SS_{noon}-SS_{error}\\)\n= 4.02557{4}-2.47041{4}-319.76458-7025.97838 = 8205.93974\nAnalysis-In program의 ANOVA 결과 table과 일치하는 것을 확인할 수 있습니다.\n위의 결과들을 종합하면 아래와 같이 요약됩니다.\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   7026     370    1.18   0.29\nnoon         1    339     339    1.08   0.30\nDay:noon    19   8187     431    1.38   0.16\nResiduals   79  24704     313               \n결측으로 인하여 1개의 관측치가 삭제되었습니다.\n\n\n\nRepeatability SD = \\(\\sqrt{V_{error}}=\\sqrt{MS_{error}}\\) = 17.6836\nRepeatability CV = \\(\\frac{repeatability \\space SD}{global \\space mean \\space response}\\) = 0.45662\n\n위의 결과를 간단히 해석해 보면\n\n집단간 범주 변수인 Day는 p-value =0.29>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, 일별로 평균 response값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, 일별로 평균 response값이 다르지 않습니다.\n\n집단간 범주 변수인 noon은 p-value =0.30>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, 오전/오후별 평균 response값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, 오전/오후별 평균 response값이 다르지 않습니다.\n\nDay와 noon두 변수의 상호작용 변수는 p-value =0.16>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, Day별 Noon(오전/오후)별 평균 response값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, Day별 Noon(오전/오후)별 평균 response값이 다르지 않습니다.\n\n\n최종 결론, 제품의 response값이 Day별 오전/오후별 안정적인 performance를 보인다고 조심스럽게 결론을 내릴 수 있습니다.\n이제 까지는 질문에 대한 답이 되는 ANOVA의 원리 및 통계량의 재현 및 해석법에 대하여 알아봤습니다. 하지만 직관적으로 어떤 의미가 있을 까요? 원래는 시각화를 통해 데이터의 패턴을 짐작하고 통계 검정 결과를 예상하는데 우리는 반대로 가고 있네요 ㅎㅎ 시각화를 통해 ANOVA 결과가 얼마나 직관적인지 알아보겠습니다.\n\n\n\n2.4 Visualization\n\n2.4.1 One-way: Day\n\n\n\n\n\n\n\n\n자세히 보면 일별로 시간의 경과에 따라 전체적으로 평균 response값이 약간 하향하는 것으로 보입니다. 하지만 좀 더 세부적으로 관찰하면 1일~8일 평균 response의 경향이 constant한 패턴을 보입니다. 9일~13일 평균 response가 진동 하향하는 패턴을 보입니다. 14일~20일 평균 response가 상향하는 패턴을 보입니다.\n\n\nTables of means\nGrand mean\n       \n38.727 \n\n Day \n        1    2     3     4     5     6     7     8     9    10    11    12\n    47.34 41.2 43.26 33.04 44.01 40.53 40.92 42.98 55.89 34.64 49.19 26.94\nrep  6.00  6.0  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00\n       13    14    15    16    17    18    19    20\n    43.86 28.82 28.68 31.52 30.06 35.28 32.97 44.35\nrep  6.00  6.00  6.00  6.00  6.00  6.00  6.00  5.00\n\n\nTables of effects\n\n Day \n        1     2     3      4    5     6     7     8     9     10    11     12\n    8.615 2.476 4.534 -5.685 5.28 1.802 2.193 4.251 17.17 -4.088 10.46 -11.79\nrep 6.000 6.000 6.000  6.000 6.00 6.000 6.000 6.000  6.00  6.000  6.00   6.00\n       13     14     15     16     17     18     19   20\n    5.138 -9.907 -10.05 -7.211 -8.666 -3.449 -5.756 5.62\nrep 6.000  6.000   6.00  6.000  6.000  6.000  6.000 5.00\n\n\n위에 첫 번째표에서 Global Sample response Mean = 38.727 과 각 집단의 평균 response를 확인할 수 있습니다. 위에 두 번째표에서 Global Sample response Mean = 37.322 과 각 집단의 평균 response의 차이를 확인할 수 있습니다.\n\nDay 9에서 차이가 가장 큰 것으로 보아 9일째 실험에서 performance가 가장 낮은 것이 관측됐습니다.\n반대로, 12일에 performance 가장 좋은 것으로 관측됐습니다.\n\n9일과 12일에 response값에 영향을 미쳤던 요인이 있었는지 복기 하는것도 도움이 되겠군요.\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   7026     370     1.1   0.36\nResiduals   99  33230     336               \n결측으로 인하여 1개의 관측치가 삭제되었습니다.\n\n\nOne-way ANOVA의 결과값입니다. Day별 평균 response의 차이는 거의 없는 것으로 보입니다. 따라서 Day 별 평균 response의 경향이 일관되지 않고 One-way ANOVA에서 역시 통계적으로 유의하지 않아 Day 변수는 평균 response에 영향을 미치지 않는 것 같습니다.\n\n\n2.4.2 One-way: AM/PM\n\n\n\n\n\n\n\n\n오후에 시간의 경과에 따라 전체적으로 평균 response값이 약간 하향하는 것으로 보입니다.\n\n\nTables of means\nGrand mean\n       \n38.727 \n\n noon \n       AM   PM\n    40.38 37.1\nrep 59.00 60.0\n\n\nTables of effects\n\n noon \n        AM     PM\n     1.653 -1.626\nrep 59.000 60.000\n\n\n위 첫 번째 표에서 AM/PM 간의 평균 response차이는 0.15 (농도가 약 0.5배) 차이가 나는 것을 확인할 수 있습니다. 생물학적으로 의미가 있는 수치일까요? 위 두 번째 표에서 Global Sample Mean 37.322와 오전/오후 별 약 0.07씩(농도가 약 0.25배) 차이가 납니다.\n\n\n             Df Sum Sq Mean Sq F value Pr(>F)\nnoon          1    320     320    0.94   0.34\nResiduals   117  39936     341               \n결측으로 인하여 1개의 관측치가 삭제되었습니다.\n\n\n오전 오후별 One way ANOVA를 실행한 결과가 오전/오부 평균 response값의 차이가 다르지 않다는 것을 시사하고 있습니다. 아무래도 위의 차이는 우연에 의해 발생한 현상인 것 같습니다.\n\n\n\n\n\n일별로 평균 response값이 차이를 모든 경우의 수에서 차이가 나는지 확인한 결과 신뢰구간 모두가 0을 포함하고 있으므로 Day 변수는 유의하다고 볼 수 없습니다.\n\n\n\n\n\n오전/오후별로 평균 response값이 차이를 모든 경우의 수에서 차이가 나는지 확인한 결과 신뢰구간 모두가 0을 포함하고 있으므로 오전/오후 변수는 유의하다고 볼 수 없습니다.\n여기 까지 각 변수별 평균 response로의 영향도를 통계적으로 시각적으로 관찰했습니다. 하지만 Day별 오전/오후별 영향도가 있는지 확인하겠습니다. (이미 위에서 통계적으로 없다고 검정됐습니다.)\n\n\n\n2.5 Two way Anova\n\n\n       AM     PM\n1  35.987 58.697\n2  40.032 42.374\n3  33.148 53.374\n4  36.697 29.387\n5  44.432 43.581\n6  45.781 35.277\n7  40.458 41.381\n8  51.103 34.852\n9  69.981 41.806\n10 26.619 42.658\n11 67.923 30.452\n12 25.981 27.897\n13 40.600 47.129\n14 25.839 31.800\n15 18.458 38.897\n16 36.910 26.123\n17 41.594 18.529\n18 38.471 32.084\n19 45.923 20.019\n20 42.303 45.710\n\n\n        AM      PM\n1  20.9797  7.4516\n2  18.6291 12.0856\n3  22.1450 34.0565\n4  16.3483  7.5583\n5   4.6107 11.0032\n6  23.6862 11.7618\n7  12.3380 30.0128\n8  33.4416 12.0135\n9   7.7381 30.4715\n10  3.4483  7.8927\n11 28.8752  9.1557\n12  5.9053 12.9054\n13 15.2088  7.3824\n14  8.4556 27.2253\n15  4.5063 35.2180\n16 18.3756  1.6899\n17 13.3474  8.0068\n18  6.0067 21.7672\n19 19.7931  1.4176\n20  3.9142 14.0145\n\n\n\n\nTables of means\nGrand mean\n       \n38.727 \n\n Day \n        1    2     3     4     5     6     7     8     9    10    11    12\n    47.34 41.2 43.26 33.04 44.01 40.53 40.92 42.98 55.89 34.64 49.19 26.94\nrep  6.00  6.0  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00\n       13    14    15    16    17    18    19    20\n    43.86 28.82 28.68 31.52 30.06 35.28 32.97 44.35\nrep  6.00  6.00  6.00  6.00  6.00  6.00  6.00  5.00\n\n noon \n       AM    PM\n    40.43 37.05\nrep 59.00 60.00\n\n Day:noon \n     noon\nDay   AM    PM   \n  1   35.99 58.70\n  rep  3.00  3.00\n  2   40.03 42.37\n  rep  3.00  3.00\n  3   33.15 53.37\n  rep  3.00  3.00\n  4   36.70 29.39\n  rep  3.00  3.00\n  5   44.43 43.58\n  rep  3.00  3.00\n  6   45.78 35.28\n  rep  3.00  3.00\n  7   40.46 41.38\n  rep  3.00  3.00\n  8   51.10 34.85\n  rep  3.00  3.00\n  9   69.98 41.81\n  rep  3.00  3.00\n  10  26.62 42.66\n  rep  3.00  3.00\n  11  67.92 30.45\n  rep  3.00  3.00\n  12  25.98 27.90\n  rep  3.00  3.00\n  13  40.60 47.13\n  rep  3.00  3.00\n  14  25.84 31.80\n  rep  3.00  3.00\n  15  18.46 38.90\n  rep  3.00  3.00\n  16  36.91 26.12\n  rep  3.00  3.00\n  17  41.59 18.53\n  rep  3.00  3.00\n  18  38.47 32.08\n  rep  3.00  3.00\n  19  45.92 20.02\n  rep  3.00  3.00\n  20  42.30 45.71\n  rep  2.00  3.00\n\n\nTables of effects\n\n Day \n        1     2     3      4    5     6     7     8     9     10    11     12\n    8.615 2.476 4.534 -5.685 5.28 1.802 2.193 4.251 17.17 -4.088 10.46 -11.79\nrep 6.000 6.000 6.000  6.000 6.00 6.000 6.000 6.000  6.00  6.000  6.00   6.00\n       13     14     15     16     17     18     19   20\n    5.138 -9.907 -10.05 -7.211 -8.666 -3.449 -5.756 5.62\nrep 6.000  6.000   6.00  6.000  6.000  6.000  6.000 5.00\n\n noon \n        AM     PM\n     1.701 -1.672\nrep 59.000 60.000\n\n Day:noon \n     noon\nDay   AM      PM     \n  1   -13.044  13.044\n  rep   3.000   3.000\n  2    -2.860   2.860\n  rep   3.000   3.000\n  3   -11.802  11.802\n  rep   3.000   3.000\n  4     1.966  -1.966\n  rep   3.000   3.000\n  5    -1.263   1.263\n  rep   3.000   3.000\n  6     3.562  -3.562\n  rep   3.000   3.000\n  7    -2.151   2.151\n  rep   3.000   3.000\n  8     6.437  -6.437\n  rep   3.000   3.000\n  9    12.398 -12.398\n  rep   3.000   3.000\n  10   -9.709   9.709\n  rep   3.000   3.000\n  11   17.046 -17.046\n  rep   3.000   3.000\n  12   -2.647   2.647\n  rep   3.000   3.000\n  13   -4.954   4.954\n  rep   3.000   3.000\n  14   -4.670   4.670\n  rep   3.000   3.000\n  15  -11.909  11.909\n  rep   3.000   3.000\n  16    3.704  -3.704\n  rep   3.000   3.000\n  17    9.843  -9.843\n  rep   3.000   3.000\n  18    1.504  -1.504\n  rep   3.000   3.000\n  19   11.262 -11.262\n  rep   3.000   3.000\n  20   -4.071   2.714\n  rep   2.000   3.000\n\n\none way ANOVA와 같이 해석\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   7026     370    1.18   0.29\nnoon         1    339     339    1.08   0.30\nDay:noon    19   8187     431    1.38   0.16\nResiduals   79  24704     313               \n결측으로 인하여 1개의 관측치가 삭제되었습니다.\n\n\n위 그림을 보듯이 두 변수의 영향도가 없음, ANOVA 역시 유의하지 않음\n\n\n\n\n\n\n 누락된 행들: 117 \n\n\n\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = response ~ Day + noon + Day:noon, data = data)\n\n$Day\n            diff     lwr     upr   p adj\n2-1    -6.138710 -43.575 31.2979 1.00000\n3-1    -4.080645 -41.517 33.3560 1.00000\n4-1   -14.300000 -51.737 23.1366 0.99700\n5-1    -3.335484 -40.772 34.1011 1.00000\n6-1    -6.812903 -44.250 30.6237 1.00000\n7-1    -6.422581 -43.859 31.0140 1.00000\n8-1    -4.364516 -41.801 33.0721 1.00000\n9-1     8.551613 -28.885 45.9882 1.00000\n10-1  -12.703226 -50.140 24.7334 0.99934\n11-1    1.845161 -35.591 39.2818 1.00000\n12-1  -20.403226 -57.840 17.0334 0.89330\n13-1   -3.477419 -40.914 33.9592 1.00000\n14-1  -18.522581 -55.959 18.9140 0.95243\n15-1  -18.664516 -56.101 18.7721 0.94905\n16-1  -15.825806 -53.262 21.6108 0.99024\n17-1  -17.280645 -54.717 20.1560 0.97539\n18-1  -12.064516 -49.501 25.3721 0.99968\n19-1  -14.370968 -51.808 23.0657 0.99682\n20-1   -2.994839 -42.259 36.2690 1.00000\n3-2     2.058065 -35.379 39.4947 1.00000\n4-2    -8.161290 -45.598 29.2753 1.00000\n5-2     2.803226 -34.633 40.2399 1.00000\n6-2    -0.674194 -38.111 36.7624 1.00000\n7-2    -0.283871 -37.721 37.1528 1.00000\n8-2     1.774194 -35.662 39.2108 1.00000\n9-2    14.690323 -22.746 52.1270 0.99585\n10-2   -6.564516 -44.001 30.8721 1.00000\n11-2    7.983871 -29.453 45.4205 1.00000\n12-2  -14.264516 -51.701 23.1721 0.99709\n13-2    2.661290 -34.775 40.0979 1.00000\n14-2  -12.383871 -49.821 25.0528 0.99953\n15-2  -12.525806 -49.962 24.9108 0.99946\n16-2   -9.687097 -47.124 27.7495 0.99999\n17-2  -11.141935 -48.579 26.2947 0.99990\n18-2   -5.925806 -43.362 31.5108 1.00000\n19-2   -8.232258 -45.669 29.2044 1.00000\n20-2    3.143871 -36.120 42.4077 1.00000\n4-3   -10.219355 -47.656 27.2173 0.99997\n5-3     0.745161 -36.691 38.1818 1.00000\n6-3    -2.732258 -40.169 34.7044 1.00000\n7-3    -2.341935 -39.779 35.0947 1.00000\n8-3    -0.283871 -37.721 37.1528 1.00000\n9-3    12.632258 -24.804 50.0689 0.99939\n10-3   -8.622581 -46.059 28.8140 1.00000\n11-3    5.925806 -31.511 43.3624 1.00000\n12-3  -16.322581 -53.759 21.1140 0.98634\n13-3    0.603226 -36.833 38.0399 1.00000\n14-3  -14.441935 -51.879 22.9947 0.99662\n15-3  -14.583871 -52.021 22.8528 0.99620\n16-3  -11.745161 -49.182 25.6915 0.99978\n17-3  -13.200000 -50.637 24.2366 0.99891\n18-3   -7.983871 -45.421 29.4528 1.00000\n19-3  -10.290323 -47.727 27.1463 0.99997\n20-3    1.085806 -38.178 40.3497 1.00000\n5-4    10.964516 -26.472 48.4011 0.99992\n6-4     7.487097 -29.950 44.9237 1.00000\n7-4     7.877419 -29.559 45.3140 1.00000\n8-4     9.935484 -27.501 47.3721 0.99998\n9-4    22.851613 -14.585 60.2882 0.76824\n10-4    1.596774 -35.840 39.0334 1.00000\n11-4   16.145161 -21.291 53.5818 0.98785\n12-4   -6.103226 -43.540 31.3334 1.00000\n13-4   10.822581 -26.614 48.2592 0.99993\n14-4   -4.222581 -41.659 33.2140 1.00000\n15-4   -4.364516 -41.801 33.0721 1.00000\n16-4   -1.525806 -38.962 35.9108 1.00000\n17-4   -2.980645 -40.417 34.4560 1.00000\n18-4    2.235484 -35.201 39.6721 1.00000\n19-4   -0.070968 -37.508 37.3657 1.00000\n20-4   11.305161 -27.959 50.5690 0.99994\n6-5    -3.477419 -40.914 33.9592 1.00000\n7-5    -3.087097 -40.524 34.3495 1.00000\n8-5    -1.029032 -38.466 36.4076 1.00000\n9-5    11.887097 -25.550 49.3237 0.99974\n10-5   -9.367742 -46.804 28.0689 0.99999\n11-5    5.180645 -32.256 42.6173 1.00000\n12-5  -17.067742 -54.504 20.3689 0.97827\n13-5   -0.141935 -37.579 37.2947 1.00000\n14-5  -15.187097 -52.624 22.2495 0.99387\n15-5  -15.329032 -52.766 22.1076 0.99318\n16-5  -12.490323 -49.927 24.9463 0.99948\n17-5  -13.945161 -51.382 23.4915 0.99780\n18-5   -8.729032 -46.166 28.7076 1.00000\n19-5  -11.035484 -48.472 26.4011 0.99991\n20-5    0.340645 -38.923 39.6045 1.00000\n7-6     0.390323 -37.046 37.8270 1.00000\n8-6     2.448387 -34.988 39.8850 1.00000\n9-6    15.364516 -22.072 52.8011 0.99300\n10-6   -5.890323 -43.327 31.5463 1.00000\n11-6    8.658065 -28.779 46.0947 1.00000\n12-6  -13.590323 -51.027 23.8463 0.99841\n13-6    3.335484 -34.101 40.7721 1.00000\n14-6  -11.709677 -49.146 25.7270 0.99979\n15-6  -11.851613 -49.288 25.5850 0.99975\n16-6   -9.012903 -46.450 28.4237 1.00000\n17-6  -10.467742 -47.904 26.9689 0.99996\n18-6   -5.251613 -42.688 32.1850 1.00000\n19-6   -7.558065 -44.995 29.8786 1.00000\n20-6    3.818065 -35.446 43.0819 1.00000\n8-7     2.058065 -35.379 39.4947 1.00000\n9-7    14.974194 -22.462 52.4108 0.99480\n10-7   -6.280645 -43.717 31.1560 1.00000\n11-7    8.267742 -29.169 45.7044 1.00000\n12-7  -13.980645 -51.417 23.4560 0.99773\n13-7    2.945161 -34.491 40.3818 1.00000\n14-7  -12.100000 -49.537 25.3366 0.99966\n15-7  -12.241935 -49.679 25.1947 0.99960\n16-7   -9.403226 -46.840 28.0334 0.99999\n17-7  -10.858065 -48.295 26.5786 0.99993\n18-7   -5.641935 -43.079 31.7947 1.00000\n19-7   -7.948387 -45.385 29.4882 1.00000\n20-7    3.427742 -35.836 42.6916 1.00000\n9-8    12.916129 -24.521 50.3528 0.99918\n10-8   -8.338710 -45.775 29.0979 1.00000\n11-8    6.209677 -31.227 43.6463 1.00000\n12-8  -16.038710 -53.475 21.3979 0.98869\n13-8    0.887097 -36.550 38.3237 1.00000\n14-8  -14.158065 -51.595 23.2786 0.99735\n15-8  -14.300000 -51.737 23.1366 0.99700\n16-8  -11.461290 -48.898 25.9753 0.99984\n17-8  -12.916129 -50.353 24.5205 0.99918\n18-8   -7.700000 -45.137 29.7366 1.00000\n19-8  -10.006452 -47.443 27.4302 0.99998\n20-8    1.369677 -37.894 40.6335 1.00000\n10-9  -21.254839 -58.691 16.1818 0.85576\n11-9   -6.706452 -44.143 30.7302 1.00000\n12-9  -28.954839 -66.391  8.4818 0.35269\n13-9  -12.029032 -49.466 25.4076 0.99969\n14-9  -27.074194 -64.511 10.3624 0.47719\n15-9  -27.216129 -64.653 10.2205 0.46731\n16-9  -24.377419 -61.814 13.0592 0.66832\n17-9  -25.832258 -63.269 11.6044 0.56535\n18-9  -20.616129 -58.053 16.8205 0.88455\n19-9  -22.922581 -60.359 14.5140 0.76389\n20-9  -11.546452 -50.810 27.7174 0.99991\n11-10  14.548387 -22.888 51.9850 0.99631\n12-10  -7.700000 -45.137 29.7366 1.00000\n13-10   9.225806 -28.211 46.6624 0.99999\n14-10  -5.819355 -43.256 31.6173 1.00000\n15-10  -5.961290 -43.398 31.4753 1.00000\n16-10  -3.122581 -40.559 34.3140 1.00000\n17-10  -4.577419 -42.014 32.8592 1.00000\n18-10   0.638710 -36.798 38.0753 1.00000\n19-10  -1.667742 -39.104 35.7689 1.00000\n20-10   9.708387 -29.555 48.9723 0.99999\n12-11 -22.248387 -59.685 15.1882 0.80373\n13-11  -5.322581 -42.759 32.1140 1.00000\n14-11 -20.367742 -57.804 17.0689 0.89472\n15-11 -20.509677 -57.946 16.9270 0.88898\n16-11 -17.670968 -55.108 19.7657 0.96936\n17-11 -19.125806 -56.562 18.3108 0.93692\n18-11 -13.909677 -51.346 23.5270 0.99787\n19-11 -16.216129 -53.653 21.2205 0.98726\n20-11  -4.840000 -44.104 34.4239 1.00000\n13-12  16.925806 -20.511 54.3624 0.98004\n14-12   1.880645 -35.556 39.3173 1.00000\n15-12   1.738710 -35.698 39.1753 1.00000\n16-12   4.577419 -32.859 42.0140 1.00000\n17-12   3.122581 -34.314 40.5592 1.00000\n18-12   8.338710 -29.098 45.7753 1.00000\n19-12   6.032258 -31.404 43.4689 1.00000\n20-12  17.408387 -21.855 56.6723 0.98369\n14-13 -15.045161 -52.482 22.3915 0.99450\n15-13 -15.187097 -52.624 22.2495 0.99387\n16-13 -12.348387 -49.785 25.0882 0.99955\n17-13 -13.803226 -51.240 23.6334 0.99807\n18-13  -8.587097 -46.024 28.8495 1.00000\n19-13 -10.893548 -48.330 26.5431 0.99992\n20-13   0.482581 -38.781 39.7464 1.00000\n15-14  -0.141935 -37.579 37.2947 1.00000\n16-14   2.696774 -34.740 40.1334 1.00000\n17-14   1.241935 -36.195 38.6786 1.00000\n18-14   6.458065 -30.979 43.8947 1.00000\n19-14   4.151613 -33.285 41.5882 1.00000\n20-14  15.527742 -23.736 54.7916 0.99545\n16-15   2.838710 -34.598 40.2753 1.00000\n17-15   1.383871 -36.053 38.8205 1.00000\n18-15   6.600000 -30.837 44.0366 1.00000\n19-15   4.293548 -33.143 41.7302 1.00000\n20-15  15.669677 -23.594 54.9335 0.99493\n17-16  -1.454839 -38.891 35.9818 1.00000\n18-16   3.761290 -33.675 41.1979 1.00000\n19-16   1.454839 -35.982 38.8915 1.00000\n20-16  12.830968 -26.433 52.0948 0.99961\n18-17   5.216129 -32.221 42.6528 1.00000\n19-17   2.909677 -34.527 40.3463 1.00000\n20-17  14.285806 -24.978 53.5497 0.99837\n19-18  -2.306452 -39.743 35.1302 1.00000\n20-18   9.069677 -30.194 48.3335 1.00000\n20-19  11.376129 -27.888 50.6400 0.99993\n\n$noon\n         diff     lwr    upr   p adj\nPM-AM -3.3731 -9.8265 3.0804 0.30135\n\n$`Day:noon`\n                  diff      lwr     upr   p adj\n2:AM-1:AM     4.045161  -54.344 62.4344 1.00000\n3:AM-1:AM    -2.838710  -61.228 55.5505 1.00000\n4:AM-1:AM     0.709677  -57.680 59.0989 1.00000\n5:AM-1:AM     8.445161  -49.944 66.8344 1.00000\n6:AM-1:AM     9.793548  -48.596 68.1827 1.00000\n7:AM-1:AM     4.470968  -53.918 62.8602 1.00000\n8:AM-1:AM    15.116129  -43.273 73.5053 1.00000\n9:AM-1:AM    33.993548  -24.396 92.3827 0.92887\n10:AM-1:AM   -9.367742  -67.757 49.0215 1.00000\n11:AM-1:AM   31.935484  -26.454 90.3247 0.96592\n12:AM-1:AM  -10.006452  -68.396 48.3827 1.00000\n13:AM-1:AM    4.612903  -53.776 63.0021 1.00000\n14:AM-1:AM  -10.148387  -68.538 48.2408 1.00000\n15:AM-1:AM  -17.529032  -75.918 40.8602 1.00000\n16:AM-1:AM    0.922581  -57.467 59.3118 1.00000\n17:AM-1:AM    5.606452  -52.783 63.9956 1.00000\n18:AM-1:AM    2.483871  -55.905 60.8731 1.00000\n19:AM-1:AM    9.935484  -48.454 68.3247 1.00000\n20:AM-1:AM    6.316129  -58.965 71.5972 1.00000\n1:PM-1:AM    22.709677  -35.680 81.0989 0.99990\n2:PM-1:AM     6.387097  -52.002 64.7763 1.00000\n3:PM-1:AM    17.387097  -41.002 75.7763 1.00000\n4:PM-1:AM    -6.600000  -64.989 51.7892 1.00000\n5:PM-1:AM     7.593548  -50.796 65.9827 1.00000\n6:PM-1:AM    -0.709677  -59.099 57.6795 1.00000\n7:PM-1:AM     5.393548  -52.996 63.7827 1.00000\n8:PM-1:AM    -1.135484  -59.525 57.2537 1.00000\n9:PM-1:AM     5.819355  -52.570 64.2086 1.00000\n10:PM-1:AM    6.670968  -51.718 65.0602 1.00000\n11:PM-1:AM   -5.535484  -63.925 52.8537 1.00000\n12:PM-1:AM   -8.090323  -66.480 50.2989 1.00000\n13:PM-1:AM   11.141935  -47.247 69.5311 1.00000\n14:PM-1:AM   -4.187097  -62.576 54.2021 1.00000\n15:PM-1:AM    2.909677  -55.480 61.2989 1.00000\n16:PM-1:AM   -9.864516  -68.254 48.5247 1.00000\n17:PM-1:AM  -17.458065  -75.847 40.9311 1.00000\n18:PM-1:AM   -3.903226  -62.292 54.4860 1.00000\n19:PM-1:AM  -15.967742  -74.357 42.4215 1.00000\n20:PM-1:AM    9.722581  -48.667 68.1118 1.00000\n3:AM-2:AM    -6.883871  -65.273 51.5053 1.00000\n4:AM-2:AM    -3.335484  -61.725 55.0537 1.00000\n5:AM-2:AM     4.400000  -53.989 62.7892 1.00000\n6:AM-2:AM     5.748387  -52.641 64.1376 1.00000\n7:AM-2:AM     0.425806  -57.963 58.8150 1.00000\n8:AM-2:AM    11.070968  -47.318 69.4602 1.00000\n9:AM-2:AM    29.948387  -28.441 88.3376 0.98570\n10:AM-2:AM  -13.412903  -71.802 44.9763 1.00000\n11:AM-2:AM   27.890323  -30.499 86.2795 0.99521\n12:AM-2:AM  -14.051613  -72.441 44.3376 1.00000\n13:AM-2:AM    0.567742  -57.821 58.9569 1.00000\n14:AM-2:AM  -14.193548  -72.583 44.1956 1.00000\n15:AM-2:AM  -21.574194  -79.963 36.8150 0.99997\n16:AM-2:AM   -3.122581  -61.512 55.2666 1.00000\n17:AM-2:AM    1.561290  -56.828 59.9505 1.00000\n18:AM-2:AM   -1.561290  -59.950 56.8279 1.00000\n19:AM-2:AM    5.890323  -52.499 64.2795 1.00000\n20:AM-2:AM    2.270968  -63.010 67.5521 1.00000\n1:PM-2:AM    18.664516  -39.725 77.0537 1.00000\n2:PM-2:AM     2.341935  -56.047 60.7311 1.00000\n3:PM-2:AM    13.341935  -45.047 71.7311 1.00000\n4:PM-2:AM   -10.645161  -69.034 47.7440 1.00000\n5:PM-2:AM     3.548387  -54.841 61.9376 1.00000\n6:PM-2:AM    -4.754839  -63.144 53.6344 1.00000\n7:PM-2:AM     1.348387  -57.041 59.7376 1.00000\n8:PM-2:AM    -5.180645  -63.570 53.2086 1.00000\n9:PM-2:AM     1.774194  -56.615 60.1634 1.00000\n10:PM-2:AM    2.625806  -55.763 61.0150 1.00000\n11:PM-2:AM   -9.580645  -67.970 48.8086 1.00000\n12:PM-2:AM  -12.135484  -70.525 46.2537 1.00000\n13:PM-2:AM    7.096774  -51.292 65.4860 1.00000\n14:PM-2:AM   -8.232258  -66.621 50.1569 1.00000\n15:PM-2:AM   -1.135484  -59.525 57.2537 1.00000\n16:PM-2:AM  -13.909677  -72.299 44.4795 1.00000\n17:PM-2:AM  -21.503226  -79.892 36.8860 0.99997\n18:PM-2:AM   -7.948387  -66.338 50.4408 1.00000\n19:PM-2:AM  -20.012903  -78.402 38.3763 0.99999\n20:PM-2:AM    5.677419  -52.712 64.0666 1.00000\n4:AM-3:AM     3.548387  -54.841 61.9376 1.00000\n5:AM-3:AM    11.283871  -47.105 69.6731 1.00000\n6:AM-3:AM    12.632258  -45.757 71.0215 1.00000\n7:AM-3:AM     7.309677  -51.080 65.6989 1.00000\n8:AM-3:AM    17.954839  -40.434 76.3440 1.00000\n9:AM-3:AM    36.832258  -21.557 95.2215 0.84321\n10:AM-3:AM   -6.529032  -64.918 51.8602 1.00000\n11:AM-3:AM   34.774194  -23.615 93.1634 0.90946\n12:AM-3:AM   -7.167742  -65.557 51.2215 1.00000\n13:AM-3:AM    7.451613  -50.938 65.8408 1.00000\n14:AM-3:AM   -7.309677  -65.699 51.0795 1.00000\n15:AM-3:AM  -14.690323  -73.080 43.6989 1.00000\n16:AM-3:AM    3.761290  -54.628 62.1505 1.00000\n17:AM-3:AM    8.445161  -49.944 66.8344 1.00000\n18:AM-3:AM    5.322581  -53.067 63.7118 1.00000\n19:AM-3:AM   12.774194  -45.615 71.1634 1.00000\n20:AM-3:AM    9.154839  -56.126 74.4359 1.00000\n1:PM-3:AM    25.548387  -32.841 83.9376 0.99897\n2:PM-3:AM     9.225806  -49.163 67.6150 1.00000\n3:PM-3:AM    20.225806  -38.163 78.6150 0.99999\n4:PM-3:AM    -3.761290  -62.150 54.6279 1.00000\n5:PM-3:AM    10.432258  -47.957 68.8215 1.00000\n6:PM-3:AM     2.129032  -56.260 60.5182 1.00000\n7:PM-3:AM     8.232258  -50.157 66.6215 1.00000\n8:PM-3:AM     1.703226  -56.686 60.0924 1.00000\n9:PM-3:AM     8.658065  -49.731 67.0473 1.00000\n10:PM-3:AM    9.509677  -48.880 67.8989 1.00000\n11:PM-3:AM   -2.696774  -61.086 55.6924 1.00000\n12:PM-3:AM   -5.251613  -63.641 53.1376 1.00000\n13:PM-3:AM   13.980645  -44.409 72.3698 1.00000\n14:PM-3:AM   -1.348387  -59.738 57.0408 1.00000\n15:PM-3:AM    5.748387  -52.641 64.1376 1.00000\n16:PM-3:AM   -7.025806  -65.415 51.3634 1.00000\n17:PM-3:AM  -14.619355  -73.009 43.7698 1.00000\n18:PM-3:AM   -1.064516  -59.454 57.3247 1.00000\n19:PM-3:AM  -13.129032  -71.518 45.2602 1.00000\n20:PM-3:AM   12.561290  -45.828 70.9505 1.00000\n5:AM-4:AM     7.735484  -50.654 66.1247 1.00000\n6:AM-4:AM     9.083871  -49.305 67.4731 1.00000\n7:AM-4:AM     3.761290  -54.628 62.1505 1.00000\n8:AM-4:AM    14.406452  -43.983 72.7956 1.00000\n9:AM-4:AM    33.283871  -25.105 91.6731 0.94386\n10:AM-4:AM  -10.077419  -68.467 48.3118 1.00000\n11:AM-4:AM   31.225806  -27.163 89.6150 0.97452\n12:AM-4:AM  -10.716129  -69.105 47.6731 1.00000\n13:AM-4:AM    3.903226  -54.486 62.2924 1.00000\n14:AM-4:AM  -10.858065  -69.247 47.5311 1.00000\n15:AM-4:AM  -18.238710  -76.628 40.1505 1.00000\n16:AM-4:AM    0.212903  -58.176 58.6021 1.00000\n17:AM-4:AM    4.896774  -53.492 63.2860 1.00000\n18:AM-4:AM    1.774194  -56.615 60.1634 1.00000\n19:AM-4:AM    9.225806  -49.163 67.6150 1.00000\n20:AM-4:AM    5.606452  -59.675 70.8876 1.00000\n1:PM-4:AM    22.000000  -36.389 80.3892 0.99995\n2:PM-4:AM     5.677419  -52.712 64.0666 1.00000\n3:PM-4:AM    16.677419  -41.712 75.0666 1.00000\n4:PM-4:AM    -7.309677  -65.699 51.0795 1.00000\n5:PM-4:AM     6.883871  -51.505 65.2731 1.00000\n6:PM-4:AM    -1.419355  -59.809 56.9698 1.00000\n7:PM-4:AM     4.683871  -53.705 63.0731 1.00000\n8:PM-4:AM    -1.845161  -60.234 56.5440 1.00000\n9:PM-4:AM     5.109677  -53.280 63.4989 1.00000\n10:PM-4:AM    5.961290  -52.428 64.3505 1.00000\n11:PM-4:AM   -6.245161  -64.634 52.1440 1.00000\n12:PM-4:AM   -8.800000  -67.189 49.5892 1.00000\n13:PM-4:AM   10.432258  -47.957 68.8215 1.00000\n14:PM-4:AM   -4.896774  -63.286 53.4924 1.00000\n15:PM-4:AM    2.200000  -56.189 60.5892 1.00000\n16:PM-4:AM  -10.574194  -68.963 47.8150 1.00000\n17:PM-4:AM  -18.167742  -76.557 40.2215 1.00000\n18:PM-4:AM   -4.612903  -63.002 53.7763 1.00000\n19:PM-4:AM  -16.677419  -75.067 41.7118 1.00000\n20:PM-4:AM    9.012903  -49.376 67.4021 1.00000\n6:AM-5:AM     1.348387  -57.041 59.7376 1.00000\n7:AM-5:AM    -3.974194  -62.363 54.4150 1.00000\n8:AM-5:AM     6.670968  -51.718 65.0602 1.00000\n9:AM-5:AM    25.548387  -32.841 83.9376 0.99897\n10:AM-5:AM  -17.812903  -76.202 40.5763 1.00000\n11:AM-5:AM   23.490323  -34.899 81.8795 0.99980\n12:AM-5:AM  -18.451613  -76.841 39.9376 1.00000\n13:AM-5:AM   -3.832258  -62.221 54.5569 1.00000\n14:AM-5:AM  -18.593548  -76.983 39.7956 1.00000\n15:AM-5:AM  -25.974194  -84.363 32.4150 0.99860\n16:AM-5:AM   -7.522581  -65.912 50.8666 1.00000\n17:AM-5:AM   -2.838710  -61.228 55.5505 1.00000\n18:AM-5:AM   -5.961290  -64.350 52.4279 1.00000\n19:AM-5:AM    1.490323  -56.899 59.8795 1.00000\n20:AM-5:AM   -2.129032  -67.410 63.1521 1.00000\n1:PM-5:AM    14.264516  -44.125 72.6537 1.00000\n2:PM-5:AM    -2.058065  -60.447 56.3311 1.00000\n3:PM-5:AM     8.941935  -49.447 67.3311 1.00000\n4:PM-5:AM   -15.045161  -73.434 43.3440 1.00000\n5:PM-5:AM    -0.851613  -59.241 57.5376 1.00000\n6:PM-5:AM    -9.154839  -67.544 49.2344 1.00000\n7:PM-5:AM    -3.051613  -61.441 55.3376 1.00000\n8:PM-5:AM    -9.580645  -67.970 48.8086 1.00000\n9:PM-5:AM    -2.625806  -61.015 55.7634 1.00000\n10:PM-5:AM   -1.774194  -60.163 56.6150 1.00000\n11:PM-5:AM  -13.980645  -72.370 44.4086 1.00000\n12:PM-5:AM  -16.535484  -74.925 41.8537 1.00000\n13:PM-5:AM    2.696774  -55.692 61.0860 1.00000\n14:PM-5:AM  -12.632258  -71.021 45.7569 1.00000\n15:PM-5:AM   -5.535484  -63.925 52.8537 1.00000\n16:PM-5:AM  -18.309677  -76.699 40.0795 1.00000\n17:PM-5:AM  -25.903226  -84.292 32.4860 0.99867\n18:PM-5:AM  -12.348387  -70.738 46.0408 1.00000\n19:PM-5:AM  -24.412903  -82.802 33.9763 0.99957\n20:PM-5:AM    1.277419  -57.112 59.6666 1.00000\n7:AM-6:AM    -5.322581  -63.712 53.0666 1.00000\n8:AM-6:AM     5.322581  -53.067 63.7118 1.00000\n9:AM-6:AM    24.200000  -34.189 82.5892 0.99964\n10:AM-6:AM  -19.161290  -77.550 39.2279 1.00000\n11:AM-6:AM   22.141935  -36.247 80.5311 0.99994\n12:AM-6:AM  -19.800000  -78.189 38.5892 1.00000\n13:AM-6:AM   -5.180645  -63.570 53.2086 1.00000\n14:AM-6:AM  -19.941935  -78.331 38.4473 1.00000\n15:AM-6:AM  -27.322581  -85.712 31.0666 0.99660\n16:AM-6:AM   -8.870968  -67.260 49.5182 1.00000\n17:AM-6:AM   -4.187097  -62.576 54.2021 1.00000\n18:AM-6:AM   -7.309677  -65.699 51.0795 1.00000\n19:AM-6:AM    0.141935  -58.247 58.5311 1.00000\n20:AM-6:AM   -3.477419  -68.759 61.8037 1.00000\n1:PM-6:AM    12.916129  -45.473 71.3053 1.00000\n2:PM-6:AM    -3.406452  -61.796 54.9827 1.00000\n3:PM-6:AM     7.593548  -50.796 65.9827 1.00000\n4:PM-6:AM   -16.393548  -74.783 41.9956 1.00000\n5:PM-6:AM    -2.200000  -60.589 56.1892 1.00000\n6:PM-6:AM   -10.503226  -68.892 47.8860 1.00000\n7:PM-6:AM    -4.400000  -62.789 53.9892 1.00000\n8:PM-6:AM   -10.929032  -69.318 47.4602 1.00000\n9:PM-6:AM    -3.974194  -62.363 54.4150 1.00000\n10:PM-6:AM   -3.122581  -61.512 55.2666 1.00000\n11:PM-6:AM  -15.329032  -73.718 43.0602 1.00000\n12:PM-6:AM  -17.883871  -76.273 40.5053 1.00000\n13:PM-6:AM    1.348387  -57.041 59.7376 1.00000\n14:PM-6:AM  -13.980645  -72.370 44.4086 1.00000\n15:PM-6:AM   -6.883871  -65.273 51.5053 1.00000\n16:PM-6:AM  -19.658065  -78.047 38.7311 1.00000\n17:PM-6:AM  -27.251613  -85.641 31.1376 0.99674\n18:PM-6:AM  -13.696774  -72.086 44.6924 1.00000\n19:PM-6:AM  -25.761290  -84.150 32.6279 0.99880\n20:PM-6:AM   -0.070968  -58.460 58.3182 1.00000\n8:AM-7:AM    10.645161  -47.744 69.0344 1.00000\n9:AM-7:AM    29.522581  -28.867 87.9118 0.98840\n10:AM-7:AM  -13.838710  -72.228 44.5505 1.00000\n11:AM-7:AM   27.464516  -30.925 85.8537 0.99629\n12:AM-7:AM  -14.477419  -72.867 43.9118 1.00000\n13:AM-7:AM    0.141935  -58.247 58.5311 1.00000\n14:AM-7:AM  -14.619355  -73.009 43.7698 1.00000\n15:AM-7:AM  -22.000000  -80.389 36.3892 0.99995\n16:AM-7:AM   -3.548387  -61.938 54.8408 1.00000\n17:AM-7:AM    1.135484  -57.254 59.5247 1.00000\n18:AM-7:AM   -1.987097  -60.376 56.4021 1.00000\n19:AM-7:AM    5.464516  -52.925 63.8537 1.00000\n20:AM-7:AM    1.845161  -63.436 67.1263 1.00000\n1:PM-7:AM    18.238710  -40.150 76.6279 1.00000\n2:PM-7:AM     1.916129  -56.473 60.3053 1.00000\n3:PM-7:AM    12.916129  -45.473 71.3053 1.00000\n4:PM-7:AM   -11.070968  -69.460 47.3182 1.00000\n5:PM-7:AM     3.122581  -55.267 61.5118 1.00000\n6:PM-7:AM    -5.180645  -63.570 53.2086 1.00000\n7:PM-7:AM     0.922581  -57.467 59.3118 1.00000\n8:PM-7:AM    -5.606452  -63.996 52.7827 1.00000\n9:PM-7:AM     1.348387  -57.041 59.7376 1.00000\n10:PM-7:AM    2.200000  -56.189 60.5892 1.00000\n11:PM-7:AM  -10.006452  -68.396 48.3827 1.00000\n12:PM-7:AM  -12.561290  -70.950 45.8279 1.00000\n13:PM-7:AM    6.670968  -51.718 65.0602 1.00000\n14:PM-7:AM   -8.658065  -67.047 49.7311 1.00000\n15:PM-7:AM   -1.561290  -59.950 56.8279 1.00000\n16:PM-7:AM  -14.335484  -72.725 44.0537 1.00000\n17:PM-7:AM  -21.929032  -80.318 36.4602 0.99996\n18:PM-7:AM   -8.374194  -66.763 50.0150 1.00000\n19:PM-7:AM  -20.438710  -78.828 37.9505 0.99999\n20:PM-7:AM    5.251613  -53.138 63.6408 1.00000\n9:AM-8:AM    18.877419  -39.512 77.2666 1.00000\n10:AM-8:AM  -24.483871  -82.873 33.9053 0.99954\n11:AM-8:AM   16.819355  -41.570 75.2086 1.00000\n12:AM-8:AM  -25.122581  -83.512 33.2666 0.99925\n13:AM-8:AM  -10.503226  -68.892 47.8860 1.00000\n14:AM-8:AM  -25.264516  -83.654 33.1247 0.99916\n15:AM-8:AM  -32.645161  -91.034 25.7440 0.95531\n16:AM-8:AM  -14.193548  -72.583 44.1956 1.00000\n17:AM-8:AM   -9.509677  -67.899 48.8795 1.00000\n18:AM-8:AM  -12.632258  -71.021 45.7569 1.00000\n19:AM-8:AM   -5.180645  -63.570 53.2086 1.00000\n20:AM-8:AM   -8.800000  -74.081 56.4811 1.00000\n1:PM-8:AM     7.593548  -50.796 65.9827 1.00000\n2:PM-8:AM    -8.729032  -67.118 49.6602 1.00000\n3:PM-8:AM     2.270968  -56.118 60.6602 1.00000\n4:PM-8:AM   -21.716129  -80.105 36.6731 0.99996\n5:PM-8:AM    -7.522581  -65.912 50.8666 1.00000\n6:PM-8:AM   -15.825806  -74.215 42.5634 1.00000\n7:PM-8:AM    -9.722581  -68.112 48.6666 1.00000\n8:PM-8:AM   -16.251613  -74.641 42.1376 1.00000\n9:PM-8:AM    -9.296774  -67.686 49.0924 1.00000\n10:PM-8:AM   -8.445161  -66.834 49.9440 1.00000\n11:PM-8:AM  -20.651613  -79.041 37.7376 0.99999\n12:PM-8:AM  -23.206452  -81.596 35.1827 0.99985\n13:PM-8:AM   -3.974194  -62.363 54.4150 1.00000\n14:PM-8:AM  -19.303226  -77.692 39.0860 1.00000\n15:PM-8:AM  -12.206452  -70.596 46.1827 1.00000\n16:PM-8:AM  -24.980645  -83.370 33.4086 0.99933\n17:PM-8:AM  -32.574194  -90.963 25.8150 0.95647\n18:PM-8:AM  -19.019355  -77.409 39.3698 1.00000\n19:PM-8:AM  -31.083871  -89.473 27.3053 0.97602\n20:PM-8:AM   -5.393548  -63.783 52.9956 1.00000\n10:AM-9:AM  -43.361290 -101.750 15.0279 0.52759\n11:AM-9:AM   -2.058065  -60.447 56.3311 1.00000\n12:AM-9:AM  -44.000000 -102.389 14.3892 0.49403\n13:AM-9:AM  -29.380645  -87.770 29.0086 0.98920\n14:AM-9:AM  -44.141935 -102.531 14.2473 0.48663\n15:AM-9:AM  -51.522581 -109.912  6.8666 0.17781\n16:AM-9:AM  -33.070968  -91.460 25.3182 0.94789\n17:AM-9:AM  -28.387097  -86.776 30.0021 0.99364\n18:AM-9:AM  -31.509677  -89.899 26.8795 0.97131\n19:AM-9:AM  -24.058065  -82.447 34.3311 0.99968\n20:AM-9:AM  -27.677419  -92.959 37.6037 0.99943\n1:PM-9:AM   -11.283871  -69.673 47.1053 1.00000\n2:PM-9:AM   -27.606452  -85.996 30.7827 0.99595\n3:PM-9:AM   -16.606452  -74.996 41.7827 1.00000\n4:PM-9:AM   -40.593548  -98.983 17.7956 0.67322\n5:PM-9:AM   -26.400000  -84.789 31.9892 0.99812\n6:PM-9:AM   -34.703226  -93.092 23.6860 0.91136\n7:PM-9:AM   -28.600000  -86.989 29.7892 0.99284\n8:PM-9:AM   -35.129032  -93.518 23.2602 0.89961\n9:PM-9:AM   -28.174194  -86.563 30.2150 0.99436\n10:PM-9:AM  -27.322581  -85.712 31.0666 0.99660\n11:PM-9:AM  -39.529032  -97.918 18.8602 0.72631\n12:PM-9:AM  -42.083871 -100.473 16.3053 0.59536\n13:PM-9:AM  -22.851613  -81.241 35.5376 0.99989\n14:PM-9:AM  -38.180645  -96.570 20.2086 0.78850\n15:PM-9:AM  -31.083871  -89.473 27.3053 0.97602\n16:PM-9:AM  -43.858065 -102.247 14.5311 0.50145\n17:PM-9:AM  -51.451613 -109.841  6.9376 0.17988\n18:PM-9:AM  -37.896774  -96.286 20.4924 0.80070\n19:PM-9:AM  -49.961290 -108.350  8.4279 0.22767\n20:PM-9:AM  -24.270968  -82.660 34.1182 0.99962\n11:AM-10:AM  41.303226  -17.086 99.6924 0.63649\n12:AM-10:AM  -0.638710  -59.028 57.7505 1.00000\n13:AM-10:AM  13.980645  -44.409 72.3698 1.00000\n14:AM-10:AM  -0.780645  -59.170 57.6086 1.00000\n15:AM-10:AM  -8.161290  -66.550 50.2279 1.00000\n16:AM-10:AM  10.290323  -48.099 68.6795 1.00000\n17:AM-10:AM  14.974194  -43.415 73.3634 1.00000\n18:AM-10:AM  11.851613  -46.538 70.2408 1.00000\n19:AM-10:AM  19.303226  -39.086 77.6924 1.00000\n20:AM-10:AM  15.683871  -49.597 80.9650 1.00000\n1:PM-10:AM   32.077419  -26.312 90.4666 0.96397\n2:PM-10:AM   15.754839  -42.634 74.1440 1.00000\n3:PM-10:AM   26.754839  -31.634 85.1440 0.99763\n4:PM-10:AM    2.767742  -55.621 61.1569 1.00000\n5:PM-10:AM   16.961290  -41.428 75.3505 1.00000\n6:PM-10:AM    8.658065  -49.731 67.0473 1.00000\n7:PM-10:AM   14.761290  -43.628 73.1505 1.00000\n8:PM-10:AM    8.232258  -50.157 66.6215 1.00000\n9:PM-10:AM   15.187097  -43.202 73.5763 1.00000\n10:PM-10:AM  16.038710  -42.350 74.4279 1.00000\n11:PM-10:AM   3.832258  -54.557 62.2215 1.00000\n12:PM-10:AM   1.277419  -57.112 59.6666 1.00000\n13:PM-10:AM  20.509677  -37.880 78.8989 0.99999\n14:PM-10:AM   5.180645  -53.209 63.5698 1.00000\n15:PM-10:AM  12.277419  -46.112 70.6666 1.00000\n16:PM-10:AM  -0.496774  -58.886 57.8924 1.00000\n17:PM-10:AM  -8.090323  -66.480 50.2989 1.00000\n18:PM-10:AM   5.464516  -52.925 63.8537 1.00000\n19:PM-10:AM  -6.600000  -64.989 51.7892 1.00000\n20:PM-10:AM  19.090323  -39.299 77.4795 1.00000\n12:AM-11:AM -41.941935 -100.331 16.4473 0.60287\n13:AM-11:AM -27.322581  -85.712 31.0666 0.99660\n14:AM-11:AM -42.083871 -100.473 16.3053 0.59536\n15:AM-11:AM -49.464516 -107.854  8.9247 0.24541\n16:AM-11:AM -31.012903  -89.402 27.3763 0.97675\n17:AM-11:AM -26.329032  -84.718 32.0602 0.99821\n18:AM-11:AM -29.451613  -87.841 28.9376 0.98880\n19:AM-11:AM -22.000000  -80.389 36.3892 0.99995\n20:AM-11:AM -25.619355  -90.900 39.6618 0.99988\n1:PM-11:AM   -9.225806  -67.615 49.1634 1.00000\n2:PM-11:AM  -25.548387  -83.938 32.8408 0.99897\n3:PM-11:AM  -14.548387  -72.938 43.8408 1.00000\n4:PM-11:AM  -38.535484  -96.925 19.8537 0.77279\n5:PM-11:AM  -24.341935  -82.731 34.0473 0.99959\n6:PM-11:AM  -32.645161  -91.034 25.7440 0.95531\n7:PM-11:AM  -26.541935  -84.931 31.8473 0.99794\n8:PM-11:AM  -33.070968  -91.460 25.3182 0.94789\n9:PM-11:AM  -26.116129  -84.505 32.2731 0.99846\n10:PM-11:AM -25.264516  -83.654 33.1247 0.99916\n11:PM-11:AM -37.470968  -95.860 20.9182 0.81833\n12:PM-11:AM -40.025806  -98.415 18.3634 0.70190\n13:PM-11:AM -20.793548  -79.183 37.5956 0.99999\n14:PM-11:AM -36.122581  -94.512 22.2666 0.86852\n15:PM-11:AM -29.025806  -87.415 29.3634 0.99101\n16:PM-11:AM -41.800000 -100.189 16.5892 0.61038\n17:PM-11:AM -49.393548 -107.783  8.9956 0.24802\n18:PM-11:AM -35.838710  -94.228 22.5505 0.87792\n19:PM-11:AM -47.903226 -106.292 10.4860 0.30702\n20:PM-11:AM -22.212903  -80.602 36.1763 0.99994\n13:AM-12:AM  14.619355  -43.770 73.0086 1.00000\n14:AM-12:AM  -0.141935  -58.531 58.2473 1.00000\n15:AM-12:AM  -7.522581  -65.912 50.8666 1.00000\n16:AM-12:AM  10.929032  -47.460 69.3182 1.00000\n17:AM-12:AM  15.612903  -42.776 74.0021 1.00000\n18:AM-12:AM  12.490323  -45.899 70.8795 1.00000\n19:AM-12:AM  19.941935  -38.447 78.3311 1.00000\n20:AM-12:AM  16.322581  -48.959 81.6037 1.00000\n1:PM-12:AM   32.716129  -25.673 91.1053 0.95413\n2:PM-12:AM   16.393548  -41.996 74.7827 1.00000\n3:PM-12:AM   27.393548  -30.996 85.7827 0.99645\n4:PM-12:AM    3.406452  -54.983 61.7956 1.00000\n5:PM-12:AM   17.600000  -40.789 75.9892 1.00000\n6:PM-12:AM    9.296774  -49.092 67.6860 1.00000\n7:PM-12:AM   15.400000  -42.989 73.7892 1.00000\n8:PM-12:AM    8.870968  -49.518 67.2602 1.00000\n9:PM-12:AM   15.825806  -42.563 74.2150 1.00000\n10:PM-12:AM  16.677419  -41.712 75.0666 1.00000\n11:PM-12:AM   4.470968  -53.918 62.8602 1.00000\n12:PM-12:AM   1.916129  -56.473 60.3053 1.00000\n13:PM-12:AM  21.148387  -37.241 79.5376 0.99998\n14:PM-12:AM   5.819355  -52.570 64.2086 1.00000\n15:PM-12:AM  12.916129  -45.473 71.3053 1.00000\n16:PM-12:AM   0.141935  -58.247 58.5311 1.00000\n17:PM-12:AM  -7.451613  -65.841 50.9376 1.00000\n18:PM-12:AM   6.103226  -52.286 64.4924 1.00000\n19:PM-12:AM  -5.961290  -64.350 52.4279 1.00000\n20:PM-12:AM  19.729032  -38.660 78.1182 1.00000\n14:AM-13:AM -14.761290  -73.150 43.6279 1.00000\n15:AM-13:AM -22.141935  -80.531 36.2473 0.99994\n16:AM-13:AM  -3.690323  -62.080 54.6989 1.00000\n17:AM-13:AM   0.993548  -57.396 59.3827 1.00000\n18:AM-13:AM  -2.129032  -60.518 56.2602 1.00000\n19:AM-13:AM   5.322581  -53.067 63.7118 1.00000\n20:AM-13:AM   1.703226  -63.578 66.9843 1.00000\n1:PM-13:AM   18.096774  -40.292 76.4860 1.00000\n2:PM-13:AM    1.774194  -56.615 60.1634 1.00000\n3:PM-13:AM   12.774194  -45.615 71.1634 1.00000\n4:PM-13:AM  -11.212903  -69.602 47.1763 1.00000\n5:PM-13:AM    2.980645  -55.409 61.3698 1.00000\n6:PM-13:AM   -5.322581  -63.712 53.0666 1.00000\n7:PM-13:AM    0.780645  -57.609 59.1698 1.00000\n8:PM-13:AM   -5.748387  -64.138 52.6408 1.00000\n9:PM-13:AM    1.206452  -57.183 59.5956 1.00000\n10:PM-13:AM   2.058065  -56.331 60.4473 1.00000\n11:PM-13:AM -10.148387  -68.538 48.2408 1.00000\n12:PM-13:AM -12.703226  -71.092 45.6860 1.00000\n13:PM-13:AM   6.529032  -51.860 64.9182 1.00000\n14:PM-13:AM  -8.800000  -67.189 49.5892 1.00000\n15:PM-13:AM  -1.703226  -60.092 56.6860 1.00000\n16:PM-13:AM -14.477419  -72.867 43.9118 1.00000\n17:PM-13:AM -22.070968  -80.460 36.3182 0.99995\n18:PM-13:AM  -8.516129  -66.905 49.8731 1.00000\n19:PM-13:AM -20.580645  -78.970 37.8086 0.99999\n20:PM-13:AM   5.109677  -53.280 63.4989 1.00000\n15:AM-14:AM  -7.380645  -65.770 51.0086 1.00000\n16:AM-14:AM  11.070968  -47.318 69.4602 1.00000\n17:AM-14:AM  15.754839  -42.634 74.1440 1.00000\n18:AM-14:AM  12.632258  -45.757 71.0215 1.00000\n19:AM-14:AM  20.083871  -38.305 78.4731 0.99999\n20:AM-14:AM  16.464516  -48.817 81.7456 1.00000\n1:PM-14:AM   32.858065  -25.531 91.2473 0.95170\n2:PM-14:AM   16.535484  -41.854 74.9247 1.00000\n3:PM-14:AM   27.535484  -30.854 85.9247 0.99612\n4:PM-14:AM    3.548387  -54.841 61.9376 1.00000\n5:PM-14:AM   17.741935  -40.647 76.1311 1.00000\n6:PM-14:AM    9.438710  -48.950 67.8279 1.00000\n7:PM-14:AM   15.541935  -42.847 73.9311 1.00000\n8:PM-14:AM    9.012903  -49.376 67.4021 1.00000\n9:PM-14:AM   15.967742  -42.421 74.3569 1.00000\n10:PM-14:AM  16.819355  -41.570 75.2086 1.00000\n11:PM-14:AM   4.612903  -53.776 63.0021 1.00000\n12:PM-14:AM   2.058065  -56.331 60.4473 1.00000\n13:PM-14:AM  21.290323  -37.099 79.6795 0.99998\n14:PM-14:AM   5.961290  -52.428 64.3505 1.00000\n15:PM-14:AM  13.058065  -45.331 71.4473 1.00000\n16:PM-14:AM   0.283871  -58.105 58.6731 1.00000\n17:PM-14:AM  -7.309677  -65.699 51.0795 1.00000\n18:PM-14:AM   6.245161  -52.144 64.6344 1.00000\n19:PM-14:AM  -5.819355  -64.209 52.5698 1.00000\n20:PM-14:AM  19.870968  -38.518 78.2602 1.00000\n16:AM-15:AM  18.451613  -39.938 76.8408 1.00000\n17:AM-15:AM  23.135484  -35.254 81.5247 0.99986\n18:AM-15:AM  20.012903  -38.376 78.4021 0.99999\n19:AM-15:AM  27.464516  -30.925 85.8537 0.99629\n20:AM-15:AM  23.845161  -41.436 89.1263 0.99998\n1:PM-15:AM   40.238710  -18.150 98.6279 0.69123\n2:PM-15:AM   23.916129  -34.473 82.3053 0.99972\n3:PM-15:AM   34.916129  -23.473 93.3053 0.90560\n4:PM-15:AM   10.929032  -47.460 69.3182 1.00000\n5:PM-15:AM   25.122581  -33.267 83.5118 0.99925\n6:PM-15:AM   16.819355  -41.570 75.2086 1.00000\n7:PM-15:AM   22.922581  -35.467 81.3118 0.99988\n8:PM-15:AM   16.393548  -41.996 74.7827 1.00000\n9:PM-15:AM   23.348387  -35.041 81.7376 0.99983\n10:PM-15:AM  24.200000  -34.189 82.5892 0.99964\n11:PM-15:AM  11.993548  -46.396 70.3827 1.00000\n12:PM-15:AM   9.438710  -48.950 67.8279 1.00000\n13:PM-15:AM  28.670968  -29.718 87.0602 0.99256\n14:PM-15:AM  13.341935  -45.047 71.7311 1.00000\n15:PM-15:AM  20.438710  -37.950 78.8279 0.99999\n16:PM-15:AM   7.664516  -50.725 66.0537 1.00000\n17:PM-15:AM   0.070968  -58.318 58.4602 1.00000\n18:PM-15:AM  13.625806  -44.763 72.0150 1.00000\n19:PM-15:AM   1.561290  -56.828 59.9505 1.00000\n20:PM-15:AM  27.251613  -31.138 85.6408 0.99674\n17:AM-16:AM   4.683871  -53.705 63.0731 1.00000\n18:AM-16:AM   1.561290  -56.828 59.9505 1.00000\n19:AM-16:AM   9.012903  -49.376 67.4021 1.00000\n20:AM-16:AM   5.393548  -59.888 70.6747 1.00000\n1:PM-16:AM   21.787097  -36.602 80.1763 0.99996\n2:PM-16:AM    5.464516  -52.925 63.8537 1.00000\n3:PM-16:AM   16.464516  -41.925 74.8537 1.00000\n4:PM-16:AM   -7.522581  -65.912 50.8666 1.00000\n5:PM-16:AM    6.670968  -51.718 65.0602 1.00000\n6:PM-16:AM   -1.632258  -60.021 56.7569 1.00000\n7:PM-16:AM    4.470968  -53.918 62.8602 1.00000\n8:PM-16:AM   -2.058065  -60.447 56.3311 1.00000\n9:PM-16:AM    4.896774  -53.492 63.2860 1.00000\n10:PM-16:AM   5.748387  -52.641 64.1376 1.00000\n11:PM-16:AM  -6.458065  -64.847 51.9311 1.00000\n12:PM-16:AM  -9.012903  -67.402 49.3763 1.00000\n13:PM-16:AM  10.219355  -48.170 68.6086 1.00000\n14:PM-16:AM  -5.109677  -63.499 53.2795 1.00000\n15:PM-16:AM   1.987097  -56.402 60.3763 1.00000\n16:PM-16:AM -10.787097  -69.176 47.6021 1.00000\n17:PM-16:AM -18.380645  -76.770 40.0086 1.00000\n18:PM-16:AM  -4.825806  -63.215 53.5634 1.00000\n19:PM-16:AM -16.890323  -75.280 41.4989 1.00000\n20:PM-16:AM   8.800000  -49.589 67.1892 1.00000\n18:AM-17:AM  -3.122581  -61.512 55.2666 1.00000\n19:AM-17:AM   4.329032  -54.060 62.7182 1.00000\n20:AM-17:AM   0.709677  -64.571 65.9908 1.00000\n1:PM-17:AM   17.103226  -41.286 75.4924 1.00000\n2:PM-17:AM    0.780645  -57.609 59.1698 1.00000\n3:PM-17:AM   11.780645  -46.609 70.1698 1.00000\n4:PM-17:AM  -12.206452  -70.596 46.1827 1.00000\n5:PM-17:AM    1.987097  -56.402 60.3763 1.00000\n6:PM-17:AM   -6.316129  -64.705 52.0731 1.00000\n7:PM-17:AM   -0.212903  -58.602 58.1763 1.00000\n8:PM-17:AM   -6.741935  -65.131 51.6473 1.00000\n9:PM-17:AM    0.212903  -58.176 58.6021 1.00000\n10:PM-17:AM   1.064516  -57.325 59.4537 1.00000\n11:PM-17:AM -11.141935  -69.531 47.2473 1.00000\n12:PM-17:AM -13.696774  -72.086 44.6924 1.00000\n13:PM-17:AM   5.535484  -52.854 63.9247 1.00000\n14:PM-17:AM  -9.793548  -68.183 48.5956 1.00000\n15:PM-17:AM  -2.696774  -61.086 55.6924 1.00000\n16:PM-17:AM -15.470968  -73.860 42.9182 1.00000\n17:PM-17:AM -23.064516  -81.454 35.3247 0.99987\n18:PM-17:AM  -9.509677  -67.899 48.8795 1.00000\n19:PM-17:AM -21.574194  -79.963 36.8150 0.99997\n20:PM-17:AM   4.116129  -54.273 62.5053 1.00000\n19:AM-18:AM   7.451613  -50.938 65.8408 1.00000\n20:AM-18:AM   3.832258  -61.449 69.1134 1.00000\n1:PM-18:AM   20.225806  -38.163 78.6150 0.99999\n2:PM-18:AM    3.903226  -54.486 62.2924 1.00000\n3:PM-18:AM   14.903226  -43.486 73.2924 1.00000\n4:PM-18:AM   -9.083871  -67.473 49.3053 1.00000\n5:PM-18:AM    5.109677  -53.280 63.4989 1.00000\n6:PM-18:AM   -3.193548  -61.583 55.1956 1.00000\n7:PM-18:AM    2.909677  -55.480 61.2989 1.00000\n8:PM-18:AM   -3.619355  -62.009 54.7698 1.00000\n9:PM-18:AM    3.335484  -55.054 61.7247 1.00000\n10:PM-18:AM   4.187097  -54.202 62.5763 1.00000\n11:PM-18:AM  -8.019355  -66.409 50.3698 1.00000\n12:PM-18:AM -10.574194  -68.963 47.8150 1.00000\n13:PM-18:AM   8.658065  -49.731 67.0473 1.00000\n14:PM-18:AM  -6.670968  -65.060 51.7182 1.00000\n15:PM-18:AM   0.425806  -57.963 58.8150 1.00000\n16:PM-18:AM -12.348387  -70.738 46.0408 1.00000\n17:PM-18:AM -19.941935  -78.331 38.4473 1.00000\n18:PM-18:AM  -6.387097  -64.776 52.0021 1.00000\n19:PM-18:AM -18.451613  -76.841 39.9376 1.00000\n20:PM-18:AM   7.238710  -51.150 65.6279 1.00000\n20:AM-19:AM  -3.619355  -68.900 61.6618 1.00000\n1:PM-19:AM   12.774194  -45.615 71.1634 1.00000\n2:PM-19:AM   -3.548387  -61.938 54.8408 1.00000\n3:PM-19:AM    7.451613  -50.938 65.8408 1.00000\n4:PM-19:AM  -16.535484  -74.925 41.8537 1.00000\n5:PM-19:AM   -2.341935  -60.731 56.0473 1.00000\n6:PM-19:AM  -10.645161  -69.034 47.7440 1.00000\n7:PM-19:AM   -4.541935  -62.931 53.8473 1.00000\n8:PM-19:AM  -11.070968  -69.460 47.3182 1.00000\n9:PM-19:AM   -4.116129  -62.505 54.2731 1.00000\n10:PM-19:AM  -3.264516  -61.654 55.1247 1.00000\n11:PM-19:AM -15.470968  -73.860 42.9182 1.00000\n12:PM-19:AM -18.025806  -76.415 40.3634 1.00000\n13:PM-19:AM   1.206452  -57.183 59.5956 1.00000\n14:PM-19:AM -14.122581  -72.512 44.2666 1.00000\n15:PM-19:AM  -7.025806  -65.415 51.3634 1.00000\n16:PM-19:AM -19.800000  -78.189 38.5892 1.00000\n17:PM-19:AM -27.393548  -85.783 30.9956 0.99645\n18:PM-19:AM -13.838710  -72.228 44.5505 1.00000\n19:PM-19:AM -25.903226  -84.292 32.4860 0.99867\n20:PM-19:AM  -0.212903  -58.602 58.1763 1.00000\n1:PM-20:AM   16.393548  -48.888 81.6747 1.00000\n2:PM-20:AM    0.070968  -65.210 65.3521 1.00000\n3:PM-20:AM   11.070968  -54.210 76.3521 1.00000\n4:PM-20:AM  -12.916129  -78.197 52.3650 1.00000\n5:PM-20:AM    1.277419  -64.004 66.5585 1.00000\n6:PM-20:AM   -7.025806  -72.307 58.2553 1.00000\n7:PM-20:AM   -0.922581  -66.204 64.3585 1.00000\n8:PM-20:AM   -7.451613  -72.733 57.8295 1.00000\n9:PM-20:AM   -0.496774  -65.778 64.7843 1.00000\n10:PM-20:AM   0.354839  -64.926 65.6359 1.00000\n11:PM-20:AM -11.851613  -77.133 53.4295 1.00000\n12:PM-20:AM -14.406452  -79.688 50.8747 1.00000\n13:PM-20:AM   4.825806  -60.455 70.1069 1.00000\n14:PM-20:AM -10.503226  -75.784 54.7779 1.00000\n15:PM-20:AM  -3.406452  -68.688 61.8747 1.00000\n16:PM-20:AM -16.180645  -81.462 49.1005 1.00000\n17:PM-20:AM -23.774194  -89.055 41.5069 0.99998\n18:PM-20:AM -10.219355  -75.500 55.0618 1.00000\n19:PM-20:AM -22.283871  -87.565 42.9972 1.00000\n20:PM-20:AM   3.406452  -61.875 68.6876 1.00000\n2:PM-1:PM   -16.322581  -74.712 42.0666 1.00000\n3:PM-1:PM    -5.322581  -63.712 53.0666 1.00000\n4:PM-1:PM   -29.309677  -87.699 29.0795 0.98958\n5:PM-1:PM   -15.116129  -73.505 43.2731 1.00000\n6:PM-1:PM   -23.419355  -81.809 34.9698 0.99982\n7:PM-1:PM   -17.316129  -75.705 41.0731 1.00000\n8:PM-1:PM   -23.845161  -82.234 34.5440 0.99973\n9:PM-1:PM   -16.890323  -75.280 41.4989 1.00000\n10:PM-1:PM  -16.038710  -74.428 42.3505 1.00000\n11:PM-1:PM  -28.245161  -86.634 30.1440 0.99413\n12:PM-1:PM  -30.800000  -89.189 27.5892 0.97882\n13:PM-1:PM  -11.567742  -69.957 46.8215 1.00000\n14:PM-1:PM  -26.896774  -85.286 31.4924 0.99740\n15:PM-1:PM  -19.800000  -78.189 38.5892 1.00000\n16:PM-1:PM  -32.574194  -90.963 25.8150 0.95647\n17:PM-1:PM  -40.167742  -98.557 18.2215 0.69480\n18:PM-1:PM  -26.612903  -85.002 31.7763 0.99784\n19:PM-1:PM  -38.677419  -97.067 19.7118 0.76637\n20:PM-1:PM  -12.987097  -71.376 45.4021 1.00000\n3:PM-2:PM    11.000000  -47.389 69.3892 1.00000\n4:PM-2:PM   -12.987097  -71.376 45.4021 1.00000\n5:PM-2:PM     1.206452  -57.183 59.5956 1.00000\n6:PM-2:PM    -7.096774  -65.486 51.2924 1.00000\n7:PM-2:PM    -0.993548  -59.383 57.3956 1.00000\n8:PM-2:PM    -7.522581  -65.912 50.8666 1.00000\n9:PM-2:PM    -0.567742  -58.957 57.8215 1.00000\n10:PM-2:PM    0.283871  -58.105 58.6731 1.00000\n11:PM-2:PM  -11.922581  -70.312 46.4666 1.00000\n12:PM-2:PM  -14.477419  -72.867 43.9118 1.00000\n13:PM-2:PM    4.754839  -53.634 63.1440 1.00000\n14:PM-2:PM  -10.574194  -68.963 47.8150 1.00000\n15:PM-2:PM   -3.477419  -61.867 54.9118 1.00000\n16:PM-2:PM  -16.251613  -74.641 42.1376 1.00000\n17:PM-2:PM  -23.845161  -82.234 34.5440 0.99973\n18:PM-2:PM  -10.290323  -68.680 48.0989 1.00000\n19:PM-2:PM  -22.354839  -80.744 36.0344 0.99993\n20:PM-2:PM    3.335484  -55.054 61.7247 1.00000\n4:PM-3:PM   -23.987097  -82.376 34.4021 0.99970\n5:PM-3:PM    -9.793548  -68.183 48.5956 1.00000\n6:PM-3:PM   -18.096774  -76.486 40.2924 1.00000\n7:PM-3:PM   -11.993548  -70.383 46.3956 1.00000\n8:PM-3:PM   -18.522581  -76.912 39.8666 1.00000\n9:PM-3:PM   -11.567742  -69.957 46.8215 1.00000\n10:PM-3:PM  -10.716129  -69.105 47.6731 1.00000\n11:PM-3:PM  -22.922581  -81.312 35.4666 0.99988\n12:PM-3:PM  -25.477419  -83.867 32.9118 0.99902\n13:PM-3:PM   -6.245161  -64.634 52.1440 1.00000\n14:PM-3:PM  -21.574194  -79.963 36.8150 0.99997\n15:PM-3:PM  -14.477419  -72.867 43.9118 1.00000\n16:PM-3:PM  -27.251613  -85.641 31.1376 0.99674\n17:PM-3:PM  -34.845161  -93.234 23.5440 0.90755\n18:PM-3:PM  -21.290323  -79.680 37.0989 0.99998\n19:PM-3:PM  -33.354839  -91.744 25.0344 0.94247\n20:PM-3:PM   -7.664516  -66.054 50.7247 1.00000\n5:PM-4:PM    14.193548  -44.196 72.5827 1.00000\n6:PM-4:PM     5.890323  -52.499 64.2795 1.00000\n7:PM-4:PM    11.993548  -46.396 70.3827 1.00000\n8:PM-4:PM     5.464516  -52.925 63.8537 1.00000\n9:PM-4:PM    12.419355  -45.970 70.8086 1.00000\n10:PM-4:PM   13.270968  -45.118 71.6602 1.00000\n11:PM-4:PM    1.064516  -57.325 59.4537 1.00000\n12:PM-4:PM   -1.490323  -59.880 56.8989 1.00000\n13:PM-4:PM   17.741935  -40.647 76.1311 1.00000\n14:PM-4:PM    2.412903  -55.976 60.8021 1.00000\n15:PM-4:PM    9.509677  -48.880 67.8989 1.00000\n16:PM-4:PM   -3.264516  -61.654 55.1247 1.00000\n17:PM-4:PM  -10.858065  -69.247 47.5311 1.00000\n18:PM-4:PM    2.696774  -55.692 61.0860 1.00000\n19:PM-4:PM   -9.367742  -67.757 49.0215 1.00000\n20:PM-4:PM   16.322581  -42.067 74.7118 1.00000\n6:PM-5:PM    -8.303226  -66.692 50.0860 1.00000\n7:PM-5:PM    -2.200000  -60.589 56.1892 1.00000\n8:PM-5:PM    -8.729032  -67.118 49.6602 1.00000\n9:PM-5:PM    -1.774194  -60.163 56.6150 1.00000\n10:PM-5:PM   -0.922581  -59.312 57.4666 1.00000\n11:PM-5:PM  -13.129032  -71.518 45.2602 1.00000\n12:PM-5:PM  -15.683871  -74.073 42.7053 1.00000\n13:PM-5:PM    3.548387  -54.841 61.9376 1.00000\n14:PM-5:PM  -11.780645  -70.170 46.6086 1.00000\n15:PM-5:PM   -4.683871  -63.073 53.7053 1.00000\n16:PM-5:PM  -17.458065  -75.847 40.9311 1.00000\n17:PM-5:PM  -25.051613  -83.441 33.3376 0.99929\n18:PM-5:PM  -11.496774  -69.886 46.8924 1.00000\n19:PM-5:PM  -23.561290  -81.950 34.8279 0.99979\n20:PM-5:PM    2.129032  -56.260 60.5182 1.00000\n7:PM-6:PM     6.103226  -52.286 64.4924 1.00000\n8:PM-6:PM    -0.425806  -58.815 57.9634 1.00000\n9:PM-6:PM     6.529032  -51.860 64.9182 1.00000\n10:PM-6:PM    7.380645  -51.009 65.7698 1.00000\n11:PM-6:PM   -4.825806  -63.215 53.5634 1.00000\n12:PM-6:PM   -7.380645  -65.770 51.0086 1.00000\n13:PM-6:PM   11.851613  -46.538 70.2408 1.00000\n14:PM-6:PM   -3.477419  -61.867 54.9118 1.00000\n15:PM-6:PM    3.619355  -54.770 62.0086 1.00000\n16:PM-6:PM   -9.154839  -67.544 49.2344 1.00000\n17:PM-6:PM  -16.748387  -75.138 41.6408 1.00000\n18:PM-6:PM   -3.193548  -61.583 55.1956 1.00000\n19:PM-6:PM  -15.258065  -73.647 43.1311 1.00000\n20:PM-6:PM   10.432258  -47.957 68.8215 1.00000\n8:PM-7:PM    -6.529032  -64.918 51.8602 1.00000\n9:PM-7:PM     0.425806  -57.963 58.8150 1.00000\n10:PM-7:PM    1.277419  -57.112 59.6666 1.00000\n11:PM-7:PM  -10.929032  -69.318 47.4602 1.00000\n12:PM-7:PM  -13.483871  -71.873 44.9053 1.00000\n13:PM-7:PM    5.748387  -52.641 64.1376 1.00000\n14:PM-7:PM   -9.580645  -67.970 48.8086 1.00000\n15:PM-7:PM   -2.483871  -60.873 55.9053 1.00000\n16:PM-7:PM  -15.258065  -73.647 43.1311 1.00000\n17:PM-7:PM  -22.851613  -81.241 35.5376 0.99989\n18:PM-7:PM   -9.296774  -67.686 49.0924 1.00000\n19:PM-7:PM  -21.361290  -79.750 37.0279 0.99998\n20:PM-7:PM    4.329032  -54.060 62.7182 1.00000\n9:PM-8:PM     6.954839  -51.434 65.3440 1.00000\n10:PM-8:PM    7.806452  -50.583 66.1956 1.00000\n11:PM-8:PM   -4.400000  -62.789 53.9892 1.00000\n12:PM-8:PM   -6.954839  -65.344 51.4344 1.00000\n13:PM-8:PM   12.277419  -46.112 70.6666 1.00000\n14:PM-8:PM   -3.051613  -61.441 55.3376 1.00000\n15:PM-8:PM    4.045161  -54.344 62.4344 1.00000\n16:PM-8:PM   -8.729032  -67.118 49.6602 1.00000\n17:PM-8:PM  -16.322581  -74.712 42.0666 1.00000\n18:PM-8:PM   -2.767742  -61.157 55.6215 1.00000\n19:PM-8:PM  -14.832258  -73.221 43.5569 1.00000\n20:PM-8:PM   10.858065  -47.531 69.2473 1.00000\n10:PM-9:PM    0.851613  -57.538 59.2408 1.00000\n11:PM-9:PM  -11.354839  -69.744 47.0344 1.00000\n12:PM-9:PM  -13.909677  -72.299 44.4795 1.00000\n13:PM-9:PM    5.322581  -53.067 63.7118 1.00000\n14:PM-9:PM  -10.006452  -68.396 48.3827 1.00000\n15:PM-9:PM   -2.909677  -61.299 55.4795 1.00000\n16:PM-9:PM  -15.683871  -74.073 42.7053 1.00000\n17:PM-9:PM  -23.277419  -81.667 35.1118 0.99984\n18:PM-9:PM   -9.722581  -68.112 48.6666 1.00000\n19:PM-9:PM  -21.787097  -80.176 36.6021 0.99996\n20:PM-9:PM    3.903226  -54.486 62.2924 1.00000\n11:PM-10:PM -12.206452  -70.596 46.1827 1.00000\n12:PM-10:PM -14.761290  -73.150 43.6279 1.00000\n13:PM-10:PM   4.470968  -53.918 62.8602 1.00000\n14:PM-10:PM -10.858065  -69.247 47.5311 1.00000\n15:PM-10:PM  -3.761290  -62.150 54.6279 1.00000\n16:PM-10:PM -16.535484  -74.925 41.8537 1.00000\n17:PM-10:PM -24.129032  -82.518 34.2602 0.99966\n18:PM-10:PM -10.574194  -68.963 47.8150 1.00000\n19:PM-10:PM -22.638710  -81.028 35.7505 0.99991\n20:PM-10:PM   3.051613  -55.338 61.4408 1.00000\n12:PM-11:PM  -2.554839  -60.944 55.8344 1.00000\n13:PM-11:PM  16.677419  -41.712 75.0666 1.00000\n14:PM-11:PM   1.348387  -57.041 59.7376 1.00000\n15:PM-11:PM   8.445161  -49.944 66.8344 1.00000\n16:PM-11:PM  -4.329032  -62.718 54.0602 1.00000\n17:PM-11:PM -11.922581  -70.312 46.4666 1.00000\n18:PM-11:PM   1.632258  -56.757 60.0215 1.00000\n19:PM-11:PM -10.432258  -68.821 47.9569 1.00000\n20:PM-11:PM  15.258065  -43.131 73.6473 1.00000\n13:PM-12:PM  19.232258  -39.157 77.6215 1.00000\n14:PM-12:PM   3.903226  -54.486 62.2924 1.00000\n15:PM-12:PM  11.000000  -47.389 69.3892 1.00000\n16:PM-12:PM  -1.774194  -60.163 56.6150 1.00000\n17:PM-12:PM  -9.367742  -67.757 49.0215 1.00000\n18:PM-12:PM   4.187097  -54.202 62.5763 1.00000\n19:PM-12:PM  -7.877419  -66.267 50.5118 1.00000\n20:PM-12:PM  17.812903  -40.576 76.2021 1.00000\n14:PM-13:PM -15.329032  -73.718 43.0602 1.00000\n15:PM-13:PM  -8.232258  -66.621 50.1569 1.00000\n16:PM-13:PM -21.006452  -79.396 37.3827 0.99998\n17:PM-13:PM -28.600000  -86.989 29.7892 0.99284\n18:PM-13:PM -15.045161  -73.434 43.3440 1.00000\n19:PM-13:PM -27.109677  -85.499 31.2795 0.99702\n20:PM-13:PM  -1.419355  -59.809 56.9698 1.00000\n15:PM-14:PM   7.096774  -51.292 65.4860 1.00000\n16:PM-14:PM  -5.677419  -64.067 52.7118 1.00000\n17:PM-14:PM -13.270968  -71.660 45.1182 1.00000\n18:PM-14:PM   0.283871  -58.105 58.6731 1.00000\n19:PM-14:PM -11.780645  -70.170 46.6086 1.00000\n20:PM-14:PM  13.909677  -44.480 72.2989 1.00000\n16:PM-15:PM -12.774194  -71.163 45.6150 1.00000\n17:PM-15:PM -20.367742  -78.757 38.0215 0.99999\n18:PM-15:PM  -6.812903  -65.202 51.5763 1.00000\n19:PM-15:PM -18.877419  -77.267 39.5118 1.00000\n20:PM-15:PM   6.812903  -51.576 65.2021 1.00000\n17:PM-16:PM  -7.593548  -65.983 50.7956 1.00000\n18:PM-16:PM   5.961290  -52.428 64.3505 1.00000\n19:PM-16:PM  -6.103226  -64.492 52.2860 1.00000\n20:PM-16:PM  19.587097  -38.802 77.9763 1.00000\n18:PM-17:PM  13.554839  -44.834 71.9440 1.00000\n19:PM-17:PM   1.490323  -56.899 59.8795 1.00000\n20:PM-17:PM  27.180645  -31.209 85.5698 0.99689\n19:PM-18:PM -12.064516  -70.454 46.3247 1.00000\n20:PM-18:PM  13.625806  -44.763 72.0150 1.00000\n20:PM-19:PM  25.690323  -32.699 84.0795 0.99886\n\n\n  Tukey multiple comparisons of means\n    99% family-wise confidence level\n\nFit: aov(formula = response ~ Day + noon + Day:noon, data = data)\n\n$Day\n            diff     lwr    upr   p adj\n2-1    -6.138710 -48.905 36.628 1.00000\n3-1    -4.080645 -46.847 38.686 1.00000\n4-1   -14.300000 -57.066 28.466 0.99700\n5-1    -3.335484 -46.102 39.431 1.00000\n6-1    -6.812903 -49.579 35.953 1.00000\n7-1    -6.422581 -49.189 36.344 1.00000\n8-1    -4.364516 -47.131 38.402 1.00000\n9-1     8.551613 -34.215 51.318 1.00000\n10-1  -12.703226 -55.470 30.063 0.99934\n11-1    1.845161 -40.921 44.611 1.00000\n12-1  -20.403226 -63.170 22.363 0.89330\n13-1   -3.477419 -46.244 39.289 1.00000\n14-1  -18.522581 -61.289 24.244 0.95243\n15-1  -18.664516 -61.431 24.102 0.94905\n16-1  -15.825806 -58.592 26.941 0.99024\n17-1  -17.280645 -60.047 25.486 0.97539\n18-1  -12.064516 -54.831 30.702 0.99968\n19-1  -14.370968 -57.137 28.395 0.99682\n20-1   -2.994839 -47.849 41.859 1.00000\n3-2     2.058065 -40.708 44.824 1.00000\n4-2    -8.161290 -50.928 34.605 1.00000\n5-2     2.803226 -39.963 45.570 1.00000\n6-2    -0.674194 -43.441 42.092 1.00000\n7-2    -0.283871 -43.050 42.482 1.00000\n8-2     1.774194 -40.992 44.541 1.00000\n9-2    14.690323 -28.076 57.457 0.99585\n10-2   -6.564516 -49.331 36.202 1.00000\n11-2    7.983871 -34.782 50.750 1.00000\n12-2  -14.264516 -57.031 28.502 0.99709\n13-2    2.661290 -40.105 45.428 1.00000\n14-2  -12.383871 -55.150 30.382 0.99953\n15-2  -12.525806 -55.292 30.241 0.99946\n16-2   -9.687097 -52.453 33.079 0.99999\n17-2  -11.141935 -53.908 31.624 0.99990\n18-2   -5.925806 -48.692 36.841 1.00000\n19-2   -8.232258 -50.999 34.534 1.00000\n20-2    3.143871 -41.710 47.998 1.00000\n4-3   -10.219355 -52.986 32.547 0.99997\n5-3     0.745161 -42.021 43.511 1.00000\n6-3    -2.732258 -45.499 40.034 1.00000\n7-3    -2.341935 -45.108 40.424 1.00000\n8-3    -0.283871 -43.050 42.482 1.00000\n9-3    12.632258 -30.134 55.399 0.99939\n10-3   -8.622581 -51.389 34.144 1.00000\n11-3    5.925806 -36.841 48.692 1.00000\n12-3  -16.322581 -59.089 26.444 0.98634\n13-3    0.603226 -42.163 43.370 1.00000\n14-3  -14.441935 -57.208 28.324 0.99662\n15-3  -14.583871 -57.350 28.182 0.99620\n16-3  -11.745161 -54.511 31.021 0.99978\n17-3  -13.200000 -55.966 29.566 0.99891\n18-3   -7.983871 -50.750 34.782 1.00000\n19-3  -10.290323 -53.057 32.476 0.99997\n20-3    1.085806 -43.768 45.939 1.00000\n5-4    10.964516 -31.802 53.731 0.99992\n6-4     7.487097 -35.279 50.253 1.00000\n7-4     7.877419 -34.889 50.644 1.00000\n8-4     9.935484 -32.831 52.702 0.99998\n9-4    22.851613 -19.915 65.618 0.76824\n10-4    1.596774 -41.170 44.363 1.00000\n11-4   16.145161 -26.621 58.911 0.98785\n12-4   -6.103226 -48.870 36.663 1.00000\n13-4   10.822581 -31.944 53.589 0.99993\n14-4   -4.222581 -46.989 38.544 1.00000\n15-4   -4.364516 -47.131 38.402 1.00000\n16-4   -1.525806 -44.292 41.241 1.00000\n17-4   -2.980645 -45.747 39.786 1.00000\n18-4    2.235484 -40.531 45.002 1.00000\n19-4   -0.070968 -42.837 42.695 1.00000\n20-4   11.305161 -33.549 56.159 0.99994\n6-5    -3.477419 -46.244 39.289 1.00000\n7-5    -3.087097 -45.853 39.679 1.00000\n8-5    -1.029032 -43.795 41.737 1.00000\n9-5    11.887097 -30.879 54.653 0.99974\n10-5   -9.367742 -52.134 33.399 0.99999\n11-5    5.180645 -37.586 47.947 1.00000\n12-5  -17.067742 -59.834 25.699 0.97827\n13-5   -0.141935 -42.908 42.624 1.00000\n14-5  -15.187097 -57.953 27.579 0.99387\n15-5  -15.329032 -58.095 27.437 0.99318\n16-5  -12.490323 -55.257 30.276 0.99948\n17-5  -13.945161 -56.711 28.821 0.99780\n18-5   -8.729032 -51.495 34.037 1.00000\n19-5  -11.035484 -53.802 31.731 0.99991\n20-5    0.340645 -44.513 45.194 1.00000\n7-6     0.390323 -42.376 43.157 1.00000\n8-6     2.448387 -40.318 45.215 1.00000\n9-6    15.364516 -27.402 58.131 0.99300\n10-6   -5.890323 -48.657 36.876 1.00000\n11-6    8.658065 -34.108 51.424 1.00000\n12-6  -13.590323 -56.357 29.176 0.99841\n13-6    3.335484 -39.431 46.102 1.00000\n14-6  -11.709677 -54.476 31.057 0.99979\n15-6  -11.851613 -54.618 30.915 0.99975\n16-6   -9.012903 -51.779 33.753 1.00000\n17-6  -10.467742 -53.234 32.299 0.99996\n18-6   -5.251613 -48.018 37.515 1.00000\n19-6   -7.558065 -50.324 35.208 1.00000\n20-6    3.818065 -41.036 48.672 1.00000\n8-7     2.058065 -40.708 44.824 1.00000\n9-7    14.974194 -27.792 57.741 0.99480\n10-7   -6.280645 -49.047 36.486 1.00000\n11-7    8.267742 -34.499 51.034 1.00000\n12-7  -13.980645 -56.747 28.786 0.99773\n13-7    2.945161 -39.821 45.711 1.00000\n14-7  -12.100000 -54.866 30.666 0.99966\n15-7  -12.241935 -55.008 30.524 0.99960\n16-7   -9.403226 -52.170 33.363 0.99999\n17-7  -10.858065 -53.624 31.908 0.99993\n18-7   -5.641935 -48.408 37.124 1.00000\n19-7   -7.948387 -50.715 34.818 1.00000\n20-7    3.427742 -41.426 48.281 1.00000\n9-8    12.916129 -29.850 55.682 0.99918\n10-8   -8.338710 -51.105 34.428 1.00000\n11-8    6.209677 -36.557 48.976 1.00000\n12-8  -16.038710 -58.805 26.728 0.98869\n13-8    0.887097 -41.879 43.653 1.00000\n14-8  -14.158065 -56.924 28.608 0.99735\n15-8  -14.300000 -57.066 28.466 0.99700\n16-8  -11.461290 -54.228 31.305 0.99984\n17-8  -12.916129 -55.682 29.850 0.99918\n18-8   -7.700000 -50.466 35.066 1.00000\n19-8  -10.006452 -52.773 32.760 0.99998\n20-8    1.369677 -43.484 46.223 1.00000\n10-9  -21.254839 -64.021 21.511 0.85576\n11-9   -6.706452 -49.473 36.060 1.00000\n12-9  -28.954839 -71.721 13.811 0.35269\n13-9  -12.029032 -54.795 30.737 0.99969\n14-9  -27.074194 -69.841 15.692 0.47719\n15-9  -27.216129 -69.982 15.550 0.46731\n16-9  -24.377419 -67.144 18.389 0.66832\n17-9  -25.832258 -68.599 16.934 0.56535\n18-9  -20.616129 -63.382 22.150 0.88455\n19-9  -22.922581 -65.689 19.844 0.76389\n20-9  -11.546452 -56.400 33.307 0.99991\n11-10  14.548387 -28.218 57.315 0.99631\n12-10  -7.700000 -50.466 35.066 1.00000\n13-10   9.225806 -33.541 51.992 0.99999\n14-10  -5.819355 -48.586 36.947 1.00000\n15-10  -5.961290 -48.728 36.805 1.00000\n16-10  -3.122581 -45.889 39.644 1.00000\n17-10  -4.577419 -47.344 38.189 1.00000\n18-10   0.638710 -42.128 43.405 1.00000\n19-10  -1.667742 -44.434 41.099 1.00000\n20-10   9.708387 -35.145 54.562 0.99999\n12-11 -22.248387 -65.015 20.518 0.80373\n13-11  -5.322581 -48.089 37.444 1.00000\n14-11 -20.367742 -63.134 22.399 0.89472\n15-11 -20.509677 -63.276 22.257 0.88898\n16-11 -17.670968 -60.437 25.095 0.96936\n17-11 -19.125806 -61.892 23.641 0.93692\n18-11 -13.909677 -56.676 28.857 0.99787\n19-11 -16.216129 -58.982 26.550 0.98726\n20-11  -4.840000 -49.694 40.014 1.00000\n13-12  16.925806 -25.841 59.692 0.98004\n14-12   1.880645 -40.886 44.647 1.00000\n15-12   1.738710 -41.028 44.505 1.00000\n16-12   4.577419 -38.189 47.344 1.00000\n17-12   3.122581 -39.644 45.889 1.00000\n18-12   8.338710 -34.428 51.105 1.00000\n19-12   6.032258 -36.734 48.799 1.00000\n20-12  17.408387 -27.445 62.262 0.98369\n14-13 -15.045161 -57.811 27.721 0.99450\n15-13 -15.187097 -57.953 27.579 0.99387\n16-13 -12.348387 -55.115 30.418 0.99955\n17-13 -13.803226 -56.570 28.963 0.99807\n18-13  -8.587097 -51.353 34.179 1.00000\n19-13 -10.893548 -53.660 31.873 0.99992\n20-13   0.482581 -44.371 45.336 1.00000\n15-14  -0.141935 -42.908 42.624 1.00000\n16-14   2.696774 -40.070 45.463 1.00000\n17-14   1.241935 -41.524 44.008 1.00000\n18-14   6.458065 -36.308 49.224 1.00000\n19-14   4.151613 -38.615 46.918 1.00000\n20-14  15.527742 -29.326 60.381 0.99545\n16-15   2.838710 -39.928 45.605 1.00000\n17-15   1.383871 -41.382 44.150 1.00000\n18-15   6.600000 -36.166 49.366 1.00000\n19-15   4.293548 -38.473 47.060 1.00000\n20-15  15.669677 -29.184 60.523 0.99493\n17-16  -1.454839 -44.221 41.311 1.00000\n18-16   3.761290 -39.005 46.528 1.00000\n19-16   1.454839 -41.311 44.221 1.00000\n20-16  12.830968 -32.023 57.685 0.99961\n18-17   5.216129 -37.550 47.982 1.00000\n19-17   2.909677 -39.857 45.676 1.00000\n20-17  14.285806 -30.568 59.139 0.99837\n19-18  -2.306452 -45.073 40.460 1.00000\n20-18   9.069677 -35.784 53.923 1.00000\n20-19  11.376129 -33.478 56.230 0.99993\n\n\n유의한 패턴 없음 Tukey 검정 결과 (보통 유의할때 함) 테이블 도 유의한 수치가 없음"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-07-anova/index.html#application-to-example",
    "href": "docs/blog/posts/statistics/2023-01-07-anova/index.html#application-to-example",
    "title": "ANOVA",
    "section": "2 Application to Example",
    "text": "2 Application to Example\n\n2.1 Data Description\n\n2.1.1 Raw Data\n(…민감 정보 제거 및 데이터 변환 후 컨설팅 내용 일부 발췌…)\n\n\n\nexample data는 Day, Run, response의 변수들을 포함하고 있습니다. 공유해주신 정보에 따르면 아마도 Run은 오전과 오후를 나누는 변수인 것으로 생각 됩니다. 이 data만 보면 아마도 같은 샘플에 대해서 시약 제품이 시간에 따라 얼마나 안정적인 performance를 보여주는지 검사하는 실험으로 추측됩니다. 좀 더 분석하기 용이한 형태로 data structure를 바꾸겠습니다.\n\n\n2.1.2 Processed Data\n\n\n\n\n  \n\n\n\n재가공된 data는 120개의 샘플과 5개의 변수를 갖고있습니다. 변수 목록은 다음과 같습니다.\n\nid: 열번호, 총 20일간 하루 2회 구동(AM, PM) 구동, 오전 오후 각 각 3번씩 구동 총 120 \\((=20 \\times 3 \\times 2)\\) 샘플\n\nDay: Day1~20\n\nnoon: AM= before noon, PM= after noon\nRun: 1회 구동당 3번 반복씩1, 2, 3\n\nresponse: response variable, 낮을 수록 좋음\n\nANOVA의 Assumption\n\nresponse variable should follow normal distribution.\n\nhomoscedasticity, equality of variance: 각 집단의 분포는 모두 동일한 분산을 가짐\nANOVA의 가정들을 반드시 충족하지 않아도 되지만 충족하면 Power 가 올라감\n\n\n\n\n2.2 EDA (Explorator Data Analysis)\n이 data는 아래 처럼 1의 결측치를 갖고 있습니다.\n\n\n\n\n\nid\nDay\nnoon\nRun\nresponse\n\n\n\n\n117\n20\nAM\n3\nNA\n\n\n\n\n\nCt에 대한 Global Statistics는 다음과 같습니다.\n\n\n\n\n\ncount\nglobal_response_mean\nglobal_response_sd\nglobal_response_CV\n\n\n\n\n119\n38.727\n18.47\n47.694 %\n\n\n\n\n\nDay groups의 Statistics은 다음과 같습니다.\n\n\n\n\n  \n\n\n\nAM/PM groups의 Statistics은 다음과 같습니다.\n\n\n\n\n  \n\n\n\nDays와 AM/PM 조합 groups의 Statistics은 다음과 같습니다.\n\n\n\n\n  \n\n\n\n이제 ANOVA를 수행하기 위한 basic statistics는 모두 구했습니다. ANOVA를 수행하기 위해 집단 간 분산과 집단 내 분산을 계산하도록 하겠습니다.\n\n\n2.3 집단 간 분산\n앞에서 설명 드린바로 유추해보면 예시 data의 집단 간 분산의 범주형 변수는 Day로 설정하는 것이 합리적인 것으로 보입니다.\n\n\\(g=g\\) Day의 sample size = 20, 자유도 = 20-1 = 19 입니다.\n\\(n_g=g\\) group의 sample size, \\(\\overline{X}_g=g\\) 의 sample mean은 다음과 같습니다.\n\\(\\overline{X}\\) = global sample mean = 38.72681\n집단 간 분산: \\(\\frac{집단 간 제곱합}{자유도}=\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n2.3.1 SS_Day (집단간 분산 Day)\nDay sq = \\([(\\overline{X}_g-\\overline{X})^2n_g]\\) 집단(Day)간 분산 계산, 집단(Day)간 분산 = \\(\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n\n\n  \n\n\n\nAnalysis-In program의 ANOVA결과값과 일치하는 것을 볼 수 있습니다. $SS_{day} $= 7025.97838 with \\(df=19\\).\n\n\n2.3.2 SS_noon (집단간 분산 noon)\nnoon sq = \\([(\\overline{X}_g-\\overline{X})^2n_g]\\) 집단(noon)간 분산 계산, 집단(noon)간 분산 = \\(\\frac{\\sum_g[(\\overline{X}_g-\\overline{X})^2n_g]}{g-1}\\)\n\n\n\n\n  \n\n\n\nAnalysis-In program의 결과에서 찾아 볼 수 없죠? 이 결과는 숨어 있습니다. 상호 작용에 대한 분산값을 구하고 나면 정체를 알 수 있습니다.\n\\(SS_{noon}\\) = 319.76458 with \\(df=1\\).\n\n\n2.3.3 SS_error (집단내 분산)\n\n집단 내 분산 (within-groups variability)\n\n\n\n\n\n  \n\n\n\n\\(SS_{error}\\) = 2.47041^{4}\nAnalysis-In program의 결과와 일치하는 것을 확인할 수 있습니다.\n\n\n2.3.4 SS_total\n\n\n\n\n  \n\n\n\n\\(SS_{total}\\) = 4.02557^{4}\nAnalysis-In program의 ANOVA 결과 table에 있는 SS들의 합과 일치하는 것을 확인할 수 있습니다.\n\n\n2.3.5 상호 작용 분산\n\n\n\n\\(SS_{interaction}=SS_{DayNoon}= SS_{total}-SS_{Day}-SS_{noon}-SS_{error}\\)\n= 4.02557{4}-2.47041{4}-319.76458-7025.97838 = 8205.93974\nAnalysis-In program의 ANOVA 결과 table과 일치하는 것을 확인할 수 있습니다.\n위의 결과들을 종합하면 아래와 같이 요약됩니다.\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   7026     370    1.18   0.29\nnoon         1    339     339    1.08   0.30\nDay:noon    19   8187     431    1.38   0.16\nResiduals   79  24704     313               \n결측으로 인하여 1개의 관측치가 삭제되었습니다.\n\n\n\nRepeatability SD = \\(\\sqrt{V_{error}}=\\sqrt{MS_{error}}\\) = 17.6836\nRepeatability CV = \\(\\frac{repeatability \\space SD}{global \\space mean \\space response}\\) = 0.45662\n\n위의 결과를 간단히 해석해 보면\n\n집단간 범주 변수인 Day는 p-value =0.29>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, 일별로 평균 response값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, 일별로 평균 response값이 다르지 않습니다.\n\n집단간 범주 변수인 noon은 p-value =0.30>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, 오전/오후별 평균 response값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, 오전/오후별 평균 response값이 다르지 않습니다.\n\nDay와 noon두 변수의 상호작용 변수는 p-value =0.16>0.05 이므로 5% 유의 수준에서, 유의하지 않습니다.\n\n즉, 5% 유의 수준에서, Day별 Noon(오전/오후)별 평균 response값이 같다는 가설을 기각하는데 실패했습니다.\n다시 말하면, 5% 유의 수준에서, Day별 Noon(오전/오후)별 평균 response값이 다르지 않습니다.\n\n\n최종 결론, 제품의 response값이 Day별 오전/오후별 안정적인 performance를 보인다고 조심스럽게 결론을 내릴 수 있습니다.\n이제 까지는 질문에 대한 답이 되는 ANOVA의 원리 및 통계량의 재현 및 해석법에 대하여 알아봤습니다. 하지만 직관적으로 어떤 의미가 있을 까요? 원래는 시각화를 통해 데이터의 패턴을 짐작하고 통계 검정 결과를 예상하는데 우리는 반대로 가고 있네요 ㅎㅎ 시각화를 통해 ANOVA 결과가 얼마나 직관적인지 알아보겠습니다.\n\n\n\n2.4 Visualization\n\n2.4.1 One-way: Day\n\n\n\n\n\n\n\n\n자세히 보면 일별로 시간의 경과에 따라 전체적으로 평균 response값이 약간 하향하는 것으로 보입니다. 하지만 좀 더 세부적으로 관찰하면 1일~8일 평균 response의 경향이 constant한 패턴을 보입니다. 9일~13일 평균 response가 진동 하향하는 패턴을 보입니다. 14일~20일 평균 response가 상향하는 패턴을 보입니다.\n\n\nTables of means\nGrand mean\n       \n38.727 \n\n Day \n        1    2     3     4     5     6     7     8     9    10    11    12\n    47.34 41.2 43.26 33.04 44.01 40.53 40.92 42.98 55.89 34.64 49.19 26.94\nrep  6.00  6.0  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00\n       13    14    15    16    17    18    19    20\n    43.86 28.82 28.68 31.52 30.06 35.28 32.97 44.35\nrep  6.00  6.00  6.00  6.00  6.00  6.00  6.00  5.00\n\n\nTables of effects\n\n Day \n        1     2     3      4    5     6     7     8     9     10    11     12\n    8.615 2.476 4.534 -5.685 5.28 1.802 2.193 4.251 17.17 -4.088 10.46 -11.79\nrep 6.000 6.000 6.000  6.000 6.00 6.000 6.000 6.000  6.00  6.000  6.00   6.00\n       13     14     15     16     17     18     19   20\n    5.138 -9.907 -10.05 -7.211 -8.666 -3.449 -5.756 5.62\nrep 6.000  6.000   6.00  6.000  6.000  6.000  6.000 5.00\n\n\n위에 첫 번째표에서 Global Sample response Mean = 38.727 과 각 집단의 평균 response를 확인할 수 있습니다. 위에 두 번째표에서 Global Sample response Mean = 37.322 과 각 집단의 평균 response의 차이를 확인할 수 있습니다.\n\nDay 9에서 차이가 가장 큰 것으로 보아 9일째 실험에서 performance가 가장 낮은 것이 관측됐습니다.\n반대로, 12일에 performance 가장 좋은 것으로 관측됐습니다.\n\n9일과 12일에 response값에 영향을 미쳤던 요인이 있었는지 복기 하는것도 도움이 되겠군요.\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   7026     370     1.1   0.36\nResiduals   99  33230     336               \n결측으로 인하여 1개의 관측치가 삭제되었습니다.\n\n\nOne-way ANOVA의 결과값입니다. Day별 평균 response의 차이는 거의 없는 것으로 보입니다. 따라서 Day 별 평균 response의 경향이 일관되지 않고 One-way ANOVA에서 역시 통계적으로 유의하지 않아 Day 변수는 평균 response에 영향을 미치지 않는 것 같습니다.\n\n\n2.4.2 One-way: AM/PM\n\n\n\n\n\n\n\n\n오후에 시간의 경과에 따라 전체적으로 평균 response값이 약간 하향하는 것으로 보입니다.\n\n\nTables of means\nGrand mean\n       \n38.727 \n\n noon \n       AM   PM\n    40.38 37.1\nrep 59.00 60.0\n\n\nTables of effects\n\n noon \n        AM     PM\n     1.653 -1.626\nrep 59.000 60.000\n\n\n위 첫 번째 표에서 AM/PM 간의 평균 response차이는 0.15 (농도가 약 0.5배) 차이가 나는 것을 확인할 수 있습니다. 생물학적으로 의미가 있는 수치일까요? 위 두 번째 표에서 Global Sample Mean 37.322와 오전/오후 별 약 0.07씩(농도가 약 0.25배) 차이가 납니다.\n\n\n             Df Sum Sq Mean Sq F value Pr(>F)\nnoon          1    320     320    0.94   0.34\nResiduals   117  39936     341               \n결측으로 인하여 1개의 관측치가 삭제되었습니다.\n\n\n오전 오후별 One way ANOVA를 실행한 결과가 오전/오부 평균 response값의 차이가 다르지 않다는 것을 시사하고 있습니다. 아무래도 위의 차이는 우연에 의해 발생한 현상인 것 같습니다.\n\n\n\n\n\n일별로 평균 response값이 차이를 모든 경우의 수에서 차이가 나는지 확인한 결과 신뢰구간 모두가 0을 포함하고 있으므로 Day 변수는 유의하다고 볼 수 없습니다.\n\n\n\n\n\n오전/오후별로 평균 response값이 차이를 모든 경우의 수에서 차이가 나는지 확인한 결과 신뢰구간 모두가 0을 포함하고 있으므로 오전/오후 변수는 유의하다고 볼 수 없습니다.\n여기 까지 각 변수별 평균 response로의 영향도를 통계적으로 시각적으로 관찰했습니다. 하지만 Day별 오전/오후별 영향도가 있는지 확인하겠습니다. (이미 위에서 통계적으로 없다고 검정됐습니다.)\n\n\n\n2.5 Two way Anova\n\n\n       AM     PM\n1  35.987 58.697\n2  40.032 42.374\n3  33.148 53.374\n4  36.697 29.387\n5  44.432 43.581\n6  45.781 35.277\n7  40.458 41.381\n8  51.103 34.852\n9  69.981 41.806\n10 26.619 42.658\n11 67.923 30.452\n12 25.981 27.897\n13 40.600 47.129\n14 25.839 31.800\n15 18.458 38.897\n16 36.910 26.123\n17 41.594 18.529\n18 38.471 32.084\n19 45.923 20.019\n20 42.303 45.710\n\n\n        AM      PM\n1  20.9797  7.4516\n2  18.6291 12.0856\n3  22.1450 34.0565\n4  16.3483  7.5583\n5   4.6107 11.0032\n6  23.6862 11.7618\n7  12.3380 30.0128\n8  33.4416 12.0135\n9   7.7381 30.4715\n10  3.4483  7.8927\n11 28.8752  9.1557\n12  5.9053 12.9054\n13 15.2088  7.3824\n14  8.4556 27.2253\n15  4.5063 35.2180\n16 18.3756  1.6899\n17 13.3474  8.0068\n18  6.0067 21.7672\n19 19.7931  1.4176\n20  3.9142 14.0145\n\n\n\n\nTables of means\nGrand mean\n       \n38.727 \n\n Day \n        1    2     3     4     5     6     7     8     9    10    11    12\n    47.34 41.2 43.26 33.04 44.01 40.53 40.92 42.98 55.89 34.64 49.19 26.94\nrep  6.00  6.0  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00  6.00\n       13    14    15    16    17    18    19    20\n    43.86 28.82 28.68 31.52 30.06 35.28 32.97 44.35\nrep  6.00  6.00  6.00  6.00  6.00  6.00  6.00  5.00\n\n noon \n       AM    PM\n    40.43 37.05\nrep 59.00 60.00\n\n Day:noon \n     noon\nDay   AM    PM   \n  1   35.99 58.70\n  rep  3.00  3.00\n  2   40.03 42.37\n  rep  3.00  3.00\n  3   33.15 53.37\n  rep  3.00  3.00\n  4   36.70 29.39\n  rep  3.00  3.00\n  5   44.43 43.58\n  rep  3.00  3.00\n  6   45.78 35.28\n  rep  3.00  3.00\n  7   40.46 41.38\n  rep  3.00  3.00\n  8   51.10 34.85\n  rep  3.00  3.00\n  9   69.98 41.81\n  rep  3.00  3.00\n  10  26.62 42.66\n  rep  3.00  3.00\n  11  67.92 30.45\n  rep  3.00  3.00\n  12  25.98 27.90\n  rep  3.00  3.00\n  13  40.60 47.13\n  rep  3.00  3.00\n  14  25.84 31.80\n  rep  3.00  3.00\n  15  18.46 38.90\n  rep  3.00  3.00\n  16  36.91 26.12\n  rep  3.00  3.00\n  17  41.59 18.53\n  rep  3.00  3.00\n  18  38.47 32.08\n  rep  3.00  3.00\n  19  45.92 20.02\n  rep  3.00  3.00\n  20  42.30 45.71\n  rep  2.00  3.00\n\n\nTables of effects\n\n Day \n        1     2     3      4    5     6     7     8     9     10    11     12\n    8.615 2.476 4.534 -5.685 5.28 1.802 2.193 4.251 17.17 -4.088 10.46 -11.79\nrep 6.000 6.000 6.000  6.000 6.00 6.000 6.000 6.000  6.00  6.000  6.00   6.00\n       13     14     15     16     17     18     19   20\n    5.138 -9.907 -10.05 -7.211 -8.666 -3.449 -5.756 5.62\nrep 6.000  6.000   6.00  6.000  6.000  6.000  6.000 5.00\n\n noon \n        AM     PM\n     1.701 -1.672\nrep 59.000 60.000\n\n Day:noon \n     noon\nDay   AM      PM     \n  1   -13.044  13.044\n  rep   3.000   3.000\n  2    -2.860   2.860\n  rep   3.000   3.000\n  3   -11.802  11.802\n  rep   3.000   3.000\n  4     1.966  -1.966\n  rep   3.000   3.000\n  5    -1.263   1.263\n  rep   3.000   3.000\n  6     3.562  -3.562\n  rep   3.000   3.000\n  7    -2.151   2.151\n  rep   3.000   3.000\n  8     6.437  -6.437\n  rep   3.000   3.000\n  9    12.398 -12.398\n  rep   3.000   3.000\n  10   -9.709   9.709\n  rep   3.000   3.000\n  11   17.046 -17.046\n  rep   3.000   3.000\n  12   -2.647   2.647\n  rep   3.000   3.000\n  13   -4.954   4.954\n  rep   3.000   3.000\n  14   -4.670   4.670\n  rep   3.000   3.000\n  15  -11.909  11.909\n  rep   3.000   3.000\n  16    3.704  -3.704\n  rep   3.000   3.000\n  17    9.843  -9.843\n  rep   3.000   3.000\n  18    1.504  -1.504\n  rep   3.000   3.000\n  19   11.262 -11.262\n  rep   3.000   3.000\n  20   -4.071   2.714\n  rep   2.000   3.000\n\n\none way ANOVA와 같이 해석\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDay         19   7026     370    1.18   0.29\nnoon         1    339     339    1.08   0.30\nDay:noon    19   8187     431    1.38   0.16\nResiduals   79  24704     313               \n결측으로 인하여 1개의 관측치가 삭제되었습니다.\n\n\n위 그림을 보듯이 두 변수의 영향도가 없음, ANOVA 역시 유의하지 않음\n\n\n\n\n\n\n 누락된 행들: 117 \n\n\n\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = response ~ Day + noon + Day:noon, data = data)\n\n$Day\n            diff     lwr     upr   p adj\n2-1    -6.138710 -43.575 31.2979 1.00000\n3-1    -4.080645 -41.517 33.3560 1.00000\n4-1   -14.300000 -51.737 23.1366 0.99700\n5-1    -3.335484 -40.772 34.1011 1.00000\n6-1    -6.812903 -44.250 30.6237 1.00000\n7-1    -6.422581 -43.859 31.0140 1.00000\n8-1    -4.364516 -41.801 33.0721 1.00000\n9-1     8.551613 -28.885 45.9882 1.00000\n10-1  -12.703226 -50.140 24.7334 0.99934\n11-1    1.845161 -35.591 39.2818 1.00000\n12-1  -20.403226 -57.840 17.0334 0.89330\n13-1   -3.477419 -40.914 33.9592 1.00000\n14-1  -18.522581 -55.959 18.9140 0.95243\n15-1  -18.664516 -56.101 18.7721 0.94905\n16-1  -15.825806 -53.262 21.6108 0.99024\n17-1  -17.280645 -54.717 20.1560 0.97539\n18-1  -12.064516 -49.501 25.3721 0.99968\n19-1  -14.370968 -51.808 23.0657 0.99682\n20-1   -2.994839 -42.259 36.2690 1.00000\n3-2     2.058065 -35.379 39.4947 1.00000\n4-2    -8.161290 -45.598 29.2753 1.00000\n5-2     2.803226 -34.633 40.2399 1.00000\n6-2    -0.674194 -38.111 36.7624 1.00000\n7-2    -0.283871 -37.721 37.1528 1.00000\n8-2     1.774194 -35.662 39.2108 1.00000\n9-2    14.690323 -22.746 52.1270 0.99585\n10-2   -6.564516 -44.001 30.8721 1.00000\n11-2    7.983871 -29.453 45.4205 1.00000\n12-2  -14.264516 -51.701 23.1721 0.99709\n13-2    2.661290 -34.775 40.0979 1.00000\n14-2  -12.383871 -49.821 25.0528 0.99953\n15-2  -12.525806 -49.962 24.9108 0.99946\n16-2   -9.687097 -47.124 27.7495 0.99999\n17-2  -11.141935 -48.579 26.2947 0.99990\n18-2   -5.925806 -43.362 31.5108 1.00000\n19-2   -8.232258 -45.669 29.2044 1.00000\n20-2    3.143871 -36.120 42.4077 1.00000\n4-3   -10.219355 -47.656 27.2173 0.99997\n5-3     0.745161 -36.691 38.1818 1.00000\n6-3    -2.732258 -40.169 34.7044 1.00000\n7-3    -2.341935 -39.779 35.0947 1.00000\n8-3    -0.283871 -37.721 37.1528 1.00000\n9-3    12.632258 -24.804 50.0689 0.99939\n10-3   -8.622581 -46.059 28.8140 1.00000\n11-3    5.925806 -31.511 43.3624 1.00000\n12-3  -16.322581 -53.759 21.1140 0.98634\n13-3    0.603226 -36.833 38.0399 1.00000\n14-3  -14.441935 -51.879 22.9947 0.99662\n15-3  -14.583871 -52.021 22.8528 0.99620\n16-3  -11.745161 -49.182 25.6915 0.99978\n17-3  -13.200000 -50.637 24.2366 0.99891\n18-3   -7.983871 -45.421 29.4528 1.00000\n19-3  -10.290323 -47.727 27.1463 0.99997\n20-3    1.085806 -38.178 40.3497 1.00000\n5-4    10.964516 -26.472 48.4011 0.99992\n6-4     7.487097 -29.950 44.9237 1.00000\n7-4     7.877419 -29.559 45.3140 1.00000\n8-4     9.935484 -27.501 47.3721 0.99998\n9-4    22.851613 -14.585 60.2882 0.76824\n10-4    1.596774 -35.840 39.0334 1.00000\n11-4   16.145161 -21.291 53.5818 0.98785\n12-4   -6.103226 -43.540 31.3334 1.00000\n13-4   10.822581 -26.614 48.2592 0.99993\n14-4   -4.222581 -41.659 33.2140 1.00000\n15-4   -4.364516 -41.801 33.0721 1.00000\n16-4   -1.525806 -38.962 35.9108 1.00000\n17-4   -2.980645 -40.417 34.4560 1.00000\n18-4    2.235484 -35.201 39.6721 1.00000\n19-4   -0.070968 -37.508 37.3657 1.00000\n20-4   11.305161 -27.959 50.5690 0.99994\n6-5    -3.477419 -40.914 33.9592 1.00000\n7-5    -3.087097 -40.524 34.3495 1.00000\n8-5    -1.029032 -38.466 36.4076 1.00000\n9-5    11.887097 -25.550 49.3237 0.99974\n10-5   -9.367742 -46.804 28.0689 0.99999\n11-5    5.180645 -32.256 42.6173 1.00000\n12-5  -17.067742 -54.504 20.3689 0.97827\n13-5   -0.141935 -37.579 37.2947 1.00000\n14-5  -15.187097 -52.624 22.2495 0.99387\n15-5  -15.329032 -52.766 22.1076 0.99318\n16-5  -12.490323 -49.927 24.9463 0.99948\n17-5  -13.945161 -51.382 23.4915 0.99780\n18-5   -8.729032 -46.166 28.7076 1.00000\n19-5  -11.035484 -48.472 26.4011 0.99991\n20-5    0.340645 -38.923 39.6045 1.00000\n7-6     0.390323 -37.046 37.8270 1.00000\n8-6     2.448387 -34.988 39.8850 1.00000\n9-6    15.364516 -22.072 52.8011 0.99300\n10-6   -5.890323 -43.327 31.5463 1.00000\n11-6    8.658065 -28.779 46.0947 1.00000\n12-6  -13.590323 -51.027 23.8463 0.99841\n13-6    3.335484 -34.101 40.7721 1.00000\n14-6  -11.709677 -49.146 25.7270 0.99979\n15-6  -11.851613 -49.288 25.5850 0.99975\n16-6   -9.012903 -46.450 28.4237 1.00000\n17-6  -10.467742 -47.904 26.9689 0.99996\n18-6   -5.251613 -42.688 32.1850 1.00000\n19-6   -7.558065 -44.995 29.8786 1.00000\n20-6    3.818065 -35.446 43.0819 1.00000\n8-7     2.058065 -35.379 39.4947 1.00000\n9-7    14.974194 -22.462 52.4108 0.99480\n10-7   -6.280645 -43.717 31.1560 1.00000\n11-7    8.267742 -29.169 45.7044 1.00000\n12-7  -13.980645 -51.417 23.4560 0.99773\n13-7    2.945161 -34.491 40.3818 1.00000\n14-7  -12.100000 -49.537 25.3366 0.99966\n15-7  -12.241935 -49.679 25.1947 0.99960\n16-7   -9.403226 -46.840 28.0334 0.99999\n17-7  -10.858065 -48.295 26.5786 0.99993\n18-7   -5.641935 -43.079 31.7947 1.00000\n19-7   -7.948387 -45.385 29.4882 1.00000\n20-7    3.427742 -35.836 42.6916 1.00000\n9-8    12.916129 -24.521 50.3528 0.99918\n10-8   -8.338710 -45.775 29.0979 1.00000\n11-8    6.209677 -31.227 43.6463 1.00000\n12-8  -16.038710 -53.475 21.3979 0.98869\n13-8    0.887097 -36.550 38.3237 1.00000\n14-8  -14.158065 -51.595 23.2786 0.99735\n15-8  -14.300000 -51.737 23.1366 0.99700\n16-8  -11.461290 -48.898 25.9753 0.99984\n17-8  -12.916129 -50.353 24.5205 0.99918\n18-8   -7.700000 -45.137 29.7366 1.00000\n19-8  -10.006452 -47.443 27.4302 0.99998\n20-8    1.369677 -37.894 40.6335 1.00000\n10-9  -21.254839 -58.691 16.1818 0.85576\n11-9   -6.706452 -44.143 30.7302 1.00000\n12-9  -28.954839 -66.391  8.4818 0.35269\n13-9  -12.029032 -49.466 25.4076 0.99969\n14-9  -27.074194 -64.511 10.3624 0.47719\n15-9  -27.216129 -64.653 10.2205 0.46731\n16-9  -24.377419 -61.814 13.0592 0.66832\n17-9  -25.832258 -63.269 11.6044 0.56535\n18-9  -20.616129 -58.053 16.8205 0.88455\n19-9  -22.922581 -60.359 14.5140 0.76389\n20-9  -11.546452 -50.810 27.7174 0.99991\n11-10  14.548387 -22.888 51.9850 0.99631\n12-10  -7.700000 -45.137 29.7366 1.00000\n13-10   9.225806 -28.211 46.6624 0.99999\n14-10  -5.819355 -43.256 31.6173 1.00000\n15-10  -5.961290 -43.398 31.4753 1.00000\n16-10  -3.122581 -40.559 34.3140 1.00000\n17-10  -4.577419 -42.014 32.8592 1.00000\n18-10   0.638710 -36.798 38.0753 1.00000\n19-10  -1.667742 -39.104 35.7689 1.00000\n20-10   9.708387 -29.555 48.9723 0.99999\n12-11 -22.248387 -59.685 15.1882 0.80373\n13-11  -5.322581 -42.759 32.1140 1.00000\n14-11 -20.367742 -57.804 17.0689 0.89472\n15-11 -20.509677 -57.946 16.9270 0.88898\n16-11 -17.670968 -55.108 19.7657 0.96936\n17-11 -19.125806 -56.562 18.3108 0.93692\n18-11 -13.909677 -51.346 23.5270 0.99787\n19-11 -16.216129 -53.653 21.2205 0.98726\n20-11  -4.840000 -44.104 34.4239 1.00000\n13-12  16.925806 -20.511 54.3624 0.98004\n14-12   1.880645 -35.556 39.3173 1.00000\n15-12   1.738710 -35.698 39.1753 1.00000\n16-12   4.577419 -32.859 42.0140 1.00000\n17-12   3.122581 -34.314 40.5592 1.00000\n18-12   8.338710 -29.098 45.7753 1.00000\n19-12   6.032258 -31.404 43.4689 1.00000\n20-12  17.408387 -21.855 56.6723 0.98369\n14-13 -15.045161 -52.482 22.3915 0.99450\n15-13 -15.187097 -52.624 22.2495 0.99387\n16-13 -12.348387 -49.785 25.0882 0.99955\n17-13 -13.803226 -51.240 23.6334 0.99807\n18-13  -8.587097 -46.024 28.8495 1.00000\n19-13 -10.893548 -48.330 26.5431 0.99992\n20-13   0.482581 -38.781 39.7464 1.00000\n15-14  -0.141935 -37.579 37.2947 1.00000\n16-14   2.696774 -34.740 40.1334 1.00000\n17-14   1.241935 -36.195 38.6786 1.00000\n18-14   6.458065 -30.979 43.8947 1.00000\n19-14   4.151613 -33.285 41.5882 1.00000\n20-14  15.527742 -23.736 54.7916 0.99545\n16-15   2.838710 -34.598 40.2753 1.00000\n17-15   1.383871 -36.053 38.8205 1.00000\n18-15   6.600000 -30.837 44.0366 1.00000\n19-15   4.293548 -33.143 41.7302 1.00000\n20-15  15.669677 -23.594 54.9335 0.99493\n17-16  -1.454839 -38.891 35.9818 1.00000\n18-16   3.761290 -33.675 41.1979 1.00000\n19-16   1.454839 -35.982 38.8915 1.00000\n20-16  12.830968 -26.433 52.0948 0.99961\n18-17   5.216129 -32.221 42.6528 1.00000\n19-17   2.909677 -34.527 40.3463 1.00000\n20-17  14.285806 -24.978 53.5497 0.99837\n19-18  -2.306452 -39.743 35.1302 1.00000\n20-18   9.069677 -30.194 48.3335 1.00000\n20-19  11.376129 -27.888 50.6400 0.99993\n\n$noon\n         diff     lwr    upr   p adj\nPM-AM -3.3731 -9.8265 3.0804 0.30135\n\n$`Day:noon`\n                  diff      lwr     upr   p adj\n2:AM-1:AM     4.045161  -54.344 62.4344 1.00000\n3:AM-1:AM    -2.838710  -61.228 55.5505 1.00000\n4:AM-1:AM     0.709677  -57.680 59.0989 1.00000\n5:AM-1:AM     8.445161  -49.944 66.8344 1.00000\n6:AM-1:AM     9.793548  -48.596 68.1827 1.00000\n7:AM-1:AM     4.470968  -53.918 62.8602 1.00000\n8:AM-1:AM    15.116129  -43.273 73.5053 1.00000\n9:AM-1:AM    33.993548  -24.396 92.3827 0.92887\n10:AM-1:AM   -9.367742  -67.757 49.0215 1.00000\n11:AM-1:AM   31.935484  -26.454 90.3247 0.96592\n12:AM-1:AM  -10.006452  -68.396 48.3827 1.00000\n13:AM-1:AM    4.612903  -53.776 63.0021 1.00000\n14:AM-1:AM  -10.148387  -68.538 48.2408 1.00000\n15:AM-1:AM  -17.529032  -75.918 40.8602 1.00000\n16:AM-1:AM    0.922581  -57.467 59.3118 1.00000\n17:AM-1:AM    5.606452  -52.783 63.9956 1.00000\n18:AM-1:AM    2.483871  -55.905 60.8731 1.00000\n19:AM-1:AM    9.935484  -48.454 68.3247 1.00000\n20:AM-1:AM    6.316129  -58.965 71.5972 1.00000\n1:PM-1:AM    22.709677  -35.680 81.0989 0.99990\n2:PM-1:AM     6.387097  -52.002 64.7763 1.00000\n3:PM-1:AM    17.387097  -41.002 75.7763 1.00000\n4:PM-1:AM    -6.600000  -64.989 51.7892 1.00000\n5:PM-1:AM     7.593548  -50.796 65.9827 1.00000\n6:PM-1:AM    -0.709677  -59.099 57.6795 1.00000\n7:PM-1:AM     5.393548  -52.996 63.7827 1.00000\n8:PM-1:AM    -1.135484  -59.525 57.2537 1.00000\n9:PM-1:AM     5.819355  -52.570 64.2086 1.00000\n10:PM-1:AM    6.670968  -51.718 65.0602 1.00000\n11:PM-1:AM   -5.535484  -63.925 52.8537 1.00000\n12:PM-1:AM   -8.090323  -66.480 50.2989 1.00000\n13:PM-1:AM   11.141935  -47.247 69.5311 1.00000\n14:PM-1:AM   -4.187097  -62.576 54.2021 1.00000\n15:PM-1:AM    2.909677  -55.480 61.2989 1.00000\n16:PM-1:AM   -9.864516  -68.254 48.5247 1.00000\n17:PM-1:AM  -17.458065  -75.847 40.9311 1.00000\n18:PM-1:AM   -3.903226  -62.292 54.4860 1.00000\n19:PM-1:AM  -15.967742  -74.357 42.4215 1.00000\n20:PM-1:AM    9.722581  -48.667 68.1118 1.00000\n3:AM-2:AM    -6.883871  -65.273 51.5053 1.00000\n4:AM-2:AM    -3.335484  -61.725 55.0537 1.00000\n5:AM-2:AM     4.400000  -53.989 62.7892 1.00000\n6:AM-2:AM     5.748387  -52.641 64.1376 1.00000\n7:AM-2:AM     0.425806  -57.963 58.8150 1.00000\n8:AM-2:AM    11.070968  -47.318 69.4602 1.00000\n9:AM-2:AM    29.948387  -28.441 88.3376 0.98570\n10:AM-2:AM  -13.412903  -71.802 44.9763 1.00000\n11:AM-2:AM   27.890323  -30.499 86.2795 0.99521\n12:AM-2:AM  -14.051613  -72.441 44.3376 1.00000\n13:AM-2:AM    0.567742  -57.821 58.9569 1.00000\n14:AM-2:AM  -14.193548  -72.583 44.1956 1.00000\n15:AM-2:AM  -21.574194  -79.963 36.8150 0.99997\n16:AM-2:AM   -3.122581  -61.512 55.2666 1.00000\n17:AM-2:AM    1.561290  -56.828 59.9505 1.00000\n18:AM-2:AM   -1.561290  -59.950 56.8279 1.00000\n19:AM-2:AM    5.890323  -52.499 64.2795 1.00000\n20:AM-2:AM    2.270968  -63.010 67.5521 1.00000\n1:PM-2:AM    18.664516  -39.725 77.0537 1.00000\n2:PM-2:AM     2.341935  -56.047 60.7311 1.00000\n3:PM-2:AM    13.341935  -45.047 71.7311 1.00000\n4:PM-2:AM   -10.645161  -69.034 47.7440 1.00000\n5:PM-2:AM     3.548387  -54.841 61.9376 1.00000\n6:PM-2:AM    -4.754839  -63.144 53.6344 1.00000\n7:PM-2:AM     1.348387  -57.041 59.7376 1.00000\n8:PM-2:AM    -5.180645  -63.570 53.2086 1.00000\n9:PM-2:AM     1.774194  -56.615 60.1634 1.00000\n10:PM-2:AM    2.625806  -55.763 61.0150 1.00000\n11:PM-2:AM   -9.580645  -67.970 48.8086 1.00000\n12:PM-2:AM  -12.135484  -70.525 46.2537 1.00000\n13:PM-2:AM    7.096774  -51.292 65.4860 1.00000\n14:PM-2:AM   -8.232258  -66.621 50.1569 1.00000\n15:PM-2:AM   -1.135484  -59.525 57.2537 1.00000\n16:PM-2:AM  -13.909677  -72.299 44.4795 1.00000\n17:PM-2:AM  -21.503226  -79.892 36.8860 0.99997\n18:PM-2:AM   -7.948387  -66.338 50.4408 1.00000\n19:PM-2:AM  -20.012903  -78.402 38.3763 0.99999\n20:PM-2:AM    5.677419  -52.712 64.0666 1.00000\n4:AM-3:AM     3.548387  -54.841 61.9376 1.00000\n5:AM-3:AM    11.283871  -47.105 69.6731 1.00000\n6:AM-3:AM    12.632258  -45.757 71.0215 1.00000\n7:AM-3:AM     7.309677  -51.080 65.6989 1.00000\n8:AM-3:AM    17.954839  -40.434 76.3440 1.00000\n9:AM-3:AM    36.832258  -21.557 95.2215 0.84321\n10:AM-3:AM   -6.529032  -64.918 51.8602 1.00000\n11:AM-3:AM   34.774194  -23.615 93.1634 0.90946\n12:AM-3:AM   -7.167742  -65.557 51.2215 1.00000\n13:AM-3:AM    7.451613  -50.938 65.8408 1.00000\n14:AM-3:AM   -7.309677  -65.699 51.0795 1.00000\n15:AM-3:AM  -14.690323  -73.080 43.6989 1.00000\n16:AM-3:AM    3.761290  -54.628 62.1505 1.00000\n17:AM-3:AM    8.445161  -49.944 66.8344 1.00000\n18:AM-3:AM    5.322581  -53.067 63.7118 1.00000\n19:AM-3:AM   12.774194  -45.615 71.1634 1.00000\n20:AM-3:AM    9.154839  -56.126 74.4359 1.00000\n1:PM-3:AM    25.548387  -32.841 83.9376 0.99897\n2:PM-3:AM     9.225806  -49.163 67.6150 1.00000\n3:PM-3:AM    20.225806  -38.163 78.6150 0.99999\n4:PM-3:AM    -3.761290  -62.150 54.6279 1.00000\n5:PM-3:AM    10.432258  -47.957 68.8215 1.00000\n6:PM-3:AM     2.129032  -56.260 60.5182 1.00000\n7:PM-3:AM     8.232258  -50.157 66.6215 1.00000\n8:PM-3:AM     1.703226  -56.686 60.0924 1.00000\n9:PM-3:AM     8.658065  -49.731 67.0473 1.00000\n10:PM-3:AM    9.509677  -48.880 67.8989 1.00000\n11:PM-3:AM   -2.696774  -61.086 55.6924 1.00000\n12:PM-3:AM   -5.251613  -63.641 53.1376 1.00000\n13:PM-3:AM   13.980645  -44.409 72.3698 1.00000\n14:PM-3:AM   -1.348387  -59.738 57.0408 1.00000\n15:PM-3:AM    5.748387  -52.641 64.1376 1.00000\n16:PM-3:AM   -7.025806  -65.415 51.3634 1.00000\n17:PM-3:AM  -14.619355  -73.009 43.7698 1.00000\n18:PM-3:AM   -1.064516  -59.454 57.3247 1.00000\n19:PM-3:AM  -13.129032  -71.518 45.2602 1.00000\n20:PM-3:AM   12.561290  -45.828 70.9505 1.00000\n5:AM-4:AM     7.735484  -50.654 66.1247 1.00000\n6:AM-4:AM     9.083871  -49.305 67.4731 1.00000\n7:AM-4:AM     3.761290  -54.628 62.1505 1.00000\n8:AM-4:AM    14.406452  -43.983 72.7956 1.00000\n9:AM-4:AM    33.283871  -25.105 91.6731 0.94386\n10:AM-4:AM  -10.077419  -68.467 48.3118 1.00000\n11:AM-4:AM   31.225806  -27.163 89.6150 0.97452\n12:AM-4:AM  -10.716129  -69.105 47.6731 1.00000\n13:AM-4:AM    3.903226  -54.486 62.2924 1.00000\n14:AM-4:AM  -10.858065  -69.247 47.5311 1.00000\n15:AM-4:AM  -18.238710  -76.628 40.1505 1.00000\n16:AM-4:AM    0.212903  -58.176 58.6021 1.00000\n17:AM-4:AM    4.896774  -53.492 63.2860 1.00000\n18:AM-4:AM    1.774194  -56.615 60.1634 1.00000\n19:AM-4:AM    9.225806  -49.163 67.6150 1.00000\n20:AM-4:AM    5.606452  -59.675 70.8876 1.00000\n1:PM-4:AM    22.000000  -36.389 80.3892 0.99995\n2:PM-4:AM     5.677419  -52.712 64.0666 1.00000\n3:PM-4:AM    16.677419  -41.712 75.0666 1.00000\n4:PM-4:AM    -7.309677  -65.699 51.0795 1.00000\n5:PM-4:AM     6.883871  -51.505 65.2731 1.00000\n6:PM-4:AM    -1.419355  -59.809 56.9698 1.00000\n7:PM-4:AM     4.683871  -53.705 63.0731 1.00000\n8:PM-4:AM    -1.845161  -60.234 56.5440 1.00000\n9:PM-4:AM     5.109677  -53.280 63.4989 1.00000\n10:PM-4:AM    5.961290  -52.428 64.3505 1.00000\n11:PM-4:AM   -6.245161  -64.634 52.1440 1.00000\n12:PM-4:AM   -8.800000  -67.189 49.5892 1.00000\n13:PM-4:AM   10.432258  -47.957 68.8215 1.00000\n14:PM-4:AM   -4.896774  -63.286 53.4924 1.00000\n15:PM-4:AM    2.200000  -56.189 60.5892 1.00000\n16:PM-4:AM  -10.574194  -68.963 47.8150 1.00000\n17:PM-4:AM  -18.167742  -76.557 40.2215 1.00000\n18:PM-4:AM   -4.612903  -63.002 53.7763 1.00000\n19:PM-4:AM  -16.677419  -75.067 41.7118 1.00000\n20:PM-4:AM    9.012903  -49.376 67.4021 1.00000\n6:AM-5:AM     1.348387  -57.041 59.7376 1.00000\n7:AM-5:AM    -3.974194  -62.363 54.4150 1.00000\n8:AM-5:AM     6.670968  -51.718 65.0602 1.00000\n9:AM-5:AM    25.548387  -32.841 83.9376 0.99897\n10:AM-5:AM  -17.812903  -76.202 40.5763 1.00000\n11:AM-5:AM   23.490323  -34.899 81.8795 0.99980\n12:AM-5:AM  -18.451613  -76.841 39.9376 1.00000\n13:AM-5:AM   -3.832258  -62.221 54.5569 1.00000\n14:AM-5:AM  -18.593548  -76.983 39.7956 1.00000\n15:AM-5:AM  -25.974194  -84.363 32.4150 0.99860\n16:AM-5:AM   -7.522581  -65.912 50.8666 1.00000\n17:AM-5:AM   -2.838710  -61.228 55.5505 1.00000\n18:AM-5:AM   -5.961290  -64.350 52.4279 1.00000\n19:AM-5:AM    1.490323  -56.899 59.8795 1.00000\n20:AM-5:AM   -2.129032  -67.410 63.1521 1.00000\n1:PM-5:AM    14.264516  -44.125 72.6537 1.00000\n2:PM-5:AM    -2.058065  -60.447 56.3311 1.00000\n3:PM-5:AM     8.941935  -49.447 67.3311 1.00000\n4:PM-5:AM   -15.045161  -73.434 43.3440 1.00000\n5:PM-5:AM    -0.851613  -59.241 57.5376 1.00000\n6:PM-5:AM    -9.154839  -67.544 49.2344 1.00000\n7:PM-5:AM    -3.051613  -61.441 55.3376 1.00000\n8:PM-5:AM    -9.580645  -67.970 48.8086 1.00000\n9:PM-5:AM    -2.625806  -61.015 55.7634 1.00000\n10:PM-5:AM   -1.774194  -60.163 56.6150 1.00000\n11:PM-5:AM  -13.980645  -72.370 44.4086 1.00000\n12:PM-5:AM  -16.535484  -74.925 41.8537 1.00000\n13:PM-5:AM    2.696774  -55.692 61.0860 1.00000\n14:PM-5:AM  -12.632258  -71.021 45.7569 1.00000\n15:PM-5:AM   -5.535484  -63.925 52.8537 1.00000\n16:PM-5:AM  -18.309677  -76.699 40.0795 1.00000\n17:PM-5:AM  -25.903226  -84.292 32.4860 0.99867\n18:PM-5:AM  -12.348387  -70.738 46.0408 1.00000\n19:PM-5:AM  -24.412903  -82.802 33.9763 0.99957\n20:PM-5:AM    1.277419  -57.112 59.6666 1.00000\n7:AM-6:AM    -5.322581  -63.712 53.0666 1.00000\n8:AM-6:AM     5.322581  -53.067 63.7118 1.00000\n9:AM-6:AM    24.200000  -34.189 82.5892 0.99964\n10:AM-6:AM  -19.161290  -77.550 39.2279 1.00000\n11:AM-6:AM   22.141935  -36.247 80.5311 0.99994\n12:AM-6:AM  -19.800000  -78.189 38.5892 1.00000\n13:AM-6:AM   -5.180645  -63.570 53.2086 1.00000\n14:AM-6:AM  -19.941935  -78.331 38.4473 1.00000\n15:AM-6:AM  -27.322581  -85.712 31.0666 0.99660\n16:AM-6:AM   -8.870968  -67.260 49.5182 1.00000\n17:AM-6:AM   -4.187097  -62.576 54.2021 1.00000\n18:AM-6:AM   -7.309677  -65.699 51.0795 1.00000\n19:AM-6:AM    0.141935  -58.247 58.5311 1.00000\n20:AM-6:AM   -3.477419  -68.759 61.8037 1.00000\n1:PM-6:AM    12.916129  -45.473 71.3053 1.00000\n2:PM-6:AM    -3.406452  -61.796 54.9827 1.00000\n3:PM-6:AM     7.593548  -50.796 65.9827 1.00000\n4:PM-6:AM   -16.393548  -74.783 41.9956 1.00000\n5:PM-6:AM    -2.200000  -60.589 56.1892 1.00000\n6:PM-6:AM   -10.503226  -68.892 47.8860 1.00000\n7:PM-6:AM    -4.400000  -62.789 53.9892 1.00000\n8:PM-6:AM   -10.929032  -69.318 47.4602 1.00000\n9:PM-6:AM    -3.974194  -62.363 54.4150 1.00000\n10:PM-6:AM   -3.122581  -61.512 55.2666 1.00000\n11:PM-6:AM  -15.329032  -73.718 43.0602 1.00000\n12:PM-6:AM  -17.883871  -76.273 40.5053 1.00000\n13:PM-6:AM    1.348387  -57.041 59.7376 1.00000\n14:PM-6:AM  -13.980645  -72.370 44.4086 1.00000\n15:PM-6:AM   -6.883871  -65.273 51.5053 1.00000\n16:PM-6:AM  -19.658065  -78.047 38.7311 1.00000\n17:PM-6:AM  -27.251613  -85.641 31.1376 0.99674\n18:PM-6:AM  -13.696774  -72.086 44.6924 1.00000\n19:PM-6:AM  -25.761290  -84.150 32.6279 0.99880\n20:PM-6:AM   -0.070968  -58.460 58.3182 1.00000\n8:AM-7:AM    10.645161  -47.744 69.0344 1.00000\n9:AM-7:AM    29.522581  -28.867 87.9118 0.98840\n10:AM-7:AM  -13.838710  -72.228 44.5505 1.00000\n11:AM-7:AM   27.464516  -30.925 85.8537 0.99629\n12:AM-7:AM  -14.477419  -72.867 43.9118 1.00000\n13:AM-7:AM    0.141935  -58.247 58.5311 1.00000\n14:AM-7:AM  -14.619355  -73.009 43.7698 1.00000\n15:AM-7:AM  -22.000000  -80.389 36.3892 0.99995\n16:AM-7:AM   -3.548387  -61.938 54.8408 1.00000\n17:AM-7:AM    1.135484  -57.254 59.5247 1.00000\n18:AM-7:AM   -1.987097  -60.376 56.4021 1.00000\n19:AM-7:AM    5.464516  -52.925 63.8537 1.00000\n20:AM-7:AM    1.845161  -63.436 67.1263 1.00000\n1:PM-7:AM    18.238710  -40.150 76.6279 1.00000\n2:PM-7:AM     1.916129  -56.473 60.3053 1.00000\n3:PM-7:AM    12.916129  -45.473 71.3053 1.00000\n4:PM-7:AM   -11.070968  -69.460 47.3182 1.00000\n5:PM-7:AM     3.122581  -55.267 61.5118 1.00000\n6:PM-7:AM    -5.180645  -63.570 53.2086 1.00000\n7:PM-7:AM     0.922581  -57.467 59.3118 1.00000\n8:PM-7:AM    -5.606452  -63.996 52.7827 1.00000\n9:PM-7:AM     1.348387  -57.041 59.7376 1.00000\n10:PM-7:AM    2.200000  -56.189 60.5892 1.00000\n11:PM-7:AM  -10.006452  -68.396 48.3827 1.00000\n12:PM-7:AM  -12.561290  -70.950 45.8279 1.00000\n13:PM-7:AM    6.670968  -51.718 65.0602 1.00000\n14:PM-7:AM   -8.658065  -67.047 49.7311 1.00000\n15:PM-7:AM   -1.561290  -59.950 56.8279 1.00000\n16:PM-7:AM  -14.335484  -72.725 44.0537 1.00000\n17:PM-7:AM  -21.929032  -80.318 36.4602 0.99996\n18:PM-7:AM   -8.374194  -66.763 50.0150 1.00000\n19:PM-7:AM  -20.438710  -78.828 37.9505 0.99999\n20:PM-7:AM    5.251613  -53.138 63.6408 1.00000\n9:AM-8:AM    18.877419  -39.512 77.2666 1.00000\n10:AM-8:AM  -24.483871  -82.873 33.9053 0.99954\n11:AM-8:AM   16.819355  -41.570 75.2086 1.00000\n12:AM-8:AM  -25.122581  -83.512 33.2666 0.99925\n13:AM-8:AM  -10.503226  -68.892 47.8860 1.00000\n14:AM-8:AM  -25.264516  -83.654 33.1247 0.99916\n15:AM-8:AM  -32.645161  -91.034 25.7440 0.95531\n16:AM-8:AM  -14.193548  -72.583 44.1956 1.00000\n17:AM-8:AM   -9.509677  -67.899 48.8795 1.00000\n18:AM-8:AM  -12.632258  -71.021 45.7569 1.00000\n19:AM-8:AM   -5.180645  -63.570 53.2086 1.00000\n20:AM-8:AM   -8.800000  -74.081 56.4811 1.00000\n1:PM-8:AM     7.593548  -50.796 65.9827 1.00000\n2:PM-8:AM    -8.729032  -67.118 49.6602 1.00000\n3:PM-8:AM     2.270968  -56.118 60.6602 1.00000\n4:PM-8:AM   -21.716129  -80.105 36.6731 0.99996\n5:PM-8:AM    -7.522581  -65.912 50.8666 1.00000\n6:PM-8:AM   -15.825806  -74.215 42.5634 1.00000\n7:PM-8:AM    -9.722581  -68.112 48.6666 1.00000\n8:PM-8:AM   -16.251613  -74.641 42.1376 1.00000\n9:PM-8:AM    -9.296774  -67.686 49.0924 1.00000\n10:PM-8:AM   -8.445161  -66.834 49.9440 1.00000\n11:PM-8:AM  -20.651613  -79.041 37.7376 0.99999\n12:PM-8:AM  -23.206452  -81.596 35.1827 0.99985\n13:PM-8:AM   -3.974194  -62.363 54.4150 1.00000\n14:PM-8:AM  -19.303226  -77.692 39.0860 1.00000\n15:PM-8:AM  -12.206452  -70.596 46.1827 1.00000\n16:PM-8:AM  -24.980645  -83.370 33.4086 0.99933\n17:PM-8:AM  -32.574194  -90.963 25.8150 0.95647\n18:PM-8:AM  -19.019355  -77.409 39.3698 1.00000\n19:PM-8:AM  -31.083871  -89.473 27.3053 0.97602\n20:PM-8:AM   -5.393548  -63.783 52.9956 1.00000\n10:AM-9:AM  -43.361290 -101.750 15.0279 0.52759\n11:AM-9:AM   -2.058065  -60.447 56.3311 1.00000\n12:AM-9:AM  -44.000000 -102.389 14.3892 0.49403\n13:AM-9:AM  -29.380645  -87.770 29.0086 0.98920\n14:AM-9:AM  -44.141935 -102.531 14.2473 0.48663\n15:AM-9:AM  -51.522581 -109.912  6.8666 0.17781\n16:AM-9:AM  -33.070968  -91.460 25.3182 0.94789\n17:AM-9:AM  -28.387097  -86.776 30.0021 0.99364\n18:AM-9:AM  -31.509677  -89.899 26.8795 0.97131\n19:AM-9:AM  -24.058065  -82.447 34.3311 0.99968\n20:AM-9:AM  -27.677419  -92.959 37.6037 0.99943\n1:PM-9:AM   -11.283871  -69.673 47.1053 1.00000\n2:PM-9:AM   -27.606452  -85.996 30.7827 0.99595\n3:PM-9:AM   -16.606452  -74.996 41.7827 1.00000\n4:PM-9:AM   -40.593548  -98.983 17.7956 0.67322\n5:PM-9:AM   -26.400000  -84.789 31.9892 0.99812\n6:PM-9:AM   -34.703226  -93.092 23.6860 0.91136\n7:PM-9:AM   -28.600000  -86.989 29.7892 0.99284\n8:PM-9:AM   -35.129032  -93.518 23.2602 0.89961\n9:PM-9:AM   -28.174194  -86.563 30.2150 0.99436\n10:PM-9:AM  -27.322581  -85.712 31.0666 0.99660\n11:PM-9:AM  -39.529032  -97.918 18.8602 0.72631\n12:PM-9:AM  -42.083871 -100.473 16.3053 0.59536\n13:PM-9:AM  -22.851613  -81.241 35.5376 0.99989\n14:PM-9:AM  -38.180645  -96.570 20.2086 0.78850\n15:PM-9:AM  -31.083871  -89.473 27.3053 0.97602\n16:PM-9:AM  -43.858065 -102.247 14.5311 0.50145\n17:PM-9:AM  -51.451613 -109.841  6.9376 0.17988\n18:PM-9:AM  -37.896774  -96.286 20.4924 0.80070\n19:PM-9:AM  -49.961290 -108.350  8.4279 0.22767\n20:PM-9:AM  -24.270968  -82.660 34.1182 0.99962\n11:AM-10:AM  41.303226  -17.086 99.6924 0.63649\n12:AM-10:AM  -0.638710  -59.028 57.7505 1.00000\n13:AM-10:AM  13.980645  -44.409 72.3698 1.00000\n14:AM-10:AM  -0.780645  -59.170 57.6086 1.00000\n15:AM-10:AM  -8.161290  -66.550 50.2279 1.00000\n16:AM-10:AM  10.290323  -48.099 68.6795 1.00000\n17:AM-10:AM  14.974194  -43.415 73.3634 1.00000\n18:AM-10:AM  11.851613  -46.538 70.2408 1.00000\n19:AM-10:AM  19.303226  -39.086 77.6924 1.00000\n20:AM-10:AM  15.683871  -49.597 80.9650 1.00000\n1:PM-10:AM   32.077419  -26.312 90.4666 0.96397\n2:PM-10:AM   15.754839  -42.634 74.1440 1.00000\n3:PM-10:AM   26.754839  -31.634 85.1440 0.99763\n4:PM-10:AM    2.767742  -55.621 61.1569 1.00000\n5:PM-10:AM   16.961290  -41.428 75.3505 1.00000\n6:PM-10:AM    8.658065  -49.731 67.0473 1.00000\n7:PM-10:AM   14.761290  -43.628 73.1505 1.00000\n8:PM-10:AM    8.232258  -50.157 66.6215 1.00000\n9:PM-10:AM   15.187097  -43.202 73.5763 1.00000\n10:PM-10:AM  16.038710  -42.350 74.4279 1.00000\n11:PM-10:AM   3.832258  -54.557 62.2215 1.00000\n12:PM-10:AM   1.277419  -57.112 59.6666 1.00000\n13:PM-10:AM  20.509677  -37.880 78.8989 0.99999\n14:PM-10:AM   5.180645  -53.209 63.5698 1.00000\n15:PM-10:AM  12.277419  -46.112 70.6666 1.00000\n16:PM-10:AM  -0.496774  -58.886 57.8924 1.00000\n17:PM-10:AM  -8.090323  -66.480 50.2989 1.00000\n18:PM-10:AM   5.464516  -52.925 63.8537 1.00000\n19:PM-10:AM  -6.600000  -64.989 51.7892 1.00000\n20:PM-10:AM  19.090323  -39.299 77.4795 1.00000\n12:AM-11:AM -41.941935 -100.331 16.4473 0.60287\n13:AM-11:AM -27.322581  -85.712 31.0666 0.99660\n14:AM-11:AM -42.083871 -100.473 16.3053 0.59536\n15:AM-11:AM -49.464516 -107.854  8.9247 0.24541\n16:AM-11:AM -31.012903  -89.402 27.3763 0.97675\n17:AM-11:AM -26.329032  -84.718 32.0602 0.99821\n18:AM-11:AM -29.451613  -87.841 28.9376 0.98880\n19:AM-11:AM -22.000000  -80.389 36.3892 0.99995\n20:AM-11:AM -25.619355  -90.900 39.6618 0.99988\n1:PM-11:AM   -9.225806  -67.615 49.1634 1.00000\n2:PM-11:AM  -25.548387  -83.938 32.8408 0.99897\n3:PM-11:AM  -14.548387  -72.938 43.8408 1.00000\n4:PM-11:AM  -38.535484  -96.925 19.8537 0.77279\n5:PM-11:AM  -24.341935  -82.731 34.0473 0.99959\n6:PM-11:AM  -32.645161  -91.034 25.7440 0.95531\n7:PM-11:AM  -26.541935  -84.931 31.8473 0.99794\n8:PM-11:AM  -33.070968  -91.460 25.3182 0.94789\n9:PM-11:AM  -26.116129  -84.505 32.2731 0.99846\n10:PM-11:AM -25.264516  -83.654 33.1247 0.99916\n11:PM-11:AM -37.470968  -95.860 20.9182 0.81833\n12:PM-11:AM -40.025806  -98.415 18.3634 0.70190\n13:PM-11:AM -20.793548  -79.183 37.5956 0.99999\n14:PM-11:AM -36.122581  -94.512 22.2666 0.86852\n15:PM-11:AM -29.025806  -87.415 29.3634 0.99101\n16:PM-11:AM -41.800000 -100.189 16.5892 0.61038\n17:PM-11:AM -49.393548 -107.783  8.9956 0.24802\n18:PM-11:AM -35.838710  -94.228 22.5505 0.87792\n19:PM-11:AM -47.903226 -106.292 10.4860 0.30702\n20:PM-11:AM -22.212903  -80.602 36.1763 0.99994\n13:AM-12:AM  14.619355  -43.770 73.0086 1.00000\n14:AM-12:AM  -0.141935  -58.531 58.2473 1.00000\n15:AM-12:AM  -7.522581  -65.912 50.8666 1.00000\n16:AM-12:AM  10.929032  -47.460 69.3182 1.00000\n17:AM-12:AM  15.612903  -42.776 74.0021 1.00000\n18:AM-12:AM  12.490323  -45.899 70.8795 1.00000\n19:AM-12:AM  19.941935  -38.447 78.3311 1.00000\n20:AM-12:AM  16.322581  -48.959 81.6037 1.00000\n1:PM-12:AM   32.716129  -25.673 91.1053 0.95413\n2:PM-12:AM   16.393548  -41.996 74.7827 1.00000\n3:PM-12:AM   27.393548  -30.996 85.7827 0.99645\n4:PM-12:AM    3.406452  -54.983 61.7956 1.00000\n5:PM-12:AM   17.600000  -40.789 75.9892 1.00000\n6:PM-12:AM    9.296774  -49.092 67.6860 1.00000\n7:PM-12:AM   15.400000  -42.989 73.7892 1.00000\n8:PM-12:AM    8.870968  -49.518 67.2602 1.00000\n9:PM-12:AM   15.825806  -42.563 74.2150 1.00000\n10:PM-12:AM  16.677419  -41.712 75.0666 1.00000\n11:PM-12:AM   4.470968  -53.918 62.8602 1.00000\n12:PM-12:AM   1.916129  -56.473 60.3053 1.00000\n13:PM-12:AM  21.148387  -37.241 79.5376 0.99998\n14:PM-12:AM   5.819355  -52.570 64.2086 1.00000\n15:PM-12:AM  12.916129  -45.473 71.3053 1.00000\n16:PM-12:AM   0.141935  -58.247 58.5311 1.00000\n17:PM-12:AM  -7.451613  -65.841 50.9376 1.00000\n18:PM-12:AM   6.103226  -52.286 64.4924 1.00000\n19:PM-12:AM  -5.961290  -64.350 52.4279 1.00000\n20:PM-12:AM  19.729032  -38.660 78.1182 1.00000\n14:AM-13:AM -14.761290  -73.150 43.6279 1.00000\n15:AM-13:AM -22.141935  -80.531 36.2473 0.99994\n16:AM-13:AM  -3.690323  -62.080 54.6989 1.00000\n17:AM-13:AM   0.993548  -57.396 59.3827 1.00000\n18:AM-13:AM  -2.129032  -60.518 56.2602 1.00000\n19:AM-13:AM   5.322581  -53.067 63.7118 1.00000\n20:AM-13:AM   1.703226  -63.578 66.9843 1.00000\n1:PM-13:AM   18.096774  -40.292 76.4860 1.00000\n2:PM-13:AM    1.774194  -56.615 60.1634 1.00000\n3:PM-13:AM   12.774194  -45.615 71.1634 1.00000\n4:PM-13:AM  -11.212903  -69.602 47.1763 1.00000\n5:PM-13:AM    2.980645  -55.409 61.3698 1.00000\n6:PM-13:AM   -5.322581  -63.712 53.0666 1.00000\n7:PM-13:AM    0.780645  -57.609 59.1698 1.00000\n8:PM-13:AM   -5.748387  -64.138 52.6408 1.00000\n9:PM-13:AM    1.206452  -57.183 59.5956 1.00000\n10:PM-13:AM   2.058065  -56.331 60.4473 1.00000\n11:PM-13:AM -10.148387  -68.538 48.2408 1.00000\n12:PM-13:AM -12.703226  -71.092 45.6860 1.00000\n13:PM-13:AM   6.529032  -51.860 64.9182 1.00000\n14:PM-13:AM  -8.800000  -67.189 49.5892 1.00000\n15:PM-13:AM  -1.703226  -60.092 56.6860 1.00000\n16:PM-13:AM -14.477419  -72.867 43.9118 1.00000\n17:PM-13:AM -22.070968  -80.460 36.3182 0.99995\n18:PM-13:AM  -8.516129  -66.905 49.8731 1.00000\n19:PM-13:AM -20.580645  -78.970 37.8086 0.99999\n20:PM-13:AM   5.109677  -53.280 63.4989 1.00000\n15:AM-14:AM  -7.380645  -65.770 51.0086 1.00000\n16:AM-14:AM  11.070968  -47.318 69.4602 1.00000\n17:AM-14:AM  15.754839  -42.634 74.1440 1.00000\n18:AM-14:AM  12.632258  -45.757 71.0215 1.00000\n19:AM-14:AM  20.083871  -38.305 78.4731 0.99999\n20:AM-14:AM  16.464516  -48.817 81.7456 1.00000\n1:PM-14:AM   32.858065  -25.531 91.2473 0.95170\n2:PM-14:AM   16.535484  -41.854 74.9247 1.00000\n3:PM-14:AM   27.535484  -30.854 85.9247 0.99612\n4:PM-14:AM    3.548387  -54.841 61.9376 1.00000\n5:PM-14:AM   17.741935  -40.647 76.1311 1.00000\n6:PM-14:AM    9.438710  -48.950 67.8279 1.00000\n7:PM-14:AM   15.541935  -42.847 73.9311 1.00000\n8:PM-14:AM    9.012903  -49.376 67.4021 1.00000\n9:PM-14:AM   15.967742  -42.421 74.3569 1.00000\n10:PM-14:AM  16.819355  -41.570 75.2086 1.00000\n11:PM-14:AM   4.612903  -53.776 63.0021 1.00000\n12:PM-14:AM   2.058065  -56.331 60.4473 1.00000\n13:PM-14:AM  21.290323  -37.099 79.6795 0.99998\n14:PM-14:AM   5.961290  -52.428 64.3505 1.00000\n15:PM-14:AM  13.058065  -45.331 71.4473 1.00000\n16:PM-14:AM   0.283871  -58.105 58.6731 1.00000\n17:PM-14:AM  -7.309677  -65.699 51.0795 1.00000\n18:PM-14:AM   6.245161  -52.144 64.6344 1.00000\n19:PM-14:AM  -5.819355  -64.209 52.5698 1.00000\n20:PM-14:AM  19.870968  -38.518 78.2602 1.00000\n16:AM-15:AM  18.451613  -39.938 76.8408 1.00000\n17:AM-15:AM  23.135484  -35.254 81.5247 0.99986\n18:AM-15:AM  20.012903  -38.376 78.4021 0.99999\n19:AM-15:AM  27.464516  -30.925 85.8537 0.99629\n20:AM-15:AM  23.845161  -41.436 89.1263 0.99998\n1:PM-15:AM   40.238710  -18.150 98.6279 0.69123\n2:PM-15:AM   23.916129  -34.473 82.3053 0.99972\n3:PM-15:AM   34.916129  -23.473 93.3053 0.90560\n4:PM-15:AM   10.929032  -47.460 69.3182 1.00000\n5:PM-15:AM   25.122581  -33.267 83.5118 0.99925\n6:PM-15:AM   16.819355  -41.570 75.2086 1.00000\n7:PM-15:AM   22.922581  -35.467 81.3118 0.99988\n8:PM-15:AM   16.393548  -41.996 74.7827 1.00000\n9:PM-15:AM   23.348387  -35.041 81.7376 0.99983\n10:PM-15:AM  24.200000  -34.189 82.5892 0.99964\n11:PM-15:AM  11.993548  -46.396 70.3827 1.00000\n12:PM-15:AM   9.438710  -48.950 67.8279 1.00000\n13:PM-15:AM  28.670968  -29.718 87.0602 0.99256\n14:PM-15:AM  13.341935  -45.047 71.7311 1.00000\n15:PM-15:AM  20.438710  -37.950 78.8279 0.99999\n16:PM-15:AM   7.664516  -50.725 66.0537 1.00000\n17:PM-15:AM   0.070968  -58.318 58.4602 1.00000\n18:PM-15:AM  13.625806  -44.763 72.0150 1.00000\n19:PM-15:AM   1.561290  -56.828 59.9505 1.00000\n20:PM-15:AM  27.251613  -31.138 85.6408 0.99674\n17:AM-16:AM   4.683871  -53.705 63.0731 1.00000\n18:AM-16:AM   1.561290  -56.828 59.9505 1.00000\n19:AM-16:AM   9.012903  -49.376 67.4021 1.00000\n20:AM-16:AM   5.393548  -59.888 70.6747 1.00000\n1:PM-16:AM   21.787097  -36.602 80.1763 0.99996\n2:PM-16:AM    5.464516  -52.925 63.8537 1.00000\n3:PM-16:AM   16.464516  -41.925 74.8537 1.00000\n4:PM-16:AM   -7.522581  -65.912 50.8666 1.00000\n5:PM-16:AM    6.670968  -51.718 65.0602 1.00000\n6:PM-16:AM   -1.632258  -60.021 56.7569 1.00000\n7:PM-16:AM    4.470968  -53.918 62.8602 1.00000\n8:PM-16:AM   -2.058065  -60.447 56.3311 1.00000\n9:PM-16:AM    4.896774  -53.492 63.2860 1.00000\n10:PM-16:AM   5.748387  -52.641 64.1376 1.00000\n11:PM-16:AM  -6.458065  -64.847 51.9311 1.00000\n12:PM-16:AM  -9.012903  -67.402 49.3763 1.00000\n13:PM-16:AM  10.219355  -48.170 68.6086 1.00000\n14:PM-16:AM  -5.109677  -63.499 53.2795 1.00000\n15:PM-16:AM   1.987097  -56.402 60.3763 1.00000\n16:PM-16:AM -10.787097  -69.176 47.6021 1.00000\n17:PM-16:AM -18.380645  -76.770 40.0086 1.00000\n18:PM-16:AM  -4.825806  -63.215 53.5634 1.00000\n19:PM-16:AM -16.890323  -75.280 41.4989 1.00000\n20:PM-16:AM   8.800000  -49.589 67.1892 1.00000\n18:AM-17:AM  -3.122581  -61.512 55.2666 1.00000\n19:AM-17:AM   4.329032  -54.060 62.7182 1.00000\n20:AM-17:AM   0.709677  -64.571 65.9908 1.00000\n1:PM-17:AM   17.103226  -41.286 75.4924 1.00000\n2:PM-17:AM    0.780645  -57.609 59.1698 1.00000\n3:PM-17:AM   11.780645  -46.609 70.1698 1.00000\n4:PM-17:AM  -12.206452  -70.596 46.1827 1.00000\n5:PM-17:AM    1.987097  -56.402 60.3763 1.00000\n6:PM-17:AM   -6.316129  -64.705 52.0731 1.00000\n7:PM-17:AM   -0.212903  -58.602 58.1763 1.00000\n8:PM-17:AM   -6.741935  -65.131 51.6473 1.00000\n9:PM-17:AM    0.212903  -58.176 58.6021 1.00000\n10:PM-17:AM   1.064516  -57.325 59.4537 1.00000\n11:PM-17:AM -11.141935  -69.531 47.2473 1.00000\n12:PM-17:AM -13.696774  -72.086 44.6924 1.00000\n13:PM-17:AM   5.535484  -52.854 63.9247 1.00000\n14:PM-17:AM  -9.793548  -68.183 48.5956 1.00000\n15:PM-17:AM  -2.696774  -61.086 55.6924 1.00000\n16:PM-17:AM -15.470968  -73.860 42.9182 1.00000\n17:PM-17:AM -23.064516  -81.454 35.3247 0.99987\n18:PM-17:AM  -9.509677  -67.899 48.8795 1.00000\n19:PM-17:AM -21.574194  -79.963 36.8150 0.99997\n20:PM-17:AM   4.116129  -54.273 62.5053 1.00000\n19:AM-18:AM   7.451613  -50.938 65.8408 1.00000\n20:AM-18:AM   3.832258  -61.449 69.1134 1.00000\n1:PM-18:AM   20.225806  -38.163 78.6150 0.99999\n2:PM-18:AM    3.903226  -54.486 62.2924 1.00000\n3:PM-18:AM   14.903226  -43.486 73.2924 1.00000\n4:PM-18:AM   -9.083871  -67.473 49.3053 1.00000\n5:PM-18:AM    5.109677  -53.280 63.4989 1.00000\n6:PM-18:AM   -3.193548  -61.583 55.1956 1.00000\n7:PM-18:AM    2.909677  -55.480 61.2989 1.00000\n8:PM-18:AM   -3.619355  -62.009 54.7698 1.00000\n9:PM-18:AM    3.335484  -55.054 61.7247 1.00000\n10:PM-18:AM   4.187097  -54.202 62.5763 1.00000\n11:PM-18:AM  -8.019355  -66.409 50.3698 1.00000\n12:PM-18:AM -10.574194  -68.963 47.8150 1.00000\n13:PM-18:AM   8.658065  -49.731 67.0473 1.00000\n14:PM-18:AM  -6.670968  -65.060 51.7182 1.00000\n15:PM-18:AM   0.425806  -57.963 58.8150 1.00000\n16:PM-18:AM -12.348387  -70.738 46.0408 1.00000\n17:PM-18:AM -19.941935  -78.331 38.4473 1.00000\n18:PM-18:AM  -6.387097  -64.776 52.0021 1.00000\n19:PM-18:AM -18.451613  -76.841 39.9376 1.00000\n20:PM-18:AM   7.238710  -51.150 65.6279 1.00000\n20:AM-19:AM  -3.619355  -68.900 61.6618 1.00000\n1:PM-19:AM   12.774194  -45.615 71.1634 1.00000\n2:PM-19:AM   -3.548387  -61.938 54.8408 1.00000\n3:PM-19:AM    7.451613  -50.938 65.8408 1.00000\n4:PM-19:AM  -16.535484  -74.925 41.8537 1.00000\n5:PM-19:AM   -2.341935  -60.731 56.0473 1.00000\n6:PM-19:AM  -10.645161  -69.034 47.7440 1.00000\n7:PM-19:AM   -4.541935  -62.931 53.8473 1.00000\n8:PM-19:AM  -11.070968  -69.460 47.3182 1.00000\n9:PM-19:AM   -4.116129  -62.505 54.2731 1.00000\n10:PM-19:AM  -3.264516  -61.654 55.1247 1.00000\n11:PM-19:AM -15.470968  -73.860 42.9182 1.00000\n12:PM-19:AM -18.025806  -76.415 40.3634 1.00000\n13:PM-19:AM   1.206452  -57.183 59.5956 1.00000\n14:PM-19:AM -14.122581  -72.512 44.2666 1.00000\n15:PM-19:AM  -7.025806  -65.415 51.3634 1.00000\n16:PM-19:AM -19.800000  -78.189 38.5892 1.00000\n17:PM-19:AM -27.393548  -85.783 30.9956 0.99645\n18:PM-19:AM -13.838710  -72.228 44.5505 1.00000\n19:PM-19:AM -25.903226  -84.292 32.4860 0.99867\n20:PM-19:AM  -0.212903  -58.602 58.1763 1.00000\n1:PM-20:AM   16.393548  -48.888 81.6747 1.00000\n2:PM-20:AM    0.070968  -65.210 65.3521 1.00000\n3:PM-20:AM   11.070968  -54.210 76.3521 1.00000\n4:PM-20:AM  -12.916129  -78.197 52.3650 1.00000\n5:PM-20:AM    1.277419  -64.004 66.5585 1.00000\n6:PM-20:AM   -7.025806  -72.307 58.2553 1.00000\n7:PM-20:AM   -0.922581  -66.204 64.3585 1.00000\n8:PM-20:AM   -7.451613  -72.733 57.8295 1.00000\n9:PM-20:AM   -0.496774  -65.778 64.7843 1.00000\n10:PM-20:AM   0.354839  -64.926 65.6359 1.00000\n11:PM-20:AM -11.851613  -77.133 53.4295 1.00000\n12:PM-20:AM -14.406452  -79.688 50.8747 1.00000\n13:PM-20:AM   4.825806  -60.455 70.1069 1.00000\n14:PM-20:AM -10.503226  -75.784 54.7779 1.00000\n15:PM-20:AM  -3.406452  -68.688 61.8747 1.00000\n16:PM-20:AM -16.180645  -81.462 49.1005 1.00000\n17:PM-20:AM -23.774194  -89.055 41.5069 0.99998\n18:PM-20:AM -10.219355  -75.500 55.0618 1.00000\n19:PM-20:AM -22.283871  -87.565 42.9972 1.00000\n20:PM-20:AM   3.406452  -61.875 68.6876 1.00000\n2:PM-1:PM   -16.322581  -74.712 42.0666 1.00000\n3:PM-1:PM    -5.322581  -63.712 53.0666 1.00000\n4:PM-1:PM   -29.309677  -87.699 29.0795 0.98958\n5:PM-1:PM   -15.116129  -73.505 43.2731 1.00000\n6:PM-1:PM   -23.419355  -81.809 34.9698 0.99982\n7:PM-1:PM   -17.316129  -75.705 41.0731 1.00000\n8:PM-1:PM   -23.845161  -82.234 34.5440 0.99973\n9:PM-1:PM   -16.890323  -75.280 41.4989 1.00000\n10:PM-1:PM  -16.038710  -74.428 42.3505 1.00000\n11:PM-1:PM  -28.245161  -86.634 30.1440 0.99413\n12:PM-1:PM  -30.800000  -89.189 27.5892 0.97882\n13:PM-1:PM  -11.567742  -69.957 46.8215 1.00000\n14:PM-1:PM  -26.896774  -85.286 31.4924 0.99740\n15:PM-1:PM  -19.800000  -78.189 38.5892 1.00000\n16:PM-1:PM  -32.574194  -90.963 25.8150 0.95647\n17:PM-1:PM  -40.167742  -98.557 18.2215 0.69480\n18:PM-1:PM  -26.612903  -85.002 31.7763 0.99784\n19:PM-1:PM  -38.677419  -97.067 19.7118 0.76637\n20:PM-1:PM  -12.987097  -71.376 45.4021 1.00000\n3:PM-2:PM    11.000000  -47.389 69.3892 1.00000\n4:PM-2:PM   -12.987097  -71.376 45.4021 1.00000\n5:PM-2:PM     1.206452  -57.183 59.5956 1.00000\n6:PM-2:PM    -7.096774  -65.486 51.2924 1.00000\n7:PM-2:PM    -0.993548  -59.383 57.3956 1.00000\n8:PM-2:PM    -7.522581  -65.912 50.8666 1.00000\n9:PM-2:PM    -0.567742  -58.957 57.8215 1.00000\n10:PM-2:PM    0.283871  -58.105 58.6731 1.00000\n11:PM-2:PM  -11.922581  -70.312 46.4666 1.00000\n12:PM-2:PM  -14.477419  -72.867 43.9118 1.00000\n13:PM-2:PM    4.754839  -53.634 63.1440 1.00000\n14:PM-2:PM  -10.574194  -68.963 47.8150 1.00000\n15:PM-2:PM   -3.477419  -61.867 54.9118 1.00000\n16:PM-2:PM  -16.251613  -74.641 42.1376 1.00000\n17:PM-2:PM  -23.845161  -82.234 34.5440 0.99973\n18:PM-2:PM  -10.290323  -68.680 48.0989 1.00000\n19:PM-2:PM  -22.354839  -80.744 36.0344 0.99993\n20:PM-2:PM    3.335484  -55.054 61.7247 1.00000\n4:PM-3:PM   -23.987097  -82.376 34.4021 0.99970\n5:PM-3:PM    -9.793548  -68.183 48.5956 1.00000\n6:PM-3:PM   -18.096774  -76.486 40.2924 1.00000\n7:PM-3:PM   -11.993548  -70.383 46.3956 1.00000\n8:PM-3:PM   -18.522581  -76.912 39.8666 1.00000\n9:PM-3:PM   -11.567742  -69.957 46.8215 1.00000\n10:PM-3:PM  -10.716129  -69.105 47.6731 1.00000\n11:PM-3:PM  -22.922581  -81.312 35.4666 0.99988\n12:PM-3:PM  -25.477419  -83.867 32.9118 0.99902\n13:PM-3:PM   -6.245161  -64.634 52.1440 1.00000\n14:PM-3:PM  -21.574194  -79.963 36.8150 0.99997\n15:PM-3:PM  -14.477419  -72.867 43.9118 1.00000\n16:PM-3:PM  -27.251613  -85.641 31.1376 0.99674\n17:PM-3:PM  -34.845161  -93.234 23.5440 0.90755\n18:PM-3:PM  -21.290323  -79.680 37.0989 0.99998\n19:PM-3:PM  -33.354839  -91.744 25.0344 0.94247\n20:PM-3:PM   -7.664516  -66.054 50.7247 1.00000\n5:PM-4:PM    14.193548  -44.196 72.5827 1.00000\n6:PM-4:PM     5.890323  -52.499 64.2795 1.00000\n7:PM-4:PM    11.993548  -46.396 70.3827 1.00000\n8:PM-4:PM     5.464516  -52.925 63.8537 1.00000\n9:PM-4:PM    12.419355  -45.970 70.8086 1.00000\n10:PM-4:PM   13.270968  -45.118 71.6602 1.00000\n11:PM-4:PM    1.064516  -57.325 59.4537 1.00000\n12:PM-4:PM   -1.490323  -59.880 56.8989 1.00000\n13:PM-4:PM   17.741935  -40.647 76.1311 1.00000\n14:PM-4:PM    2.412903  -55.976 60.8021 1.00000\n15:PM-4:PM    9.509677  -48.880 67.8989 1.00000\n16:PM-4:PM   -3.264516  -61.654 55.1247 1.00000\n17:PM-4:PM  -10.858065  -69.247 47.5311 1.00000\n18:PM-4:PM    2.696774  -55.692 61.0860 1.00000\n19:PM-4:PM   -9.367742  -67.757 49.0215 1.00000\n20:PM-4:PM   16.322581  -42.067 74.7118 1.00000\n6:PM-5:PM    -8.303226  -66.692 50.0860 1.00000\n7:PM-5:PM    -2.200000  -60.589 56.1892 1.00000\n8:PM-5:PM    -8.729032  -67.118 49.6602 1.00000\n9:PM-5:PM    -1.774194  -60.163 56.6150 1.00000\n10:PM-5:PM   -0.922581  -59.312 57.4666 1.00000\n11:PM-5:PM  -13.129032  -71.518 45.2602 1.00000\n12:PM-5:PM  -15.683871  -74.073 42.7053 1.00000\n13:PM-5:PM    3.548387  -54.841 61.9376 1.00000\n14:PM-5:PM  -11.780645  -70.170 46.6086 1.00000\n15:PM-5:PM   -4.683871  -63.073 53.7053 1.00000\n16:PM-5:PM  -17.458065  -75.847 40.9311 1.00000\n17:PM-5:PM  -25.051613  -83.441 33.3376 0.99929\n18:PM-5:PM  -11.496774  -69.886 46.8924 1.00000\n19:PM-5:PM  -23.561290  -81.950 34.8279 0.99979\n20:PM-5:PM    2.129032  -56.260 60.5182 1.00000\n7:PM-6:PM     6.103226  -52.286 64.4924 1.00000\n8:PM-6:PM    -0.425806  -58.815 57.9634 1.00000\n9:PM-6:PM     6.529032  -51.860 64.9182 1.00000\n10:PM-6:PM    7.380645  -51.009 65.7698 1.00000\n11:PM-6:PM   -4.825806  -63.215 53.5634 1.00000\n12:PM-6:PM   -7.380645  -65.770 51.0086 1.00000\n13:PM-6:PM   11.851613  -46.538 70.2408 1.00000\n14:PM-6:PM   -3.477419  -61.867 54.9118 1.00000\n15:PM-6:PM    3.619355  -54.770 62.0086 1.00000\n16:PM-6:PM   -9.154839  -67.544 49.2344 1.00000\n17:PM-6:PM  -16.748387  -75.138 41.6408 1.00000\n18:PM-6:PM   -3.193548  -61.583 55.1956 1.00000\n19:PM-6:PM  -15.258065  -73.647 43.1311 1.00000\n20:PM-6:PM   10.432258  -47.957 68.8215 1.00000\n8:PM-7:PM    -6.529032  -64.918 51.8602 1.00000\n9:PM-7:PM     0.425806  -57.963 58.8150 1.00000\n10:PM-7:PM    1.277419  -57.112 59.6666 1.00000\n11:PM-7:PM  -10.929032  -69.318 47.4602 1.00000\n12:PM-7:PM  -13.483871  -71.873 44.9053 1.00000\n13:PM-7:PM    5.748387  -52.641 64.1376 1.00000\n14:PM-7:PM   -9.580645  -67.970 48.8086 1.00000\n15:PM-7:PM   -2.483871  -60.873 55.9053 1.00000\n16:PM-7:PM  -15.258065  -73.647 43.1311 1.00000\n17:PM-7:PM  -22.851613  -81.241 35.5376 0.99989\n18:PM-7:PM   -9.296774  -67.686 49.0924 1.00000\n19:PM-7:PM  -21.361290  -79.750 37.0279 0.99998\n20:PM-7:PM    4.329032  -54.060 62.7182 1.00000\n9:PM-8:PM     6.954839  -51.434 65.3440 1.00000\n10:PM-8:PM    7.806452  -50.583 66.1956 1.00000\n11:PM-8:PM   -4.400000  -62.789 53.9892 1.00000\n12:PM-8:PM   -6.954839  -65.344 51.4344 1.00000\n13:PM-8:PM   12.277419  -46.112 70.6666 1.00000\n14:PM-8:PM   -3.051613  -61.441 55.3376 1.00000\n15:PM-8:PM    4.045161  -54.344 62.4344 1.00000\n16:PM-8:PM   -8.729032  -67.118 49.6602 1.00000\n17:PM-8:PM  -16.322581  -74.712 42.0666 1.00000\n18:PM-8:PM   -2.767742  -61.157 55.6215 1.00000\n19:PM-8:PM  -14.832258  -73.221 43.5569 1.00000\n20:PM-8:PM   10.858065  -47.531 69.2473 1.00000\n10:PM-9:PM    0.851613  -57.538 59.2408 1.00000\n11:PM-9:PM  -11.354839  -69.744 47.0344 1.00000\n12:PM-9:PM  -13.909677  -72.299 44.4795 1.00000\n13:PM-9:PM    5.322581  -53.067 63.7118 1.00000\n14:PM-9:PM  -10.006452  -68.396 48.3827 1.00000\n15:PM-9:PM   -2.909677  -61.299 55.4795 1.00000\n16:PM-9:PM  -15.683871  -74.073 42.7053 1.00000\n17:PM-9:PM  -23.277419  -81.667 35.1118 0.99984\n18:PM-9:PM   -9.722581  -68.112 48.6666 1.00000\n19:PM-9:PM  -21.787097  -80.176 36.6021 0.99996\n20:PM-9:PM    3.903226  -54.486 62.2924 1.00000\n11:PM-10:PM -12.206452  -70.596 46.1827 1.00000\n12:PM-10:PM -14.761290  -73.150 43.6279 1.00000\n13:PM-10:PM   4.470968  -53.918 62.8602 1.00000\n14:PM-10:PM -10.858065  -69.247 47.5311 1.00000\n15:PM-10:PM  -3.761290  -62.150 54.6279 1.00000\n16:PM-10:PM -16.535484  -74.925 41.8537 1.00000\n17:PM-10:PM -24.129032  -82.518 34.2602 0.99966\n18:PM-10:PM -10.574194  -68.963 47.8150 1.00000\n19:PM-10:PM -22.638710  -81.028 35.7505 0.99991\n20:PM-10:PM   3.051613  -55.338 61.4408 1.00000\n12:PM-11:PM  -2.554839  -60.944 55.8344 1.00000\n13:PM-11:PM  16.677419  -41.712 75.0666 1.00000\n14:PM-11:PM   1.348387  -57.041 59.7376 1.00000\n15:PM-11:PM   8.445161  -49.944 66.8344 1.00000\n16:PM-11:PM  -4.329032  -62.718 54.0602 1.00000\n17:PM-11:PM -11.922581  -70.312 46.4666 1.00000\n18:PM-11:PM   1.632258  -56.757 60.0215 1.00000\n19:PM-11:PM -10.432258  -68.821 47.9569 1.00000\n20:PM-11:PM  15.258065  -43.131 73.6473 1.00000\n13:PM-12:PM  19.232258  -39.157 77.6215 1.00000\n14:PM-12:PM   3.903226  -54.486 62.2924 1.00000\n15:PM-12:PM  11.000000  -47.389 69.3892 1.00000\n16:PM-12:PM  -1.774194  -60.163 56.6150 1.00000\n17:PM-12:PM  -9.367742  -67.757 49.0215 1.00000\n18:PM-12:PM   4.187097  -54.202 62.5763 1.00000\n19:PM-12:PM  -7.877419  -66.267 50.5118 1.00000\n20:PM-12:PM  17.812903  -40.576 76.2021 1.00000\n14:PM-13:PM -15.329032  -73.718 43.0602 1.00000\n15:PM-13:PM  -8.232258  -66.621 50.1569 1.00000\n16:PM-13:PM -21.006452  -79.396 37.3827 0.99998\n17:PM-13:PM -28.600000  -86.989 29.7892 0.99284\n18:PM-13:PM -15.045161  -73.434 43.3440 1.00000\n19:PM-13:PM -27.109677  -85.499 31.2795 0.99702\n20:PM-13:PM  -1.419355  -59.809 56.9698 1.00000\n15:PM-14:PM   7.096774  -51.292 65.4860 1.00000\n16:PM-14:PM  -5.677419  -64.067 52.7118 1.00000\n17:PM-14:PM -13.270968  -71.660 45.1182 1.00000\n18:PM-14:PM   0.283871  -58.105 58.6731 1.00000\n19:PM-14:PM -11.780645  -70.170 46.6086 1.00000\n20:PM-14:PM  13.909677  -44.480 72.2989 1.00000\n16:PM-15:PM -12.774194  -71.163 45.6150 1.00000\n17:PM-15:PM -20.367742  -78.757 38.0215 0.99999\n18:PM-15:PM  -6.812903  -65.202 51.5763 1.00000\n19:PM-15:PM -18.877419  -77.267 39.5118 1.00000\n20:PM-15:PM   6.812903  -51.576 65.2021 1.00000\n17:PM-16:PM  -7.593548  -65.983 50.7956 1.00000\n18:PM-16:PM   5.961290  -52.428 64.3505 1.00000\n19:PM-16:PM  -6.103226  -64.492 52.2860 1.00000\n20:PM-16:PM  19.587097  -38.802 77.9763 1.00000\n18:PM-17:PM  13.554839  -44.834 71.9440 1.00000\n19:PM-17:PM   1.490323  -56.899 59.8795 1.00000\n20:PM-17:PM  27.180645  -31.209 85.5698 0.99689\n19:PM-18:PM -12.064516  -70.454 46.3247 1.00000\n20:PM-18:PM  13.625806  -44.763 72.0150 1.00000\n20:PM-19:PM  25.690323  -32.699 84.0795 0.99886\n\n\n  Tukey multiple comparisons of means\n    99% family-wise confidence level\n\nFit: aov(formula = response ~ Day + noon + Day:noon, data = data)\n\n$Day\n            diff     lwr    upr   p adj\n2-1    -6.138710 -48.905 36.628 1.00000\n3-1    -4.080645 -46.847 38.686 1.00000\n4-1   -14.300000 -57.066 28.466 0.99700\n5-1    -3.335484 -46.102 39.431 1.00000\n6-1    -6.812903 -49.579 35.953 1.00000\n7-1    -6.422581 -49.189 36.344 1.00000\n8-1    -4.364516 -47.131 38.402 1.00000\n9-1     8.551613 -34.215 51.318 1.00000\n10-1  -12.703226 -55.470 30.063 0.99934\n11-1    1.845161 -40.921 44.611 1.00000\n12-1  -20.403226 -63.170 22.363 0.89330\n13-1   -3.477419 -46.244 39.289 1.00000\n14-1  -18.522581 -61.289 24.244 0.95243\n15-1  -18.664516 -61.431 24.102 0.94905\n16-1  -15.825806 -58.592 26.941 0.99024\n17-1  -17.280645 -60.047 25.486 0.97539\n18-1  -12.064516 -54.831 30.702 0.99968\n19-1  -14.370968 -57.137 28.395 0.99682\n20-1   -2.994839 -47.849 41.859 1.00000\n3-2     2.058065 -40.708 44.824 1.00000\n4-2    -8.161290 -50.928 34.605 1.00000\n5-2     2.803226 -39.963 45.570 1.00000\n6-2    -0.674194 -43.441 42.092 1.00000\n7-2    -0.283871 -43.050 42.482 1.00000\n8-2     1.774194 -40.992 44.541 1.00000\n9-2    14.690323 -28.076 57.457 0.99585\n10-2   -6.564516 -49.331 36.202 1.00000\n11-2    7.983871 -34.782 50.750 1.00000\n12-2  -14.264516 -57.031 28.502 0.99709\n13-2    2.661290 -40.105 45.428 1.00000\n14-2  -12.383871 -55.150 30.382 0.99953\n15-2  -12.525806 -55.292 30.241 0.99946\n16-2   -9.687097 -52.453 33.079 0.99999\n17-2  -11.141935 -53.908 31.624 0.99990\n18-2   -5.925806 -48.692 36.841 1.00000\n19-2   -8.232258 -50.999 34.534 1.00000\n20-2    3.143871 -41.710 47.998 1.00000\n4-3   -10.219355 -52.986 32.547 0.99997\n5-3     0.745161 -42.021 43.511 1.00000\n6-3    -2.732258 -45.499 40.034 1.00000\n7-3    -2.341935 -45.108 40.424 1.00000\n8-3    -0.283871 -43.050 42.482 1.00000\n9-3    12.632258 -30.134 55.399 0.99939\n10-3   -8.622581 -51.389 34.144 1.00000\n11-3    5.925806 -36.841 48.692 1.00000\n12-3  -16.322581 -59.089 26.444 0.98634\n13-3    0.603226 -42.163 43.370 1.00000\n14-3  -14.441935 -57.208 28.324 0.99662\n15-3  -14.583871 -57.350 28.182 0.99620\n16-3  -11.745161 -54.511 31.021 0.99978\n17-3  -13.200000 -55.966 29.566 0.99891\n18-3   -7.983871 -50.750 34.782 1.00000\n19-3  -10.290323 -53.057 32.476 0.99997\n20-3    1.085806 -43.768 45.939 1.00000\n5-4    10.964516 -31.802 53.731 0.99992\n6-4     7.487097 -35.279 50.253 1.00000\n7-4     7.877419 -34.889 50.644 1.00000\n8-4     9.935484 -32.831 52.702 0.99998\n9-4    22.851613 -19.915 65.618 0.76824\n10-4    1.596774 -41.170 44.363 1.00000\n11-4   16.145161 -26.621 58.911 0.98785\n12-4   -6.103226 -48.870 36.663 1.00000\n13-4   10.822581 -31.944 53.589 0.99993\n14-4   -4.222581 -46.989 38.544 1.00000\n15-4   -4.364516 -47.131 38.402 1.00000\n16-4   -1.525806 -44.292 41.241 1.00000\n17-4   -2.980645 -45.747 39.786 1.00000\n18-4    2.235484 -40.531 45.002 1.00000\n19-4   -0.070968 -42.837 42.695 1.00000\n20-4   11.305161 -33.549 56.159 0.99994\n6-5    -3.477419 -46.244 39.289 1.00000\n7-5    -3.087097 -45.853 39.679 1.00000\n8-5    -1.029032 -43.795 41.737 1.00000\n9-5    11.887097 -30.879 54.653 0.99974\n10-5   -9.367742 -52.134 33.399 0.99999\n11-5    5.180645 -37.586 47.947 1.00000\n12-5  -17.067742 -59.834 25.699 0.97827\n13-5   -0.141935 -42.908 42.624 1.00000\n14-5  -15.187097 -57.953 27.579 0.99387\n15-5  -15.329032 -58.095 27.437 0.99318\n16-5  -12.490323 -55.257 30.276 0.99948\n17-5  -13.945161 -56.711 28.821 0.99780\n18-5   -8.729032 -51.495 34.037 1.00000\n19-5  -11.035484 -53.802 31.731 0.99991\n20-5    0.340645 -44.513 45.194 1.00000\n7-6     0.390323 -42.376 43.157 1.00000\n8-6     2.448387 -40.318 45.215 1.00000\n9-6    15.364516 -27.402 58.131 0.99300\n10-6   -5.890323 -48.657 36.876 1.00000\n11-6    8.658065 -34.108 51.424 1.00000\n12-6  -13.590323 -56.357 29.176 0.99841\n13-6    3.335484 -39.431 46.102 1.00000\n14-6  -11.709677 -54.476 31.057 0.99979\n15-6  -11.851613 -54.618 30.915 0.99975\n16-6   -9.012903 -51.779 33.753 1.00000\n17-6  -10.467742 -53.234 32.299 0.99996\n18-6   -5.251613 -48.018 37.515 1.00000\n19-6   -7.558065 -50.324 35.208 1.00000\n20-6    3.818065 -41.036 48.672 1.00000\n8-7     2.058065 -40.708 44.824 1.00000\n9-7    14.974194 -27.792 57.741 0.99480\n10-7   -6.280645 -49.047 36.486 1.00000\n11-7    8.267742 -34.499 51.034 1.00000\n12-7  -13.980645 -56.747 28.786 0.99773\n13-7    2.945161 -39.821 45.711 1.00000\n14-7  -12.100000 -54.866 30.666 0.99966\n15-7  -12.241935 -55.008 30.524 0.99960\n16-7   -9.403226 -52.170 33.363 0.99999\n17-7  -10.858065 -53.624 31.908 0.99993\n18-7   -5.641935 -48.408 37.124 1.00000\n19-7   -7.948387 -50.715 34.818 1.00000\n20-7    3.427742 -41.426 48.281 1.00000\n9-8    12.916129 -29.850 55.682 0.99918\n10-8   -8.338710 -51.105 34.428 1.00000\n11-8    6.209677 -36.557 48.976 1.00000\n12-8  -16.038710 -58.805 26.728 0.98869\n13-8    0.887097 -41.879 43.653 1.00000\n14-8  -14.158065 -56.924 28.608 0.99735\n15-8  -14.300000 -57.066 28.466 0.99700\n16-8  -11.461290 -54.228 31.305 0.99984\n17-8  -12.916129 -55.682 29.850 0.99918\n18-8   -7.700000 -50.466 35.066 1.00000\n19-8  -10.006452 -52.773 32.760 0.99998\n20-8    1.369677 -43.484 46.223 1.00000\n10-9  -21.254839 -64.021 21.511 0.85576\n11-9   -6.706452 -49.473 36.060 1.00000\n12-9  -28.954839 -71.721 13.811 0.35269\n13-9  -12.029032 -54.795 30.737 0.99969\n14-9  -27.074194 -69.841 15.692 0.47719\n15-9  -27.216129 -69.982 15.550 0.46731\n16-9  -24.377419 -67.144 18.389 0.66832\n17-9  -25.832258 -68.599 16.934 0.56535\n18-9  -20.616129 -63.382 22.150 0.88455\n19-9  -22.922581 -65.689 19.844 0.76389\n20-9  -11.546452 -56.400 33.307 0.99991\n11-10  14.548387 -28.218 57.315 0.99631\n12-10  -7.700000 -50.466 35.066 1.00000\n13-10   9.225806 -33.541 51.992 0.99999\n14-10  -5.819355 -48.586 36.947 1.00000\n15-10  -5.961290 -48.728 36.805 1.00000\n16-10  -3.122581 -45.889 39.644 1.00000\n17-10  -4.577419 -47.344 38.189 1.00000\n18-10   0.638710 -42.128 43.405 1.00000\n19-10  -1.667742 -44.434 41.099 1.00000\n20-10   9.708387 -35.145 54.562 0.99999\n12-11 -22.248387 -65.015 20.518 0.80373\n13-11  -5.322581 -48.089 37.444 1.00000\n14-11 -20.367742 -63.134 22.399 0.89472\n15-11 -20.509677 -63.276 22.257 0.88898\n16-11 -17.670968 -60.437 25.095 0.96936\n17-11 -19.125806 -61.892 23.641 0.93692\n18-11 -13.909677 -56.676 28.857 0.99787\n19-11 -16.216129 -58.982 26.550 0.98726\n20-11  -4.840000 -49.694 40.014 1.00000\n13-12  16.925806 -25.841 59.692 0.98004\n14-12   1.880645 -40.886 44.647 1.00000\n15-12   1.738710 -41.028 44.505 1.00000\n16-12   4.577419 -38.189 47.344 1.00000\n17-12   3.122581 -39.644 45.889 1.00000\n18-12   8.338710 -34.428 51.105 1.00000\n19-12   6.032258 -36.734 48.799 1.00000\n20-12  17.408387 -27.445 62.262 0.98369\n14-13 -15.045161 -57.811 27.721 0.99450\n15-13 -15.187097 -57.953 27.579 0.99387\n16-13 -12.348387 -55.115 30.418 0.99955\n17-13 -13.803226 -56.570 28.963 0.99807\n18-13  -8.587097 -51.353 34.179 1.00000\n19-13 -10.893548 -53.660 31.873 0.99992\n20-13   0.482581 -44.371 45.336 1.00000\n15-14  -0.141935 -42.908 42.624 1.00000\n16-14   2.696774 -40.070 45.463 1.00000\n17-14   1.241935 -41.524 44.008 1.00000\n18-14   6.458065 -36.308 49.224 1.00000\n19-14   4.151613 -38.615 46.918 1.00000\n20-14  15.527742 -29.326 60.381 0.99545\n16-15   2.838710 -39.928 45.605 1.00000\n17-15   1.383871 -41.382 44.150 1.00000\n18-15   6.600000 -36.166 49.366 1.00000\n19-15   4.293548 -38.473 47.060 1.00000\n20-15  15.669677 -29.184 60.523 0.99493\n17-16  -1.454839 -44.221 41.311 1.00000\n18-16   3.761290 -39.005 46.528 1.00000\n19-16   1.454839 -41.311 44.221 1.00000\n20-16  12.830968 -32.023 57.685 0.99961\n18-17   5.216129 -37.550 47.982 1.00000\n19-17   2.909677 -39.857 45.676 1.00000\n20-17  14.285806 -30.568 59.139 0.99837\n19-18  -2.306452 -45.073 40.460 1.00000\n20-18   9.069677 -35.784 53.923 1.00000\n20-19  11.376129 -33.478 56.230 0.99993\n\n\n유의한 패턴 없음 Tukey 검정 결과 (보통 유의할때 함) 테이블 도 유의한 수치가 없음"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-07-anova/index.html#terms",
    "href": "docs/blog/posts/statistics/2023-01-07-anova/index.html#terms",
    "title": "ANOVA",
    "section": "3 Terms",
    "text": "3 Terms\n\npairwise comparison: A hypothesis test (e.g., of means) between two groups among multiple groups.\nobmnibus set: A single hypothesis test of the overall variance among multiple group means.\ndecomposition of variance : Separation of components contributing to an individual value (e.g., from the overall average, from a treatment mean, and from a residual error).\nF-test\nF statistic: A standardized statistic that measures the extent to which differences among group means exceed what might be expected in a chance model.\nsum of squares: deviations from some average value"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-07-anova/index.html#anova",
    "href": "docs/blog/posts/statistics/2023-01-07-anova/index.html#anova",
    "title": "ANOVA",
    "section": "4 ANOVA",
    "text": "4 ANOVA\nA basic idea of the ANOVA is to partition variation. It is not concerend with analyzing variances but with analyzing variation in means. To be specific, it is a method of estimating the means of several populations that are often assumed to be normally distributed.\n\n4.1 One Way ANOVA\n\nwe\nAssumptions\n\n\\[ Y_{ij}=\\mu_i + \\epsilon_{ij}, \\space \\space i=1, ..., k, \\space j=1, ...,n_i \\] \\[ EY_{ij}=\\mu_i, \\space \\space , i=1, ..., k, \\space j=1, ...,n_i \\]\nwhere the \\(\\mu_i\\) are unkown parameters and the \\(\\epsilon_{ij}\\) are error random variables.\n\n\\(\\text{E}\\epsilon_{ij}=0\\), \\(\\text{Var}\\epsilon_{ij}=0<\\infty\\), for all \\(i, j\\)\n\\(\\text{Cov}(\\epsilon_{ij},\\epsilon_{i'j'})=0\\), for all \\(i, i', j\\), and \\(j'\\) unless \\(i=i'\\) and \\(j=j'\\).\nThe \\(\\epsilon_{ij}\\) are independent and normally distributed (normal errors).\n\\(\\sigma^2_{i}=\\sigma^2\\) for all \\(i\\) (homoscedasticity)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-07-anova/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-01-07-anova/index.html#blog-guide-map-link",
    "title": "ANOVA",
    "section": "5 Blog Guide Map Link",
    "text": "5 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-16_normality/index.html",
    "href": "docs/blog/posts/statistics/2023-01-16_normality/index.html",
    "title": "Normality Check",
    "section": "",
    "text": "Wiki\n논문\n원래는 표본의 크기가 50이하인 작은 데이터 셋의 정규성 검정을 위해 고안됨. R 에서는 3~5천개 사이의 표본까지 다룰 수 있도록 조정됨\n\n정규 분포 전용 검정: 모든 검정 대비 최고의 검정력을 보임 (Power), 이상치가 있으면 p value가 너무 작아짐\n\\(H_0\\): 데이터가 정규분포를 따른다\n\\(H_a\\): 데이터가 정규분포를 따르지 않는다.\n검정 통계량 \\[\n   \\mathbf W=\\frac{(\\sum_{i=1}^{n}a_ix_{(i)})^2}{\\sum_{i=1}^{n}(x_i-\\overline{x})^2}\n   \\]\n\n\\(a_i\\) : 미리 정해진 숫자들, \\(x\\)의 개수에 의해 정해짐\n\\(x_{(i)}\\) 들은 순위 표본, 즉 i 번째로 큰 표본\n분자는 순서 통계량으로 계산한 정규분포의 분산, 분모는 데이터의 표본 분산 (표본 Sum of Squares)\n이미 이론적으로 세팅된 값과 표본 분산의 비율을 보는 것\n귀무 가설이 참이면 이론적으로 1 이 나와야 함\n\\(\\mathbf W \\in (0,1)\\), 상관계수의 제곱을 측정한 계량 값이라고 생각해도 된다.\n\n\\(\\mathbf W\\) 값이 1에서 멀어질 수록 정규분포와는 다르게 분포되어 있음을 의미\n단점: 너무 민감함, 조그만 달라도 p value가 너무 작게 나와 귀무가설이 기각됨\n해결책 : 시각화 기법과 같이 사용해서 보여준다\n\nqqplot과 density 같이 사용"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-16_normality/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-01-16_normality/index.html#blog-guide-map-link",
    "title": "Normality Check",
    "section": "2 Blog Guide Map Link",
    "text": "2 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html",
    "title": "ANCOVA",
    "section": "",
    "text": "(Draft, 바쁘니까 일단 대충이라도 적어놓음 ㅠ)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#description",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#description",
    "title": "ANCOVA",
    "section": "1 Description",
    "text": "1 Description\nANCOVA (Analysis of Covariance, ANCOVA)\n\nANOVA에 공변량 (covariate)을 추가하여 분석 수행\n공변량을 조정하여 독립변수의 순수한 영향을 검정\n공변량: 연속형 변수로 한정"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#example",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#example",
    "title": "ANCOVA",
    "section": "2 Example",
    "text": "2 Example\n\n2.1 Load Libraries and Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(faraway)\nlibrary(markdown)\nlibrary(effects)\nlibrary(HH)\nlibrary(psych)\n\n\n\n\n2.2 Data Description\n\n\nCode\nstr(sexab)\n\n\n'data.frame':   76 obs. of  3 variables:\n $ cpa : num  2.048 0.839 -0.241 -1.115 2.015 ...\n $ ptsd: num  9.71 6.17 15.16 11.31 9.95 ...\n $ csa : Factor w/ 2 levels \"Abused\",\"NotAbused\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nR console에 ?sexab를 입력하면 다음과 같은 설명이 나온다.\nPost traumatic stress disorder in abused adult females\nThe data for this example come from a study of the effects of childhood sexual abuse on adult females. 45 women being treated at a clinic, who reported childhood sexual abuse, were measured for post traumatic stress disorder and childhood physical abuse both on standardized scales. 31 women also being treated at the same clinic, who did not report childhood sexual abuse were also measured. The full study was more complex than reported here and so readers interested in the subject matter should refer to the original article.\n즉, 요약하면 아동기에 성폭력을 겸험한 성인들의 정신 건강을 측정한 데이터로서, 아동기의 성폭력 경험과 학대 경험이 성인기의 정신건강에 유의한 영향을 미치는지에 대한 실험을 한 것이다.\n이 data는 3개의 변수와 76개의 samples을 포함한다.\n\ncpa : Childhood physical abuse on standard scale, covariate\nptsd : post-traumatic stress disorder on standard scale, response variable\ncsa : Childhood sexual abuse - abused or not abused, independent variable\n\n친절하게 response variable, independent variable 및 covariate을 규명해놓았다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#eda",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#eda",
    "title": "ANCOVA",
    "section": "3 EDA",
    "text": "3 EDA\n\n3.1 Descriptive Statistics\n\n\nCode\ntemp<-describeBy(ptsd~csa,data=sexab)\ntemp<-rbind('abused'=temp$Abused,'notAbused'=temp$NotAbused)%>%\nas.data.frame()\ntemp%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n\nabused\n1\n45\n11.941093\n3.440151\n11.31277\n11.883422\n3.857355\n5.98491\n18.99251\n13.00760\n0.1556159\n-0.9124483\n0.5128275\n\n\nnotAbused\n1\n31\n4.695874\n3.519743\n5.79447\n4.903441\n1.978841\n-3.34921\n10.91447\n14.26368\n-0.6589170\n-0.2008051\n0.6321645\n\n\n\n\n\n위의 요약된 기술 통계량들 중 표준 편차는 유사하지만 평균 ptsd가 약 7.245219의 차이를 보여준다. 아래의 histogram역시 성폭력을 경험한 그룹과 경험하지 않은 그룹간의 PTSD 수치가 다른것을 볼 수 있다.\n\n\nCode\nggplot(data=sexab,aes(x=ptsd,color=csa,fill=csa))+\ngeom_histogram(aes(y=..density..),position=\"identity\",fill='white')+\ngeom_density(alpha=0.5)+\nlabs(title=\"Histogram, PTSD Grouped by Childhood Sexual Abuse Experience\", x=\"PTSD\", y=\"Desnsity\")\n\n\n\n\n\n\n\n3.2 One-Way ANOVA\n성폭력 경험 유무에 따른 PTSD 평균 차이가 통계적으로 유의한지 확인하기 위해 ANOVA를 수행한다.\n\n\nCode\nsexab_aov<-aov(ptsd~csa, data=sexab)\nsummary(sexab_aov)\n\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncsa          1  963.5   963.5    79.9 2.17e-13 ***\nResiduals   74  892.4    12.1                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n집단간 변수 csa p value가 <0.05 인 것을 확인할 수 있다. csa는 5% 유의수준에서 유의하다.\n하지만 PTSD의 변동량은 아동 학대에 의해 설명될 수도 있기 때문에 ptsd의 평균은 csa뿐만 아니라 cpa에 또한 고려되어야한다.\n\n\nCode\nggplot(data=sexab,aes(x=cpa,y=ptsd))+geom_point()+geom_smooth(method=\"lm\")+\nlabs(title=\"Scatter Plot, PTSD vs CPA\", x=\"CPA\", y=\"PTSD\")\n\n\n\n\n\nCode\ncorrelation<-cor.test(sexab$cpa,sexab$ptsd, method='pearson')\n\n\n그림과 같이 CPA가 증가하면서 PTSD또한 선형적으로 증가하는 패턴을 관찰할 수 있다. 두 변수간의 상관계수 = 0.49이고 p value= 6.2715909^{-6}으로 보아 두 변수 사이에 선형적인 상관관계가 있는 것으로 보인다.\n\n\nCode\nggplot(data=sexab,aes(x=cpa,y=ptsd))+geom_point()+geom_smooth(method=\"lm\")+\nfacet_wrap(.~csa)+\nlabs(title=\"Scatter Plot, PTSD vs CPA Grouped By CSA\", x=\"CPA\", y=\"PTSD\")\n\n\n\n\n\n아동기 성폭력 경험 유/무에도 PTSD와 CPA와 선형적인 관계가 있는 것으로 보이기 때문에 CSA의 PTSD로의 효과를 검정하기 위해선 CPA를 조정할 필요가 있는것으로 보인다.\n\n\nCode\n# ptsd로의 순수한 성폭력 경험의 영향도를 얻기 위해서는 아동기 신체적 학대(공변량)에 대해서 고려를 해줘야함\n\nsexab_aov<-aov(ptsd~cpa+csa, data=sexab) \nsummary(sexab_aov)\n\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncpa          1  449.8   449.8   41.98 9.46e-09 ***\ncsa          1  624.0   624.0   58.25 6.91e-11 ***\nResiduals   73  782.1    10.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n아동기의 신체적 학대가 일정하다는 가정하에서 PTSD와 성폭력의 순수한 관계는 5% 유의수준에서 유의하고 공변량, CPA를 조정하기전과 그 유의성이 차이가 있음을 관찰할 수 있다.\n\n\nCode\n# CPA가 제거 된 후에 CSA의 순수한 효과를 알아보기\n\nancova(ptsd~cpa+csa, data=sexab) \n\n\nAnalysis of Variance Table\n\nResponse: ptsd\n          Df Sum Sq Mean Sq F value    Pr(>F)    \ncpa        1 449.80  449.80  41.984 9.462e-09 ***\ncsa        1 624.03  624.03  58.247 6.907e-11 ***\nResiduals 73 782.08   10.71                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n두 csa집단에서 두 회귀선의 기울기 같고 절편이 다르게 나타나는 것을 관찰 할 수있다. 기울기가 같은 이유는 cpa가 ptsd에 영향을 미치는 정도가 두집단에서 일정하도록 공변량으로서 통제 했기 때문이다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-01-27_ANCOVA/index.html#blog-guide-map-link",
    "title": "ANCOVA",
    "section": "4 Blog Guide Map Link",
    "text": "4 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html",
    "href": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html",
    "title": "Repeated Meausres ANOVA",
    "section": "",
    "text": "(Draft, 바쁘니까 일단 대충이라도 적어놓음 ㅠ)"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html#description",
    "href": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html#description",
    "title": "Repeated Meausres ANOVA",
    "section": "1 Description",
    "text": "1 Description\nRepeated Meausres ANOVA (반복 측정 분산 분석)\n\n동일한 대상에 대해 여러 번 반복측정하여 반복측정(3번 이상) 집단 간에 차이가 존재하는지 검정\n\n대응 표본 검정은 동일한 대상에 2번 반복 측정함 2개의 대응표본에 대한 검정\n\n이 때, 반복 측정 기간은 집단 내 요인이라 부르고 이를 반복측정 일원분산분석-집단내요인이라고 부름\n반복측정 기간외에 대상을 구분하는 집단 변수가 포함되면 반복측정 이원분산분석- 집단내요인 & 집단간 요인"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html#example",
    "href": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html#example",
    "title": "Repeated Meausres ANOVA",
    "section": "2 Example",
    "text": "2 Example\n\n\nCode\nstr(CO2)\n\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  84 obs. of  5 variables:\n $ Plant    : Ord.factor w/ 12 levels \"Qn1\"<\"Qn2\"<\"Qn3\"<..: 1 1 1 1 1 1 1 2 2 2 ...\n $ Type     : Factor w/ 2 levels \"Quebec\",\"Mississippi\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Treatment: Factor w/ 2 levels \"nonchilled\",\"chilled\": 1 1 1 1 1 1 1 1 1 1 ...\n $ conc     : num  95 175 250 350 500 675 1000 95 175 250 ...\n $ uptake   : num  16 30.4 34.8 37.2 35.3 39.2 39.7 13.6 27.3 37.1 ...\n - attr(*, \"formula\")=Class 'formula'  language uptake ~ conc | Plant\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n - attr(*, \"outer\")=Class 'formula'  language ~Treatment * Type\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n - attr(*, \"labels\")=List of 2\n  ..$ x: chr \"Ambient carbon dioxide concentration\"\n  ..$ y: chr \"CO2 uptake rate\"\n - attr(*, \"units\")=List of 2\n  ..$ x: chr \"(uL/L)\"\n  ..$ y: chr \"(umol/m^2 s)\"\n\n\nCode\nnames(CO2)\n\n\n[1] \"Plant\"     \"Type\"      \"Treatment\" \"conc\"      \"uptake\"   \n\n\nR console에 ?CO2를 입력하면 다음과 같은 설명이 나온다.\nThe CO2 data frame has 84 rows and 5 columns of data from an experiment on the cold tolerance of the grass species Echinochloa crus-galli.The \\(CO_2\\) uptake of six plants from Quebec and six plants from Mississippi was measured at several levels of ambient \\(CO_2\\) concentration. Half the plants of each type were chilled overnight before the experiment was conducted.\n즉, 식물이 저온의 환경에서 견디는 정도를 실험한 데이터로 퀘벡 지역의 6개의 나무와 미시시피 지역의 6개 나무의 이산화 탄소 흡수율을 7개의 서로 다른 이산화 탄소 농도 하에서 반복적으로 측정했다.\n\n2.1 Goals\n분석의 편의를 위해 저온 처리된 나무에 한정하여 분석\n\n두 지역간의 CO2흡수율의 차이를 검정\n7개의 서로 다른 이산화탄소 농도에 따라서 이산화 탄소의 흡수율 차이를 검정\n나무의 출신 지역과 이산화 탄소 흡수율 간의 관계가 이산화탄소 농도에 따라 달라지는 지도 검정\n\n\n\n2.2 Data Description\n\nPlant: plant id\nType: 나무의 출신 지역, 2개의 범주, 집단간 요인\nTreatment: 퀘벡 지역 나무와 미시시피 지역 나무dp 각 각 절반씩 실험 전에 저온 처리 했음, 저온 처리 여부가 treatment 변수에 저장됨\nconc: co2농도, 7개의 범주, 집단 내 요인\nuptake: 종속 변수, 이산화 탄소 흡수율\n\n\n\n2.3 Load Libraries and Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(faraway)\nlibrary(markdown)\nlibrary(effects)\nlibrary(HH)\nlibrary(psych)\n\n\n\n\n2.4 EDA\n\n\nCode\ndata=CO2%>%\nfilter(Treatment=='chilled')%>%\nmutate(conc=factor(conc),\ntype=factor(conc))\n\n\n\n\n2.5 Descriptive Statistics\n\n\nCode\ndata%>%\ngroup_by(Type)%>%\nsummarise(count=n(),\nmean_uptake=mean(uptake),\nsd_uptake=sd(uptake)\n)%>%knitr::kable()\n\n\n\n\n\nType\ncount\nmean_uptake\nsd_uptake\n\n\n\n\nQuebec\n21\n31.75238\n9.644823\n\n\nMississippi\n21\n15.81429\n4.058976\n\n\n\n\n\n두 지역의 분산과 평균 흡수율이 차이가 남\n\n\nCode\ndata%>%\ngroup_by(conc)%>%\nsummarise(count=n(),\nmean_uptake=mean(uptake),\nsd_uptake=sd(uptake)\n)%>%knitr::kable()\n\n\n\n\n\nconc\ncount\nmean_uptake\nsd_uptake\n\n\n\n\n95\n6\n11.23333\n2.860536\n\n\n175\n6\n19.45000\n5.886510\n\n\n250\n6\n25.28333\n10.569090\n\n\n350\n6\n26.20000\n10.831251\n\n\n500\n6\n26.65000\n11.445479\n\n\n675\n6\n27.88333\n10.958361\n\n\n1000\n6\n29.78333\n12.410547\n\n\n\n\n\n농도에 따라서 평균 흡수율과 분산이 약간 차이가 나는 것 같음. 다만, 고농도가 될수록 변화량이 줄어드는 것으로 보임\n\n\nCode\ndata%>%\ngroup_by(Type,conc)%>%\nsummarise(count=n(),\nmean_uptake=mean(uptake),\nsd_uptake=sd(uptake)\n)%>%knitr::kable()\n\n\n\n\n\nType\nconc\ncount\nmean_uptake\nsd_uptake\n\n\n\n\nQuebec\n95\n3\n12.86667\n3.121431\n\n\nQuebec\n175\n3\n24.13333\n3.150132\n\n\nQuebec\n250\n3\n34.46667\n3.927255\n\n\nQuebec\n350\n3\n35.80000\n2.615339\n\n\nQuebec\n500\n3\n36.66667\n3.611556\n\n\nQuebec\n675\n3\n37.50000\n2.100000\n\n\nQuebec\n1000\n3\n40.83333\n1.913984\n\n\nMississippi\n95\n3\n9.60000\n1.646208\n\n\nMississippi\n175\n3\n14.76667\n3.302020\n\n\nMississippi\n250\n3\n16.10000\n3.292416\n\n\nMississippi\n350\n3\n16.60000\n3.157531\n\n\nMississippi\n500\n3\n16.63333\n3.667879\n\n\nMississippi\n675\n3\n18.26667\n4.285246\n\n\nMississippi\n1000\n3\n18.73333\n3.883727\n\n\n\n\n\n농도가 증가함에 따라 지역간 평균 흡수율의 차이가 커지는 것을 관찰할 수 있음\n\n\nCode\nboxplot(uptake~Type*conc,data=data,col=c(\"darkblue\",\"darkred\"),\n        las=2,cex.axis=0.7,xlab=\"\",ylab=\"Carbon Dioxide Uptake Rate\",\n        main=\"Effects of Plant Ype and CO2 on Carbon Dioxide Uptake\")\nlegend(\"topleft\",inset=0.025, legend=c(\"Quebec\",\"Mississippi\"),\n       fill=c(\"darkblue\",\"darkred\"))\n\n\n\n\n\n\n\n2.6 One-Wway ANOVA\n반복 측정 일원분산분석: y~W+Error(subject) where W= a within grouping variable, subject= a sample identifier\n\n\nCode\naov(uptake~Type+Error(Plant),data=data)%>%summary()\n\n\n\nError: Plant\n          Df Sum Sq Mean Sq F value  Pr(>F)   \nType       1 2667.2  2667.2   60.41 0.00148 **\nResiduals  4  176.6    44.1                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals 36   2013   55.93               \n\n\nCode\naov(uptake~conc+Error(Plant),data=data)%>%summary()\n\n\n\nError: Plant\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals  5   2844   568.8               \n\nError: Within\n          Df Sum Sq Mean Sq F value   Pr(>F)    \nconc       6   1472  245.40   13.61 2.09e-07 ***\nResiduals 30    541   18.03                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n지역간 농도간 평균 흡수율이 차이가 5% 유의수준에서 유의하다.\n\n\n2.7 Two-Wway ANOVA\n반복측정 이원분산 분석: y~B*W+Error(subject/W) where W= within grouping variable, B =Between Group Variable, and subject= a sample identifier\n\n\nCode\naov(uptake~Type*conc+Error(Plant/conc),data=data)%>%summary()\n\n\n\nError: Plant\n          Df Sum Sq Mean Sq F value  Pr(>F)   \nType       1 2667.2  2667.2   60.41 0.00148 **\nResiduals  4  176.6    44.1                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Plant:conc\n          Df Sum Sq Mean Sq F value   Pr(>F)    \nconc       6 1472.4  245.40   52.52 1.26e-12 ***\nType:conc  6  428.8   71.47   15.30 3.75e-07 ***\nResiduals 24  112.1    4.67                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n지역간 농도간 집단간에서 흡수율 차이가 유의한것으로 나타나고 둘의 상호작용 또한 5% 유의수준에서 유의하다."
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-01-27_rmANOVA/index.html#blog-guide-map-link",
    "title": "Repeated Meausres ANOVA",
    "section": "3 Blog Guide Map Link",
    "text": "3 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html",
    "title": "MANOVA",
    "section": "",
    "text": "다변량 분산분석(Multivariate Analysis of Variance, MANOVA)\n\n2개 이상의 종속변수가 있을 경우 집단별 차이를 동시에 검정\n연구의 타당성 증가"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#example",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#example",
    "title": "MANOVA",
    "section": "2 Example",
    "text": "2 Example\n\n2.1 Load Libraries and Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(faraway)\nlibrary(markdown)\nlibrary(heplots)\nlibrary(HH)\nlibrary(psych)\n\n\n\n\n2.2 Data Description\n\n\nCode\nstr(Skulls)\n\n\n'data.frame':   150 obs. of  5 variables:\n $ epoch: Ord.factor w/ 5 levels \"c4000BC\"<\"c3300BC\"<..: 1 1 1 1 1 1 1 1 1 1 ...\n $ mb   : num  131 125 131 119 136 138 139 125 131 134 ...\n $ bh   : num  138 131 132 132 143 137 130 136 134 134 ...\n $ bl   : num  89 92 99 96 100 89 108 93 102 99 ...\n $ nh   : num  49 48 50 44 54 56 48 48 51 51 ...\n\n\nR console에 ?Skulls를 입력하면 다음과 같은 설명이 나온다.\nMeasurements made on Egyptian skulls from five epochs.\n\nThe epochs correspond to the following periods of Egyptian history:\n\nthe early predynstic period (circa 4000 BC);\nthe late predynatic period (circa 3300 BC);\nthe 12th and 13t dynasties (circa 1850 BC);\nthe Ptolemiac peiod (circa 200 BC);\nthe Roman period(circa 150 AD).\n\n\nThe question is hether the measurements change over time. Non-constant measurements of the skulls over time would indicate interbreeding with immigrant populations. Note that using polynomial contrasts for epoch essentially treats the time points as equally spaced\n즉, skulls 고대 이집트 왕조 부터 로마시대까지 이집트 지역에서 발군된 두개골의 크기를 측정한 데이터 이집트 역사를 5개의 시대로 구분하고 각 시대별로 30개씩의 두개골을 4개의 지표로 측정\n이 data는 5개의 변수와 150개의 samples을 포함한다.\n\nepoch :\nmb :\nbh :\nbl :\nnh :"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#eda",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#eda",
    "title": "MANOVA",
    "section": "3 EDA",
    "text": "3 EDA\n\n3.1 Descriptive Statistics\n\n\nCode\nlibrary(heplots)\n#skulls 고대 이집트 왕조 부터 로마시대까지 이집트 지역에서 발군된 두개골의 크기를 측정한 데이터\n# 이집트 역사를 5개의 시대로 구분하고 각 시대별로 30개씩의 두개골을 4개의 지표로 측정\n# epoch: 이집트의 시대를 5개로 구분, 독립변수\n# mb : 두개골의 폭, 종속 변수\n# bh : 두개골의 높이, 종속 변수\n# bl : 두개골의 길이, 종속 변수\n# nh : 코의 높이, 종속 변수\n\nlibrary(dplyr)\nsample_n(Skulls,10)\n\n\n      epoch  mb  bh  bl nh\n134  cAD150 137 134  93 53\n24  c4000BC 128 134 103 50\n19  c4000BC 139 136  96 50\n53  c3300BC 135 136  97 52\n115  c200BC 133 128  92 51\n59  c3300BC 135 132  98 54\n129  cAD150 132 132  99 55\n119  c200BC 132 136  92 52\n118  c200BC 136 138  94 55\n5   c4000BC 136 143 100 54\n\n\nCode\nattach(Skulls)# Skulls를 작업 경로에 포함시키기\nsearch() # 작업 경로 확인인\n\n\n [1] \".GlobalEnv\"           \"Skulls\"               \"package:psych\"       \n [4] \"package:HH\"           \"package:gridExtra\"    \"package:multcomp\"    \n [7] \"package:TH.data\"      \"package:MASS\"         \"package:survival\"    \n[10] \"package:mvtnorm\"      \"package:latticeExtra\" \"package:grid\"        \n[13] \"package:lattice\"      \"package:heplots\"      \"package:broom\"       \n[16] \"package:car\"          \"package:carData\"      \"package:markdown\"    \n[19] \"package:faraway\"      \"package:forcats\"      \"package:stringr\"     \n[22] \"package:dplyr\"        \"package:purrr\"        \"package:readr\"       \n[25] \"package:tidyr\"        \"package:tibble\"       \"package:ggplot2\"     \n[28] \"package:tidyverse\"    \"tools:quarto\"         \"tools:quarto\"        \n[31] \"package:stats\"        \"package:graphics\"     \"package:grDevices\"   \n[34] \"package:utils\"        \"package:datasets\"     \"package:methods\"     \n[37] \"Autoloads\"            \"package:base\"        \n\n\nCode\n# 종속 변수를 결합시켜 하나의 행렬로 만들기\ny<-cbind(mb,bh,bl,nh)\n# 시대별 두개골  길이의 평균 보기\naggregate(y,by=list(epoch),mean) # 언뜻 보기에 차이가 있는 것 처럼 보임\n\n\n  Group.1       mb       bh       bl       nh\n1 c4000BC 131.3667 133.6000 99.16667 50.53333\n2 c3300BC 132.3667 132.7000 99.06667 50.23333\n3 c1850BC 134.4667 133.8000 96.03333 50.56667\n4  c200BC 135.5000 132.3000 94.53333 51.96667\n5  cAD150 136.1667 130.3333 93.50000 51.36667\n\n\nCode\n# 모집단으로 일반화하기 위해 통계적 검정 시행\nskulls_manova<-manova(y~epoch)\nsummary(skulls_manova)\n\n\n           Df  Pillai approx F num Df den Df    Pr(>F)    \nepoch       4 0.35331    3.512     16    580 4.675e-06 ***\nResiduals 145                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# 시대별 두개골 측정값이 차이가 있는 것으로 보임\n\n# 구체적으로 어느 두개 골 측정값에서 차이가 나는지 확인\nsummary.aov(skulls_manova)\n\n\n Response mb :\n             Df  Sum Sq Mean Sq F value    Pr(>F)    \nepoch         4  502.83 125.707  5.9546 0.0001826 ***\nResiduals   145 3061.07  21.111                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response bh :\n             Df Sum Sq Mean Sq F value  Pr(>F)  \nepoch         4  229.9  57.477  2.4474 0.04897 *\nResiduals   145 3405.3  23.485                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response bl :\n             Df Sum Sq Mean Sq F value    Pr(>F)    \nepoch         4  803.3 200.823  8.3057 4.636e-06 ***\nResiduals   145 3506.0  24.179                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response nh :\n             Df Sum Sq Mean Sq F value Pr(>F)\nepoch         4   61.2  15.300   1.507 0.2032\nResiduals   145 1472.1  10.153               \n\n\nCode\n# nh는 차이가 없는 것으로 보임\n\n## 시간에 따라 두개골 측정이 다르다는 것은 이민족 유입의 혼혈 가능성이 있음\n\ndetach(Skulls)# 작업경로에서 삭제제\n\n\n\n\n3.2 One-Way ANOVA"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#blog-guide-map-link",
    "href": "docs/blog/posts/statistics/2023-01-28_MANOVA/index.html#blog-guide-map-link",
    "title": "MANOVA",
    "section": "4 Blog Guide Map Link",
    "text": "4 Blog Guide Map Link\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html",
    "href": "docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html",
    "title": "Bayes’ Rule",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\nBayes’ rule (베이즈 정리)는 prior probability(사전 확률)과 posterior probability(사후 확률)의 관계를 조건부 확률을 이용하여 확립한 것이다.\n\nprior probability(사전 확률)는 데이터를 얻기 전 연구자의 가설이 들어간 일종의 사건 발생의 신뢰도로 해석하기도 하고 prior probability density function (사전 확률 밀도 함수) 라고도 표현된다.\nposterior probability(사후 확률)는 데이터가 주어진 후 연구자의 가설이 들어간 사건 발생의 신뢰도로 해석하기도 하고 posterior probability desnsity function(사후 확률 밀도 함수) 라고도 표현된다.\n\n좀 더 구체적으로, 2개의 사건 A와 B로 한정시켜 생각해봤을 때, 조건부 확률 \\(P(A|B)\\) 는 각 사건의 확률 \\(P(A), P(B), P(B|A)\\) 를 사용하여 게산될 수 있다. 그래서 베이즈 정리는 \\(P(B|A)\\), \\(P(B|\\overline{A})\\), \\(P(A)\\) 의 정보를 알고있거나 계산 가능할 때 아래와 같은 \\(P(A|B)\\) 의 확률을 구할 수 있는 공식을 제공한다(Equation 1).\n\n\nBayes’ rule을 좀 더 직관적으로 이해하기 위해선 Bayes’ rule와 연관된 친숙한 개념들을 상기시킬 필요가 있다. 우리에게 친숙한 개념인 연역법과 귀납법에 대해서 간단이 살펴본다.\n\n\n\n\n연역법 (deduction method or deductive reasoning)는 하나 (=대전제) 또는 둘 이상의 명제(=대전제+소전제들)를 전제로 하여 명확한 논리에 근거해 새로운 명제(결론)를 도출하는 방법이다. 보통 일반적인 명제에서 구체적인 명제로 도출해내는 방식으로 연역법을 설명하기도 한다. 연역법은 전제와 결론의 타당성보다는 결론을 이끌어내는 논리 전개에 엄격함을 요구한다. 그래서 명쾌한 논리가 보장된다면 연역적 추론의 결론은 그 전제들이 결정적 근거가 되어 전제와 결론이 필연성을 갖게 된다. 따라서, 전제가 진리(=참)이면 결론도 항상 진리(=참)이고 전제가 거짓이면 결론도 거짓으로 도출된다. 하지만, 모든 연역적 추론에서 출발되는 최초의 명제가 결코 연역에 의해 도출될 수 없다는 약점을 갖고있다. 즉, 반드시 검증된 명제를 대전제로 하여 연역적 추리를 시작해야한다. Source: naver encyclopedia -deductive method (cf. 귀류법)\n예를 들어, 아리스토텔레스의 삼단논법의 논리 형식이 가장 많이 인용이 된다. 대전제와 소전제가 하나씩있는 둘 이상의 명제로부터 결론이 도출되는 예를 살펴보자.\n\n대전제: 모든 사람은 죽는다.\n소전제: 소크라테스는 사람이다.\n결론: 그러므로 소크라테스도 죽는다.\n\n\n\n\n귀납법 (Induction method or Inductive reasoning)은 전제와 결론을 뒷받침하는 논리에 의해 그 타당성이 평가된다. 귀납적 추론은 관찰과 실험에서 얻은 특수한 사례 (= data)를 근거로 전체에 적용시키는 귀납적 비약을 통해 이루어진다. 이와 같이 귀납에서 얻어진 결론은 일정한 개연성을 지닐 뿐이며, 특정 data에 따라 귀납적 추론의 타당성에 영향을 미친다. 그러므로, 검증된 data가 많을 수록 신뢰도와 타당성이 증가한다는 특징이 있다.하지만, 귀납적 추론의 결론이 진리인 것은 아니다. Source: naver encyclopedia - inductive method\n\n특수한 사례 (or data): 소크라테스는 죽었다, 플라톤도 죽었다, 아리스토텔레스도 죽었다.\n소전제: 소크라테스는 사람이다.\n결론: 그러므로 소크라테스도 죽는다.\n\n위와 같이 연역적 추론과 귀납적 추론은 서로 반대되는 개념으로 각 각 강점과 약점이 있으며 현실에서는 서로 상호 보완적으로 쓰이고 있다. 따라서, 전제로 삼는 대전제 역시 검증 과정이 필요하고 그 가설에서 몇 개의 명제를 연역해 실험과 관찰 등을 수행하는 가설연역법(hypothetical deductive method)이 널리 쓰이고 있다.\n\n\n\n\n통계학에선 모수(parameter)를 추정하는 여러 방법론들이 있는데 이번 블로그에서는 Frequentism와 Bayeseanism, 이 2가지 방법론에 초점을 둔다.\n\n\n통계학에서 가장 널리쓰이고 있는 방법론으로, 연역법에 근거한 결론 도출 방식을 이용한다. 간단히 말하면, 이미 알려진 분포에서 연구자의 관측치가 발생할 확률을 관찰하여 결론을 유도 하는 방법이다. p-value에 의한 결론 도출방식이 그 대표적인 예이다. 연구자의 데이터가 여러 수학자와 통계학자들이 증명해 놓은 분포하에서 발생한 사실이 입증이 됐을 때 연구자의 관측치가 그 named distribution(like normal distribution)에서 발생할 확률이 낮을 수록 p-value가 작아지고 일정 유의수준에 따라 연구자는 귀무가설을 기각하는 논리방식을 따른다.\n\n\n\n통계학에서 역시 많이 쓰이는 방법론으로, 귀납법에 근거한 결론 도출 방식을 이용한다. 간단히 말하면, 확률을 확률변수가 갖는 sample space에 대한 특정 사건이 발생한 사건의 비로 보는 것 (equally likely라고 가정)이 아니라 내가 설정한 가설에 대한 신뢰도로 바라보는 것이다. 따라서, 사전에 이미 알고있는 데이터가 있어 사전 확률 (prior probability)을 알고있고 이 사전 확률이 추가적인 data에 의해 조정되는 사후 확률 (posterior probability)이 계산된다. 이때 사전 확률자체보다는 추가적인 data와 사후 확률을 계산하는데 사용되는 likelihood의 타당성이 더 중요하다. 더 구체적인 내용은 Bayesean statistics에 기본이 되는 Bayes’ rule에서 살펴보기로 한다.\nsource: Frequentism vs Bayeseanism\n\n\n\n\n\n\nTheorem 1 \\[\n\\begin{aligned}\nP(A|B)&=\\frac{P(B|A)P(A)}{P(B)}\\\\\n      &=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\n\\end{aligned}\n\\tag{1}\\]\n\n\n\\(P(A|B)\\): posterior probability, B(data)가 주어졌을때 가설 A에 대한 신뢰도\n\\(P(A)\\): prior probability, 가설 A에대한 신뢰도\n\\(P(B|A)\\): likelihood, 가설 A가 주어졌을때 B(Data)가 발생할 신뢰도\n\\(P(B)\\): marginal probability, Data의 신뢰도\n\nEquation 1 의 두 분째 등식을 이해하기 위해선, Law of Total Probability (전 확률 법칙) 또는 Total Probability Rule (전 확률 정리)을 이해해야한다.\n\n\n\n\nTheorem 2 Let \\(A_1, A_2, ..., A_k\\) be a set of mutually exclusive and exhaustive events. Let \\(A\\) be a event and a partition of sample space \\(\\Omega\\), then\n\\[\nP(B)=\\sum_{i=1}^{n}P(B|A_i)P(A_i)\n\\]\n\n\n\n\n\n\nFigure 1: Law of Total Probability Example - Two Events\n\n\nSource: Law of ToTal Probability with Proof\n\n\n\n\nFigure 2: Law of Total Probability Example - Multiple Events\n\n\nSource: MIT RES.6-012 Introduction to Probability, Spring 2018 - Youtube\n\n\n\\[\n\\begin{aligned}\nP(B)&=P(B\\cap A) + P(B\\cap \\overline A)\\\\\n    &=P(B\\cap A) + P(B\\cap \\overline A)\\\\\n    &=P(B|A)P(A)+P(B|\\overline A)P(\\overline A)\\\\\nP(A\\cap B)&=P(B|A)P(A)=P(A|B)P(B)\\\\\n\\therefore\nP(A|B)&=\\frac{P(A \\cap B)}{P(B)}\\\\\n      &=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\n\\end{aligned}\n\\tag{2}\\]\nLaw of total probability 를 이용하여 Bayes’ rule이 Equation 2 와 같이 변형되었다. 최종식을 보면 좀 더 직관적인 해석이 가능해지는데 P(B) 가 A와의 교집합 확률의 총합이 되면서 분자가 그 일부가 되는 비율의 개념으로 해석될 수 있다. Figure 1 를 보면 \\(P(A|B)=\\frac{P(B \\cap A)}{P(B)}=\\frac{P(B \\cap A)}{P(B \\cap A)+P(B \\cap \\overline A)}\\) 로 표현되는 것을 볼 수 있다. 그 것을 조금 더 일반화 한 경우는 Figure 2 를 참고하여 유추할 수 있다.\n\n\n\n\\[\n\\begin{aligned}\nP(A|B)&=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}\\\\\nP(\\theta|x)&=\\frac{P(x|\\theta)P(\\theta)}{P(x|\\theta)P(\\theta)+P(x|\\overline \\theta)P(\\overline \\theta)}\n\\end{aligned}\n\\]\n많은 참고 문헌에서 사건 A, B를 모수, \\(\\theta\\) 와 data, \\(x\\) 로 표현하기도 한다. 즉, data \\(x\\) 가 주어졌을 때 모수 \\(\\theta\\) 가 발생할 확률이 data에 의해서 update된다.\n\n\\(P(\\theta)\\)\n\nprior probability density function\n데이터없이 초기에 임시로 부여된 모델 또는 모수의 확률\n\n\\(P(x|\\theta)\\)\n\nlikelihood\n초기에 임시로 부여된 모델 또는 모수가 주어졌을 때 data x가 발생할 우도\n좀 더 파격적으로 해석하면, 초기에 임시로 부여된 모델 또는 모수가 data x에 들어맞을(or fittng) 확률\n\n\\(P(x)\\)\n\nmarginal proability\n데이터가 발생할 확률로 \\(\\theta\\) 와 상관없기 때문에 상수로 취급한다.\n\n\\(P(\\theta|x)\\)\n\nposterior probability density function\ndata가 주어졌을 때 모델 또는 모수의 확률\nBayes’ Rule에 의한 최적화에서 다음 최적화 iteration에서 Prior로 쓰인다.\n\n\n\\(P(x)\\) 는 상수이기 때문에 생략가능 하여 아래의 식과 같이 정리 할 수 있다. \\[\nP(\\theta|x)\\propto P(x|\\theta)P(\\theta)\n\\]\n\\(P(\\theta|x)\\) 는 \\(P(x|\\theta)P(\\theta)\\) 에만 영향을 받는 것을 볼 수 있다.\n\n\n\n펭수는 평소 관심이 있던 코니에게서 초콜릿을 선물받았다. 펭수는 초콜릿을 준 코니가 나를 좋아하는지가 궁금하기 때문에 이것을 통계적으로 계산해본다.\n펭수는 먼저 다음 두 상황을 가정한다.\n\n\\(P(like)=0.5\\)\n\n코니가 펭수를 좋아한다는 가설의 신뢰도는 반 반이다. 즉, 정보없는 상태에서의 펭수의 prior probability.\n0.5로 설정한 이유는 다음의 원리를 따랐다. The Principle of Insufficient Reason(이유불충분의 원리- 하나의 사건을 기대할만한 어떤 이유가 없는 경우에는 가능한 모든 사건에 동일한 확률을 할당해야 한다는 원칙).\n\n\\(P(choco)\\)\n\n초콜릿을 받았다라는 data가 발생할 신뢰도\n\n\n펭수는 코니에게 자신을 좋아하는지 알 길이 없으니 사람이 호감이 있을 때에 대한 초콜릿 선물 데이터를 조사하기 시작한다. 즉, 호감의 근거는 초콜릿으로 한정했고 초콜릿 선물 방식의 불확실성을 호감으로 설명하는 문헌을 찾기 시작했다. 그리고 펭수는 도서관에 있는 일반인 100명을 대상으로 초콜릿과 호감과의 관계를 연구한 초콜릿과 호감 논문을 통해 두 가지 정보를 알게된다.\n\n일반적으로, 어떤 사람이 상대방에게 호감이 있어서 초콜릿을 줄 확률은 \\(40%\\) 이다. 즉, \\(P(choco|like)=0.4\\)\n일반적으로, 어떤 사람이 상대방에게 호감이 없지만 예의상 초콜릿을 줄 확률은 \\(30%\\) 이다. 즉, \\(P(choco|\\overline{like})=0.3\\)\n위의 2가지 정보로 유추 가능한 정보\n\n\\(P(\\overline{choco}|like)=0.6\\)\n\\(P(\\overline{choco}|\\overline{like})=0.7\\)\n\n초콜릿에 관한 조사를 토대로 얻은 4가지 정보로 유추할 수 있는 정보\n\n\\(P(choco|like)=0.4\\): like를 받고 있는 50명 중 \\(40%\\) 인 20명은 초콜릿을 받는다.\n\\(P(\\overline{choco}|like)=0.6\\): like를 받고 있는 50명 중 \\(60%\\) 인 30명은 초콜릿을 받지 못한다.\n\\(P(choco|\\overline{like})=0.3\\): like를 받지 않는 50명 중 \\(30%\\) 인 15명은 예의상 준 초콜릿을 받는다.\n\\(P(\\overline{choco}|\\overline{like})=0.7\\): like를 받지 않는 50명 중 \\(70%\\) 인 35명은 초콜릿을 받지 못한다.\n\n\n펭수의 관점으로 정보를 재분류\n\n펭수가 궁금한 정보\n\n\\(P(like|choco)=?\\), posterior probability\n\n펭수가 가정한 정보\n\n\\(P(like)=0.5\\), prior probability by The Principle of Insufficient Reason\n\n펭수가 조사한 정보\n\n\\(P(choco|like)=0.4\\), likelihood\n\\(P(choco)\\): marginal probability\n\n\\(P(choco)=P(choco|like)+P(choco|\\overline{like})=\\frac{20+15}{100}=0.35\\)\n\n\n\n위의 정리한 정보를 Bayes’ rule에 대입하면,\n\\[\nP(like|choco)=\\frac{P(choco|like)\\times P(like)}{P(choco)}=\\frac{0.4\\times 0.5}{0.35}=0.57\n\\]\n펭수의 prior probability(\\(P(A)=0.5\\))가 posterior probability(\\(P(A|B)=0.57\\))로 업데이트 될 수 있다. 초콜릿과 호감 논문을 읽고 코니가 자신을 좋아할 확률이 높아진 것에 대해 기대감을 얻은 용기가 없는 펭수는 100명 보다 더 많은 독립적인 사람들로 실험한 논문을 찾아 다시 자신의 업데이트 된 사전 확률을 계속해서 업데이트할 생각이다. 그리고 자신의 사전 확률을 추가적인 데이터를 갖고 사후 확률로 계속해서 업데이트시켜 정확한 확률을 구한다.\n위의 예시는 영상 자료: 초콜릿을 준 코니의 마음을 시청하고 영감을 얻은 슬기로운 통계생활 tistory에 있는 Source: 베이즈 정리(Bayes’ rule) 완벽히 정리하기 슬기로운 통계생활 블로그를 요약 및 약간의 각색을 한 것이다.\n\n\n\n\n\nTheorem 3 Let \\(A_1, A_2, ..., A_k\\) be a set of mutually exclusive and exhaustive events. Let \\(A\\) be a event, then\n\\[\nP(A_i|B)=\\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^{k}P(B|A_i)P(A_i)}\n\\]\n\n\n\n\nMaximum a posterior estimation는 statistical estimation methods의 큰 기둥 중 하나인 maximum likelihood estimation과 더불어 parameter \\(\\theta\\) 를 추정하는데 많이 사용되는 방법론이다. 사후 확률 밀도 함수 \\(f(x|\\theta)\\) 또는 \\(P(x|\\theta)\\) 를 최대화하는 \\(\\theta\\) 의 추정치를 구하는 방법이며 아래와 같은 argument로 표현할 수 있다.\n\\[\n\\begin{aligned}\n\\hat{\\theta}&=\\arg \\max_{\\theta}\\frac{f(x|\\theta)f(\\theta)}{\\int f(x|\\theta)f(\\theta)}\\\\\n            &\\propto\\arg \\max_{\\theta}f(x|\\theta)f(\\theta)\n\\end{aligned}\n\\]\n최대 우도 추정량과 달리 최대 사후 추정량에는 최대화하는 식에 사전 확률이 추가되어 있는 것을 볼 수 있다. 그러므로 분자 부분인 \\(f(x|\\theta)f(\\theta)\\) 만을 최대화 한다. 분모 부분인 \\(\\int f(x|\\theta)f(\\theta)\\) nomarlizing penalty 또는 constant로 간주한다. 여기서 \\(P(\\theta)\\) 초기 가정치 인데 아무렇게나 설정하기 보다는 good estimate로 설정해야 통계학자들로부터의 공격을 최소화시킬 수 있다. MAP는 나이브 베이즈의 알고리즘의 핵심이다.\n[참고] 최대 우도 추정량 \\[\n\\begin{aligned}\n\\hat{\\theta}=\\arg \\max_{\\theta}L(x|\\theta)=\\arg \\max_{\\theta}\\Pi_{i=1}^{n}f(x|\\theta)\n\\end{aligned}\n\\]\n\n\n\nNaive Bayes에 대한 구체적인 글은 다른 블로그에 소개한다. Naive Bayes는 Bayes’ Rule을 이용해 \\(\\theta\\) 를 최적화 시킨다. Naive Bayes의 Naive는 features 또는 explanotry variables이 서로 conditionally indepdent라고 가정한 것에서 이름 붙여졌다.\n\n\n\n\n\nBayes’ rule provides a formula how to calculate \\(P(A|B)\\) if \\(P(B|A)\\), \\(P(B|\\overline{A})\\), \\(P(A)\\) are available\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html",
    "href": "docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html",
    "title": "Conditional Probability",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n조건부 확률은 조건이 주어졌을 때 한 사건이 발생할 확률이며 하나의 사건이 다른 사건에 영향을 미쳐 그 확률값이 달라지는 것을 의미한다. 즉, 한 사건, B가 조건으로 주어지고 다른 사건A가 발생할 확률 \\(P(A|B)\\) 가 사건 A가 발생할 확률 \\(P(A)\\) 와 다르다는 것을 의미한다 (\\(P(A)\\not=P(A|B)\\)).\n예를 들어, 주사위를 던질 때 특정 주사위의 눈 (1~6)이 나올 확률은 \\(\\frac{1}{6}\\) 으로 같다 (eqaully likely)라고 가정할 때 주사위의 눈이 나올 수 있는 모든 집합 표본 공간 \\(S\\) 에 대한 특정 주사위의 눈이 나오는 사건 \\(A\\) 가 발생할 확률은 \\(\\frac{n(A)}{n(S)}\\) 와 같다. 다시 말해서, 조건부 확률은 2개 이상의 사건에 대해서 하나의 사건이 다른 사건이 발생할 확률에 영향을 미치는 개념을 말한다. 가장 간단한 2개의 사건 \\(A, B\\) 에 대해서 살펴볼 때 조건부 확률은 다음과 (Equation 1)과 같다.\n\nDefinition 1 If \\(A\\) and \\(B\\) are events in sample space \\(S\\), and \\(P(B)>0\\), then the conditional probability of \\(A\\) given \\(B\\), written \\(P(A|B)\\), is \\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n\\tag{1}\\]\nwhere \\(0 < P(B) \\le 1\\)\n\n위의 정의에서 볼 수 있듯이, sample space \\(S\\) 가 B로 update 되어 P(B|B)=1이 되고 사건 A의 outcome이 B에 관해서 조정된다.\n예를 들어, 사건 \\(A\\) 는 주사위의 눈이 1이 나오는 사건, 사건 \\(B\\) 는 주사위의 눈이 3 이하가 나오는 사건이라고 했을 때 사건 \\(A\\) 가 사건 \\(B\\) 의 부분 집합이므로 두 사건이 서로 독립이 아니다. 즉, $ AB $ 사건에서 주사위의 눈이 1 나오는 경우 밖에 없다. 이렇게 사건 \\(B\\) 가 주어졌을 때 혹은 \\(B\\) 가 먼저 일어났을 때 1이 나올 확률은 달라지게 된다. 즉, \\(A\\) 의 sample space = \\(\\{1,2,3,4,5,6\\}\\) 이고 \\(A|B\\) 의 sample space = \\(\\{1,2,3\\}\\) 이 되기 때문에 \\(P(A) \\not= P(A|B)\\) 가 된다.\n좀 더 구체적으로 계산을 하게 되면, \\(P(A)=\\frac{1}{6}, P(B)=\\frac{3}{6}, P(A\\cap B)=\\frac{1}{6}\\) 일 때,\n\\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}=\\frac{\\frac{1}{6}}{\\frac{3}{6}}=\\frac{1}{3}\n\\]\n인 것을 알 수 있다.\n\nDefinition 2 Independence \\[\n\\begin{aligned}\nP(A)&=P(A|B)\\\\\nP(B)&=P(B|A)\\\\\n\\end{aligned}\n\\tag{2}\\]\nA and B are independent.\n\n사건 A, B가 독립일 때 두 두사건이 동시에 발생할 확률은 \\(P(A \\cap B)=P(A)P(B)=P(B)P(A)\\) 이다. 반면에, 두 사건이 독립이 아니라면 Equation 1 을 이용하여 \\(P(A \\cap B)\\) 는 \\(P(B|A)P(A)\\) 또는 \\(P(A|B)P(B)\\) 로 표현될 수 있다.독립일 때 동시에 발생할 확률에서 \\(P(A)\\) 가 B를 조건으로 봤을 때 동시에 발생할 확률 \\(P(A|B)\\) 로 바뀐 것을 볼 수 있다.\n\nTheorem 1 Multiplicative Rule A,B dependent \\[\n\\begin{aligned}\nP(A\\cap B)&=P(A)P(B|A)\n\\end{aligned}\n\\]\nA,B independent \\[\n\\begin{aligned}\nP(A\\cap B)&=P(A)P(B)\n\\end{aligned}\n\\]\n\n\nTheorem 2 Generalized multiplicative rule $$\n$$\n\n\nTheorem 3 Total Probability Rule $$\n$$\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-05_probability/index.html",
    "href": "docs/blog/posts/statistics/2023-02-05_probability/index.html",
    "title": "Probability",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nexperiment(실험): 연구 수행 방식.\ntrial (시행): 연구 실험 시행.\nsample space(표본 공간, (\\(\\text{S or } \\Omega\\)): 실험의 가능한 모든 결과의 모음 또는 근원 사상의 집합.\nelement (근원 사상, \\(\\omega\\)): 표본 공간의 원소.\nevent (사건, \\(E\\)): 근원 사상의 집합 또는 표본 공간의 부분 집합.\n\n\n\n\n\n\\(\\omega \\in A\\) : \\(\\omega\\) is an element of a set A\n\\(\\omega \\not\\in A\\) : \\(\\omega\\) is not an element of a set A\n\\(B \\subset A\\) : B is a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\in A\\)\n\\(A = B\\) : \\(B \\subset A\\), \\(A \\subset B\\)\n\\(B \\not\\subset A\\) : B is not a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\not\\in A\\)\n\n\n\n\n확률은 우연 (또는 가능성)과 불확실성에 대한 연구이다. 그 이론은 집합 이론을 기반으로 한다. 고전 확률은 가능성 조합 게임, 이론 오류와 같은 도박과 같다. 확률은 통계학, 경제학, 연산연구, 심리학, 생물학, 역학, 의학 등에 사용된다. 확률을 이해하기 위해 미적분학과 집합론의 지식이 요구되며 관련 학문은 ​​해석학, 측도론, 확률과정 등이 있다. 확률을 정의하려면 사건 수집에 대한 규칙성이 필요하다.\n\n\n\n\n확률은 사건이 발생할 가능성을 나타낸다.\n고전적 정의\n\n\nDefinition 1 The probability of event A is the sum of the probabilities assigned to all sample points in event A. Therefore, \\(0 \\le P(A) \\le 1, P(\\emptyset)=0, P(\\Omega)=1\\). In addition, if \\(A_1, A_2, A_3, ...\\) are mutually exclusive,\n\\[\nP(A_1 \\cup A_2 \\cup A_3 \\dots)=P(A_1) + P(A_2) + P(A_3) + \\dots =\\sum_{i=1}^{\\infty}P(A_i)\n\\]\n\n위의 정의에서 the probabilities assigned to all sample points in event A 의 표현은 사건 A안에 있는 element의 가중치로 해석할 수 있다. 즉, 쉽게 말하면, 확률은 표본 공간, sample space \\(\\Omega\\) 의 원소 (element) \\(\\omega\\) 에 할당된 가중치를 더한 것이다.\n예를 들어, 주사위 모양을 어떤식으로든 조작해 홀수가 짝수보다 2배 더 많이 발생하게끔 만들어 1 번 던질 때 3 보다 작은 수가 나올 사건을 A라고 하면 \\(\\Omega=\\{1,2,3,4,5,6\\}\\) 이고 각 홀수 원소에 가중치가 2배씩 붙기 때문에 홀수 눈이 발생할 확률은 \\(\\frac{2x}{2x+x+2x+x+2x+x}=\\frac{2}{9}\\), 반면에, 짝수의 눈이 나올 확률은 \\(\\frac{x}{2x+x+2x+x+2x+x}=\\frac{1}{9}\\) 이다. 이 때 확률은 위의 정의를 따라야 하므로\n\n\\(0\\le P(evenNumber)=\\frac{2}{9}, P(oddNumber)=\\frac{1}{9}\\le 1\\) 이고\n\\(P(\\Omega=\\{1,2,3,4,5,6\\})=P(evenNumbers)+P(oddNumbers)=1\\)\n\n이므로 확률이라고 할 수있다.\n\n그러므로 \\(P(A={1,2})=P(1)+P(2)=\\frac{2}{9}+\\frac{1}{9}=\\frac{3}{9}\\) 이다.\n\n\nTheorem 1 Countable sample space \\(\\Omega\\) consists of \\(N\\) distinctive equally likely elements (i.e. \\(n(S)=N\\)), and an event \\(A\\) is a subset of the sample space. The event A consists of \\(n\\) distinctive equally likely elements (i.e. \\(n(A)=n\\)). Then \\[\nP(A)=\\frac{n}{N}\n\\]\n\nelement가 오직 동일한 확률로 발생할 때에만 (equally likely), 확률은 \\(\\frac{n(A)}{n(\\Omega \\space or \\space S)}\\) 의 비율(proportion)로 표현될 수 있다.\n예를 들어, 주사위의 눈이 3 보다 작은 수가 나올 사건을 A라고 하면 \\(P(A)= \\frac{n(A)}{n(S)}=\\frac{n(\\{1,2\\})}{n(\\{1,2,3,4,5,6\\})}=\\frac{2}{6}\\) 가 된다.\n\nTheorem 2 If in \\(N\\) identical and independent repeated experiments, an event \\(A\\) happens \\(n\\) times, the the probability of \\(A\\) is defined by \\[\nP(A)=\\lim_{N\\to\\infty}\\frac{n}{N}\n\\]\n\n\nTheorem 3 Basic probability theorem: the complement and additive rule. \\[\n\\begin{aligned}\nP(E^c)&=1-P(E) \\\\\nP(E_1 \\cup E_2)&= P(E_1) + P(E_2) - P(E_1 \\cap E_2) \\\\\n\\end{aligned}\n\\]\n\\(E_1\\) and \\(E_2\\) are mutually exclusive.\n\n\nTheorem 4  \nGeneralized additive rule $$\n\\[\\begin{aligned}\n\n\\end{aligned}\\]\n$$\n\n나머지는 확률 이론 서적을 살펴 보길 바란다.\n\n\n\n\n\n\n\n\nexperiment: the way carry out a study, study design.\ntrial: study experiment trial.\nsample space (\\(\\text{S or } \\Omega\\)): the set of all possible elements (i.e. the collection of all possible outcomes of an experiment).\nelement (\\(\\omega\\)): each outcome of sample space, it is also called ‘sample point’.\nevent (\\(E\\)): a set of sample points or outcomes or a subset of sample space.\n\n\n\n\n\n\\(\\omega \\in A\\) : \\(\\omega\\) is an element of a set A\n\\(\\omega \\not\\in A\\) : \\(\\omega\\) is not an element of a set A\n\\(B \\subset A\\) : B is a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\in A\\)\n\\(A = B\\) : \\(B \\subset A\\), \\(A \\subset B\\)\n\\(B \\not\\subset A\\) : B is not a subset of A, which means if \\(\\omega \\in B\\), then \\(\\omega \\not\\in A\\)\n\n\n\n\nProbability is on study of chance and uncertainty. Its theory builds on set theory. Classic probability is like a gambling of combinatorial games of chance, theory errors. Probability is used in statistics, economics, operation research, psychology, biology, epidemiology, medicine, etc. The prerequisite for probability is calculus and set theory, and the related study is real analysis, measure theory, and stochastic process. To define probability, some regularity on the collection of events is required.\n\n\n\n\nprobability shows the possibility of the occurrence of an event.\nclassic definition ::: {#def-classic}\n\nCountable sample space \\(\\Omgega\\) consists of \\(N\\) distinctive equally likely elements (i.e. \\(n(S)=N\\)), and an event \\(A\\) is a subset of the sample space. The event A consists of \\(n\\) distinctive equally likely elements (i.e. \\(n(A)=n\\)). Then \\[\nP(A)=\\frac{n}{N}\n\\]\n\n\n\nTheorem 5 If in \\(N\\) identical and independent repeated experiments, an event \\(A\\) happens \\(n\\) times, the the probability of \\(A\\) is defined by \\[\nP(A)=\\lim_{N\\to\\infty}\\frac{n}{N}\n\\]\n\n\nThe case of the sample space consisting of \\(N\\) distinctive not equally likely elements,\nThe case of the uncountable sample space\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-21_transformation/index.html",
    "href": "docs/blog/posts/statistics/2023-02-21_transformation/index.html",
    "title": "Transformation of Random Variables",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n확률 변수 2개 이상에 대한 확률 분포를 joint probability distribution (결합확률분포)라고 하는데 두 확률 변수 \\(X\\) 와 \\(Y\\) 의 관계에 대해서 규명해야 할 때가 있다. 예를 들어, X 와 Y의 높은 상관계수라든지 또는 비선형적인 관계가 관찰될 때 그 관계가 수리적으로 모델링이 가능하고 한 확률 변수의 분포에 대한 정보를 알고있다면 미지의 다른 확률 변수의 분포가 추정가능해진다. 이 때 두 변수에 대한 관계 정도가 높으면 높을수록 추정이 쉬워진다.\n이번 블로그에서는 주어진 확률 변수 \\(X\\) 에 대해서 \\(X\\) 의 pmf (probability mass function) 또는 pdf (probability density function) \\(f_x(x)\\) 를 알고있을 때 확률 변수에 \\(X\\) 에 적절한 함수의 변환을 적용해 확률 변수 \\(Y\\) 를 \\(Y=u(X)\\) 라는 관계식이 정의 가능할 때 \\(Y\\) 의 pmf 또는 pdf를 구하는 방법에 집중한다. 후에 MGF (Momment Generating Function) 학습에 응용될 수 있는 개념으로 잘 정리할 필요가 있다.\n\n\n\n\nTheorem 1 Discrete random variable \\(X\\) 의 probability distribution이 \\(f_X(x)\\) 이고 \\(X\\) 와 \\(Y\\) 사이에는 \\(Y=u(X)\\) 라는 one-to-one relation이 성립될 때 \\(y=u(x)\\) 를 유일한 \\(x\\) 를 \\(y\\) 에 대한 함수인 \\(x=w(y)\\) 로 표현 가능하다면 \\(Y\\) 의 probability distribution는 \\[\nf_Y(y)=f_X(w(y))\n\\] 이다.\n\n\n\n\nIn the case of a One-to-One relation\n\n동전을 독립적으로 2번 던질 때, 확률 변수 \\(X\\) 가 앞면이 나오는 수라고 정의했을 때, 확률 분포 아래 표 (a)와 같다. \\(Y=2X+1\\) 라는 관계가 성립할 때 \\(Y\\) 의 분포는 아래 표 (b)와 같다.\n\n\nTable 1: Exmaple: Transformation of Discrete Random Variable (One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\\(X\\)\n0\n1\n2\n\n\n\n\n\\(P_X(X=x)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y=2X+1\\)\n\n\n\\(Y=2X+1\\)\n1\n3\n5\n\n\n\n\n\\(P_Y(Y=y)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n\nIn the case of not a One-to-One relation\n\n동전을 독립적으로 2번 던질 때, 확률 변수 \\(X\\) 가 앞면이 나오는 수의 합이라고 정의 했을때, \\(X\\) 의 확률 분포는 아래 표 (a) 와 같다. 이 때 \\(Y=mod(X,2)\\) 라는 관계가 성립할 때 \\(Y\\) 의 분포는 아래 표 (b) 같다.\n\n\nTable 2: Exmaple: Transformation of Discrete Random Variable (Not One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\\(X\\)\n0\n1\n2\n\n\n\n\n\\(P_X(X=x)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y=mod(x,2)\\)\n\n\n\\(Y=mod(x,2)\\)\n0\n1\n\n\n\n\n\\(P_Y(Y=y)\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{2}{4}\\)\n\n\n\n\n\n\n위의 예시와 같이 두 확률 변수가 one to one 관계일 때는 확률분포가 그대로 유지되어서 쉽게 변환된 확률 변수의 분포가 추정가능하지만 one to one 관계가 아닐 경우 확률 분포가 바뀌게 된다.\n\nanother example (In the case of not a One-to-One relation)\n\n기하분포 (Geometric Distribution)란 동일한 베르누이 (Bernoulli) 분포의 시행을 독립적으로 반복할 때 첫 성공까지의 시행 횟수를 확률변수 \\(X\\) 로 하는 분포이다. 즉, \\(x-1\\) 번째까지 베르누이 시행이 실패하고 \\(x\\) 번째 시행에서 성공할 확률 분포를 말한다.\nNotation은 \\(X \\sim Geometric(p)\\) 또는 \\(X \\sim Geo(p)\\) 라고 표현하고, \\(p\\) 는 독립 시행에서 성공할 확률이다. (참고: \\(text{E}(X)=\\frac{1}{p}\\), \\(\\text{Var}(X)=\\frac{1-p}{p^2}\\))\n\\(X \\sim Geometric(\\frac{4}{5})\\) 일 때 \\(X\\) 의 확률분포 \\(f(x)=\\frac{4}{5}(\\frac{1}{5})^{(x-1)}\\), \\(x=1,2,3 ...\\) 가 기하분포일 때 \\(Y=X^2\\) 의 확률분포는\n\\[\n\\begin{aligned}\n  y&=u(x)=x^2 \\\\\n  x&=w(y)=\\sqrt{y} \\text{ } (x>0)\\\\\n  f_Y(y)&=f_X(w(y))=f_X(\\sqrt{y})=\\frac{4}{5}(\\frac{1}{5}^{(\\sqrt{y}-1)})\\\\\n  \\therefore f_Y(y)&=\n  \\begin{cases}\n    \\frac{4}{5}(\\frac{1}{5}^{(\\sqrt{y}-1)}) &\\text{if} \\text{  } y =1, 4, 9, ...\\\\\n     0 \\text{  } & \\text{otherwise}\n    \\end{cases}\n\\end{aligned}\n\\]\n이다.\n\n\n\n\n\nTheorem 2 Two discrete random variables \\(X_1\\) 과 \\(X_2\\) 의 joint probability distribution이 \\(f(x_1,x_2)\\) 이고 \\((x_1,x_2)\\) 와 \\((y_1,y_2)\\) 가 one-to-one relation이 성립되어 \\(y_1=u_1(x_1,x_2)\\) 와 \\(y_2=u_2(x_1,x_2)\\) 를 유일한 \\(x_1=w_1(y_1,y_2)\\) 와 \\(x_2=w_2(y_1,y_2)\\) 의 함수로 표현 가능하다면 새로운 확률변수 \\(\\mathbf{Y}\\), 즉, \\(Y_1\\) 과 \\(Y_2\\) 의 joint probability distribution는 \\[\nf_Y(y_1,y_2)=f_X(w_1(y_1,y_2),w_2(y_1,y_2))\n\\] 이다.\n\n\n\n\nexmaple 1\n\n다항 분포(multinomial distribution)란 2개 이상의 독립적인 확률 변수 \\(\\mathbf{X}=X_1, X_2, ...\\) 들에 대한 확률분포로, 여러 독립 시행에서 각 각의 값이 특정 횟수가 나타날 확률을 정의하고 독립 변수가 2개인 경우 다항 분포의 특별한 case로 이항 분포 (binomial distribution)가 된다.\n참고( Source: wiki) :\n\\[\n\\begin{aligned}\n  f_(x) & =\\frac{n!}{x_1!x_2!\\dots x_k!}p_1^{x_1}p_2^{x_2}\\dots p_k^{x_k}\\\\\n  \\text{E}(x)&=np_i\\\\\n  \\text{Var}(x)&=np_i(1-p_i)\n\\end{aligned}\n\\]\n\\(X_1\\) and \\(X_2\\) 가 다음과 같은 multinomial distribution을 따를 때 \\[\nf(x_1,x_2) = \\binom{2}{x_1,x_2,2-x_1-x_2}\\frac{1}{4}^{x_1}\\frac{1}{3}^{x_2}\\frac{5}{12}^{2-x_1-x_2} x_1=0,1,2, \\space x_2=0,1,2, \\space x_1+x_2\\le 2\n\\]\n\\(Y_1=X_1+X_2\\) 와 \\(Y_2=X_1-X_2\\) 의 결합 확률 분포는\n\\[\n\\begin{aligned}\n  y_1&=u_1(x_1,x_2)=x_1+x_2\\\\\n  y_2&=u_2(x_1,x_2)=x_1-x_2\\\\\n  x_1&=w_1(y_1,y_2)=\\frac{y_1+y_2}{2}\\\\\n  x_2&=w_2(y_1,y_2)=\\frac{y_1-y_2}{2}\\\\\n  f_Y(y_1,y_2)&=\\binom{2}{\\frac{y_1+y_2}{2},\\frac{y_1-y_2}{2},2-y_1 }\\frac{1}{4}^{\\frac{y_1+y_2}{2}}\\frac{5}{12}^{2-y_1} y_1=0,1,2, \\space y_2=-2,-1,0,1,2\n\\end{aligned}\n\\] 이다.\n\nexmaple 2\n\n\\(X_1\\) 과 \\(X_2\\) 의 결합확률분포가 다음과 같을 때 변환된 \\(Y_1=X_1+X_2\\) 와 \\(Y_2=X_1X_2\\) 의 결합확률 분포는?\n\n\nTable 3: Exmaple: Transformation of Joint Discrete Random Variable (Not One to One)\n\n\n\n\n(a) Probability Distribution of \\(X\\)\n\n\n\n\n\n\n\n\n\n\\((x_1,x_2)\\)\n(0, 0)\n(0, 1)\n(1, 0)\n(1, 1)\n\n\n\n\n\\(f_X(x_1,x_2)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n(b) Probability Distribution of \\(Y\\)\n\n\n\\((y_1,y_2)\\)\n(0, 0)\n(1, 0)\n(2, 1)\n\n\n\n\n\n\\(f_Y(y_1,y_2)\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{2}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 3 continuous random variable \\(X\\) 의 probability distribution이 \\(f_X(x)\\) 이고 \\(X\\) 와 \\(Y\\) 사이에는 \\(Y=u(X)\\) 라는 one-to-one relation이 성립될 때 \\(y=u(x)\\) 를 유일한 \\(x\\) 를 \\(y\\) 에 대한 함수인 \\(x=w(y)\\) 로 표현 가능할 때 Y의 확률 분포는 \\[\nf_Y(y)=f_X(w(y))|J|\n\\] 이다. (\\(J=w'(y)\\), called ‘Jacobian’)\n\n\n\n\nIn the case of a One-to-One relation\n\n지수분포(exponential distribution)는 연속 확률 분포 중 하나로, 사건이 서로 독립적일 때, 일정 시간동안 발생하는 사건의 횟수가 푸아송 분포를 따른다면, 다음 사건이 발생할 때까지의 대기 시간을 확률 변수 \\(X \\in [0, \\infty)\\) 로 하는 분포이다. (Source: Wiki).\n참고: \\[\n\\begin{aligned}\n  X&\\sim Exp(\\lambda) \\\\\n  \\text E[X] &= \\frac{1}{\\lambda}\\\\\n  \\text{Var}[X] &= \\frac{1}{\\lambda^2}\\\\\n  f(x;\\lambda)&=\n  \\begin{cases}\n    \\lambda e^{-\\lambda x} & x \\ge 0\\\\\n    0 & x<0\n  \\end{cases}\\\\\n\\end{aligned}\n\\] \\(\\lambda>0\\) is the parameter of the distribution, called ‘rate parameter’.\nTransformation of Linear Function. When two random variables \\(X \\sim \\text{exp}(\\lambda)\\) and \\(Y\\) has the relation, \\(Y=aX+b\\) \\((a\\ne 0)\\). When \\(f_X(x)=\\lambda e^{-\\lambda x}\\), \\(f_Y(y)\\) is\nBy Theorem\n\\[\n\\begin{aligned}\n  Y&=u(X)=aX+b\\\\\n  y&=u(x)=ax+b\\\\\n  x&=w(y)=\\frac{y-b}{a}\\\\\n  f_Y(y)&=f_X(w(y))|J|=f_X(\\frac{y-b}{a})|J|\\\\\n        &=f_X(\\frac{y-b}{a})|w'(y)|\\\\\n        &=f_X(\\frac{y-b}{a})|\\frac{1}{a}|\\\\\n        &=\\frac{f_X(\\frac{y-b}{a})}{|a|}\\\\\n\\end{aligned}\n\\]\nBy Definition\n\n\n\n\\(a>0\\) \\[\n\\begin{aligned}\nF_Y(Y)&=P_Y(Y\\le y)\\\\\n      &=P_Y(aX+b\\le y)\\\\\n      &=P_Y(X\\le \\frac{y-b}{a})\\\\\n      &=F_X(\\frac{y-b}{a})\\\\\nF'_Y(Y)&=\\frac{d}{dy}F_X(\\frac{y-b}{a})\\\\\n       &=f_X(\\frac{y-b}{a})\\frac{1}{a}\\\\\n\\end{aligned}\n\\]\n\n\n\n\\(a\\le 0\\) \\[\n\\begin{aligned}\nF_Y(Y)&=P_Y(Y\\le y)\\\\\n      &=P_Y(aX+b\\le y)\\\\\n      &=P_Y(X> \\frac{y-b}{a})\\\\\n      &=1-F_X(\\frac{y-b}{a})\\\\\nF'_Y(Y)&=\\frac{d}{dy}(1-F_X(\\frac{y-b}{a}))\\\\\n       &=-f_X(\\frac{y-b}{a})\\frac{1}{a}\\\\  \n\\end{aligned}\n\\]\n\n\\[\n\\therefore f_Y(y)=\\frac{f_X(\\frac{y-b}{a})}{|a|}=\\frac{\\lambda exp(-\\lambda (\\frac{y-b}{a}))}{|a|}\n\\]\n\n위의 정리를 유도를 하진 않았지만 증명을 해보면 사실, \\(|J|\\) 는 역함수의 미분의 계수인 것을 할 수 있다. 위의 예시를 By definition 으로 푼 것을 보면 \\(y=u(x)\\) 와 \\(x=w(y)\\) 의 관계인 것을 알 수 있고 \\(|J|\\) 에 해당되는 \\(\\frac{1}{a}\\) 는 CDF \\(F(Y)\\) 의 derivative를 구하는 과정에서 발생하는 chain rule에 의해 생긴 것을 알 수 있다.\n\\[\n\\begin{aligned}\n  y&=u(x)\\\\\n  \\frac{dy}{dx}&=u'(x)\\\\\n  x&=w(y)=u^{-1}(y)\\\\\n  \\frac{dx}{dy}&=w'(y) \\\\\n  w'(y)&=\\frac{dx}{dy}=(\\frac{dy}{dx})^{-1}=\\frac{1}{u'(x)}\n\\end{aligned}\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 20, 1000)\ny = 2*x+1\ny2 = x/2+1\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,x,color='black',label=r'$y=x$', linestyle='dashed')\nax.plot(x,2*x-2,color='black',label=r'$x=w(y) \\rightarrow y=2x-2 $')\nax.plot(x,y2,color='red',label=r'$y=u(x)=\\frac{1}{2}x+1$')\nax.set_aspect(1)\nax.set_title(r\"Example of Relation of X and Y, $y=u(x)$ vs $x=w(y)$\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\nax.set_xlim(-20, 40)\n\nplt.legend(shadow=True, loc=(-0.5, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 20, 1000)\ny = 2*x+1\ny2 = x/2+1\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,y,color='black',label=r'$y=2x+1$')\nax.plot(x,y2,color='red',label=r'$y=\\frac{1}{2}x+1$')\nax.set_aspect(1)\nax.set_title(r\"Transformation of Linear Function, $f_Y(y)=\\frac{f_X(\\frac{y-b}{a})}{|a|}$\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y=u(X)\")\nax.set_xlim(-20, 40)\nax.text(10, 0,'|', color='black', horizontalalignment='center', verticalalignment='center')\nax.text(10, -5, r'$\\frac{y-b}{a}$', horizontalalignment='center',color='black')\nax.text(0, 5,'--', color='black', horizontalalignment='center', verticalalignment='center')\nax.text(-9, 5, r'$ax+b$', verticalalignment='center',color='black')\n\nplt.legend(shadow=True, loc=(-0.5, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\\(P(Y\\le y)=P(X\\le \\frac{y-b}{a})\\) 이기 때문에 \\(X\\) 의 특정 구간의 확률과 대응되는 \\(Y\\) 의 구간의 확률이 같다고 했을 때 \\(Y \\le y\\) 의 범위는 \\(X\\) 를 \\(b\\) 만큼 이동한 후 \\(a\\) 배한 \\(x\\) 좌표 이하 구간에 대응 된다 즉, \\(X\\le \\frac{y-b}{a}\\). 이 때, 두 구간에서의 확률 밀도가 같아야 하기 때문에 \\(f_x(x)\\) 가 \\(f_Y(y)\\) 의 \\(\\frac{1}{a}\\) 배 인 것을 알 수 있다.\n위의 그림에서 처럼, 두 확률 변수 \\(X\\) 와 \\(Y\\) 의 관계를 \\(y = 2x+1\\) \\((a>1)\\) 와 \\(y = \\frac{1}{2}x+1\\) \\((0 \\le a\\le 1)\\) 로 두 가지의 예로 들면, 위의 그래프의 경우, \\(y = 2x+1\\) \\((a>1)\\) 일 때 \\(0<X<10\\) 는 \\(1<Y<21\\) 의 구간이 2배가 되는 것을 관찰 할 수 있다. 반대로, \\(y = \\frac{1}{2}x+1\\) \\((0 \\le a\\le 1)\\) 일 때 \\(0<X<10\\) 는 \\(1<Y<6\\) 의 구간이 \\(\\frac{1}{2}\\) 배가 된다. 이 때 각 예시의 경우에 \\(X\\) 와 \\(Y\\) 의 확률 값이 같기 때문에 (\\(P(Y\\le y)=P(X\\le \\frac{y-1}{2})\\) 와 \\(P(Y\\le y)=P(X\\le \\frac{y-1}{\\frac{1}{2}})\\)). \\(f_x(x)\\) 가 \\(f_Y(y)\\) 의 각 각 \\(2\\) 배와 \\(\\frac{1}{2}\\) 배 인 것을 추정 할 수 있다.\n\nIn the case of not a One-to-One relation\n\n\\(Y=X^2\\) 일 때,\n\\[\n\\begin{aligned}\n  F_Y(Y)&=P_Y(Y\\le y)\\\\\n        &=P_Y(X^2\\le y)\\\\\n        &=P_Y(-\\sqrt{y}\\le X \\le \\sqrt y)\\\\\n        &=F_X(\\sqrt{y})-F_X(-\\sqrt{y})\\\\\n  F'_Y(Y)&=\\frac{d}{dy}(F_X(\\sqrt{y})-F_X(-\\sqrt{y}))\\\\\n        &=f_X(\\sqrt{y})\\frac{1}{2\\sqrt{y}}+f_X(-\\sqrt{y})\\frac{1}{2\\sqrt{y}}\\\\\n        &=\\frac{f_X(\\sqrt{y})}{2\\sqrt{y}}+\\frac{f_X(-\\sqrt{y})}{2\\sqrt{y}}\\\\\n\\end{aligned}\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 1000)\ny = x*x\n\nfig, ax = plt.subplots()\n\n\nax.axhline(y=0, color='k')\nax.axvline(x=0, color='k')\nax.grid(True, which='both')\n\nax.plot(x,y,color='black',label=r'$y=x^2$')\n#ax.text(3.0,0.5,r'$\\sqrt{y}$')\n#ax.text(-4,0.5,r'$-\\sqrt{y}$')\nax.set_xlim([-10, 10])\nax.set_ylim([0, 10])\n\nax.set_aspect(1)\nax.set_title(r\"Transformation of Random Variables, $y=x^2$\")\nax.set_xlabel(\"x-axis\")\nax.set_ylabel(\"y-axis\")\n\nplt.legend(shadow=True, loc=(-0.2, 1.1), handlelength=1.5, fontsize=8)\nplt.show()\n\n\n\n\n\n\\(X\\) 의 분포를 알고 \\(X\\) 와 \\(Y\\) 의 relation을 알고있을 때 \\(X\\) 의 분포로부터 \\(Y\\) 의 분포를 추정 가능한 것의 이점 중 하나는 \\(Y\\) 의 통계량을 \\(Y\\) 의 분포를 사용하지 않고 계산해낼 수 있다는 점이다.\n예를 들어, \\(X \\sim N(0,1)\\) 이고 \\(Y=u(X)\\) 라는 관계를 알고있을 때, \\(Y\\) 의 통계량 \\(E(Y)\\), \\(Var(Y)\\) 를 알고싶다면 굳이 힘들게 \\(Y\\) 의 분포를 계산할 필요가 없다. 이미 \\(X\\) 의 확률 분포 \\(f_X(x)\\) 를 알고있기 때문에 아래와 같이 \\(Y\\) 의 통계량을 계산해낼 수 있다.\n\\[\n\\begin{aligned}\n  \\text{E}(Y)&=\\int Y f_Y(y) dy\\\\\n            &=\\int u(X) f_X(x) dx\\\\\n  \\text{E}(Y^2)&=\\int Y^2 f_Y(y) dy\\\\\n            &=\\int u(X)^2 f_X(x) dx\\\\\n  \\text{Var}(Y)&=\\text{E}(Y^2)-\\text{E}(Y)^2\\\\\n  &=\\text{E}(g(X)^2)-\\text{E}(g(X))^2\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\nTheorem 4 Two continuous random variable \\(X_1\\) 과 \\(X_2\\) 의 joint probability distribution이 \\(f(x_1,x_2)\\) 이고 \\((x_1,x_2)\\) 와 \\((y_1,y_2)\\) 가 one-to-one relation이 성립될 때 \\(y_1=u_1(x_1,x_2)\\) 와 \\(y_2=u_2(x_1,x_2)\\) 를 유일한 \\(x_1=w_1(y_1,y_2)\\) 와 \\(x_2=w_2(y_1,y_2)\\) 의 함수로 표현 가능할 때 \\(Y_1=u_1(X_1,X_2)\\) 와 \\(Y_2=u_2(X_1,X_2)\\) 로 정의되는 새로운 확률변수 \\(Y_1\\) 과 \\(Y_2\\) 의 joint probability distribution는 \\[\ng(y_1.y_2)=f(w_1(y_1,y_2),w_2(y_1,y_2)) |J|\n\\] 이다.\n여기서, \\[\n\\begin{aligned}\n  |J|=|\n  \\begin{bmatrix}\n  \\frac{\\partial x_1}{\\partial y_1}&\\frac{\\partial x_1}{\\partial y_2}\\\\\n  \\frac{\\partial x_2}{\\partial y_1}&\\frac{\\partial x_2}{\\partial y_2}\\\\\n  \\end{bmatrix}| = | \\begin{bmatrix}\n  \\frac{\\partial w_1(y_1,y_2)}{\\partial y_1}&\\frac{\\partial w_1(y_1,y_2)}{\\partial y_2}\\\\\n  \\frac{\\partial w_2(y_1,y_2)}{\\partial y_1}&\\frac{\\partial w_2(y_1,y_2)}{\\partial y_2}\\\\\n  \\end{bmatrix} |\n\\end{aligned}\n\\] called ‘the determinant of jacobian matrix’\n\n\n\n\n예제\n\n연속확률변수 \\(X_1\\) 과 \\(X_2\\) 는 결합확률분포 \\[\nf(x_1,x_2)=\n  \\begin{cases}\n    4x_1x_2, & \\text{if  } 0<x_1<1,& 0< x_2 <1 \\\\\n    0, & \\text{otherwise}\n  \\end{cases}\n\\] 일 때, \\(Y_1=X_1^2\\) 과 \\(Y_2=X_1X_2\\) 의 결합확률 분포는\n\\[\n\\begin{aligned}\ny_1&=u_1(x_1,x_2)= x_1^2\\\\\ny_2&=u_2(x_1,x_2)= x_1x_2\\\\\nx_1&=w_1(y_1,y_2)= \\sqrt{y_1} & (y_1>0)\\\\\nx_2&=w_2(y_1,y_2)= \\frac{y_2}{x_1} = \\frac{y_2}{\\sqrt{y_1}}  & (y_1>0)\\\\\ng(y_1.y_2)&=f(w_1(y_1,y_2),w_2(y_1,y_2))|J|\\\\\ng(y_1.y_2)&=f(w_1(y_1,y_2),w_2(y_1,y_2))|J|\\\\\n          &=4\\sqrt{y_1}\\frac{y_2}{\\sqrt{y_1}}\\frac{1}{2y_1}\\\\\n          &=\\begin{cases}\n          \\frac{2y_2}{y_1} & \\text{if  } y_2^2<y_1<1, \\text{  } 0<y_2<1\\\\\n          0 & \\text{otherwise}\n          \\end{cases}\\\\\n          |J|&=|\n          \\begin{bmatrix}\n          \\frac{\\partial x_1}{\\partial y_1}&\\frac{\\partial x_1}{\\partial y_2}\\\\\n          \\frac{\\partial x_2}{\\partial y_1}&\\frac{\\partial x_2}{\\partial y_2}\\\\\n          \\end{bmatrix}|\\\\\n           &=|\n          \\begin{bmatrix}\n          \\frac{1}{2\\sqrt{y_1}}&0\\\\\n          y_2(-\\frac{1}{2}y_1^{\\frac{-3}{2}})&\\frac{1}{\\sqrt{y_1}}\\\\\n          \\end{bmatrix}|\\\\\n          &=\\frac{1}{2y_1}\n\\end{aligned}\n\\]\n이다.\n\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 2.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 3.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 4.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/bernoulli copy 5.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n::: {#Korean .tab-pane .fade .show .active role=“tabpanel” aria-labelledby=“Korean-tab”}\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}Xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html",
    "title": "Bernoulli Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 확률 변수 \\(X\\) 가 1(=성공) 또는 0(=실패) 중 하나이고 성공일 확률이 \\(p\\) 일 때 \\(X\\) 는 bernoulli distirbution 따르고 probability function은 \\(f(x=1)=p\\) 와 \\(f(x=0)=1-p\\) 이다. 즉, \\[\n\\begin{aligned}\n  f_X(x;p)&=\n  \\begin{cases}\n    p   & x=1 \\\\\n    1-p & x=0\n  \\end{cases}\n\\end{aligned}\n\\] 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=p\\\\\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}x^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=p\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=p-p^2\\\\\n             &=p(1-p)\\\\\n\\end{aligned}\n\\]\n\n\n\n주사위의 눈이 짝수일 때 확률 변수 X = 0, 그 외의 경우는 X = 1 일 때 기댓값과 분산은\n\\[\n\\begin{aligned}\n   \\text{E}(X)&=\\sum_{x\\in\\{1,0\\}}xf(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n              &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{E}(X^2)&=\\sum_{x \\in \\{1,0\\}}X^2f(X=x)\\\\\n             &=1 f(X=1) + 0f(X=0)\\\\\n             &=\\frac{3}{6}=\\frac{1}{2}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n             &=\\frac{1}{2}\\frac{1}{2}\\\\\n             &=\\frac{1}{4}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html",
    "title": "Binomial Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 성공 확률이 p인 bernoulli distribution을 n 번 시행했을 때 성공횟수를 확률 변수 X로 갖는 probability distribution을 binomial distribution이라 한다. probability mass function은\n즉, \\[\n\\begin{aligned}\n  f_X(x;n,p)&=\\binom{n}{x}p^{x}(1-p)^{n-x} \\text{ } (y=0,1,2, ..., n)\\\\\n\\end{aligned}\n\\] 이고 Notation은 보통 \\(X \\sim Bin(n,p) \\text{ or } X \\sim B(n,p) \\text{ or } X \\sim Binomial(n,p)\\) 와 같이 쓰인다 (binomial distribution은 bernoulli distribution을 전제로 한다).\n\n\n\n\\[\n\\begin{aligned}\n    \\text{Let } &I_i \\text{ be } 1\\{x_i=1\\} \\\\\n    X&=\\sum_{i=1}^{n}I_i=I_1+I_2+ ... +I_n\\\\\n  \\text{E}(X)&=\\text{E}(\\sum_{i=1}^{n}I_i)\\\\\n             &=\\text{E}(I_1+I_2+ ... +I_n)\\\\\n             &=\\text{E}(I_1)+\\text{E}(I_2)+ ... +\\text{E}(I_n)\\\\\n             &=p+p+...+p\\\\\n             &=np\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{Var}(X)&=\\text{Var}(I_1+I_2+ ... +I_n)\\\\\n             &=\\text{Var}(I_1)+\\text{Var}(I_2)+ ... +\\text{Var}(I_n)\\\\\n             &=p(1-p)+p(1-p)+...+p(1-p)\\\\\n             &=np(1-p)\n\\end{aligned}\n\\]\n\n\n\n쌍란이 나올 확률이 0.05라고 가정했을 때 Super Market에서 1 pack of 12 eggs을 구매했을 때\n\n3개의 eggs에서 쌍란이 나올 확률은 \\[\n\\begin{aligned}\nX&\\sim Bin(12,0.05)\\\\\nf(X=3)&=\\binom{12}{3}0.05^3 0.95^9\n\\end{aligned}\n\\] 이다.\n적어도 3개의 eggs에서 쌍란이 나올 확률은 \\[\n\\begin{aligned}\nP(X\\ge3)&=1-F_X(3)\\\\\n         &=1-(f(X=3)+f(X=2)+f(X=1)+f(X=0))\\\\\n         &=1-(\\binom{12}{3}0.05^3 0.95^9+\\binom{12}{2}0.05^2 0.95^{10}+\\\\\n         &\\binom{12}{1}0.05^1 0.95^{11}+\\binom{12}{0}0.05^0 0.95^{12})\n\\end{aligned}\n\\] 이다.\n\n다른 예시로는, 분자 진단 시장에서 golden standard라고 평가받는 PCR (Polynomial Chain Reaction)에 사용되는 medical device가 2000 대 중 5대 꼴로 기계적 결함이 발견된다고 가정할 때, 1년에 평균 100대의 분잔 진단 장비를 공급받는 구매자 입장에서 장비의 결함이 발생할 연간 평균과 분산의 추정은 다음과 같다.\n\\[\n\\begin{aligned}\n    X&\\sim Bin(100,\\frac{5}{2000})\\\\\n    f(X=x)&=\\binom{100}{x}\\frac{5}{2000}^x (1-\\frac{5}{2000})^{100-x}\\\\\n    \\text{E}(X)&=100(\\frac{5}{2000})\\\\\n    \\text{Var}(X)&=100(\\frac{5}{2000})(1-\\frac{5}{2000})\n\\end{aligned}\n\\]\n\n\n\n\n\nDefinition 2 n번의 독립 시행에서 각 각 p_1, p_2, …, p_n 의 성공 확률로 E_1, E_2, …, E_n 중 어느 하나를 발생시킬 때 각 event E_i에 대응되는 발생 횟수를 확률 변수 X_1, X_2, …, X_n 로 갖는 joint probability mass function은\n\\[\n\\begin{aligned}\n  f_X(\\mathbf X = x_1,x_2, ...,x_n)&=\\binom{n}{x_1,x_2, ..., x_n}p_1^{x_1}(p_2)^{x_2}\\dots (p_n)^{x_n} \\\\\n\\end{aligned}\n\\] 이다. (단, \\(\\sum_{i=1}^{n}x_i=n, \\sum_{i=1}^{n}p_i=1\\))\n\n\n\n주사위를 5 번 던질 때 1 또는 6의 눈이 1번, 3, 4 또는 5의 눈이 2번 , 2의 눈이 2번 나올 확률은\n\\[\n\\begin{aligned}\n    &x_1= 1, x_2=2, x_3=2\\\\\n    f(X=(1,2,2))&=\\binom{5}{1,2,3}\\frac{1}{3}^1\\frac{1}{2}^2\\frac{1}{6}^2\n\\end{aligned}\n\\] 이다.\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html",
    "title": "Geometric Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 성공 확률이 \\(p\\) 인 independent Bernoulli trials을 시행할 때 첫 성공할때까지의 시행 횟수를 확률 변수 \\(X\\) 로 갖는 분포를 geometric distribution이라고 하고 \\(X\\) 의 probability mass function은 \\[\n\\begin{aligned}\n  f_X(x;p)&=p(1-p)^{x-1}\n\\end{aligned}\n\\] 이다. (단, \\(x=1,2,...\\))\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x=1}^{\\infty} x f(x)\\\\\n             &=\\sum_{x=1}^{\\infty} x pq^{x-1} &\\text{ }(q=1-p)\\\\\n             &=p\\sum_{x=1}^{\\infty} x q^{x-1} \\\\\n             &=p\\frac{d}{dq}\\sum_{x=1}^{\\infty} q^{x} \\\\\n             &=p\\frac{d}{dq}\\frac{q}{1-q} \\\\\n             &=p\\frac{(1-q)+q}{(1-q)^2} \\\\\n             &=p\\frac{(1-q)+q}{p^2} \\\\\n             &=p\\frac{1}{p^2}\\\\\n             &=\\frac{1}{p}\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X(X-1))&=\\sum_{x=1}^{\\infty}x(x-1)pq^{x-1}\\\\\n                  &=pq\\sum_{x=1}^{\\infty}x(x-1)q^{x-2}\\\\\n                  &=pq\\sum_{x=1}^{\\infty}\\frac{d^2}{dq^2}q^{x}\\\\\n                  &=pq\\frac{d^2}{dq^2}\\sum_{x=1}^{\\infty}q^{x}\\\\\n                  &=pq\\frac{d^2}{dq^2}(\\frac{q}{1-q})\\\\\n                  &=pq\\frac{d}{dq}(\\frac{1}{(1-q)^2})\\\\\n                  &=pq(\\frac{2(1-q)}{(1-q)^4})\\\\\n                  &=\\frac{2q}{p^2}\\\\\n  \\text{E}(X^2)&=\\frac{2q}{p^2}+\\text{E}(X)=\\frac{2q}{p^2}+\\frac{1}{p}\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-\\text{E}(X)^2\\\\\n               &=\\frac{2q}{p^2}+\\frac{1}{p}-\\frac{1}{p^2}\\\\\n               &=\\frac{1-p}{p^2}\\\\\n\\end{aligned}\n\\]\n\n\n\n한 장비 제조 업체에서 사람의 건강과 생명을 진단하는데 사용되는 medical device가 1000 개마다 2개 꼴로 불량이 발생한다고 가정할 때 그 장비를 공급받는 구매자가 medical device의 공급 업체의 불량 평가 기준으로 quality control을 1대 씩 진행할 때 불량 장비가 3번째 검사에서 발생할 확률은\n\\[\n\\begin{aligned}\n   \\text{f}(x=3;p=\\frac{2}{1000})&=(\\frac{998}{1000})^2\\frac{2}{1000}\\\\\n\\end{aligned}\n\\] 이다. 평균과 분산 각 각 \\((\\frac{2}{1000})^{-1}\\), \\(\\frac{1-\\frac{2}{1000}}{(\\frac{2}{1000})^2}\\) 이다.\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html",
    "href": "docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html",
    "title": "Poisson Distribution",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\nDefinition 1 모수 (parameter)가 단위 시간 또는 공간 당 평균 발생 횟수 \\(\\lambda\\) 일 때 주어진 단위 시간 또는 공간 내에 발생하는 사건의 횟수를 확률 변수 \\(X\\) 로 하는 분포를 Poisson Distribution이라 한다. \\(X\\) 의 probability mass function은 \\[\n\\begin{aligned}\n  f_X(x;\\lambda)&=\\frac{e^{-\\lambda}\\lambda^{x}}{x!} &(x=0,1,2, ..)\n\\end{aligned}\n\\] (\\(\\lambda\\) 는 단위 시간 또는 공간 당 평균 발생 횟수) 이다.\n\n\n\n\\[\n\\begin{aligned}\n  \\text{E}(X)&=\\sum_{x=0}^{\\infty}xf(x)\\\\\n             &=\\sum_{x=0}^{\\infty}x\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n             &=\\sum_{x=1}^{\\infty}x\\frac{e^{-\\lambda}\\lambda^{x}}{x!} &\\because x=0 \\rightarrow \\text{equation}=0\\\\\n             &=\\sum_{x=1}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x}}{(x-1)!}\\\\\n             &=\\lambda\\sum_{x-1=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x-1}}{(x-1)!}\\\\\n             &=\\lambda\\sum_{y=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{y}}{(y)!} &\\because x-1=y \\text{  }(y=0,1,2, ...)\\\\\n             &=\\lambda\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n    \\text{E}(X(X-1))&=\\sum_{x=0}^{\\infty}x(x-1)f(x)\\\\\n             &=\\sum_{x=0}^{\\infty}x(x-1)\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n             &=\\lambda^2\\sum_{x=2}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{x-2}}{(x-2)!}\\\\\n             &=\\lambda^2\\sum_{y=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^{y}}{(y)!} &\\because x-2=y \\text{  }(y=0,1,2, ...)\\\\\n             &=\\lambda^2\\\\\n    \\text{E}(X^2)&=\\lambda^2+\\lambda\\\\\n    \\text{Var}(X)&=\\lambda^2+\\lambda-\\lambda^2=\\lambda\\\\\n\\end{aligned}\n\\]\n\n\n\n어느 의료 장비 제조 업체의 의료 장비 불량률이 2% 라고 가정했을 때 임의로 100대의 의료 장비를 구매하여 제조 업체의 Quality Control (QC) guide line을 따라 Quality Control (QC)를 진행 했을 때 불량품이 하나도 발생하지 않을 확률은 다음과 같다.\n\\[\n\\begin{aligned}\n  \\lambda &= 100*0.02=2\\\\\n  f(x)&=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}=\\frac{2^{x}e^{-2}}{x!}\\\\\n  f(0)&=\\frac{2^{0}e^{-2}}{0!}=e^{-2}\n\\end{aligned}\n\\]\n\n\nDefinition 2 \\(X\\sim B(n,p)\\) 일 때 \\(p\\) 가 충분히 작고 \\(n \\rightarrow \\infty\\) 고 \\(np=\\lambda\\) 한다면 \\(x=0,1,2, ...\\) 에 대하여\n\\[\n\\begin{aligned}\n  \\lim_{n \\to \\infty}\\binom{n}{x}p^{x}(1-p)^{n-x}=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}\n\\end{aligned}\n\\] (\\(\\lambda\\) 는 단위 시간 또는 공간 당 평균 발생 횟수) 이다.\n\n\n\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-02-28_mgf/index.html",
    "href": "docs/blog/posts/statistics/2023-02-28_mgf/index.html",
    "title": "Momment Generating Function",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nDefinition 1 확률 변수 \\(X^r\\) 의 expectation, \\(\\text{E}((X-\\mu)^r)\\) 를 확률 변수 \\(X\\) 의 평균 \\(\\mu\\) 에 대한 \\(r\\) 차 중심 적률(moment) 이라하고 그 notation은 \\(\\mu_r'=\\text{E}((X-\\mu)^r)\\) 로 한다. \\(\\mu_r'=\\text{E}(X^r)\\) 은 원점에 대한 \\(r\\) 차 중심 적률이라 한다.\n즉,\n\\[\n\\begin{aligned}\n  \\mu_{r}'&=\n    \\begin{cases}\n      \\text{E}((X-\\mu)^r)&=\n        \\begin{cases}\n          \\sum_{x}(x-\\mu)^rf(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}(x-\\mu)^rf(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases} & \\text{moment about }\\mu\\\\\n      \\text{E}(X^r)&=\n        \\begin{cases}\n          \\sum_{x}x^rf(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}x^rf(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases} & \\text{moment about origin}\\\\\\\\\n    \\end{cases}  \n\\end{aligned}\n\\]\n이다.\n\n분포의 특징을 묘사하는 parameters 중 많은 종류가 확률 변수의 적률을 이용해 계산될 수 있다. 그 대표적인 예가\n\n평균 (mean): 분포의 위치를 나타내는 척도, 1차 중심 적률로 계산\n분산 (variance): 분포가 평균으로부터 퍼진 정도를 나타내는 척도, 2차 중심 적률로 계산\n왜도 (skewedness): 분포가 기울어진 방향과 정도를 나타내는 척도, 3차 중심 적률로 계산\n첨도 (kurtosis): 분포가 위로 뾰족한 정도를 나타내는 척도, 4차 중심 적률로 계산\n\n\nDefinition 2 확률 변수 \\(X\\) 의 적률 생성 함수 (Moment Generating Function, mgf), \\(\\text{M}_X(t) =\\text{E}(e^{tX})\\) 로 정의한다.\n즉,\n\\[\n\\begin{aligned}\n  M_X(t)=\\text{E}(e^{tX})&=\n        \\begin{cases}\n          \\sum_{x}e^{tx}f(x), & \\text{if }x \\text{ is discrete}\\\\\n          \\int_{-\\infty}^{\\infty}e^{tx}f(x)dx, & \\text{if }x \\text{ is continuous}\\\\\n        \\end{cases}\n\\end{aligned}\n\\]\n이다.\n\n\nTheorem 1 확률 변수 \\(X\\) 의 적률 생성 함수 (Moment Generating Function, mgf), \\(\\text{M}_X(t) =\\text{E}(e^{tX})\\) 로 r차 적률 계산은 다음과 같이 할 수 있다.\n\\[\n\\begin{aligned}\n  \\frac{d^r}{dt^r}M_X(t) \\bigg|_{t=0}=M_X^r(0)=\\text{E}(X^r)=\\mu_r'\n\\end{aligned}\n\\]\n이다. 즉, 적률 생성 함수 (mgf) \\(M_X(t)\\) 를 구하고 r 번 미번한 후에 \\(t=0\\) 대입하면 r차 중심 적률을 구할 수 있다.\n\nproof (for the only continuous case)\n\\[\n\\begin{aligned}\n  \\text{First Order Moment}\\\\\n  \\frac{d}{dt}M_X(t)&= \\frac{d}{dt}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d}{dt}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}\\frac{d}{dt}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}xe^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}xf(x)dx\\\\\n    &=  \\text{E}(X)\\\\\n    &=  \\mu_1'\\\\\n  \\text{Second Order Moment}\\\\\n  \\frac{d^2}{dt^2}M_X(t)&= \\frac{d^2}{dt^2}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d^2}{dt^2}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}\\frac{d}{dt}xe^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^2e^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^2f(x)dx\\\\\n    &=  \\text{E}(X^2)\\\\\n    &=  \\mu_2'\\\\\n  \\vdots\\\\\n  \\text{r th Order Moment}\\\\\n  \\frac{d^r}{dt^r}M_X(t)&= \\frac{d^r}{dt^r}\\text{E}(e^{tX})\\\\\n    &=  \\frac{d^r}{dt^r}\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^re^{tx}f(x)dx\\bigg|_{t=0}\\\\\n    &=  \\int_{-\\infty}^{\\infty}x^rf(x)dx\\\\\n    &=  \\text{E}(X^r)\\\\\n    &=  \\mu_r'\n\\end{aligned}\n\\]\n\n\n\n\n\nMGF of \\(X\\sim B(n,p)\\)\n\n확률 변수 \\(X \\sim B(n,p)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n\\[\n\\begin{aligned}\n  f(x)&=\\binom{n}{x}p^xq^{n-x}\\\\\n  \\sum_{x=0}^{n}f(x)&=\\sum_{x=0}^{n}\\binom{n}{x}p^xq^{n-x}=(p+q)^n\\\\\n  M_X(t)&=\\sum_{x=0}^{n}e^{tx}f(x)=\\sum_{x=0}^{n}e^{tx}\\binom{n}{x}p^xq^{n-x}\\\\\n        &=\\sum_{x=0}^{n}\\binom{n}{x}(pe^t)^xq^{n-x}=(pe^t+q)^n\\\\\n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}(pe^t+q)^n\\\\\n    &=n(pe^t+q)^{n-1}pe^t\\bigg|_{t=0}\\\\\n    &=n(p+q)^{n-1}p=np\\\\\n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}(n(pe^t+q)^{n-1}pe^t)\\bigg|_{t=0}\\\\\n    &=(n(n-1)(pe^t+q)^{n-2}pe^tpe^t+n(pe^t+q)^{n-1}pe^t)\\bigg|_{t=0}\\\\\n    &=n(n-1)(p+q)^{n-2}p^2+n(p+q)^{n-1}p\\\\\n    &=n(n-1)p^2+np\\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=n(n-1)p^2+np-(np)^2\\\\\n    &=n^2p^2-np^2+np-n^2p^2\\\\\n    &=np(1-p)=npq\n\\end{aligned}\n\\]\n\nMGF of \\(X\\sim Poisson(\\lambda)\\)\n\n확률 변수 \\(X \\sim Poisson(\\lambda)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n  M_X(t)&=\\sum_{x=0}^{\\infty}e^{tx}f(x)=\\sum_{x=0}^{\\infty}e^{tx}\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\\\\n        &=e^{-\\lambda}\\sum_{x=0}^{\\infty}\\frac{{e^{t}\\lambda^{x}}^x}{x!}\\\\\n        &=e^{-\\lambda}e^{\\lambda e^t} \\because \\text{(Maclaurin's Series)}\\\\\n        &=e^{\\lambda(e^t-1)} \\\\\n        (&\\text{Maclaurin's Series: } \\sum_{n=0}^{\\infty}\\frac{x^n}{n!}=1+x+\\frac{x^2}{2!}+\\frac{x^3}{3!}+ ... =e^x)\\\\     \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda e^{e^t}e^{\\lambda(e^t-1)}\\bigg|_{t=0}=\\lambda\\\\\n  \n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}\\lambda e^{e^t}e^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda e^te^{\\lambda (e^t-1)}+\\lambda e^t\\lambda e^te^{\\lambda(e^t-1)}\\bigg|_{t=0}\\\\\n    &=\\lambda + \\lambda^2\\\\\n\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=\\lambda + \\lambda^2 -\\lambda^2\\\\\n    &=\\lambda\n\\end{aligned}\\]\n$$\n\n\n\n\nMGF of \\(X\\sim N(0,1)\\)\n\n확률 변수 \\(X \\sim N(0,1)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}(-\\infty < x < \\infty)\\\\\n  M_X(t)&=\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx=\\int_{-\\infty}^{\\infty}e^{tx}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{x^2}{2}+tx}dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(x-t)^2+\\frac{1}{2}t^2}dx\\\\\n        &=e^{\\frac{t^2}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(x-t)^2}dx\\\\\n        &=e^{\\frac{t^2}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}u^2}du \\text{  }(\\because x-t=u \\rightarrow dx=du)\\\\\n        &=e^{\\frac{t^2}{2}} \\\\     \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=te^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=0\\\\\n  \n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}te^{\\frac{t^2}{2}}\\bigg|_{t=0}\\\\\n    &=e^{\\frac{t^2}{2}}+t(te^{\\frac{t^2}{2}})\\bigg|_{t=0}\\\\\n    &=1\\\\\n\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=1\n    \n\\end{aligned}\\]\n$$\n\nMGF of \\(X\\sim N(\\mu,\\sigma^2)\\)\n\n확률 변수 \\(X\\sim N(\\mu,\\sigma^2)\\) \\(X\\) 의 mgf, \\(M_X(t)\\), \\(\\text{E}(X)\\) 및 \\(\\text{Var}(X)\\) 는 다음과 같다.\n$$\n\\[\\begin{aligned}\n  f(x)&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}(-\\infty < x < \\infty)\\\\\n  M_X(t)&=\\int_{-\\infty}^{\\infty}e^{tx}f(x)dx\\\\\n        &=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^{\\infty}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}+tx}dx\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}(\\frac{(x-(t\\sigma^2+\\mu))}{\\sigma})^2}dx\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{u^2}{2}}du\\\\\n        (&\\because \\frac{x-(t\\sigma^2+\\mu)}{\\sigma}=u \\rightarrow \\frac{dx}{\\sigma}=du) \\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^2}{2}}du\\\\\n        &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\\\\n  \n  \\text{E}(X)&=\\mu_1'=\\frac{d}{dt}M_X(t)\\bigg|_{t=0}\\\\\n  \\frac{d}{dt}M_X(t)\\bigg|_{t=0}&=\\frac{d}{dt}e^{\\frac{t^2\\sigma^2}{2}+t\\mu}\\bigg|_{t=0}\\\\\n    &=e^{\\frac{t^2\\sigma^2}{2}+t\\mu}(\\sigma^2t+\\mu)\\bigg|_{t=0}\\\\\n    &=\\mu\\\\\n  \\text{E}(X^2)&=\\mu_2'=\\frac{d^2}{dt^2}M_X(t)\\bigg|_{t=0}\\\\\n    &=\\mu_2'=(\\mu_1')'=\\frac{d}{dt}\\mu_1'\\bigg|_{t=0}\\\\\n    &=\\frac{d}{dt}e^{\\frac{t^2\\sigma^2}{2}+t\\mu}(\\sigma^2t+\\mu)\\bigg|_{t=0}\\\\\n    &=\\sigma^2+\\mu^2 \\\\\n  \\text{Var}(X)&=\\text{E}(X^2)-(\\text{E}(X))^2\\\\\n    &=\\sigma^2\n\\end{aligned}\\]\n$$\n\n\n\n\n\nTheorem 2 확률 변수 \\(X\\) 와 \\(Y\\) 가 유한한 같은 적률 생성 함수를 가지면 두 확률 변수는 같은 확률 분포를 갖는다. (단, \\(t\\in[-c,c]\\) where \\(c\\) is a positive constant) \\[\n\\text{M}_X(t) =\\text{M}_Y(t) \\rightarrow F_X(a)=F_Y(a) \\text{ for } a \\in \\mathbb{R}\n\\] 다시 말해서, 확률 변수의 분포의 특징이 적률 생성 함수에 의하여 유일하게 결정된다.\n\nProof Reference-Washington University\nMGF reference\n\nTheorem 3 \\[\n\\text{M}_{X+a}(t) =e^{at}\\text{M}_X(t)\n\\]\n\nProof) \\(\\text{E}(e^{t(x+a)})=\\text{E}(e^{at}e^{tx})=e^{at}\\text{E}(e^{tx})=e^{at}\\text{M}_X(t)\\)\n\nTheorem 4 \\[\n\\text{M}_{aX}(t) =\\text{M}_X(at)\n\\]\n\nProof) \\(\\text{E}(e^{atx})=\\text{E}(e^{at(x)})=\\text{M}_X(at)\\)\n\nExample) when \\(X \\sim N(0,1)\\), the mgf of \\(Y=aX+b\\) is ?\n\n\\[\n\\begin{aligned}\n  \\text{M}_Y(t)&=\\text{M}_{aX+b}(t)\\\\\n               &=e^{bt}\\text{M}_{X}(at)\\\\\n               &=e^{bt}e^{\\frac{a^2t^2}{2}}\\\\\n               &=e^{bt+\\frac{a^2t^2}{2}}\n\\end{aligned}\n\\]\n\nTheorem 5 서로 독립인 확률 변수 \\(X_1,X_2, ..., X_n\\) 의 적률 생성 함수가 각 각 \\(\\text{M}_{X_1}(t), \\text{M}_{X_2}(t), ..., \\text{M}_{X_n}(t)\\) 일 때, 확률 변수 \\(Y=X_1+X_2+...+X_n\\) 의 적률 생성함수 \\(\\text{M}_Y(t)\\) 는 \\(\\text{M}_{X_1}(t)\\text{M}_{X_2}(t) \\dots \\text{M}_{X_n}(t)\\) 이다.\n\\[\n\\begin{aligned}\n  \\text{M}_Y(t) &= E(e^{Yt})\\\\\n                &= E(e^{t(X_1+X_2+ ...+ X_n)})\\\\\n                &= E(e^{tX_1}e^{tX_2}\\dots e^{tX_n})\\\\\n                &= E(e^{tX_1})E(e^{tX_2})\\dots E(e^{tX_n}) \\because X_i \\text{ are independent}\\\\\n                &= \\text{M}_{X_1}(t)\\text{M}_{X_2}(t) \\dots \\text{M}_{X_n}(t)\n\\end{aligned}\n\\]\n\n\nExample) \\(X_1, X_2, ..., X_n\\) 가 서로 독립이고 parameter 가 각 각 \\(\\lambda_1, \\lambda_2, ..., \\lambda_n\\) 인 poisson 분포를 따른다면 \\(Y=X_1+X_2+...+X_n\\) 의 mgf는\n\n$$\n\\[\\begin{aligned}\n  \\text{M}_X(t) &= e^{\\lambda(e^t-1)}\\\\\n  \\text{M}_Y(t) &= E(e^{Yt})\\\\\n                &= E(e^{t(X_1+X_2+ ...+ X_n)})\\\\\n                &= E(e^{tX_1}e^{tX_2}\\dots e^{tX_n})\\\\\n                &= E(e^{tX_1})E(e^{tX_2})\\dots E(e^{tX_n}) \\because X_i \\text{ are independent}\\\\\n                &= \\text{M}_{X_1}(t)\\text{M}_{X_2}(t) \\dots \\text{M}_{X_n}(t)\\\\\n                &= e^{\\lambda_1(e^t-1)}+e^{\\lambda_2(e^t-1)}+\\dots+e^{\\lambda_n(e^t-1)}\\\\\n                &= e^{(\\lambda_1+\\lambda_2+\\dots+\\lambda_n)(e^t-1)}\\\\\n                &= e^{\\sum_{i=1}^{n}\\lambda_i(e^t-1)}\\\\\n\\end{aligned}\\]\n$$\n이다. 즉, Y는 parameter가 \\(\\sum_{i=1}^{n}\\lambda_i\\) 인 poisson 분포를 따른다. 뿐만 아니라, 정규 분포, poisson 분포, 카이제곱 분포의 경우 독립 변수들의 합으로 만든 분포도 같은 종류의 분포를 따르게 된다.\n\n\n\n\nStatistics Blog\nEngineering Blog\nDeep Learning Blog\nMachine Learning Blog\nMathematics Blog\nPatent Blog\nValidation Blog"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-03-21_mixed_model/index.html",
    "href": "docs/blog/posts/statistics/2023-03-21_mixed_model/index.html",
    "title": "template",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe radius of the circle is 10.\n\n\n\n1 Go to Project Content List\nProject Content List\n\n\n2 Go to Blog Content List\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/statistics/2023-03-25_MLE/index.html",
    "href": "docs/blog/posts/statistics/2023-03-25_MLE/index.html",
    "title": "Maximum Likelihood Estimation, Statistical Bias, and Point Estimation",
    "section": "",
    "text": "0.1 Definition\nTo talk about MLE (Maximum Likelihood Estimation), we need to recap the concepts and definitions of probability and likelihood. They are related but distinct concepts.\n\nprobability is a measure of the chance that an event will occur, given some prior knowledge or assumptions.\nlikelihood is a measure of the plausibility or compatibility of a particular set of model parameters, given the observed data.\n\n\n\n\n\n\n\nChance vs Plausibility (personal opinion)\n\n\n\n\nchance is a general term but closer term to statistics, which is used in the situation of the probability or likelihood of an event occurring based on randomness or uncertainty.\n\nplausibility chance is a term more often used in everyday life, which refers to the degree to which something is believable based on the available evidence or information.\n\n\n\n\nDefinition 1 Let \\(X_1, X_2, ..., X_n\\) be a set of iid random variables with pdf or pmf \\(f(x_i | \\theta)\\), where \\(\\theta\\) is a vector of unknown parameters. Then, the likelihood function is defined as the joint pdf or pmf of the observed data, given the values of the parameters:\n\\[\nL(\\theta | x_1, x_2, ..., x_n) = L(\\theta | \\mathbf x) = \\prod_{i=1}^n f(x_i | \\theta)\n\\]\n\nThe likelihood function is a function of the parameters \\(\\theta\\). The function measures the probability of observing a set of data, given the values of the parameters of a statistical model in order to estimate the values of the parameters by finding the values that maximize the likelihood function. The likelihood function is often used in the maximum likelihood estimation (MLE) method, where the MLE estimator is the set of parameter values that maximize the likelihood function.\nThe likelihood function is related to the concept of conditional probability. Given a set of observed data \\(x_1, x_2, ..., x_n\\), the likelihood function measures the probability of observing these data, assuming a particular set of parameter values. The likelihood function is not a probability distribution, but it can be used to derive a probability distribution for the parameters, known as the posterior distribution, using Bayes’ theorem.\n\n\n\n\n\n\nProbability\n\n\n\nProbability is a measure of the likelihood or chance that an event will occur, which is used to quantify uncertainty and randomness.\nprobability is a function that maps a real number mapped from a random variable into \\([0,1]\\). The probability function, denoted by \\(P\\), satisfies the following axioms:\n\nNon-negativity: For any event \\(A \\in \\Omega\\), \\(P(A) \\geq 0\\).\nNormalization: The probability of the entire sample space is 1, i.e., \\(P(\\Omega) = 1\\).\nAdditivity: For any two disjoint events \\(A, B \\in \\Omega\\), or \\(A \\cap B = \\emptyset\\), the probability of their union is equal to the sum of their individual probabilities, i.e., \\(P(A \\cup B) = P(A) + P(B)\\).\n\n\n\n\n0.1.1 Probability vs Likelihood\nProbability and likelihood are related but distinct concepts in statistics.\n\nProbability refers to the measure of the likelihood that a particular event will occur, scaled on \\([0,1]\\). It is calculated based on a known probability distribution (= some prior knowledge or assumptions) before the data is observed.\nOn the other hand, likelihood refers to the probability of observing a set of data given a particular set of parameter values in a statistical model. It is calculated based on the unknown parameters after the data is observed.\n\nThe likelihood function is used to estimate the values of the parameters by finding the parameter values that maximize the likelihood function.\nFor instance of a coin flip, the probability of getting heads on a coin flip is 0.5, regardless of whether the coin has been flipped or not (i.e., without data). In contrast, the likelihood of observing heads after a coin has been flipped depends on the parameter of interest. We need to find the parameter given data, the results of multiple coin flips.\nIf we want to estimate the probability of heads, we can use the maximum likelihood estimation (MLE) approach, which involves finding the value of the coin bias that maximizes the likelihood of observing the observed sequence of heads and tails. The likelihood of the data is calculated using the binomial distribution, which gives the probability of observing a certain number of heads, given the number of tosses and the coin bias. In this case, the likelihood function is a function of the coin bias, and the probability of heads is the value of the coin bias that maximizes the likelihood function.\n\n\n0.1.2 Relation between Likelihood and PMF or PDF\nThe likelihood function is closely related to pmf or pdf of the data, which is a function that describes the probability of observing a particular value or range of values for the data, given the model parameters. The pmf or pdf is a function of the data, not the parameters, and is often written as \\(f(x|\\theta)\\). The likelihood function is proportional to the pdf because is a product of pdfs when \\(X_i\\) is independent, but with the data fixed and the parameter values treated as variables.\n\n\nCode\nlibrary(tidyverse)\n\n# Probability of getting heads on a fair coin flip\nprob <- 0.5\n\n# likelihood\n## Simulate a coin flip with a biased coin\nset.seed(123) # Set random seed for reproducibility\nn <- 100 # flipping numbers\np <- 0.2 # Probability of getting heads\nx <- rbinom(n, size = 1, prob = p) # Simulate n coin flips\nx%>%head(10)\n\n\n [1] 0 0 0 1 1 0 0 1 0 0\n\n\nCode\n## Calculate the likelihood of observing the data given the parameter value p\nlikelihood <- prod(dbinom(x, size = 1, prob = p))\nlikelihood%>%round()\n\n\n[1] 0\n\n\nIn this example, prob is the assumed probability of getting heads on a fair coin flip. The probability of getting heads on a fair coin flip is 0.5, which is a fixed value that does not depend on any specific data. On the other hand, p is the probability of getting heads for the biased coin that we are simulating. The likelihood of observing a set of coin flips depends on the parameter value, which is unknown (actually, we know that it was \\(p=0.2\\)), and the observed data. We simulate a set of 100 coin flips with a biased coin that has a probability of 0.2 of getting heads. We then calculate the likelihood of observing this data given the parameter value of 0.2, which is the product of the probability mass function for each flip.\nHowever, in a real-world scenario, we would not know the true value of p and we would need to estimate it based on the observed data. By finding the Maximum Likelihood Estimation (MLE) of p, we are estimating the value of p that is most likely to have generated the observed data. To find MLE of p that maximizes the likelihood function, we can use numerical optimization methods.\nAs n increases, the product term in the likelihood function \\(\\prod_{i=1}^{n} f(x_i; \\theta)\\), where \\(f\\) is the pdf or pmf of the distribution being used, can become very small (since it is a product of values less than 1) and may result in numerical underflow (i.e., the product becomes so small that it rounds down to 0 in computer calculations). In practice, we typically take the logarithm of the likelihood function, called the log-likelihood, to avoid this issue:\n\\[\n\\log L(\\theta|x_1, x_2, ..., x_n) = \\sum_{i=1}^{n} \\log f(x_i; \\theta)\n\\]\nUsing the logarithm allows us to convert the product of small probabilities into a sum of log-probabilities, which are typically easier to work with numerically and mathematically. In this case, as n increases, the sum term in the log-likelihood can decrease (since it is a sum of negative values), but the decrease may not be as severe as in the product term of the likelihood function because of the log scale converting very small or large values into larger or smaller values .\n\nDefinition 2 The MLE estimator \\(\\hat{\\theta}\\) is the value of the parameter vector that maximizes the likelihood function:\n\\[\n\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} L(\\theta | x_1, x_2, ..., x_n)\n\\]\nor equivalently, maximizes the log-likelihood function:\n\\[\n\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\log L(\\theta | x_1, x_2, ..., x_n)\n\\]\n\nThe Maximum Likelihood Estimation (MLE) is a method of estimating the parameters of a statistical model by finding the values of the parameters that maximize the likelihood function. The likelihood function is the probability of observing the data, given the parameters of the model. The MLE estimator is the set of parameter values that maximize the likelihood function. In other words, the MLE is the set of parameter values that make the observed data most probable, given the assumed probability distribution. The likelihood function is typically the product or the sum (depending on whether the observations are assumed to be independent or not) of the probabilities or probability densities of the observations, evaluated at the values of the parameters.\nThe MLE estimator has desirable statistical properties, such as consistency, efficiency, and asymptotic normality, under certain regularity conditions on the likelihood function and the parameter space. However, it is important to note that the MLE is not always the best estimator for a given problem, and other estimation methods may be more appropriate depending on the specific characteristics of the data and the model.\nThe likelihood function is the joint probability density (or mass) function of the data, viewed as a function of the parameters, and we find the maximum of this function by differentiating it with respect to the parameters and setting the derivative to zero.\n\n\n\n0.2 MLE of OLS\nIn a linear regression, the maximum likelihood estimate of the ordinary least squares (OLS) coefficients is equivalent to the least squares estimate. To derive this, we assume that \\(\\epsilon_i \\sim N(0,\\sigma^2)\\) and that the observations are independent. Then, the likelihood function for the data \\(Y = (Y_1, Y_2, \\dots, Y_n)\\) is:\n\\[\nL(Y|\\theta) =L(Y|\\beta,\\sigma^2) = (2\\pi\\sigma^2)^{-\\frac{n}{2}} e^{-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - X_i\\beta)^2}\n\\]\nwhere \\(X_i\\) is the \\(i\\) th row of the design matrix \\(X\\) and \\(\\beta\\) is the vector of regression coefficients.\nTo find the maximum likelihood estimates of \\(\\beta\\) and \\(\\sigma^2\\), we maximize the likelihood function with respect to these parameters. Taking the log of the likelihood function and simplifying, we obtain:\n\\[\n\\log L(Y|\\theta) = \\log L(Y|\\beta,\\sigma^2) = -\\frac{n}{2} \\log (2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - X_i\\beta)^2\n\\]\nTo maximize this function with respect to \\(\\beta\\), we differentiate with respect to \\(\\beta\\) and set the derivative to zero, \\(\\frac{\\partial}{\\partial \\theta} \\log L(Y|\\theta) = 0\\):\nSolving for \\(\\beta\\), we obtain:\n\\[\n\\frac{\\partial}{\\partial \\theta} \\log L(Y|\\beta,\\sigma^2) = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n 2X_i(Y_i - X_i\\beta) = 0\n\\]\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n\\] , which is the OLS estimate of \\(\\beta\\).\nSolving for \\(\\sigma^2\\), we differentiate with respect to \\(sigma^2\\) and set the derivative to zero:\n\\[\n\\frac{\\partial}{\\partial \\sigma^2} log L(Y|\\beta,\\sigma^2) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^n (Y_i - X_i\\beta)^2 = 0\n\\] \\[\n\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (Y_i - X_i\\beta)^2}{n}\n\\]\nwhich is the OLS estimate of \\(\\sigma^2\\).\nTherefore, we see that the maximum likelihood estimates of \\(\\beta\\) and \\(\\sigma^2\\) in linear regression with normally distributed errors are equivalent to the OLS estimates of these parameters.\n\n\n0.3 Statistical Bias\nStatistical bias refers to a systematic error or deviation in the results of a statistical analysis that is caused by factors other than chance. A biased estimator is one that consistently produces estimates that are systematically different from the true value of the parameter being estimated.\n\n\n\n\n\n\nThere are 5 Types of bias\n\n\n\nThe above article discusses five types of statistical bias that analysts, data scientists, and other business professionals should be aware of to minimize their effects on the final results.\n\nselection bias: data selection methods are not truly random, leading to unequal representation of the population.\nbias in assignment: pre-existing differences between groups in an experiment can affect the outcome, a.k.a allocation bias, treatment assignment bias, or exposure assignment bias.\nconfounders: additional variables not accounted for in the experimental design can impact the results.\nself-serving bias: individuals tend to downplay undesirable qualities and overemphasize desirable ones a.k.a cognitive bias. In other words, people tend to take credit for their successes and blame outside factors for their failures.\nexperimenter expectations: researchers can unconsciously influence the data through verbal or non-verbal cues.\n\nBeing aware of these biases can lead to better models and more reliable insights for data-backed business decisions.\nSource: Article Written by Jenny Gutbezahl\n\n\nIt is important to detect and correct for bias in statistical analyses, as biased estimates can lead to incorrect conclusions and decisions. One way to correct for bias is to use an unbiased estimator, which is one that has a zero bias, i.e., its expected value is equal to the true value of the parameter being estimated.\n\nDefinition 3 An estimator \\(\\hat{\\theta}\\) is said to be biased if\n\\[\n\\operatorname{E}(\\hat{\\theta})\\ne \\theta\n\\]\nwhere \\(\\operatorname{E}(\\hat{\\theta})\\) is the expected value of the estimator \\(\\hat{\\theta}\\), and \\(\\theta\\) is the true value of the parameter being estimated.\n\nAn estimator is said to be unbiased if its expected value is equal to the true value of the parameter being estimated. In other words, an estimator is unbiased if, on average, it gives an estimate that is equal to the true value of the parameter."
  },
  {
    "objectID": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html",
    "href": "docs/blog/posts/statistics/categorical_data/2023-03-17_introduction/index.html",
    "title": "Categorical Data Analysis",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nVisualization Methods\n\nfor EDA: visualize patterns, trends, anomalies in data\nfor model diagnostic methods: visualize to assess violations of assumptions\nfor summary methods: visualize to provide an interpretable summary of data\n\napply theory to practice\n\nconert research questions into statistical hypotheses and models\nlook into the difference between non-parametric (ex. fisher exact test) vs parametric (ex. \\(\\chi^2 test for independence\\)) vs model-based methods (ex. logistic regression)\nfor summary methods: visualize to provide an interpretable summary of data\n\n\n\n\n\n\ncategorical (or frequency) data consist of a discrete set of categories, which may be ordered or unordered.\n\nunordered\n\ngener: {male, female, transgender}\nmarital status: {never married, married, separated, divorced, widowed}\nparty preference: {NDP, liberal, conservative, green}\ntreatment improvement: {none, some, marked}\n\nordered\n\nage group: {0s,10s,20s,30s, …}\nnumber of children: {0, 1 , 2 ,3, …} ## Structures\n\n\n\nCategorical data appears in various forms like:\n\ntables\n\none way\ntwo way\nthree way\n\nmatrices\narray\ndata frames\n\ncase form\nfrequency form"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html",
    "href": "docs/blog/posts/statistics/guide_map/index.html",
    "title": "Content List, Statistics",
    "section": "",
    "text": "(Draft)"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#basic",
    "href": "docs/blog/posts/statistics/guide_map/index.html#basic",
    "title": "Content List, Statistics",
    "section": "Basic",
    "text": "Basic\n\nProbability Theory\n\n2023-02-05, Set Theory\n2023-02-05, [Basics of Probability Theory - Axiomatic Foundations]\n2023-02-05, [Basics of Probability Theory - Calculus of Probabilities]\n2023-02-05, Basics of Probability Theory - Probability\n2023-02-05, Conditional Probability\n2023-02-05, [Independence]\n2023-02-05, Bayes’ Rule\n2023-02-05, Random Variable\n1111-11-11, Probability Distribution\n\n\n\nTransformations and Expectations\n\n2023-02-21, Transformation of Random Variables\n1111-11-11, Expected Value vs Realizaed Value\n1111-11-11, Variance\n1111-11-11, Covariance and Correlation\n2023-02-28, Moment Generating Function, MGF\n\n\n\nExponential Family Distributions\n\nDiscrete Random Variable\n\n2023-02-27,Bernoulli Distribution\n2023-02-28,Binomial Distribution\n2023-03-01,Poisson Distribution\n2023-03-01,Geometric Distribution\n1111-11-11, Hypergeometric Distribution\n\nContinuous Random Variable\n\n1111-11-11, Normal Distribution\n1111-11-11, Exponential Distribution\n1111-11-11, Beta Distribution\n1111-11-11, Chi-squared Distribution\n\n1111-11-11,\n\n\n\nMultiple Random Variables\n\n1111-11-11, Joint Distribution and Marginal Distribution\n\n\n\nPoint Estimation\n\n1111-11-11, Estimation Methods - Method of Moments\n2023-03-29, Estimation Methods - Maximum Likelihood Estimation & Statistical Bias\n1111-11-11, Estimation Methods - Bayesian Estimation\n1111-11-11, Estimation Methods - The EM Algorithm\n1111-11-11, Evaluation Methods of Estimators - Mean Squared Error\n1111-11-11, Evaluation Methods of Estimators - Best Unbiased Estimators\n1111-11-11, Evaluation Methods of Estimators - Sufficiency and Unbiasedness\n1111-11-11, Evaluation Methods of Estimators - Loss Function Optimality"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#hypothesis-testing",
    "href": "docs/blog/posts/statistics/guide_map/index.html#hypothesis-testing",
    "title": "Content List, Statistics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n1111-11-11, Hypothesis Testing\n1111-11-11, Permutation Test\n\n\nMethods of Finding Tests\n\n1111-11-11, Likelihood Ratio Tests\n1111-11-11, Bayesian Tests\n1111-11-11, Union-Intersection and Intersection-Union Tets\n\n\n\nMethods of Evaluating Tests\n\n1111-11-11, Power\n1111-11-11, Error Proabilities and the Power Function\n1111-11-11, Most Powerful Tests\n2022-12-28, p-values\n1111-11-11, Loss Function Optimality\n1111-11-11, Multiple Testing\n1111-11-11, Sample Size Calculation\n1111-11-11, A/B Testing\n2023-01-07, ANOVA\n\n2023-01-27, ANCOVA\n2023-01-27, repeated measures ANOVA\n2023-01-28, MANOVA"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#categorical-data-analysis",
    "href": "docs/blog/posts/statistics/guide_map/index.html#categorical-data-analysis",
    "title": "Content List, Statistics",
    "section": "Categorical Data Analysis",
    "text": "Categorical Data Analysis\n\n1111-11-11, Introduction\n1111-11-11,\n1111-11-11,\n2022-12-28,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n1111-11-11,\n2023-01-07,\n2023-01-27,\n2023-01-27,\n2023-01-28,"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#regression",
    "href": "docs/blog/posts/statistics/guide_map/index.html#regression",
    "title": "Content List, Statistics",
    "section": "Regression",
    "text": "Regression\n\n1111-11-11, Least Square and Simple Linear Regression\n1111-11-11, Multiple Linear Regression\n\n\nGeneralized Linear Models\n\n1111-11-11, Logistic Regression\n1111-11-11, Multinomial Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression\n1111-11-11, Poisson Regression"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#longitudinal-data-analysis",
    "href": "docs/blog/posts/statistics/guide_map/index.html#longitudinal-data-analysis",
    "title": "Content List, Statistics",
    "section": "Longitudinal Data Analysis",
    "text": "Longitudinal Data Analysis\n\n2023-03-23, LDA (1) - Intro\n2023-03-23, LDA (2) - Concepts & Covariance Models\n2023-03-25, LDA (3) - WLS & REML\n2023-03-25, LDA (4) - Respiratory Infection Data Example\n2023-03-28, LDA (5) - Epileptic Seizures Data Example\n\n\nMixed Models\n\n1111-11-11, Linear Mixed Models"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#generalized-additive-models",
    "href": "docs/blog/posts/statistics/guide_map/index.html#generalized-additive-models",
    "title": "Content List, Statistics",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models"
  },
  {
    "objectID": "docs/blog/posts/statistics/guide_map/index.html#survival-analysis",
    "href": "docs/blog/posts/statistics/guide_map/index.html#survival-analysis",
    "title": "Content List, Statistics",
    "section": "Survival Analysis",
    "text": "Survival Analysis\n\n1111-11-11, Cox-Hazard Model"
  },
  {
    "objectID": "docs/blog/posts/statistics/LDA/2_covariance_model.html",
    "href": "docs/blog/posts/statistics/LDA/2_covariance_model.html",
    "title": "LDA (2) - Concept & Covariance Models",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\\(y_{ij}\\) : the univariate response (i.e. scalar) for the \\(i\\) th subject at the \\(j\\) th occasion or measurement\n\nlater when I use the vector case, I will re-define this notation, but focus on the scalar case for now.\n\n\\(x_{ij}\\) : the predictor at time \\(t_{ij}\\), which is either a scalr or vector.\n\na scalar case: \\(x_{ij}\\) where \\(i\\) is the \\(i\\) th subject, and \\(j\\) is the \\(j\\) th measurement.\na vector case: \\(x_{ijk}\\) where \\(i\\) is the \\(i\\) th subject, \\(j\\) is the \\(j\\) th measurement, and \\(k \\in [1,p]\\) is the \\(k\\) th predictor.\nsometimes, covariate for different measurements could be the same. In this case, the notation could be written in \\(x_{i}\\)\n\nex) a gender does not change over time in the most cases.\n\n\n\\(i=1, \\dots, m\\) : i is the index for the \\(i\\) th subject\n\\(j=1, \\dots, n_i\\) : j is the index for the \\(j\\) th measurement of the \\(i\\) th subject\n\n\\({n_i}\\) is the number of measurements of the \\(i\\) th subject, each \\({n_i}\\) does not have to the same.\nbalanced desgin: \\({n_i}\\) is the same.\nunbalanced desgin: \\({n_i}\\) is different.\n\n\\(\\mathbf y_i\\) : a vector (not a matrix), \\((y_{i1},y_{i2},\\dots ,y_{in_i})\\) of the \\(i\\) th subject\n\\(\\mathbf Y\\) : the reponse matrix\n\\(\\mathbf X\\) : the predictor matrix\n\\(\\text{E}(y_{ij})\\) : \\(\\mu_{ij}\\)\n\\(\\text{E}(\\mathbf y_i)\\) : \\(\\mathbf \\mu_{i}\\)\n\\(\\text{Var}(\\mathbf y_i)\\) : \\(\\text{Var}(\\mathbf y_i)\\) is a variance-covariance matrix of the different measurement for the \\(i\\) th subject\n\nfor now, we do not care of the variance covariance of the different subjects because we assume that the measurements of different subjects are indpendent. \\[\n\\begin{bmatrix}\n\\text{Var}(y_{i1}) & \\text{Cov}( y_{i1}, y_{i2}) & \\dots & \\text{Cov}( y_{i1}, y_{in_i}) \\\\\n                           & \\text{Var}( y_{i2}) & \\dots & \\text{Cov}( y_{i2}, y_{in_i}) \\\\\n                             &                           & \\ddots & \\vdots \\\\\n                             &&                            \\dots & \\text{Var}( y_{in_i})\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\nthe measurements for the same subject are not independent.\nthe measurements for the different subject are independent.\nsome correlation structures of the different measurements.\n\n\n\n\n\nMarginal Models\n\n\\(\\text{E}(y_{ij}) = \\mathbf x_{ij} \\mathbf \\beta\\)\n\\(\\text{Var}(\\mathbf y_i)= \\mathbf V_i\\)\nto build a marginal model, we just need info on the 3 things\n\nthe distribution : a multivariate normal distribution\nmean and variance-covariance\n\n\\(\\beta\\) is fixed. That’s why we call this marginal models ‘fixed effect’\n\n\n\n\n\n\n\n\nRecall\n\n\n\nWe find MLE for the linear regression with the 3 things: the normal distribution (iid), \\(\\mu\\) and \\(\\sigma^2\\)\n\n\n\nMixed Effects Models\n\n\\(\\text{E}(y_{ij}|\\mathbf \\beta_i) = \\mathbf x_{ij} \\mathbf \\beta_i\\)\n\\(\\mathbf \\beta_i = \\mathbf \\beta (\\text{fixed effect}) + \\mathbf u_i (\\text{subject-specific random effect})\\)\n\\(\\mathbf \\beta_i\\) is a random coefficient specific for the \\(i\\) th subject, That’s why we call this mixed effect models ‘random effect’\nsubject-specific random effect: differenct subjects have different \\(\\mathbf \\beta_i\\)\n\nTransition Models\n\n\\(\\text{E}(y_{ij}|y_{i,j-1},\\dots,y_{i,1},\\mathbf x_{ij})\\)\nMarkov Process: the response variable in the previous time point will affect the measurement in the current time point.\n\n\n\n\nConsider an example of a simple linear model (i.e., a univaiable linear model) \\[\ny_{ij}=\\beta_0+\\beta_1t_{ij} + \\epsilon_{ij}\n\\]\n\nmean part: \\(\\text{E}(y_{ij})\\)\nvariance part: \\(\\text{Var}(\\mathbf y_{i})=\\text{Var}(\\mathbf \\epsilon_{i})\\)\n\nmore often, a correlation matrix is used in LDA because correlation is more interpretable.\n\n\n\\[\n\\text{Corr}(\\mathbf y_i) =\n\\begin{bmatrix}\n1 & \\rho_{12}& \\dots & \\rho_{1n_i} \\\\\n\\rho_{21} & 1 & \\dots & \\rho_{2n_i} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n\\rho_{n_i1} & \\rho_{n_i2}& \\dots & 1\n\\end{bmatrix}\n\\]\n\nin this correlation matrix, there are \\(\\frac{n(n-1)}{2}\\) parameters to estimate\nin the mean part, there are 2 parameters, \\(\\mathbf \\beta\\) to estimate Likewise, the number of the estimators depends on the number of the measurements and the covriates.\n\nIn LDA, since the responses are multiple, we need to look into the correlation characteristics.\n\n\n\nIn empirical observations about the nature of the correlation among repeated measures,\n\ncorrelations among the repeated measures are usually positive\ncorrelations tend to decrease with increasing time separation\ncorrelations among repeated measures rarely approach zero\ncorrelations between any pair of repeated meausres regardless of distance in time is constrained by the reliability of the measurement process.\n\nif the measurement process is not very reliable or consistent, then even if two measurements are taken close together in time, their correlation will not be very strong. Similarly, if the measurement process is highly reliable or consistent, then two measurements taken far apart in time may still be highly correlated. Reliability refers to the degree to which a measurement process produces consistent and accurate results over time.\n\n\n\n\n\nThere are 2 types of covariance structure: unbalanced design and balanced design. For now, let’s focus on the balanced design.\n\n\n\nobservations for each subject are not made on the same grid\nthese observations can be made at different time points and different numbers of observations may be made for each subject.\nMissing observations falls into this category.\n\n\n\n\n\nobservations for each subject are made on the same grid and there is no missing data.\n\nnumber and timing of the repeated measurements are the same for all individuals.\n\nThen, \\(t_{ij}\\) can be denoted as \\(t_j\\) where \\(j \\in 1, \\dots, n\\) because the size of the measurements is the same (\\(n_i\\) is the same)\nThe covariance of the response variable \\(\\mathbf Y_{m\\times n}\\) :\n\n$$\n\\[\\begin{aligned}\n  \\text{Cov}(\\mathbf Y)\n  &=\\text{Cov}(\\mathbf y_1,\\dots,y_m) \\\\\n  &=\n  \\begin{bmatrix}\n    \\text{Var}(\\mathbf y_1) & 0 & \\dots & 0 \\\\\n    0 & \\text{Var}(\\mathbf y_2) & \\dots & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & \\dots & \\text{Var}(\\mathbf y_m)\n  \\end{bmatrix}\\\\\n  &=\n  \\begin{bmatrix}\n    \\begin{bmatrix}\n    \\text{Var}(y_{11}) & \\text{Cov}( y_{11}, y_{12}) & \\dots & \\text{Cov}( y_{11}, y_{1n_1}) \\\\\n                               & \\text{Var}( y_{12}) & \\dots & \\text{Cov}( y_{12}, y_{1n_1}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{1n_1})\n\\end{bmatrix} & 0 & \\dots & 0 \\\\\n    0 & \\begin{bmatrix}\n    \\text{Var}(y_{21}) & \\text{Cov}( y_{21}, y_{22}) & \\dots & \\text{Cov}( y_{21}, y_{in_2}) \\\\\n                               & \\text{Var}( y_{22}) & \\dots & \\text{Cov}( y_{22}, y_{in_2}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{2n_2})\n\\end{bmatrix} & \\dots & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & \\dots & \\begin{bmatrix}\n    \\text{Var}(y_{m1}) & \\text{Cov}( y_{m1}, y_{m2}) & \\dots & \\text{Cov}( y_{m1}, y_{mn_m}) \\\\\n                               & \\text{Var}( y_{m2}) & \\dots & \\text{Cov}( y_{m2}, y_{mn_m}) \\\\\n                                 &                           & \\ddots & \\vdots \\\\\n                                 &&                            \\dots & \\text{Var}( y_{mn_m})\n\\end{bmatrix}\n\n  \\end{bmatrix} \\\\\n  &=\n  \\begin{bmatrix}\n    \\Sigma_1 & 0 & \\dots & 0 \\\\\n    0 & \\Sigma_1 & \\dots & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & \\dots & \\Sigma_m\n  \\end{bmatrix}\n\\end{aligned}\\]\n$$\nIf we assume the covariance matrices for different subjects are the same, we can denote \\(\\text{Cov}(\\mathbf Y)=\\Sigma\\).\n\n\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho & \\rho & \\dots & \\rho \\\\\n    \\rho & 1 & \\rho & \\dots & \\rho \\\\\n    \\rho & \\rho & 1 & \\dots & \\rho \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho & \\rho & \\rho & \\dots & 1\n  \\end{bmatrix}\n\\]\n\ncompound symmetry is a.k.a Exchangeable\nAssume variance is constant across visits (say \\(\\sigma^2\\))\nAssume correlation between any two visits are constant (say \\(\\rho\\)).\nParsimonious: there are two parameters in the covariance, \\(\\sigma^2\\) and \\(\\rho\\) (computational benefit)\nWithout any contraint on \\(\\sigma^2\\), you will get closed form estimate.\nCovariance variance matrix is plugged into likelihood function to estimate 3 kinds of parameters \\(\\sigma^2\\), \\(\\rho\\), and \\(\\beta\\)\nThis structure is so parsimonuous that it could be unrealistic: not commonly used\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho_1 & \\rho_2 & \\dots & \\rho_{n-1} \\\\\n    \\rho_1 & 1 & \\rho_1 & \\dots & \\rho_{n-2} \\\\\n    \\rho_2 & \\rho_1 & 1 & \\dots & \\rho_{n-3} \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho_{n-1} & \\rho_{n-2} & \\rho_{n-3} & \\dots & 1\n  \\end{bmatrix}\n\\]\n\nToeplitz structure is more flexible than compound symmetry\nAssume variance is constant across visits and \\(\\text{Corr}(y_{ij}, y_{i,j+k}) = \\rho_k\\).\nAssume correlation among responses at adjacent measurements is constant.\nOnly suitable for measurements made at equal intervals of time between different measurement.\nWithout any contraint on \\(\\sigma^2\\), you will get closed form estimate.\nToeplitz covariance has free \\(n\\) parameters to estimate (\\(1\\) for variance and \\(n-1\\) correlation parameters)\nThe larger time differences, the smaller its correlations\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho^1 & \\rho^2 & \\dots & \\rho^{n-1} \\\\\n    \\rho^1 & 1 & \\rho^1 & \\dots & \\rho^{n-2} \\\\\n    \\rho^2 & \\rho^1 & 1 & \\dots & \\rho^{n-3} \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    \\rho^{n-1} & \\rho^{n-2} & \\rho^{n-3} & \\dots & 1\n  \\end{bmatrix}\n\\]\n\nA special case of toeplitz structure with \\(\\text{Corr}(y_{ij},y_{i,j+k})=\\rho^k\\)\nsimpler than toeplitz, only 2 parameters\nOnly suitable for measurements made at equal intervals of time between different measurement.\n\n\n\n\n\\[\n  \\text{Cov}(\\mathbf y_i)=\n  \\sigma^2  \n  \\begin{bmatrix}\n    1 & \\rho^1 & 0 & \\dots & 0 \\\\\n    \\rho^1 & 1 & \\rho^1 & \\dots & 0 \\\\\n    0 & \\rho^1 & 1 & \\dots & 0 \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n    0 & 0 & 0 & \\dots & 1\n  \\end{bmatrix}\n\\]\nLook at the more general case of the banded structure in Wiki.\n\nAssume correlation is 0 beyond some specified interval.\nCan be combined with the previous patterns.\nVery strong assumption about how quickly the correlation decays to 0 with increasing time separation.\n\n\n\n\n\nA generalization of autoregressive pattern\nThe most general and reasonable structure\nSuitable for unevenly spaced measurements, take actual time points (time difference), the larger time difference the smaller correlation\nAssumption that the variance of different measurements over time is the same, which can be easily generalized. You can put different variance on the diagonal.\nLet \\(\\{t_{i1},\\dots,t_{in_i}\\}\\) denote the observation times for the \\(i\\) th individual. Then, the correlation is \\(\\text{Corr}(Y_{ij} ,Y_{ik}) = \\rho^{|t_{ij}-t_{ik}|}\\)\nCorrelation decreases exponentially with the time separations between them.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Content List"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html",
    "title": "FDA Software Validation Guidance Summary",
    "section": "",
    "text": "I am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto.\nThe FDA validation guidance document is a bit difficult to understand because its explanations provides abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\n\n\n2022-12-28, download this article as PDF\n2022-12-28, summary with diagrams\n\n\n\n\nFDA: General Principles of Software Validation\n\n\n\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\n\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#purpose",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#purpose",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.1 Purpose",
    "text": "2.1 Purpose\nThe purpose is to make a sketch of general validation principle of the validation of medical device software or software used to design or develop."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#scope",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#scope",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.2 Scope",
    "text": "2.2 Scope\nThe scope of this guidance is broad. The important activities for the software validation include at least:\n\nplanning,\nverfication,\ntesting,\ntraceability, and\nconfiguration management.\n\nAll of the activities above should be\n\nintegrated\nbe able to describe software life cycle management and\nbe able to describe software risk management.\n\nThe software validation and verification activities should be focused into the entire software life cycle. (It does not necessarily mean that the activies must follow any technical models.)\nThe guidance is applicable to any software related to a regulated medical device and anyone who is employed in a bio or medical industry.\n\n2.2.1 The Least Burdensome Approach\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\n2.2.2 Regulatory Requirements for Software Validation\n\nSoftware validation: a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997. (See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\nSpecific requirements for validation of device software are found in 21 CFR §820.30(g). Other design controls, such as planning, input, verification, and reviews, are required for medical device software. (See 21 CFR §820.30.)\ncomputer systems used to create, modify, and maintain electronic records and to manage electronic signatures are also subject to the validation requirements. (See 21 CFR §11.10(a).)\n\n\n2.2.2.1 Objective\nThe objective of software validation is to ensure:\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\n2.2.2.2 What to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\n\n2.2.3 Quality System Regulation vs Pre-market Submissions\nThis document does not address any specific requirements but general ones. Specific issues should be addressed to\n\nthe Office of Device Evaluation (ODE),\nCenter for Devices and Radiological Health (CDRH)\nthe Office of Blood Research and Review,\nCenter for Biologics Evaluation and Research (CBER). See the references in Appendix A for applicable FDA guidance documents for pre-market submissions."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#context-for-software-validation",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#context-for-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.3 Context for Software Validation",
    "text": "2.3 Context for Software Validation\n\nValidation elements that FDA expects to do for the Quality System regulation, using the principles and tasks are listed in Sections 4 and 5.\nAdditional specific information is available from many of the references listed in Appendix A\n\n\n2.3.1 Definition and Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology between the medical device Quality System regulation and the software industry:\n\nrequirements,\nspecification,\nverification, and\nvalidation.\n\n\n\n2.3.1.1 Requirements and Specifications\nThe Quality System regulation states\n\nthat design input requirements must be documented and\nthat specified requirements must be verified\n\nBut, the regulation does not further clarify the distinction between the terms “requirement” and “specification.”\n\nRequirement\n\ncan be any need or expectation for a system or for its software.\nreflects the stated or implied needs of the customer: requirements may be\n\nmarket-based,\ncontractual,\nstatutory, or\nan organization’s internal requirements.\n\nvarious examples of requirements\n\ndesign, functional, implementation, interface, performance, or physical requirements\n\nSoftware requirements derived from the system requirements for those aspects of system functionality\nSoftware requirements are typically stated in functional terms and are defined, refined, and updated as a development project progresses.\nSuccess in accurately and completely documenting software requirements is a crucial factor in successful validation of the resulting software.\n\nSpecification\n\ndefined as “a document that states requirements.” (See 21 CFR §820.3(y).)\nIt may refer to or include drawings, patterns, or other relevant documents\nIt usually indicates the means and the criteria whereby conformity with the requirement can be checked.\nVarious examples of written specifications\n\nsystem requirements specification,\nsoftware requirements specification,\nsoftware design specification,\nsoftware test specification,\nsoftware integration specification, etc.\n\nAll of these documents are design outputs for which various forms of verification are necessary.\n\n\n\n\n2.3.1.2 Verifiaction and Validation\nThe Quality System regulation is harmonized with ISO 8402:1994, which treats “verification” and “validation” as separate and distinct terms.\n\nSoftware verification\n\nIt provides objective evidence that the design outputs of a particular phase of the software development life cycle meet all of the specified requirements for that phase.\nIt looks for\n\nconsistency,\ncompleteness, and\ncorrectness of the software and its supporting documentation\n\nSoftware testing\n\nverification activities intended to confirm that software development output meets its input requirements.\n\nTypes of verification activities include\n\nvarious static and dynamic analyses,\ncode and document inspections,\nwalkthroughs, and other techniques.\n\n\nSoftware Validation\n\nConfirmation by examination and provision of the following objective evidence:\nEvidence 1: software specifications conform to user needs and intended uses, and\nEvidnece 2: the particular requirements implemented through software can be consistently fulfilled.\nEvidnece 3: all software requirements have been implemented correctly and completely and are traceable to system requirements.\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device.\n\nDifficulty in Software verification and validation\n\na developer cannot test forever, and\nit is difficult to know how much evidence is enough.\nIn large measure, software validation is a matter of developing a “level of confidence” that the device meets all requirements and user expectations for the software automated functions and features of the device.\nConsiderations for an acceptable level of confidence\n\nmeasures such as defects found in specifications documents,\nestimates of defects remaining,\ntesting coverage, and other techniques are all used to develop before shipping the product.\nHowever, a level of confidence varies depending upon the safety risk (hazard) posed by the automated functions of the device. (Info on safety risk is found in Section 4 and in the international standards ISO/IEC 14971-1 and IEC 60601-1-4 referenced in Appendix A).\n\n\n\n\n\n2.3.1.3 IQ/OQ/PQ\nIQ/OQ/PQ are the terminology related to user site software validation\n\nInstallation qualification (IQ)\nOperational qualification (OQ)\nPerformance qualification (PQ).\n\nDefinitions of these terms may be found in FDA’s Guideline on General Principles of Process Validation, dated May 11, 1987, and in FDA’s Glossary of Computerized System and Software Development Terminology, dated August 1995. Both FDA personnel and device manufacturers need to be aware of these differences in terminology as they ask for and provide information regarding software validation.\n\n\n\n2.3.2 Software Development as Part of System Design\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nthe user’s needs\nintended uses from which the product is developed.\n\nA primary goal of software validation is to then demonstrate that all completed software products comply with all documented software and system requirements.\n\n\n2.3.3 Software Is Different from Hardware\nSoftware engineering needs an even greater level of managerial scrutiny and control than does hardware engineering.\n\n\n2.3.4 Benefits of Software Validation\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nSoftware validation can also reduce long term costs by making it easier and less costly to reliably modify software and revalidate software changes.\n\n\n\n2.3.5 Design Review\nDesign reviews are documented, comprehensive, and systematic examinations of a design to evaluate\n\nthe adequacy of the design requirements,\nthe capability of the design to meet these requirements, and\nto identify problems.\n\nDesign review is a primary tool for managing and evaluating development projects.\n\nIt is strongly recommended that it should be formal design because it is more structured than the informal one.\nIt includes participation from others outside the development team.\nIt may review reference or include results from other formal and informal reviews.\nDesign reviews should include\n\nexamination of development plans,\nrequirements specifications,\ndesign specifications,\ntesting plans and procedures,\nall other documents and activities associated with the project,\nverification results from each stage of the defined life cycle, and\nvalidation results for the overall device.\n\nThe Quality System regulation requires that at least one formal design review be conducted during the device design process. However, it is recommended that multiple design reviews be conducted\n\n(e.g., at the end of each software life cycle activity, in preparation for proceeding to the next activity).\n\nFormal design reviews documented should include:\n\nthe appropriate tasks and expected results, outputs, or products been established for each software life cycle activity\ncorrectness, completeness, consistency, and accuracy\nsatisfaction for the standards, practices, and conventions of that activity\nestablishment of a proper basis for initiating tasks for the next software life cycle activity"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#principles-of-software-validation",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#principles-of-software-validation",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.4 Principles of Software Validation",
    "text": "2.4 Principles of Software Validation\n\n2.4.1 Requirements\nA documented software requirements specification provides a baseline for both validation and verification. The software validation process must include an established software requirements specification (Ref: 21 CFR 820.3(z) and (aa) and 820.30(f) and (g)).\n\n\n2.4.2 Defect Prevention\nIn order to establish that confidence, software developers should use a mixture of methods and techniques to prevent software errors and to detect software errors that do occur.\n\n\n2.4.3 Time and Effort\nPreparation for software validation should begin early, i.e., during design and development planning and design input. The final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\n2.4.4 Software Life Cycle\n\nSoftware validation takes place within the environment of an established software life cycle.\nThe software life cycle contains software engineering tasks and documentation necessary to support the software validation effort.\nspecific verification and validation tasks need to be appropriate for the intended use of the software\n\n\n\n2.4.5 Plans\n\nThe software validation process is defined and controlled through the use of a plan.\nThe software validation plan defines “what” is to be accomplished through the software validation effort.\nSoftware validation plans specify areas such as\n\nscope,\napproach,\nresources,\nschedules and the types and extent of activities,\ntasks, and\nwork items.\n\n\n\n\n2.4.6 Procedures\nThe software validation process is executed through the use of procedures. These procedures establish “how” to conduct the software validation effort. The procedures should identify the specific actions or sequence of actions that must be taken to complete individual validation activities, tasks, and work items.\n\n\n2.4.7 Software Validation After a Change\n\nDue to the complexity of software, a small local change may have a significant global system impact.\nIf a change exists in the software, the whole validation status of the software needs to be re-established.\nneed to determine the extent and impact of that change on the entire software system.\nthe software developer should then conduct an appropriate level of software regression testing to show that unchanged but vulnerable portions of the system have not been adversely affected.\n\n\n\n2.4.8 Validation Coverage\n\nValidation coverage should be based on the software’s complexity and safety risk.\nThe selection of validation activities, tasks, and work items should be commensurate with the complexity of the software design and the risk associated with the use of the software for the specified intended use.\n\n\n\n2.4.9 Independence of Review\n\nValidation activities should be based on the basic quality assurance precept of “independence of review.”\nSelf-validation is extremely difficult.\nWhen possible, an independent evaluation is always better (like a contracted third-party independent verification and validation)\nAnother approach is to assign internal staff members that are not involved in a particular design or its implementation, but who have sufficient knowledge to evaluate the project and conduct the verification and validation activities.\n\n\n\n2.4.10 Flexibility and Responsibility\nThe device manufacturer has flexibility in choosing how to apply these validation principles, but retains ultimate responsibility for demonstrating that the software has been validated. FDA regulated medical device applications include software that:\n\nIs a component, part, or accessory of a medical device;\n\ncomponents: e.g., application software, operating systems, compilers, debuggers, configuration management tools, and many more\n\nIs itself a medical device; or\nIs used in manufacturing, design and development, or other parts of the quality system.\nNo matter how complex and disperse the software is, the manufacturer is in charge of responsibility for software validation."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#activities-and-tasks",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#activities-and-tasks",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.5 Activities and Tasks",
    "text": "2.5 Activities and Tasks\nSoftware validation is accomplished through a series of activities and tasks that are planned and executed at various stages of the software development life cycle. These tasks may be\n\none time occurrences\niterated many times\n\n\n2.5.1 Software Life Cycle Activities\n\nSoftware developers should establish a software life cycle model that is appropriate for their product and organization.\nThe selected software life cycle model should cover the software from its birth to its retirement.\nActivities in a typical software life cycle model:\n\nQuality Planning\nSystem Requirements Definition\nDetailed Software Requirements Specification\nSoftware Design Specification\nConstruction or Coding\nTesting\nInstallation\nOperation and Support\nMaintenance\nRetirement\n\nVerification, testing, and other tasks that support software validation occur during each of these activities.\nSeveral software life cycle models defined in FDA’s Glossary of Computerized System and Software Development\n\nTerminology dated August 1995:\n\nwaterfall\nspiral\nrapid prototyping\nincremental development, etc.\n\n\n\n2.5.2 Typical Tasks Supporting Validation\nthe software developer should at least consider each of the risk-related tasks and should define and document which tasks are or are not appropriate for their specific application.\n\n2.5.2.1 Quality Planning\nDesign and development planning should culminate in a plan that identifies\n\nnecessary tasks,\nprocedures for anomaly reporting and resolution,\nnecessary resources, and\nmanagement review requirements including formal design reviews.\n\nThe plan should include:\n\nThe specific tasks for each life cycle activity;\nEnumeration of important quality factors (e.g., reliability, maintainability, and usability);\nMethods and procedures for each task;\nTask acceptance criteria;\nCriteria for defining and documenting outputs in terms that will allow evaluation of their conformance to input requirements;\nInputs for each task;\nOutputs from each task;\nRoles, resources, and responsibilities for each task;\nRisks and assumptions; and\nDocumentation of user needs.\n\nThe plan should identify\n\nthe personnel,\nthe facility and equipment resources for each task, and\nthe role that risk (hazard) management will play.\n\nA configuration management plan should be developed that will guide and control multiple parallel development activities and ensure proper communications and documentation.\nControls are necessary to ensure positive and correct correspondence among all approved versions of the specifications documents, source code, object code, and test suites that comprise a software system. The controls also should ensure accurate identification of, and access to, the currently approved versions.\nProcedures should be created for reporting and resolving software anomalies found through validation or other activities.\nManagement should identify the reports and specify the contents, format, and responsible organizational elements for each report. Procedures also are necessary for the review and approval of software development results, including the responsible organizational elements for such reviews and approvals.\nTypical Tasks – Quality Planning\n\nRisk (Hazard) Management Plan\nConfiguration Management Plan\nSoftware Quality Assurance Plan\n\nSoftware Verification and Validation Plan\n\nVerification and Validation Tasks, and Acceptance Criteria\nSchedule and Resource Allocation (for software verification and validation activities)\nReporting Requirements\n\nFormal Design Review Requirements\nOther Technical Review Requirements\n\nProblem Reporting and Resolution Procedures\nOther Support Activities\n\n\n\n2.5.2.2 Requirements\nRequirements development includes the\n\nidentification,\nanalysis, and\ndocumentation of information about the device and its intended use.\n\nAreas of special importance include allocation of system functions to\n\nhardware/software,\noperating conditions,\nuser characteristics,\npotential hazards, and\nanticipated tasks.\n\nIn addition, the requirements should state clearly the intended use of the software. It is not possible to validate software without predetermined and documented software requirements. Typical software requirements specify the following:\n\nAll software system inputs;\nAll software system outputs;\nAll functions that the software system will perform;\nAll performance requirements that the software will meet, (e.g., data throughput, reliability, and timing);\nThe definition of all external and user interfaces, as well as any internal software-to-system interfaces;\nHow users will interact with the system;\nWhat constitutes an error and how errors should be handled;\nRequired response times;\nThe intended operating environment for the software, if this is a design constraint (e.g., hardware platform, operating system);\nAll ranges, limits, defaults, and specific values that the software will accept; and\nAll safety related requirements, specifications, features, or functions that will be implemented in software.\n\nSoftware requirement specifications should identify clearly the potential hazards that can result from a software failure in the system as well as any safety requirements to be implemented in software.\nThe consequences of software failure should be evaluated, along with means of mitigating such failures (e.g., hardware mitigation, defensive programming, etc.).\nThe Quality System regulation requires a mechanism for addressing incomplete, ambiguous, or conflicting requirements. (See 21 CFR 820.30(c).) Each requirement (e.g., hardware, software, user, operator interface, and safety) identified in the software requirements specification should be evaluated for accuracy, completeness, consistency, testability, correctness, and clarity.\nFor example, software requirements should be evaluated to verify that:\n\nThere are no internal inconsistencies among requirements;\nAll of the performance requirements for the system have been spelled out;\nFault tolerance, safety, and security requirements are complete and correct;\nAllocation of software functions is accurate and complete;\nSoftware requirements are appropriate for the system hazards; and\nAll requirements are expressed in terms that are measurable or objectively verifiable.\n\nA software requirements traceability analysis should be conducted to trace software requirements to (and from) system requirements and to risk analysis results. In addition to any other analyses and documentation used to verify software requirements, a formal design review is recommended to confirm that requirements are fully specified and appropriate before extensive software design efforts begin. Requirements can be approved and released incrementally, but care should be taken that interactions and interfaces among software (and hardware) requirements are properly reviewed, analyzed, and controlled.\nTypical Tasks – Requirements\n\nPreliminary Risk Analysis\nTraceability Analysis\n\nSoftware Requirements to System Requirements (and vice versa)\nSoftware Requirements to Risk Analysis\n\nDescription of User Characteristics\nListing of Characteristics and Limitations of Primary and Secondary Memory\nSoftware Requirements Evaluation\nSoftware User Interface Requirements Analysis\nSystem Test Plan Generation\nAcceptance Test Plan Generation\nAmbiguity Review or Analysis\n\n\n\n2.5.2.3 Design\nIn the design process, the software requirements specification is translated into a logical and physical representation of the software to be implemented. The software design specification is a description of what the software should do and how it should do it. The design specification may contain both a high level summary of the design and detailed design information. Human factors engineering should be woven into\n\nthe entire design and development process,\nthe device design requirements,\nanalyses, and\ntests.\n\nDevice safety and usability issues should be considered when developing\n\nflowcharts,\nstate diagrams,\nprototyping tools, and\ntest plans.\n\nAlso, task and function analyses, risk analyses, prototype tests and reviews, and full usability tests should be performed. Participants from the user population should be included when applying these methodologies.\nThe software design specification should include:\n\nSoftware requirements specification, including predetermined criteria for acceptance of the software;\nSoftware risk analysis;\nDevelopment procedures and coding guidelines (or other programming procedures);\nSystems documentation (e.g., a narrative or a context diagram) that describes the systems context in which the program is intended to function, including the relationship of hardware, software, and the physical environment;\nHardware to be used;\nParameters to be measured or recorded;\nLogical structure (including control logic) and logical processing steps (e.g., algorithms);\nData structures and data flow diagrams;\nDefinitions of variables (control and data) and description of where they are used;\nError, alarm, and warning messages;\nSupporting software (e.g., operating systems, drivers, other application software);\nCommunication links (links among internal modules of the software, links with the supporting software, links with the hardware, and links with the user);\nSecurity measures (both physical and logical security); and\nAny additional constraints not identified in the above elements.\n\nThe first four of the elements noted above usually are separate pre-existing documents that are included by reference in the software design specification. Software requirements specification was discussed in the preceding section, as was software risk analysis.\nSoftware design evaluations criteria:\n\ncomplete,\ncorrect,\nconsistent,\nunambiguous,\nfeasible,\nmaintainable,\nanalyses of control flow,\ndata flow,\ncomplexity,\ntiming,\nsizing,\nmemory allocation,\ncriticality analysis, and many other aspects of the design\n\nAppropriate consideration of software architecture (e.g., modular structure) during design can reduce the magnitude of future validation efforts when software changes are needed.\nA traceability analysis should be conducted to verify that the software design implements all of the software requirements. As a technique for identifying where requirements are not sufficient, the traceability analysis should also verify that all aspects of the design are traceable to software requirements.\nAn analysis of communication links should be conducted to evaluate the proposed design with respect to hardware, user, and related software requirements. At the end of the software design activity, a Formal Design Review should be conducted to verify that the design is correct, consistent, complete, accurate, and testable, before moving to implement the design.\nSeveral versions of both the software requirement specification and the software design specification should be maintained. All approved versions should be archived and controlled in accordance with established configuration management procedures.\nTypical Tasks – Design\n\nUpdated Software Risk Analysis\nTraceability Analysis - Design Specification to Software Requirements (and vice versa)\nSoftware Design Evaluation\nDesign Communication Link Analysis\nModule Test Plan Generation\nIntegration Test Plan Generation\nTest Design Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.4 Construction or Coding\nSoftware may be constructed either by coding. Coding is the software activity where the detailed design specification is implemented as source code. It is the last stage in decomposition of the software requirements where module specifications are translated into a programming language.\nCoding usually involves the use of a high-level programming language, but may also entail the use of assembly language (or microcode) for time-critical operations.\nA source code traceability analysis is an important tool to verify that all code is linked to established specifications and established test procedures. A source code traceability analysis should be conducted and documented to verify that:\n\nEach element of the software design specification has been implemented in code;\nModules and functions implemented in code can be traced back to an element in the software design specification and to the risk analysis;\nTests for modules and functions can be traced back to an element in the software design specification and to the risk analysis; and\nTests for modules and functions can be traced to source code for the same modules and functions.\n\nTypical Tasks – Construction or Coding\n\nTraceability Analyses\n\nSource Code to Design Specification (and vice versa)\nTest Cases to Source Code and to Design Specification\n\nSource Code and Source Code Documentation Evaluation\nSource Code Interface Analysis\nTest Procedure and Test Case Generation (module, integration, system, and acceptance)\n\n\n\n2.5.2.5 Testing by the Software Developer\nSoftware testing entails running software products under known conditions with defined inputs and documented outcomes that can be compared to their predefined expectations. It is a time consuming, difficult, and imperfect activity.\nAs such, it requires early planning in order to be effective and efficient. Test plans and test cases should be created as early in the software development process as feasible.\nThey should identify\n\nthe schedules,\nenvironments,\nresources (personnel, tools, etc.),\nmethodologies,\ncases (inputs, procedures, outputs, expected results),\ndocumentation, and\nreporting criteria.\n\nDescriptions of categories of software and software testing effort appear in the literature\n\nNIST Special Publication 500-235, Structured Testing: A Testing Methodology Using the Cyclomatic Complexity Metric;\nNUREG/CR-6293, Verification and Validation Guidelines for High Integrity Systems; and\nIEEE Computer Society Press, Handbook of Software Reliability Engineering.\n\nTesting of all program functionality does not mean all of the program has been tested. Testing of all of a program’s code does not mean all necessary functionality is present in the program. Testing of all program functionality and all program code does not mean the program is 100% correct! Software testing that finds no errors should not be interpreted to mean that errors do not exist in the software product; it may mean the testing was superficial.\nAn essential element of a software test case is the expected result. It is the key detail that permits objective evaluation of the actual test result. This necessary testing information is obtained from the corresponding, predefined definition or specification.\nA software testing process should be based on principles that foster effective examinations of a software product. Applicable software testing tenets include:\n\nThe expected test outcome is predefined;\nA good test case has a high probability of exposing an error;\nA successful test is one that finds an error;\nThere is independence from coding;\nBoth application (user) and software (programming) expertise are employed;\nTesters use different tools from coders;\nExamining only the usual case is insufficient;\nTest documentation permits its reuse and an independent confirmation of the pass/fail status of a test outcome during subsequent review.\n\nCode-based testing is also known as structural testing or “white-box” testing. It identifies test cases based on knowledge obtained from the source code, detailed design specification, and other development documents. Structural testing can identify “dead” code that is never executed when the program is run. Structural testing is accomplished primarily with unit (module) level testing, but can be extended to other levels of software testing.\nThe level of structural testing can be evaluated using metrics that are designed to show what percentage of the software structure has been evaluated during structural testing. These metrics are typically referred to as “coverage” and are a measure of completeness with respect to test selection criteria. The amount of structural coverage should be commensurate with the level of risk posed by the software. Use of the term “coverage” usually means 100% coverage. Common structural coverage metrics include:\n\nStatement Coverage – This criteria requires sufficient test cases for each program statement to be executed at least once; however, its achievement is insufficient to provide confidence in a software product’s behavior.\nDecision (Branch) Coverage – This criteria requires sufficient test cases for each program decision or branch to be executed so that each possible outcome occurs at least once. It is considered to be a minimum level of coverage for most software products, but decision coverage alone is insufficient for high-integrity applications.\nCondition Coverage – This criteria requires sufficient test cases for each condition in a program decision to take on all possible outcomes at least once. It differs from branch coverage only when multiple conditions must be evaluated to reach a decision.\nMulti-Condition Coverage – This criteria requires sufficient test cases to exercise all possible combinations of conditions in a program decision.\nLoop Coverage – This criteria requires sufficient test cases for all program loops to be executed for zero, one, two, and many iterations covering initialization, typical running and termination (boundary) conditions.\nPath Coverage – This criteria requires sufficient test cases for each feasible path, basis path, etc., from start to exit of a defined program segment, to be executed at least once. Because of the very large number of possible paths through a software program, path coverage is generally not achievable. The amount of path coverage is normally established based on the risk or criticality of the software under test.\nData Flow Coverage – This criteria requires sufficient test cases for each feasible data flow to be executed at least once. A number of data flow testing strategies are available.\n\nThe following types of functional software testing involve generally increasing levels of effort:\n\nNormal Case – Testing with usual inputs is necessary. However, testing a software product only with expected, valid inputs does not thoroughly test that software product. By itself, normal case testing cannot provide sufficient confidence in the dependability of the software product.\nOutput Forcing – Choosing test inputs to ensure that selected (or all) software outputs are generated by testing.\nRobustness – Software testing should demonstrate that a software product behaves correctly when given unexpected, invalid inputs. Methods for identifying a sufficient set of such test cases include Equivalence Class Partitioning, Boundary Value Analysis, and Special Case Identification (Error Guessing). While important and necessary, these techniques do not ensure that all of the most appropriate challenges to a software product have been identified for testing.\nCombinations of Inputs – The functional testing methods identified above all emphasize individual or single test inputs. Most software products operate with multiple inputs under their conditions of use. Thorough software product testing should consider the combinations of inputs a software unit or system may encounter during operation. Error guessing can be extended to identify combinations of inputs, but it is an ad hoc technique. Cause-effect graphing is one functional software testing technique that systematically identifies combinations of inputs to a software product for inclusion in test cases.\n\nFunctional and structural software test case identification techniques provide specific inputs for testing, rather than random test inputs. One weakness of these techniques is the difficulty in linking structural and functional test completion criteria to a software product’s reliability.\nAdvanced software testing methods, such as statistical testing, can be employed to provide further assurance that a software product is dependable. Statistical testing uses randomly generated test data from defined distributions based on an operational profile (e.g., expected use, hazardous use, or malicious use of the software product). Large amounts of test data are generated and can be targeted to cover particular areas or concerns, providing an increased possibility of identifying individual and multiple rare operating conditions that were not anticipated by either the software product’s designers or its testers. Statistical testing also provides high structural coverage. It does require a stable software product. Thus, structural and functional testing are prerequisites for statistical testing of a software product.\nAnother aspect of software testing is the testing of software changes. Changes occur frequently during software development. These changes are the result of\n\ndebugging that finds an error and it is corrected,\nnew or changed requirements (“requirements creep”), and\nmodified designs as more effective or efficient implementations are found.\n\nOnce a software product has been baselined (approved), any change to that product should have its own “mini life cycle,” including testing. Testing of a changed software product requires additional effort. It should demonstrate\n\nthat the change was implemented correctly, and\nthat the change did not adversely impact other parts of the software product.\n\nRegression analysis is the determination of the impact of a change based on review of the relevant documentation in order to identify the necessary regression tests to be run. Regression testing is the rerunning of test cases that a program has previously executed correctly and comparing the current result to the previous result in order to detect unintended effects of a software change. Regression analysis and regression testing should also be employed when using integration methods to build a software product to ensure that newly integrated modules do not adversely impact the operation of previously integrated modules.\nIn order to provide a thorough and rigorous examination of a software product, development testing is typically organized into levels: unit, integration, and system levels of testing.\n\nUnit (module or component) level testing focuses on the early examination of sub-program functionality and ensures that functionality not visible at the system level is examined by testing. Unit testing ensures that quality software units are furnished for integration into the finished software product.\nIntegration level testing focuses on the transfer of data and control across a program’s internal and external interfaces. External interfaces are those with\n\nother software (including operating system software),\nsystem hardware, and\nthe users and can be described as communications links.\n\nSystem level testing demonstrates that all specified functionality exists and that the software product is trustworthy. This testing verifies the as-built program’s functionality and performance with respect to the requirements for the software product as exhibited on the specified operating platform(s). System level software testing addresses functional concerns and the following elements of a device’s software that are related to the intended use(s):\n\nPerformance issues (e.g., response times, reliability measurements);\nResponses to stress conditions, e.g., behavior under maximum load, continuous use;\nOperation of internal and external security features;\nEffectiveness of recovery procedures, including disaster recovery;\nUsability; (Usability vs Utility??)\nCompatibility with other software products;\nBehavior in each of the defined hardware configurations; and\nAccuracy of documentation.\n\n\nControl measures (e.g., a traceability analysis) should be used to ensure that the intended coverage is achieved.\nSystem level testing also exhibits the software product’s behavior in the intended operating environment. The location of such testing is dependent upon the software developer’s ability to produce the target operating environment(s). Depending upon the circumstances, simulation and/or testing at (potential) customer locations may be utilized.\nTest plans should identify the controls needed to ensure\n\nthat the intended coverage is achieved and\nthat proper documentation is prepared when planned system level testing is conducted at sites not directly controlled by the software developer.\n\nTest procedures, test data, and test results\n\nshould be documented in a manner permitting objective pass/fail decisions to be reached.\nshould also be suitable for review and objective decision making subsequent to running the test,\nshould be suitable for use in any subsequent regression testing.\n\nErrors detected during testing should be\n\nlogged,\nclassified,\nreviewed, and\nresolved prior to release of the software.\n\nSoftware error data that is collected and analyzed during a development life cycle may be used to determine the suitability of the software product for release for commercial distribution. Test reports should comply with the requirements of the corresponding test plans.\nSoftware testing tools are frequently used to ensure consistency, thoroughness, and efficiency in the testing of such software products and to fulfill the requirements of the planned testing activities.\nAppropriate documentation providing evidence of the validation of these software tools for their intended use should be maintained (see section 6 of this guidance).\nTypical Tasks – Testing by the Software Developer\n\nTest Planning\nStructural Test Case Identification\nFunctional Test Case Identification\nTraceability Analysis - Testing\nUnit (Module) Tests to Detailed Design\nIntegration Tests to High Level Design\nSystem Tests to Software Requirements\nUnit (Module) Test Execution\nIntegration Test Execution\nFunctional Test Execution\nSystem Test Execution\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.6 User Site Testing\nTesting at the user site is an essential part of software validation. The Quality System regulation requires\n\ninstallation and\ninspection procedures (including testing where appropriate) as well as\ndocumentation of inspection and\ntesting to demonstrate proper installation. (See 21 CFR §820.170.)\n\nLikewise, manufacturing equipment must meet specified requirements, and automated systems must be validated for their intended use. (See 21 CFR §820.70(g) and 21 CFR §820.70(i) respectively.)\nTerminology regarding user site testing can be confusing. Terms such as\n\nbeta test,\nsite validation,\nuser acceptance test,\ninstallation verification, and\ninstallation testing have all been used to describe user site testing.\n\nFor the purposes of this guidance, the term “user site testing” encompasses all of these and any other testing that takes place outside of the developer’s controlled environment.\nThis testing should take place at a user’s site with the actual hardware and software that will be part of the installed system configuration. The testing is accomplished through either actual or simulated use of the software being tested within the context in which it is intended to function.\nTest planners should check with the FDA Center(s) with the corresponding product jurisdiction to determine whether there are any additional regulatory requirements for user site testing.\nUser site testing should follow a pre-defined written plan with\n\na formal summary of testing and\na record of formal acceptance.\n\nThe following documented evidence should be retained:\n\nall testing procedures,\ntest input data, and\ntest results\n\nThere should be evidence that hardware and software are installed and configured as specified. Measures should ensure that all system components are exercised during the testing and that the versions of these components are those specified. The testing plan should specify testing throughout the full range of operating conditions and should specify continuation for a sufficient time to allow the system to encounter a wide spectrum of conditions and events in an effort to detect any latent faults that are not apparent during more normal activities.\nSome of the evaluations of the system’s ability that have been performed earlier by the software developer at the developer’s site should be repeated at the site of actual use. These may include tests for:\n\na high volume of data,\nheavy loads or stresses,\nsecurity,\nfault testing (avoidance, detection, tolerance, and recovery),\nerror messages, and\nimplementation of safety requirements.\n\nThere should be an evaluation of the ability of the users of the system to understand and correctly interface with it.\nOperators should be able to perform the intended functions and respond in an appropriate and timely manner to all alarms, warnings, and error messages.\nRecords should be maintained of both proper system performance and any system failures that are encountered.\nThe revision of the system to compensate for faults detected during this user site testing should follow the same procedures and controls as for any other software change.\nThe developers of the software may or may not be involved in the user site testing.\n\nIf the developers are involved, they may seamlessly carry over to the user’s site the last portions of design-level systems testing.\nIf the developers are not involved, it is all the more important that the user have persons who understand the importance of careful test planning, the definition of expected test results, and the recording of all test outputs.\n\nTypical Tasks – User Site Testing\n\nAcceptance Test Execution\nTest Results Evaluation\nError Evaluation/Resolution\nFinal Test Report\n\n\n\n2.5.2.7 Maintenance and Software Changes\n\n2.5.2.7.1 Hardware vs Software\nHardware maintenance typically includes\n\npreventive hardware maintenance actions,\ncomponent replacement, and\ncorrective changes.\n\nSoftware maintenance includes\n\ncorrective,\nperfective, and\nadaptive maintenance\nbut does not include preventive maintenance actions or software component replacement.\n\n\n\n2.5.2.7.2 Maintenance Types\n\nCorrective maintenance: Changes made to correct errors and faults in the software.\nPerfective maintenance: Changes made to the software to improve the performance, maintainability, or other attributes of the software system .\nAdaptive maintenance: Changes to make the software system usable in a changed environment.\n\nSufficient regression analysis and testing should be conducted to demonstrate that portions of the software not involved in the change were not adversely impacted. When changes are made to a software system,\n\neither during initial development or\nduring post release maintenance,\n\nThis is in addition to testing that evaluates the correctness of the implemented change(s). The specific validation effort necessary for each software change is determined by\n\nthe type of change,\nthe development products affected, and the\nimpact of those products on the operation of the software.\n\n\n\n2.5.2.7.3 Factors of Limitting Validation Effort Needed When a Change Is Made\n\ncareful and complete documentation of the design structure and\ncareful and complete documentation of interrelationships of various modules,\ninterfaces, etc.\nFor example,\n\ntest documentation,\ntest cases, and\nresults of previous verification and validation testing All of them need to be archived if they are to be available for performing subsequent regression testing.\n\n\nThe following additional maintenance tasks should be addressed:\n\nSoftware Validation Plan Revision - For software that was previously validated, the existing software validation plan should be revised to support the validation of the revised software. If no previous software validation plan exists, such a plan should be established to support the validation of the revised software.\nAnomaly Evaluation – Software organizations frequently maintain documentation, such as software problem reports that describe software anomalies discovered and the specific corrective action taken to fix each anomaly.\n\nToo often, however, mistakes are repeated because software developers do not take the next step to determine the root causes of problems and make the process and procedural changes needed to avoid recurrence of the problem.\nSoftware anomalies should be evaluated in terms of their severity and their effects on system operation and safety,\nbut they should also be treated as symptoms of process deficiencies in the quality system.\nA root cause analysis of anomalies can identify specific quality system deficiencies.\nWhere trends are identified (e.g., recurrence of similar software anomalies), appropriate corrective and preventive actions must be implemented and documented to avoid further recurrence of similar quality problems. (See 21 CFR 820.100.)\n\nProblem Identification and Resolution Tracking - All problems discovered during maintenance of the software should be documented. The resolution of each problem should be tracked to ensure it is fixed, for historical reference, and for trending.\nProposed Change Assessment - All proposed modifications, enhancements, or additions should be assessed to determine the effect each change would have on the system. This information should determine the extent to which verification and/or validation tasks need to be iterated.\nTask Iteration - For approved software changes, all necessary verification and validation tasks should be performed to ensure that planned changes are implemented correctly, all documentation is complete and up to date, and no unacceptable changes have occurred in software performance.\nDocumentation Updating – Documentation should be carefully reviewed to determine which documents have been impacted by a change. All approved documents (e.g., specifications, test procedures, user manuals, etc.) that have been affected should be updated in accordance with configuration management procedures. Specifications should be updated before any maintenance and software changes are made."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "href": "docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html#validation-of-automated-process-equipment-and-quality-system-software",
    "title": "FDA Software Validation Guidance Summary",
    "section": "2.6 Validation of Automated Process Equipment and Quality System Software",
    "text": "2.6 Validation of Automated Process Equipment and Quality System Software\nThe Quality System regulation requires that “when computers or automated data processing systems are used as part of production or the quality system, the [device] manufacturer shall validate computer software for its intended use according to an established protocol.” (See 21 CFR §820.70(i)). This has been a regulatory requirement of FDA’s medical device Good Manufacturing Practice (GMP) regulations since 1978.\nComputer systems that implement part of a device manufacturer’s production processes or quality system (or that are used to create and maintain records required by any other FDA regulation) are subject to the Electronic Records; Electronic Signatures regulation. (See 21 CFR Part 11.) This regulation establishes additional security, data integrity, and validation requirements when records are created or maintained electronically. These additional Part 11 requirements should be carefully considered and included in system requirements and software requirements for any automated record keeping systems. System validation and software validation should demonstrate that all Part 11 requirements have been met.\nComputers and automated equipment are used extensively throughout all aspects of\n\nmedical device design,\nlaboratory testing and analysis,\nproduct inspection and acceptance,\nproduction and process control,\nenvironmental controls,\npackaging,\nlabeling,\ntraceability,\ndocument control,\ncomplaint management, and many other aspects of the quality system.\n\nIncreasingly, automated plant floor operations can involve extensive use of embedded systems in:\n\nprogrammable logic controllers;\ndigital function controllers;\nstatistical process control;\nsupervisory control and data acquisition;\nrobotics;\nhuman-machine interfaces;\ninput/output devices; and\ncomputer operating systems.\n\nAll software tools used for software design are subject to the requirement for software validation, but the validation approach used for each application can vary widely.\nValidation is typically supported by:\n\nverifications of the outputs from each stage of that software development life cycle; and\nchecking for proper operation of the finished software in the device manufacturer’s intended use environment.\n\n\n2.6.1 How Much Validation Evidence Is Needed?\nThe level of validation effort should be commensurate with\n\nthe risk posed by the automated operation,\nthe complexity of the process software,\nthe degree to which the device manufacturer is dependent upon that automated process to produce a safe and effective device\n\nDocumented requirements and risk analysis of the automated process help to define the scope of the evidence needed to show that the software is validated for its intended use. Without a plan, extensive testing may be needed for:\n\na plant-wide electronic record and electronic signature system;\nan automated controller for a sterilization cycle; or\nautomated test equipment used for inspection and acceptance of finished circuit boards in a lifesustaining / life-supporting device.\n\nHigh risk applications should not be running in the same operating environment with non-validated software functions, even if those software functions are not used. Risk mitigation techniques such as memory partitioning or other approaches to resource protection may need to be considered when high risk applications and lower risk applications are to be used in the same operating environment.\nWhen software is upgraded or any changes are made to the software, the device manufacturer should consider how those changes may impact the “used portions” of the software and must reconfirm the validation of those portions of the software that are used. (See 21 CFR §820.70(i).)\n\n\n2.6.2 Defined User Equipment\nA very important key to software validation is a documented user requirements specification that defines:\n\nthe “intended use” of the software or automated equipment; and\nthe extent to which the device manufacturer is dependent upon that software or equipment for production of a quality medical device.\n\nThe device manufacturer (user) needs to define the expected operating environment including any required hardware and software configurations, software versions, utilities, etc. The user also needs to:\n\ndocument requirements for system performance, quality, error handling, startup, shutdown, security, etc.;\nidentify any safety related functions or features, such as sensors, alarms, interlocks, logical processing steps, or command sequences; and\ndefine objective criteria for determining acceptable performance.\n\nThe validation must be conducted in accordance with a documented protocol, and the validation results must also be documented. (See 21 CFR §820.70(i).) Test cases should be documented that will exercise the system to challenge its performance against the pre-determined criteria, especially for its most critical parameters.\nTest cases should address\n\nerror and alarm conditions,\nstartup, shutdown,\nall applicable user functions and operator controls,\npotential operator errors,\nmaximum and minimum ranges of allowed values, and\nstress conditions applicable to the intended use of the equipment.\n\nThe test cases should be executed and the results should be recorded and evaluated to determine whether the results support a conclusion that the software is validated for its intended use.\nA device manufacturer may conduct a validation using their own personnel or may depend on a third party such as the equipment/software vendor or a consultant. In any case, the device manufacturer retains the ultimate responsibility for ensuring that the production and quality system software:\n\nis validated according to a written procedure for the particular intended use; and\nwill perform as intended in the chosen application.\n\nThe device manufacturer should have documentation including:\n\ndefined user requirements;\nvalidation protocol used;\nacceptance criteria;\ntest cases and results; and\na validation summary that objectively confirms that the software is validated for its intended use.\n\n\n\n2.6.3 Validation of Off-The-Shelf Software and Automated Equipment\nMost of the automated equipment and systems used by device manufacturers are supplied by thirdparty vendors and are purchased off-the-shelf (OTS). The device manufacturer is responsible for ensuring that the product development methodologies used by the OTS software developer are appropriate and sufficient for the device manufacturer’s intended use of that OTS software.\nWhere possible and depending upon the device risk involved, the device manufacturer should consider auditing the vendor’s design and development methodologies used in the construction of the OTS software and should assess the development and validation documentation generated for the OTS software. Such audits can be conducted by the device manufacturer or by a qualified third party.\nThe audit should demonstrate that the vendor’s procedures for and results of the verification and validation activities performed the OTS software are appropriate and sufficient for the safety and effectiveness requirements of the medical device to be produced using that software."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "",
    "text": "Notice\nLast Update\nIntroduction\n\nDefinition of SW Validation\nSome Terminology\nRationale\nObjective of SW Validation\nWhat to validate\nMain Institutions\n\nQuality System Regulation\nVerification\nValidation\nBenefits and Difficulty in SW V&V\nSW Development as Part of System Design\n\nOverview\nDesign Reveiw\n\n\n\n\nValidation Pinciples\n\nOverview\nConditions\nPlanning\nAfter SW Change\nSW Lifecycle\n\nSW Lifecycle Tasks\n\nOverview\nQuality Planning\nConfiguration Management\nTask Requirements\nDesign Overview\n\nDesign Consideration\nDesign Specification\nDesign Activity and Task\n\n\n\n\n\nTesting Tasks\n\nOverview\nConsideration Before Testing Tasks\nCode Based Testing\nSolution to White Box Testing\nDevelopment Testing\nUser Site Testing\n\nOverview\nTesting\n\n\nMaintenance and SW Changes\nValidation of Quality System SW\n\nOverview\nFactors in Validation"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#notice",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#notice",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Notice",
    "text": "Notice\n\nI am so sorry not for providing a compfortab visualization. Although I have tried to use revealjs provided in the guide section in the Quarto website, I am still clumsy at handling it. I will update this article as I get proficient at revealjs using Quarto. (it seems that Quarto system has some issues on mermaid diagrams.)\nThe FDA validation guidance document is a bit difficult to understand because its explanations provide abstract, general, and present broad cocepts. For this reason, I compiled and made a summary of the document with many diagrams. However, some diagrams are too small to see. Please, scroll up your mouse wheel with the ‘Ctrl’ key on your keyboard pressed to zoom in on the small text in the diagrams.\n(Writing in Progress) It is hard to say that this version of summary is suitable for representing and covering the original document. Some of the content of this document has been excluded for personal use (less than 10% of it have been excluded).\n\n\nLast Update\n\n2022-12-28, Summary of Document"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#introduction",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#introduction",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Introduction",
    "text": "Introduction\n\nDefinition of Software Validation\nSoftware Validation is a requirement of the Quality System regulation, which was published in the Federal Register on October 7, 1996 and took effect on June 1, 1997.\n(See Title 21 Code of Federal Regulations (CFR) Part 820, and 61 Federal Register (FR) 52602, respectively.)\n\n\nSome Terminology\nThe medical device Quality System regulation (21 CFR 820.3(k)) defines\n\n“establish” = “define, document, and implement”\n“establish” = “established”\nConfusing terminology: requirements, specification, verification, and validation.\n\n\n\nRationale\nFDA has reported the following analysis:\n\n242 of 3140 (7.7%) medical device recalls between 1992 and 1998 are attributable to software failures.\n192 of the 242 (79.3%) failures were caused by software defects that were introduced when changes were made to the software after its initial production and distribution.\nThe software validation check is a principal means of avoiding such defects and resultant recalls.\n\n\n\nObjective of SW validation is to ensure\n\naccuracy\nreliability\nconsistent intended performance, and\nthe ability to discern invalid or altered records.\n\n\n\nWhat to validate\nAny software used to automate device design, testing, component acceptance, manufacturing, labeling, packaging, distribution, complaint handling, or to automate any other aspect of the quality system, including any off-the-shelf software.\n\n\nMain Institutions\n\nCenter for Devices and Radiological Health (CDRH)\nU.S. Department Of Health and Human Services\nFood and Drug Administration\nCenter for Biologics Evaluation and Research"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#quality-system-regulation",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#quality-system-regulation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Quality System Regulation",
    "text": "Quality System Regulation\nThe guidance reflects that the minimum list of the relavant scientific and legal requirements that you must comply with.\n\n\n\n\nflowchart TB\n    subgraph Quality_System_Regulation\n        direction LR\n        subgraph Requirement\n            direction TB\n            user_requirements\n        end\n        subgraph Specification\n           direction TB\n           document_user_requirements \n        end \n        subgraph Verification\n           direction TB\n           verify_spacified_requirements\n        end\n        subgraph Validation\n           direction TB\n           Confirmation_by_Examinations\n           Provision_of_objective_3evidences\n        end\n        Requirement--> Specification --> Verification --> Validation                    \n    end\n    subgraph First_Detail\n        direction TB\n        subgraph User_Requirement\n            direction TB\n            any_need_for_customer---\n            any_need_for_system---\n            any_need_for_software\n        end\n            subgraph Document_User_Requirement\n            direction TB\n            define_means_for_requirements---\n          define_criteria_for_requirements\n        end         \n        subgraph Verify_Spacified_Requirement\n            direction TB\n            Objective_Evidence--->|needs|Software_Testing\n        end\n        subgraph SW_Validation\n            direction TB\n            subgraph Confirmation_by_Examination\n            direction TB\n                subgraph Examination_List_of_SW_LifeCycle\n                    direction TB\n                    comprehensiveness_of_software_testing---\n                    inspection_verification_test---\n                    analysis_verification_test---\n                    other_varification_tests    \n                end \n            end             \n            subgraph Provision_of_Objective_3evidences\n                direction TB\n                Software_specifications_conformity---\n                Consistent_SW_Implementation---\n                Correctness_Completeness_Traceability\n            end\n        end\n        Requirement---User_Requirement\n        Specification---Document_User_Requirement\n        Verification---Verify_Spacified_Requirement\n        Confirmation_by_Examinations---Confirmation_by_Examination\n        Provision_of_objective_3evidences---Provision_of_Objective_3evidences             \n    end"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#verification",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#verification",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Verification",
    "text": "Verification\n\n\n\n\nflowchart LR\n    subgraph Objective_Evidence\n        direction LR\n        subgraph Design_Outputs_of_SW_life_cycle_for_Specified_Requirements\n            direction TB\n            Consistency---\n            Completeness---\n            Correctness---\n            Documentation\n        end       \n        subgraph Software_Testing\n            direction LR\n            subgraph Testing_Environments\n                direction TB\n                satisfaction_for_input_requirements\n                satisfaction_for_input_requirements---Simulated_Use_Environment\n                subgraph User_Site_Testing\n                    direction TB                            \n                    Installation_Qualification---\n                    Operational_Qualification---\n                    Performance_Qualification\n                end\n            end\n            satisfaction_for_input_requirements---User_Site_Testing\n            subgraph Testing_Activities\n                direction TB\n                static_analyses---\n                dynamic_analyses---\n                code_and_document_inspections---\n                walkthroughs\n            end \n        Testing_Environments-->Testing_Activities\n        end\n    Design_Outputs_of_SW_life_cycle_for_Specified_Requirements-->Software_Testing-->Testing_Activities\nend    \n\n\n\n\n\n\n\n\n\n\nInstallation_Qualification (IQ): documentation of correct installations according to requirements, specifications, vendor’s recommendations, and the FDA’s guidance for all hardware, software, equipment and systems.\nOperational_Qualification (OQ): establishment of confidence that the software shows constant performances according to specified requirements.\nPerformance_Qualification (PQ): confirmation of the performance in the intended use according to the specified requirements for functionality and safety throughout the SW life cycle."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation",
    "text": "Validation\n\n\n\n\nflowchart LR\n    subgraph Validation\n    direction LR\n        subgraph Confirmation_by_Examination\n            direction TB\n            subgraph Examination_List_at_each_stage_of_SW_Life_Cycle\n                direction TB\n                comprehensiveness_of_software_testing---\n                inspection_verification_test---\n                analysis_verification_test---\n                other_varification_tests    \n            end \n        end\n        subgraph Provision_of_objective_3evidences\n            direction TB\n            subgraph Software_specifications_conform_to\n                direction TB\n                user_needs \n                intended_uses\n            end\n            subgraph Consistent_SW_Implementation\n                direction TB\n                particular_requirements\n            end\n            subgraph Correctness_Completeness_Traceability\n                direction TB\n                correct_complete_implementation_by_all_SW_requirements---\n                traceable_to_system_requirements\n            end\n            Software_specifications_conform_to---\n            Consistent_SW_Implementation---\n            Correctness_Completeness_Traceability\n        end\n        Confirmation_by_Examination-->\n        Provision_of_objective_3evidences\n    end\n\n\n\n\n\n\n\n\n\n\n\n\nA conclusion that software is validated is highly dependent upon comprehensive software testing, inspections, analyses, and other verification tasks performed at each stage of the software development life cycle.\nTesting of device software functionality in a simulated* use environment, and user site testing are typically included as components of an overall design validation program for a software automated device."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#benefits-and-difficulty-of-sw-vv",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#benefits-and-difficulty-of-sw-vv",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Benefits and Difficulty of SW V&V",
    "text": "Benefits and Difficulty of SW V&V\n\nBenefits of SW V&V\n\nIncrease the usability and reliability of the device,\nResulting in decreased failure rates, fewer recalls and corrective actions, less risk to patients and users, and reduced liability to device manufacturers.\nReduce long term costs by making V&V easier and less costly to reliably modify software and revalidate software changes.\n\n\n\nDifficulty in SW V&V\n\na developer cannot test forever, and\n\nit is difficult to know how much evidence is enough.\na matter of developing a level of confidence that the device meets all requirements\n\nConsiderations for an acceptable level of confidence\nmeasures and estimates such as defects found in specifications documents\ntesting coverage, and other techniques are all used before shipping the product.\na level of confidence varies depending upon the safety risk (hazard) of a SW or device"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-development-as-part-of-system-design",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-development-as-part-of-system-design",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Development as Part of System Design",
    "text": "SW Development as Part of System Design\n\nOverview\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        purpose_design_review---\n        design_review_types---\n        design_review_requirements---\n        design_review_outputs\n    end\n\n\n\n\n\n\n\n\nSoftware validation must be considered within the context of the overall design validation for the system. A documented requirements specification represents\n\nuser’s needs\nintended uses from which the product is developed.\n\nA primary goal of SW validation is to demonstrate that all completed SW products comply with all documented requirements.\n\n\nDesign Review\n\n\n\n\nflowchart LR\n    subgraph Design_Review\n        direction LR\n        subgraph Purpose_Design_Review\n            direction TB\n            documented_structured_comprehensive_systematic_examinations---\n            adequacy_of_design_requirements---\n            capability_of_design_for_requirements---\n            identification_of_problem   \n        end\n        subgraph Design_Reivew_Types\n            direction TB\n            subgraph Formal_Design_Review\n                direction TB\n                3rd_parties_outside_development_team\n            end\n            subgraph Informal_Design_Review\n                direction TB\n                within_development_team\n            end\n        Formal_Design_Review---Informal_Design_Review    \n        end\n        subgraph Design_Review_Requirements\n            direction TB\n               necessary_at_least_one_formal_design_review---\n               optinal_informal_design_review---\n               recommended_multiple_design_reviews\n        end\n        subgraph Formal_Design_Review_Outputs\n            direction TB\n            more_than_10_outputs\n        end\n        Purpose_Design_Review--> Design_Reivew_Types--> Design_Review_Requirements\n        Design_Review_Requirements-->Formal_Design_Review_Outputs\n    end\n\n\n\n\n\n\n\n\n\n\nDesign review is a primary tool for managing and evaluating development projects.\nAt least one formal design review must be conducted during the device design process.\nIt is recommended that multiple design reviews be conducted.\nProblems found at this point can\n\nbe resolved more easily,\nsave time and money, and\nreduce the likelihood of missing a critical issue."
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-principles",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-principles",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation Principles",
    "text": "Validation Principles\n\nOverview\n\n\n\n\nflowchart LR\n  subgraph Validation_Principles\n        direction LR\n        subgraph Validation_Starting_Point\n            direction TB\n            during_design_planning---\n            during_development_planning---\n            all_results_should_be_supported_by_evidence_collected_from_planning_SW_lifecylce\n        end\n        subgraph Validation_Conditions\n            direction TB\n            Requirements---Estabilishment_Confidence---SW_Lifecycle\n        end\n\n        subgraph Validation_Planning\n            direction TB\n            Specify_Areas\n            subgraph Validation_Coverage\n                direction TB\n            end\n            subgraph Validation_Process_Establishment\n                direction TB\n            end\n        Specify_Areas---Validation_Coverage---Validation_Process_Establishment\n        end\n\n        subgraph After_Self_Validation\n            direction TB\n            subgraph Validation_After_SW_Change\n        direction TB\n        end\n\n        subgraph Independence_of_Review\n        direction TB\n\n        end\n        Validation_After_SW_Change---Independence_of_Review\n        end\n            Validation_Starting_Point-->Validation_Conditions-->Validation_Planning-->\nAfter_Self_Validation\n    end\n\n\n\n\n\n\n\n\n\nPreparation for software validation should begin as early as possible because the final conclusion that the software is validated should be based on evidence collected from planned efforts conducted throughout the software lifecycle.\n\n\n\nConditions\n\n\n\n\nflowchart LR\n\nsubgraph Validation_Conditions\n    direction LR\n    subgraph SW_Requirments\n        direction TB\n        subgraph Documented_SW_Requirments_Specification\n            direction TB\n            Baseline_Provision_for_V&V---\n            establishment_of_software_requirements_specification\n        end\n    end\n    subgraph Estabilishment_Confidence\n        direction TB\n            mixture_of_methods_techinques---\n            preventing_SW_errors---\n            detecting_SW_errors                 \n    end\n    subgraph SW_Lifecycle\n        direction TB\n        validation_must_be_conducted_within_established_environment_across_lifecycle---\n        lifecycle_contains_SW_engineering_tasks_and_documentation---\n        V&V_tasks_must_reflect_intended_use\n    end\nend\nSW_Requirments---Estabilishment_Confidence---SW_Lifecycle\n\n\n\n\n\n\n\n\n\n\nPlanning\n\n\n\n\nflowchart LR\n    subgraph Validation_Planning\n        direction LR\n        define_what_to_accomplish\n        subgraph Specify_Areas\n            direction TB\n            scope---\n            approach---\n            resources---\n            schedules_activities---\n            types_activitieis---\n            extent_of_activities---\n            tasks---\n            work_items\n        end\n            define_what_to_accomplish-->Specify_Areas\n        subgraph Validation_Coverage\n               direction TB\n            depending_on_SW_complexity_of_SW_design---\n            depending_on_safety_risk_for_specified_intended_use---\n            select_activities_tasks_work_items_for_complexity_safety_risk\n        end\n        subgraph Validation_Process_Establishment\n            direction TB\n            establish_how_to_conduct-->\n            identify_sequence_of_specific_actions-->\n            identify_specific_activitieis-->\n            identify_specific_tasks-->\n            identify_specific_work_items\n        end\n    Specify_Areas-->Validation_Coverage-->Validation_Process_Establishment\n    end\n\n\n\n\n\n\n\n\n\n\nAfter SW Change\n\n\n\n\nflowchart LR\n\nsubgraph After_Self_Validation\n    direction LR\n    subgraph Validation_After_SW_Change\n        direction TB\n        determine_extent_of_change_on_entire_SW_system-->\n        determine_impact_of_change_on_entire_SW_system-->\n        conduct_SW_regression_testing_on_unchanged_but_vulnerable_modules\n    end\n    subgraph Independence_of_Review\n        direction TB\n        follow_basic_quality_assurance_precept_of_independence_of_review---\n        avoid_self_validation---\n        should_conduct_contracted_3rd_party_independent_V&V---\n        or_conduct_blind_test_with_internal_staff\n    end\n    Validation_After_SW_Change---Independence_of_Review\nend\n    \n\n\n\n\n\n\n\n\n\n\nSW Lifecycle\n\n\n\n\nflowchart LR\nsubgraph SW_Lifecycle\n    direction TB\n    validation_must_be_conducted_within_the_established_environment_across_lifecycle---\n    lifecycle_contains_SW_engineering_tasks_and_documentation---\n    V&V_tasks_must_reflect_intended_use\nend\n\nsubgraph SW_Lifecycle_Activities\n    direction TB\n    subgraph should_establish_lifecycle_model\n        direction TB\n        subgraph SW_Lifecycle_Model_List_Defined_in_FDA\n            direction TB\n            waterfall---\n            spiral---\n            rapid_prototyping---\n            incremental_development---\n            etc\n        end     \n    end\n    subgraph should_cover_SW_birth_to_retirement\n        direction TB\n        subgraph Lifecycle_Activities\n            direction TB\n            Quality_Plan-->\n            System_Requirements_Definition-->\n            Detailed_Software_Requirements_Specification-->\n            Software_Design_Specification-->\n            Construction_or_Coding-->\n            Testing-->\n            Installation-->\n            Operation_and_Support-->\n            Maintenance-->\n            Retirement\n        end\n    end\n    should_establish_lifecycle_model-->should_cover_SW_birth_to_retirement\n    should_cover_SW_birth_to_retirement-->Lifecycle_Activities\nend\nsubgraph SW_Lifecycle_Tasks\n    direction TB\n    should_define_and_document_risk_related_tasks---\n    should_define_and_document_which_tasks_are_appropriate_in_vice_versa---\n    Quality_Planning---\n    Quality_Planning_Tasks---\n    Inclusion_Task_List_for_Plan---\n    Identification_Task_List_for_Plan---\n    Configuration_Management---\n    Control---\n    Management---\n    Procedures---\n    ensure_proper_communications_and_documentation---\n    Task_Requirements\nend\nSW_Lifecycle-->SW_Lifecycle_Activities-->SW_Lifecycle_Tasks"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-lifecycle-tasks",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#sw-lifecycle-tasks",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "SW Lifecycle Tasks",
    "text": "SW Lifecycle Tasks\n\nOverview\n\n\n\n\n \nflowchart TB\n\nsubgraph SW_Lifecycle_Tasks\n    direction LR\n    subgraph Define_and_Document_List\n        direction TB\n        risk_related_tasks---\n        whether_or_not_tasks_are_appropriate\n    end\n    \n    subgraph Quality_Planning\n        direction TB\n        subgraph Quality_Planning_Tasks\n            direction TB\n        \n        end\n        subgraph Inclusion_List_for_Plan\n            direction TB\n            \n        end\n        subgraph Identification_List_for_Plan\n            direction TB\n            \n        end\n    Quality_Planning_Tasks-->Inclusion_List_for_Plan-->Identification_List_for_Plan\n    end\n    \n    subgraph Configuration_Management\n        direction TB\n        subgraph Control\n            direction TB\n            \n        end\n        subgraph Management\n            direction TB\n        end\n        subgraph Procedures\n            direction TB\n        end\n        ensure_proper_communications_and_documentation\n        Control-->Management-->Procedures-->ensure_proper_communications_and_documentation \n    end\n    subgraph Task_Requirements\n        direction TB\n        identification---\n        analysis---\n        predetermined_documentation_about_device_its_intended_use---\n        Requirements_Specification_List---\n        Verfification_List_by_Evaluation---\n        Requirements_Tasks    \n    end\nDefine_and_Document_List-->Quality_Planning-->Configuration_Management-->Task_Requirements\nend     \n\n\n\n\n\n\n\n\n\n\nQuality Planning\n\n\n\n\nflowchart TB\nsubgraph Quality_Planning\n    direction LR\n    subgraph Quality_Planning_Tasks\n        direction TB\n        Risk_Hazard_Management_Plan---\n        Configuration_Management_Plan---\n        Software_Quality_Assurance_Plan---\n        Software_Verification_and_Validation_Plan---\n        Verification_and_Validation_Tasks---\n        Acceptance_Criteria---\n        Schedule_and_Resource_Allocation_for_V&V_activities---\n        Reporting_Requirements---\n        Formal_Design_Review_Requirements---\n        Other_Technical_Review_Requirements---\n        Problem_Reporting_and_Resolution_Procedures---\n        Other_Support_Activities\n    end\n    subgraph Inclusion_List_for_Plan\n        direction TB\n        specific_tasks_for_each_life_cycle_activity---\n        Enumeration_of_important_quality_factors--- \n        like_reliability_maintainability_usability---\n        Methods_and_procedures_for_each_task---\n        Task_acceptance_criteria---\n        Criteria_for_defining_and_documenting_outputs_for_input_requirements---\n        Inputs_for_each_task---\n        Outputs_from_each_task---\n        Roles_resources_and_responsibilities_for_each_task---\n        Risks_and_assumptions---\n        Documentation_of_user_needs    \n    end\n    subgraph Identification_List_for_Plan\n        direction TB\n        personnel---\n        facility_and_equipment_resources_for_each_task---\n        role_that_risk_hazard_management        \n    end\nQuality_Planning_Tasks-->Inclusion_List_for_Plan-->Identification_List_for_Plan\nend\n\n\n\n\n\n\n\n\n\n\nConfiguration Management\n\n\n\n\nflowchart LR\nsubgraph Configuration_Management\n    direction LR\n    subgraph Control\n        direction TB\n        control_multiple_parallel_development_activities---\n        ensure_positive_and_correct_correspondence_of---\n        specifications_documents---\n        source_code---\n        object_code---\n        test_suites---\n        ensure_accurate_identification_of_approved_versions---\n        ensure_access_to_approved_versions---\n        create_procedures_for_reporting---\n        create_procedures_for_resolving_SW_anomalies                            \n    end\n    subgraph Management\n        direction TB\n        identify_reports---\n        specify_contents---\n        specify_format---\n        specify_responsible_organizational_elements_for_each_report\n    end\n    subgraph Procedures\n        direction TB\n        necessary_for_review_of_SW_development_results---\n        necessary_for_approval_of_SW_development_results\n    end\n    ensure_proper_communications_and_documentation\n    Control-->Management-->Procedures-->ensure_proper_communications_and_documentation \nend\n\n\n\n\n\n\n\n\n\n\nTask Requirements\n\n\n\n\n\nflowchart TB\n    subgraph Task_Requirements\n        direction LR\n        subgraph group\n            direction TB\n            identification---\n            analysis---\n            predetermined_documentation_about_device_its_intended_use\n        end\n        \n        subgraph Requirements_Specification_List\n            direction TB\n            All_software_system_inputs---\n            All_software_system_outputs---\n            All_functions_that_software_system_will_perform---\n            All_performance_requirements_that_software_will_meet---\n            requirement_example_data_throughput_reliability_timing---\n            definition_of_all_external_and_user_interfaces---\n            any_internal_software_to_system_interfaces---\n            How_users_will_interact_with_system---\n            What_constitutes_error---\n            how_errors_should_be_handled---\n            Required_response_times---\n            Intended_operating_environment_for_software---\n            All_acceptable_ranges_limits_defaults_specific_values---\n            All_safety_related_requirements_that_will_be_implemented_in_SW---\n            All_safety_related_specifications_that_will_be_implemented_in_SW---\n            All_safety_related_features_that_will_be_implemented_in_SW---\n            All_safety_related_functions_that_will_be_implemented_in_SW---\n            clearly_identify_potential_hazards---\n            risk_evaluation_for_accuracy---\n            risk_evaluation_for_completeness---\n            risk_evaluation_for_consistency---\n            risk_evaluation_for_testability---\n            risk_evaluation_for_correctness---\n            risk_evaluation_for_clarity\n        end\n        subgraph Verfification_List_by_Evaluation\n            direction TB\n            no_internal_inconsistencies_among_requirements---\n            All_of_performance_requirements_for_system---\n            Complete_correct_Fault_tolerance_safety_security_requirements---\n            Accurate_Complete_Allocation_of_software_functions---\n            Appropriate_Software_requirements_for_system_hazards---\n            mesurable_requirements---\n            objectively_verifiable_requirements---\n            traceable_requirements\n        end\n        subgraph Requirements_Tasks\n            direction TB\n            Preliminary_Risk_Analysis---\n            Traceability_Analysis---\n            ex_Software_Requirements_to_System_Requirements_vice_versa---\n            ex_Software_Requirements_to_Risk_Analysis---\n            Description_of_User_Characteristics---\n            Listing_of_Characteristics_and_Limitations_of_Memory---\n            Software_Requirements_Evaluation---\n            Software_User_Interface_Requirements_Analysis---\n            System_Test_Plan_Generation---\n            Acceptance_Test_Plan_Generation---\n            Ambiguity_Review_or_Analysis\n        end\n    group-->Requirements_Specification_List \n    Verfification_List_by_Evaluation-->Requirements_Tasks\n    end\n\n\n\n\n\n\n\n\n\n\nDesign Overview\n\n\n\n\nflowchart TB\n    subgraph Deign_Task\n        direction LR\n    subgraph Design_Consideration_List\n        direction TB\n        subgraph Description\n                    direction TB\n                end\n        subgraph Human_Factors_Engineering\n          direction TB\n    \n        end\n        subgraph Safety_Usability_Issues_Conisderation\n            direction TB\n\n            end\n        Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n    subgraph Design_Specificiation\n        direction TB\n        subgraph Performing_List\n            direction TB\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n        end\n    Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design \n    end\n    subgraph Design_Activity_and_Task_List\n        direction TB\n        subgraph Final_Design_activity\n            direction TB\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n            end\n            subgraph Coding_Tasks\n                direction TB\n            end\n        traceability_analysis-->Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end\n    Design_Consideration_List---Design_Specificiation---Design_Activity_and_Task_List\n\n    end\n\n\n\n\n\n\n\n\n\nDesign Consideration\n\n\n\n\nflowchart TB\nsubgraph Design_Consideration_List\n    direction LR\n        subgraph Requirement_Specification\n            direction TB\n            logical_representation---\n            physical_representation\n        end\n    subgraph Description\n            direction TB\n            what_to_do---\n            how_to_do                   \n        end\n    subgraph Human_Factors_Engineering\n      direction TB\n            entire_design_and_development_process---\n            device_design_requirements---\n            analyses---\n            tests\n    end\n    subgraph Safety_Usability_Issues_Conisderation\n        direction TB\n                flowcharts--- \n                state_diagrams--- \n                prototyping_tools---\n                test_plans\n        end\n        Requirement_Specification---Description---Human_Factors_Engineering---Safety_Usability_Issues_Conisderation\n    end\n\n\n\n\n\n\n\n\n\n\nDesign Specification\n\n\n\n\nflowchart TB\nsubgraph Design_Specificiation\n        direction LR\n        subgraph Conceptual_Specification\n            direction TB\n            requirements_specification---\n            predetermined_criteria---\n            Software_risk_analysis---\n            Development_procedures---\n            coding_guidelines\n        end\n        subgraph Performing_List\n            direction TB\n            task---\n            function_analyses---\n            risk_analyses---\n            prototype_tests_and_reviews---\n            full_usability_tests\n        end\n        subgraph Design_Specification_Inclusion_List\n            direction TB\n            SW_requirements_specification---\n            predetermined_criteria_for_SW_acceptance---\n            SW_risk_analysis---\n            Development_procedure_list---\n            coding_guidance---\n            Systems_documentation---\n            Hardware_to_be_used---\n            Parameters_to_be_measured---\n            Logical_structure---\n            Control_logic---\n            logical_processing_steps_aka_algorithms---\n            Data_structures_diagram---\n            data_flow_diagrams---\n            Definitions_of_variables---\n            description_of_where_they_are_used---\n            Error_alarm_and_warning_messages---\n            Supporting_software---\n            internal_modules_Communication_links---\n            supporting_sw_links---\n            link_with_hardware---\n            link_with_user---\n            physical_Security_measures---\n            logical_security_measures\n        end\n        subgraph Evaludations_Criteria_of_Design\n            direction TB\n            complete--- \n            correct---\n            consistent--- \n            unambiguous--- \n            feasible---\n            maintainable---\n            analyses_of_control_flow---\n            data_flow--- \n            complexity--- \n            timing--- \n            sizing--- \n            memory_allocation---\n            module_architecture---\n            traceability_analysis_of_modules--- \n            criticality_analysis\n        end\n    Conceptual_Specification---Performing_List---Design_Specification_Inclusion_List---Evaludations_Criteria_of_Design  \n    end\n\n\n\n\n\n\n\n\n\n\nDesign Activity and Task\n\n\n\n\n\nflowchart TB\nsubgraph Design_Activity_and_Task_List\n        direction LR\n        subgraph Final_Design_activity\n            direction TB\n            Formal_Design_Review_Before_Design_Implementation---\n            correct_consistent_complete_accurate_testable\n        end\n        subgraph Specific_Design_Tasks\n            direction TB\n            Updated_Software_Risk_Analysis---\n            Traceability_Analysis---\n            Software_Design_Evaluation---\n            Design_Communication_Link_Analysis---\n            Module_Test_Plan_Generation---\n            Integration_Test_Plan_Generation---\n            module_Test_Design_Generation---\n            integration_Test_Design_Generation---\n            system_Test_Design_Generation---\n            acceptance_Test_Design_Generation   \n        end\n        subgraph Coding_Activity\n            direction TB\n            subgraph traceability_analysis\n                direction TB\n                each_element_implementation---\n                each_module_implementation_to_element_and_risk_analysis---\n                each_functions_implemented_to_element_and_risk_analysis---\n                Tests_for_modules_to_element_and_risk_analysis--- \n                Tests_for_functions_to_element_and_risk_analysis---\n                Tests_for_modules_to_source_code---\n                Tests_for_functions_to_source_code\n            end\n            subgraph Coding_Tasks\n                direction TB\n                Traceability_Analyses---\n                Source_Code_to_Design_Specification_and_vice_versa---\n                Test_Cases_to_Source_Code_and_to_Design_Specification---\n                Source_Code_and_Source_Code_Documentation_Evaluation---\n                Source_Code_Interface_Analysis---\n                Test_Procedure_and_Test_Case_Generation \n            end\n        traceability_analysis-->Coding_Tasks\n        end\n        Final_Design_activity---Specific_Design_Tasks---Coding_Activity\n    end"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#testing-task",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#testing-task",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Testing Task",
    "text": "Testing Task\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction TB\n            subgraph Test_Plans\n                direction TB\n            end\n            subgraph Conditions\n                direction TB\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n        subgraph Code_Based_Testing\n            direction TB\n            subgraph white_box_testing\n                direction TB\n            end\n            subgraph Evaluation_of_level_of_white_box_testing\n                direction TB\n            end\n            subgraph Coverage_Metrics_of_White_Box_Testing\n                direction TB\n            end\n        white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n        end\n        subgraph Alternatives_to_White_Box_Testing\n            direction TB\n            subgraph Types_of_Functional_Software_Testing_Increasing_Cost\n                direction TB\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n            end\n            subgraph Change_in_SW\n                direction TB    \n            end\n        Types_of_Functional_Software_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW\n        end\n        \n\n        subgraph Development_Testing\n            direction TB\n            subgraph unit_level_testing\n                direction TB    \n            end\n            subgraph integration_level_testing\n                direction TB\n            end\n            subgraph system_level_testing\n                direction TB\n            end\n            subgraph Error_Detected\n                direction TB        \n            end\n        unit_level_testing-->integration_level_testing-->system_level_testing-->Error_Detected\n        end\n\n        subgraph Testing_Tasks\n            direction TB\n        end\n        subgraph User_Site_Testing\n            direction TB\n            subgraph Quality_System_Rregulation\n                direction TB\n            end\n            subgraph Understand_Terminology\n                direction TB\n            end\n            subgraph Testing\n                direction TB\n            end\n            Quality_System_Rregulation---Understand_Terminology---Testing\n        end\nConsideration_Before_Testing_Tasks---Code_Based_Testing---Alternatives_to_White_Box_Testing\nDevelopment_Testing---Testing_Tasks---User_Site_Testing\n    end\n\n\n\n\n\n\n\n\n\n\nConsideration Before Testing Tasks\n\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Consideration_Before_Testing_Tasks\n            direction LR\n            subgraph Test_Plans\n                direction TB\n                should_identify_control_measures_like_traceability_analysis---\n                ensure_that_intended_coverage_is_achieved---\n                ensure_that_proper_documentation_is_prepared---\n                conduct_tests_not_by_SW_developers_but_in_other_sites\n            end\n            subgraph Conditions\n                direction TB\n                use_defined_inputs---\n                documented_outcomes---\n                gonnabe_time_consuming_activity---\n                gonnabe_difficult_activity---\n                gonnabe_imperfect_activity---\n                testing_all_program_functionality---\n                does_not_mean_100_prcnt_correction_perfection---\n                make_detailed_objective_evaluation---\n                requires_sophisticated_definition_specificiation---\n                all_test_procedures_data_results_are_documented---\n                all_test_procedures_data_results_are_suitable_for_review---\n                all_test_procedures_data_results_are_suitable_for_objective_decision_making---\n                all_test_procedures_data_results_are_suitable_for_subsequent_regression_testing\n            end\n            subgraph Start_test_planning_as_early_as_possible\n                direction TB\n                make_test_plans---\n                make_test_cases---\n                plan_schedules---\n                plan_environments---\n                plan_resources_of_personnel_tools---\n                plan_methodologies---\n                plan_inputs_procedures_outputs_expected_results---\n                plan_documentation---\n                plan_reporting_criteria\n            end\n            subgraph Testing_Tenets_Inclusion_List\n                direction TB\n                expected_test_outcome_is_predefined---\n                good_test_case_has_high_probability_of_exposing_errors---\n                successful_test_is_one_that_finds_errors---\n                There_is_independence_from_coding---\n                Both_application_for_user_and_SW_for_programming_expertise_are_employed---\n                Testers_use_different_tools_from_coders---\n                Examining_only_the_usual_case_is_insufficient---\n                Test_documentation_permits_its_reuse---\n                Test_documentation_permits_independent_confirmation_---\n                of_pass/fail_test_outcome_during_subsequent_review\n            end\n        Test_Plans---Conditions---Start_test_planning_as_early_as_possible---Testing_Tenets_Inclusion_List\n        end\n\nend\n\n\n\n\n\n\n\n\n\n\nCode Based Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n            subgraph Code_Based_Testing\n                direction LR\n                subgraph white_box_testing\n                    direction TB\n                    identify_dead_code_never_executed---\n                    conduct_unit_test---\n                    conduct_other_level_tests\n                end\n                subgraph Evaluation_of_level_of_white_box_testing\n                    direction TB\n                    use_coverage_metrics---\n                    metrics_of_completeness_of_test_selection_criteria---\n                    coverage_should_be_commensurate_with_level_of_SW_risk---\n                    coverage_means_100_prcnt_coverage\n                end\n                subgraph Coverage_Metrics_of_White_Box_Testing\n                    direction TB\n                    Statement_Coverage---\n                    Decision_or_Branch_Coverage---\n                    Condition_Coverage---\n                    Multi_Condition_Coverage\n                    Loop_Coverage---\n                    Path_Coverage---\n                    Data_Flow_Coverage\n                end\n            white_box_testing---Evaluation_of_level_of_white_box_testing---Coverage_Metrics_of_White_Box_Testing\n            end\nend\n\n\n\n\n\n\n\n\n\n\nSolution to White Box Testing\n\n\n\n\n\n \nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Alternatives_to_White_Box_Testing\n            direction LR\n            subgraph Types_of_Testing_Increasing_Cost\n                direction TB\n                    Normal_Case---\n                    Output_Forcing---\n                    Robustness---\n                    Combinations_of_Inputs\n            end\n            subgraph Weakness_of_functional_and_white_box_testings\n                direction TB\n                difficulty_in_linking_---\n                tests_completion_criteria_to_SW_reliability\n            end\n            subgraph Advanced_software_testing_methods\n                direction TB\n                statistical_testing---\n                provide_further_assurance_of_reliability---\n                generate_randomly_test_data_from_defined_distributions---\n                distribution_defined_by_expected_use---\n                distribution_defined_by_hazardous_use---\n                distribution_defined_by_malicious_use---\n                large_test_data_cover_particular_areas_or_concerns---\n                statistical_testing_provides_high_structural_coverage---\n                statistical_testing_requires_stable_system---\n                structural_and_functional_testing_are_prerequisites_for_statistical_testing\n            end\n            subgraph Change_in_SW\n                direction TB\n                conduct_regression_analysis_and_testing---\n                should_demonstrate_correct_implementation---\n                should_demonstrate_no_adverse_impact_on_other_modules   \n            end\n            subgraph Testing_Tasks\n                direction TB\n                Test_Planning---\n                Structural_Test_Case_Identification---\n                Functional_Test_Case_Identification---\n                Traceability_Analysis_Testing---\n                Unit_Tests_to_Detailed_Design---\n                Integration_Tests_to_High_Level_Design---\n                System_Tests_to_Software_Requirements---\n                Unit_Test_Execution---\n                Integration_Test_Execution---\n                Functional_Test_Execution---\n                System_Test_Execution---\n                Acceptance_Test_Execution---\n                Test_Results_Evaluation---\n                Error_Evaluation_Resolution---\n                Final_Test_Report\n            end\n        Types_of_Testing_Increasing_Cost---Weakness_of_functional_and_white_box_testings---\n        Advanced_software_testing_methods---Change_in_SW---Testing_Tasks\n        end\nend\n\n\n\n\n\n\n\n\n\n\nDevelopment Testing\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph Development_Testing\n            direction LR\n            subgraph unit_level_testing\n                direction TB    \n                focus_on_early_examination_of_sub_program_functionality---\n                ensure_functionality_invisible_at_system_level_examined---\n                ensure_quality_software_units_furnished_for_integration\n            end\n            subgraph integration_level_testing\n                direction TB\n                focuses_on_transfer_of_data---\n                focuses_on_control_across_program's_internal_and_external_interfaces\n            end\n            subgraph system_level_testing\n                direction TB\n                demonstrate_all_specified_functionality_exists---\n                demonstrate_SW_is_trustworthy---\n                verifies_as_built_program's_functionality_and_performance_on_requirements---\n                addresses_functional_concerns_and_intended_uses---\n                like_Performance_issues---\n                like_Responses_to_stress_conditions---\n                like_Operation_of_internal_and_external_security_features---\n                like_Effectiveness_of_recovery_procedures---\n                like_disaster_recovery---\n                like_Usability---\n                like_Compatibility_with_other_SW---\n                like_Behavior_in_each_of_the_defined_hardware_configurations---\n                like_Accuracy_of_documentation\n            end\n            subgraph Error_Detected\n                direction TB        \n                should_be_logged---\n                should_be_classified---\n                should_be_reviewed---\n                should_be_resolved_before_SW_release\n            end\n        unit_level_testing-->integration_level_testing-->system_level_testing-->Error_Detected\n        end\nend\n\n\n\n\n\n\n\n\n\n\nUser Site Testing\n\nOverview\n\n\n\n\nflowchart TB\n    subgraph Testing_Task\n        direction LR\n        subgraph User_Site_Testing\n            direction LR\n            subgraph Quality_System_Rregulation\n                direction TB\n                installation---\n                inspection_procedures---\n                testing_appropriateness---\n                documentation_of_inspection---\n                testing_to_demonstrate_proper_installation\n            end\n            subgraph Understand_Terminology\n                direction TB\n                beta_test---\n                site_validation---\n                user_acceptance_test---\n                installation_verification---\n                installation_testing\n            end\n            subgraph Testing\n                direction TB\n                subgraph Requirements\n                    direction TB\n                    either_actual_or_simulated_use---\n                    verification_of_intended_functionality---\n                    constant_contact_FDA_center\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n    \n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_System_Ability\n                        direction TB\n        \n                    end\n                  subgraph Evaluation_of_User_Ability\n                        direction TB\n        \n                    end \n                    subgraph Evaluation_of_Operator_Ability\n                        direction TB\n        \n                    end\n                constant_contact_FDA_center-->Follow_Predefiened_Plan-->Documented_Evidence-->\n            Evaluation_of_System_Ability-->Evaluation_of_User_Ability-->Evaluation_of_Operator_Ability    \n                end \n                        \n            \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements-->User_Site_Testing_Task\n        end\n        Quality_System_Rregulation-->    Understand_Terminology-->Testing-->User_Site_Testing_Task\n        end\nend\n\n\n\n\n\n\n\n\n\n\nTesting\n\n\n\n\nflowchart TB\n            subgraph Testing\n                direction LR\n                subgraph Requirements\n                    direction LR\n                    subgraph Follow_Predefiened_Plan\n                        direction TB\n                        either_actual_or_simulated_use---\n                        verification_of_intended_functionality---\n                        constant_contact_FDA_center---\n                        formal_summary_of_testing---\n                        record_of_formal_acceptance\n                    end\n                    subgraph Documented_Evidence\n                        direction TB\n                        testing_plan_of_full_range_of_operating_conditions---\n                        testing_plan_to_detect_any_latent_faults---\n                        all_testing_procedures---\n                        test_input_data---\n                        test_results---\n                        hardware_installation_and_configuration---\n                        software_installation_and_configuration---\n                        exercising_measure_of_all_system_components---\n                        versions_of_all_system_components           \n                    end\n                    subgraph Evaluation\n                        direction TB\n                      subgraph Evaluation_of_System_Ability\n                            direction TB\n                            high_volume_of_data---\n                            heavy_loads_or_stresses---\n                            security\n                            subgraph fault_testing\n                                direction TB\n                                avoidance---\n                                detection---\n                                tolerance---\n                                recovery\n                            end\n                        security---fault_testing---\n                        error_message---\n                        implementation_of_safety_requirements\n                        end\n                      subgraph Evaluation_of_User_Ability\n                            direction TB\n                            ability_to_understand_system---\n                            ability_to_interface_with_system\n                        end \n                        subgraph Evaluation_of_Operator_Ability\n                            direction TB\n                            ability_to_perform_intended_functions---\n                            ability_to_respond_in_alarms---\n                            ability_to_respond_in_warnings---\n                            ability_to_respond_in_error_messages\n                        end\n\n                    end\n            Follow_Predefiened_Plan-->Documented_Evidence-->\n            Evaluation_of_System_Ability-->Evaluation_of_User_Ability-->Evaluation_of_Operator_Ability    \n            end     \n            subgraph User_Site_Testing_Task\n                direction TB\n                Acceptance_Test_Execution2---\n                Test_Results_Evaluation2---\n                Error_Evaluation_Resolution2---\n                Final_Test_Report2  \n            end\n        Requirements-->User_Site_Testing_Task\n        end"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#maintenance-and-software-changes",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#maintenance-and-software-changes",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Maintenance and Software Changes",
    "text": "Maintenance and Software Changes\n\n\n\n\nflowchart LR\n    subgraph Hardware_VS_Software\n        direction LR\n        subgraph HW_maintenance_Inclusion\n            direction TB\n            preventive_hardware_maintenance_actions--- \n            component_replacement---\n            corrective_changes\n        end\n        subgraph SW_maintenance_Inclusion\n            direction TB\n            corrective---\n            perfective---\n            adaptive_maintenance---\n            not_include_preventive_maintenance_actions---\n            not_include_software_component_replacement\n        end\n    end\n    subgraph Maintenance_Type\n        direction TB\n        Corrective_maintenance---\n        Perfective_maintenance---\n        Adaptive_maintenance---\n        Sufficient_regression_analysis---\n        Sufficient_regression_testing\n    end\n    subgraph Factors_of_Validation_for_SW_change\n        direction TB\n        type_of_change---\n        development_products_affected---\n        impact_of_those_products_on_operation\n    end\n    subgraph Factors_of_Limitting_Validation_Effort\n        direction TB\n        documentation_of_design_structure---\n        documentation_of_interrelationships_of_modules---\n        documentation_of_interrelationships_of_interfaces---\n        test_documentation---\n    test_cases---\n        results_of_previous_verification_and_validation_testing\n    end\n    subgraph Maintenance_tasks\n        direction TB\n        Software_Validation_Plan_Revision---\n        Anomaly_Evaluation---\n        Problem_Identification_and_Resolution_Tracking---\n        Proposed_Change_Assessment---\n        Task_Iteration---\n        Documentation_Updating\n    end\nHardware_VS_Software-->Maintenance_Type-->Factors_of_Validation_for_SW_change-->\nFactors_of_Limitting_Validation_Effort-->Maintenance_tasks"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-of-quality-system-software",
    "href": "docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html#validation-of-quality-system-software",
    "title": "FDA Software Validation Guidance Presentation",
    "section": "Validation of Quality System Software",
    "text": "Validation of Quality System Software\n\nOverview\n\n\n\n\nflowchart LR\n    subgraph Use_of_Computers_and_automated_equipment\n        direction TB\n        medical_device_design---\n        laboratory_testing_and_analysis---\n        product_inspection_and_acceptance---\n        production_and_process_control---\n        environmental_controls---\n        packaging---\n        labeling---\n        traceability---\n        document_control---\n        complaint_management---\n        programmable_logic_controllers---\n        digital_function_controllers---\n        statistical_process_control---\n        supervisory_control_and_data_acquisition---\n        robotics---\n        human_machine_interfaces---\n        input_output_devices---\n        computer_operating_systems\n    end\n    subgraph Factors_in_Validation\n        direction TB\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System-->Validation_Supporting_Factors-->Factors_of_Validation_Evidence_Level-->\nFactors_of_Easing_Validation_Effort\n    end\n    subgraph Documented_User_Requirements\n        direction TB\n        intended_use_of_software_or_automated_equipment---\n      level_of_dependency_on_software_or_equipment\n    end\n    subgraph List_That_Must_Be_Defined_by_User\n        direction TB\n        \n    end\n    subgraph Documentation_List\n        direction TB\n        documented_protocol---\n        documented_validation_results\n        subgraph Documented_Test_Cases\n            direction TB\n        \n        end\n        documented_validation_results---Documented_Test_Cases\n    end\n\n    subgraph Manufaturer's_Responsbility\n        direction TB\n        \n    end\nUse_of_Computers_and_automated_equipment---Factors_in_Validation---Documented_User_Requirements---\nList_That_Must_Be_Defined_by_User---Documentation_List---Manufaturer's_Responsbility\n\n\n\n\n\n\n\n\n\n\nFactors in Validation\n\n\n\n\nflowchart LR\n    subgraph Factors_in_Validation\n        direction LR\n        subgraph Validation_Factors_of_Quality_System\n            direction TB\n            subgraph 21_CFR_Part_11_Requirements\n                direction TB\n                electronic_records_regulation---\n                electronic_signatures_regulation---\n                regulations_establishment---\n                security---\n                data_integrity---\n                validation_requirements \n            end\n        end\n        subgraph Validation_Supporting_Factors\n            direction TB\n            verifications_of_outputs_from_each_stage--- \n            verifications_of_outputs_throught_SW_life_cycle---\n            checking_for_proper_operation_in_intended_use_environment\n        end\n        subgraph Factors_of_Validation_Evidence_Level\n            direction TB\n            risk_posed_by_automated_operation---\n            complexity_of_process_software---\n            degree_of_dependence_on_automated_process\n        end\n        subgraph Factors_of_Easing_Validation_Effort\n            direction TB\n            planning---\n            documented_requirments---\n            risk_analysis   \n        end\n    Validation_Factors_of_Quality_System-->Validation_Supporting_Factors-->Factors_of_Validation_Evidence_Level-->\nFactors_of_Easing_Validation_Effort\n    end"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html",
    "title": "Content List, Validation",
    "section": "",
    "text": "0000-00-00, EN62304"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#fda",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#fda",
    "title": "Content List, Validation",
    "section": "2 FDA",
    "text": "2 FDA\n\n2023-01-27, General Principles of SW Validation\n2023-01-27, General Principles of SW Validation - Diagram Summary\n1111-11-11, Guidance for the Content of Premarket Submissions for Software Contained in Medical Devices"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#dhf",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#dhf",
    "title": "Content List, Validation",
    "section": "3 DHF",
    "text": "3 DHF"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#public-health",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#public-health",
    "title": "Content List, Validation",
    "section": "4 Public Health",
    "text": "4 Public Health"
  },
  {
    "objectID": "docs/blog/posts/Surveilance/guide_map/index.html#wet-lab",
    "href": "docs/blog/posts/Surveilance/guide_map/index.html#wet-lab",
    "title": "Content List, Validation",
    "section": "5 Wet Lab",
    "text": "5 Wet Lab\n\n0000-00-00, PCR (Polymerase Chain Reaction) Experiment"
  },
  {
    "objectID": "docs/CV/index.html",
    "href": "docs/CV/index.html",
    "title": "CV",
    "section": "",
    "text": "Curriculum Vitae\n\nResume(2 Pages) and CV(more than 2 pages)\n\n\n\n[Resume] English Version\n[Resume] Korean Version\n[CV] English Version\n[CV] Korean Version\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/projects/index.html",
    "href": "docs/projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\n\nPLAN, MATH, SP, STAT, ML, and DL stands for 'Intellectual Property Planning', 'Mathematics', 'Signal Processing', 'Statistics', 'Machine Learning', and \"Deep Learning\", respectively.\n\n\n\n[STAT][Surveilance] Diagnostic Algorithm Validation for FDA (working on)\n[STAT][ML] LLFS (Long Life Family Study) (working on)\n[STAT][ML] Diagnostic Device QC Platform (Completed)\n[PLAN] Platform IP Planning (To be written)\n[ML] Data-Driven Diagnostic Algorithm (To be written)\n[STAT][SP] Clinical Data Analysis for QC (To be written)\n[ML][MATH][Biology] Heavy Metal Removal Algorihtm using Tea Leaves (To be written)\n[ML] Diffusion Model of Social Networks using Genetic Algorithm (To be written)\n[Biochemistry] Effects of Phellinus Linteus toward Formation of Lymphatic Vessel (To be written)\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/projects/LLFS/data_preparation.html",
    "href": "docs/projects/LLFS/data_preparation.html",
    "title": "Data Preparation",
    "section": "",
    "text": "flowchart TB\n    subgraph Simulation\n        direction TB\n        subgraph Assign_Setting_Values\n            direction LR\n            Assign_Sample_Size---\n            Assign_Dimension_Size---\n            Assign_Covariance_Correlation---\n            Assign_Several_Proportions---\n            Assign_Noise_Intensity\n        end\n        subgraph Generate_Metabolite_Variables\n            direction LR\n            Generate_Covariance_Matrix---\n            Apply_Noise_to_Covariance---\n            Generate_Weights_Matrix---\n            Use_MVN_Distribution---\n            Generate_Metabolite_Data\n        end\n        subgraph Generate_Outcome_Variable\n            direction LR\n            Calculate_Score_Matrix---\n            Use_Logit_Link---\n            Calculate_Outcome_Probabilities---\n            Use_Binomial_Distribution1---\n            Generate_Binary_Outcome_Data\n        end\n        subgraph Generate_Sex_Variable\n            direction LR\n            Use_Binomial_Distribution2---\n            Generate_Sex_Data\n        end\n        subgraph Generate_Age_Variable\n            direction LR\n            Search_the_Strongest_Metabolite---\n            Rescale_It_to_Age---\n            Generate_Age_Data\n        end\n        subgraph Generate_Genotype_Variable\n            direction LR       \n            Calculate_Marginal_Proportions---\n            Calcualte_Joint_Proportions---\n            Generate_Genotype_data\n        end\n        subgraph Merge_All_Data\n            direction LR\n            Outcome_Variable---\n            Sex_Variable---\n            Age_Variable---\n            Genotype_Variable---\n            Metabolite_Data\n        end\n        Assign_Setting_Values-->Generate_Metabolite_Variables-->Generate_Outcome_Variable-->\n        Generate_Sex_Variable-->Generate_Age_Variable-->Generate_Genotype_Variable-->\n        Merge_All_Data\n    end\n    subgraph Data_Analytics\n        direction LR\n        exploratory_data_analysis---\n        statistical_analysis---\n        machine_learning\n    end\n    subgraph Conclusion\n        direction LR\n    end\n    Simulation-->Data_Analytics-->Conclusion\n\n\n\n\n\n\n\n\n\nMVN: Multivariate Normal Distirubtion\n\n\n\n\nKorean\n\n\n\n\nEnglish\n\n\n\n\n\n대략적인 분석 방법론을 간단히 보여주기 위해 Simulation을 수행했다. 이해를 돕기위해 Simulation 순서도를 간략히 설명하자면 크게 7 단계로 Simulation을 수행했다.\n\n\nData Set Size Setting\nSimulation에 필요한 몇 가지 설정값들을 Global Variables로 설정하여 후차적으로 작성된 스크립트에서 호출이 자유롭도록 작성했다. 변수들은 아래의 Simulation section에 있는 Global Variables (see Section 2.2) 에서 확인 가능하다.\nCategorial Data Setting\n먼저, 고차원 데이터의 차원을 설정하기 위해 Sample Size와 변수의 수를 설정한 후 Categorical predictors를 만들기 위해 잘 알려진 분포, 내가 정한 분포, 혹은 임의로 발생하게 만든 분포를 설정하였다. (see Section 2.2, Section 2.3, and Section 2.4)\nSex Variable Setting\nSex 변수는 \\(X \\sim \\text{Bernoulli}(0.5)\\) 을 통해 data를 생성했다. (see Section 2.4)\nGenotype Variable Setting\nGenotype 변수의 data는 아직도 어떻게 통계적으로 생성해야하는지 감을 못잡은 상태이기 때문에 더 연구가 필요하다. 하지만, 질병에 대한 유전적 영향도는 반영해야하기 때문에 outcome variable과 이미 잘 알려진 genotype의 분포를 반영하려고 노력했다. 두 변수에 연관성을 갖게하기 위해 각 변수의 proportion을 marginal distirubtion으로 설정하여 두 변수의 joint proportion을 계산하여 Genotype data를 생성했다. (see Section 2.3 and Section 2.4)\nMetabolite Data Setting\n고차원의 metabolite data를 만드는 설정으로, 고차원이면서 그룹내 서로 상관 관계가 있는 변수들을 생성하기 위해 난수에 의해 발생되는 임의의 Covariance를 생성하여 MVN (Multivariate Normal Distribution)에 반영되게 했고 각 그룹의 반응 변수로의 영향(또는 가중치)도 또한 난수로 임의적으로 발생되게 설정했다. 이때, 난수에 의해 임의적으로 발생하는 수치는 내가 임의적으로 범위를 한정했다. 재현성을 위해 seed number를 고정했다. (see Section 2.4)\nOutcome Variable Setting\nMVN에 의해 만들어진 Data와 미리 만들어 놓은 가중치 Matrix의 곱을 통해 Score Matrix를 만들고 Logit Link를 이용하여 각 Sample의 확률값을 만들었다. 각 Sample의 확률값을 기반으로 \\(X \\sim \\text{Bernoulli}(p)\\), (여기서 \\(p\\)는 각 sample이 갖는 확률값을 뜻한다), 을 통해 disease status의 정보를 담은 binary outcome variable를 만들었다. (see Section 2.4)\nAge Variable Setting\nAge 변수는 생물학적, 의학적으로 치매와 연관성이 높은 요인으로 Outcome 변수로 가장 설명이 잘되는 metabolite를 탐색해 선별하여 Age 형태로 변환을 했다. 제일 어린 사람을 65세 그리고 제일 연장자를 105세로 설정하여 min max normalization을 적용했다. (see (see Section 2.3 and Section 2.4)\nMerge All Data\nSimulation을 통해 만들어진 각 변수들을 index를 만들어 병합시켜 data frame의 형태로 만들었다. (see Section 2.4)\nAnalytics & Conclusion\n분석 부분은 이 data preparation section에서는 자세히 기술하지 않고 EDA, Statistical Approach 및 ML Approach Section에서 자세히 다룰예정이다. 간략히 말하면, outcome 변수와 통계적으로 유의한 관계를 갖는 metabolite를 선별하고 그 결과가 machine learning을 이용하여 얻은 결과와 얼마나 같은지 비교 분석을 하여 Outcome variable에 가장 연관성이 있는 변수들을 규명하는 방법을 기술할 예정이다.\n\n\n\n\n\n\n\n\n\n\nShow the code\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nShow the code\n# the number of samples\nsample_size <- 500 #1000\n# the number of predictors\npredictor_size <- 1000 #5000\n# the number of groups\ngroup_size <- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors <- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors<-floor(significant_predictors*0.4) \nnegatively_associated_predictors<-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list<-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%>%round(3) \nnames(group_proportion_list)<-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix <- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data<-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%>%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%>%round(0),\n            # the 1st index of predictors in each group\n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE),\n            # effect of each group on an outcome variable \n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]<-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]<-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata<-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix<-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients <- rnorm(predictor_size,0,0.05)\n\nanswer_list<-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nShow the code\n## Function List\n\nscale_function=function(vector=x,min=NULL,max=NULL,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data \n    # that are statistically associated with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)<-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite<-\n        temp_df%>%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%>%\n        filter(abs_mean_diff==max(abs_mean_diff))%>%\n        dplyr::select(metabolite)%>%pull\n    \n    ## generate age data with min max normalization\n    age_data<-\n        data%>%\n        dplyr::select(strong_metabolite)%>%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%>%\n        rename(age=1)%>%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated with \n    # some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n    binary_proportion<-as.numeric(table(in_response)/sample_size) #the simulated proportion for the disease vs non-disease cases\n    genotype_proportion<-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion<-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nShow the code\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range <- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] <- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) <- 1\n    data[, group_range] <- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] <-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names<-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] <- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]<-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities <- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%>%\n    ifelse(.>1,1,.)%>%\n    ifelse(.<0,0,.)\n\nresponse <-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%>%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data <- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data <- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data <- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data<-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%>%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nShow the code\n# load simulation data\nsimulated_data<-read_rds(datapath)\n\n# simple data pre-processing\nall_data<-\n    simulated_data%>%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]<-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\nThis data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers.\n\n\n\n\nSimulations were performed to outline the approximate analysis methodology. For your better understanding, I will briefly describe the simulation flow diagram. The simulation was conducted in 9 steps.\n\n\nData Set Size Setting\nSome of the setting values ​​required for simulation are set as global variables so that they can be freely called in the later scripts. (see Section 2.2)\nCategorial Data Setting\nI first set the dimensions of my high-dimensional data by setting the sample size and number of variables, then I created categorical data by choosing a well-known distribution, a distribution I determined, or a distribution that occurred randomly set. (see Section 2.2, Section 2.3, and Section 2.4)\nSex Variable Setting\nThe data of the sex variable were generated through \\(X \\sim \\text{Bernoulli}(0.5)\\). (Section 2.4)\nGenotype Variable Setting\nPersonally, I have not yet figured out how to generate data for genotype (categorical) variables statistically, so further research is needed. However, since the genetic influence on the disease should be reflected, I tried to reflect the distribution of outcome variables and well-known distribution of the genotypes, APOE (from Wiki). To make an association between the two variables, the proportions of each variable were set as marginal distirubtion and the joint distribution of the two variables was calculated to generate genotype data. (Section 2.3 and Section 2.4)\nMetabolite Data Setting\nAs a setting for generating high-dimensional metabolomic data, a covariance matrix generated by random numbers is generated to create high-dimensional and mutually correlated metabolites within a group, which is used as input in the MVN (Multivariate Normal Distribution) function, and for each group, the metabolites’ effect (or weight) toward the outcome variable is also set to be randomly generated with a random number. At this time, the range of numbers randomly generated by random numbers was arbitrarily limited by myself. A seed number was fixed for reproducibility. (Section 2.4)\nOutcome Variable Setting\nA score matrix was created through the matrix multiplication of the data created by MVN and a pre-made weight matrix with the probability values of samples that were created using the Logit Link. Based on the probability value of each sample, a binary outcome variable representing disease status information was created through \\(X \\sim \\text{Bernoulli}(p)\\), (where \\(p\\) means the probability value of each sample). (Section 2.4)\nAge Variable Setting\nSince the Age variable is a important factor related to dementia biologically and medically, the metabolite best explained as an Outcome variable was selected and converted into an age scale using min-max normalization by setting the youngest to 65 and the oldest to 105. (Section 2.3 and Section 2.4)\nMerge All Data\nEach variable created through simulation was merged into a data frame. (Section 2.4)\nAnalytics & Conclusion\nThe analysis part will not be discussed in detail in this data preparation section, but will be covered in detail in the EDA, Statistical Approaches and ML Approaches section. Briefly, I describe a method to identify the variables most associated with the outcome variable by selecting metabolites that have a statistically significant relationship with the outcome variable and comparing how similar the results are to those obtained through machine learning.\n\n\n\n\n\n\n\n\n\n\nShow the code\nif(!require(janitor)) install.packages(\"janitor\") \nif(!require(tidyverse)) install.packages(\"tidyverse\") \nif(!require(tidymodels)) install.packages(\"tidymodels\") \nif(!require(glmnet)) install.packages(\"glmnet\") \nif(!require(MASS)) install.packages(\"MASS\") \nif(!require(ggpubr)) install.packages(\"ggpubr\") \nif(!require(car)) install.packages(\"car\") \nif(!require(mixOmics)) install.packages(\"mixOmics\") \n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(ggpubr) \nlibrary(car) \nlibrary(mixOmics)\nset.seed(20230103) \nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n\n\n\n\n\n\n\nShow the code\n# the number of samples\nsample_size <- 500 #1000\n# the number of predictors\npredictor_size <- 1000 #5000\n# the number of groups\ngroup_size <- sample(6:10,1) # at least more than 6, the number of the genotypes\n# the number of predictors truly associated with a response variable\nsignificant_predictors <- floor(predictor_size*sample((50:100)/1000,1)) \n\n## set the predictors associated with an outcome\n### the number of predictors positively associated with an outcome\n### the number of predictors negatively associated with an outcome\npositively_associated_predictors<-floor(significant_predictors*0.4) \nnegatively_associated_predictors<-significant_predictors-positively_associated_predictors \n\n## set the proportion of the groups in which the predictors are correlated with one another\n### randomly sampling proportions to become their sum equal to 1\ngroup_proportion_list<-sample(seq(1,1+2*(100-group_size)/group_size,\n                            by=2*(100-group_size)/(group_size*(group_size-1)))/100,\n                        group_size,replace=FALSE)%>%round(3) \nnames(group_proportion_list)<-paste0(\"group\",1:length(group_proportion_list))\n### initialize a matrix with a size as sample_size by predictor_size\npredictor_matrix <- matrix(0, ncol = predictor_size, nrow = sample_size)\n### initialize a data frame and assign meta information used to generate simulated data\ngroup_meta_data<-\n    data.frame(\n        group_name=c(names(group_proportion_list)),\n        proportion=group_proportion_list)%>%\n        mutate(\n            # the number of predictors within each group \n            group_n=(predictor_size*group_proportion_list)%>%round(0),\n            # the 1st index of predictors in each group \n            first_index=c(1,cumsum(group_n[-length(group_proportion_list)])+1), \n            # the last index of predictors in each group\n            last_index=cumsum(group_n),\n            # within-group correlations among the within-group predictors \n            group_correlation=sample((0:700)/1000,length(group_proportion_list),replace=TRUE), \n            # effect of each group on an outcome variable\n            group_effect=sample((-40:30)/100,length(group_proportion_list),replace=TRUE)) \n### set a group effect as 0.7 into a group with the smallest group number \ngroup_meta_data[which.min(group_meta_data[,\"group_n\"]),\"group_effect\"]<-0.7\n\n### set a group effect as -0.5 into a group with the second smallest group number \ngroup_meta_data[group_meta_data[,\"group_n\"]==(sort(group_meta_data[,\"group_n\"])[2]),\"group_effect\"]<-(-0.5)\n\n# initialize a data matrix to assign simulated values\n## add some noise to data\ndata<-matrix(rnorm(sample_size*predictor_size,mean=0,sd=0.05), \n             nrow = sample_size, ncol = predictor_size)\n\n# initialize a covariance matrix to assign simulated values\ncovariance_matrix<-matrix(rnorm(predictor_size*predictor_size,mean=0,sd=0.05),\n                          nrow=predictor_size, ncol=predictor_size)\nbeta_coefficients <- rnorm(predictor_size,0,0.05)\n\nanswer_list<-list(\n    'sample size'=sample_size,\n    'predictor size'=predictor_size,\n    'group size'=group_size,\n    'significant predictors'=significant_predictors,\n    'positively associated predictors'=positively_associated_predictors,\n    'negatively associated predictors'=negatively_associated_predictors,\n    'group proportion list'=group_proportion_list,\n    'group meta data'=group_meta_data,\n    'data noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'covariance noise intensity'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'effect noise intensity on response'=c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'link function between the response and predictors' = 'canonical logit link function',\n    'link function noise intensity' = c('distribution'='rnorm','mean'=0,'sd'=0.05),\n    'age_distirbution'='used data of a variable with the highest effect on outcome',\n    'sex_distribution'='rbinom(n=sample size,p=0.5)',\n    'genotype_distirbution'=c('e2' = '8.4%','e3' = '77.9%','e4' = '13.7%'))\n\n\n\n\n\n\n\nShow the code\n## Function List\n\nscale_function=function(vector=x,min=a,max=b,method=\"customized\"){\n    if(method==\"min-max\"){\n        result=(vector-min(vector))/(max(vector)-min(vector))\n    }else if(method==\"customized\"){\n        result=(max-min)*(vector-min(vector))/(max(vector)-min(vector))+min\n    }else{\n        result=(vector-mean(vector))/sd(vector)\n    }\n  return(result)\n}\n\nage_data_generator=function(in_data,in_response,fun=scale_function){\n    # this function generates a age (continuous) data that are statistically associated \n    # with a simulated variable as designed above.\n\n    ## conduct t test with the response and each variable generated by multivariate normal distributions.\n    ## search a variable with the largest difference in mean between the two groups or with the lowest p value\n    ## In this case, I will pick the former one. \n    ## (I don't care about the multiple testing problems for now)\n    temp_df=as.data.frame(matrix(ncol=5)) # initialize an empty data frame\n    for(i in 1:ncol(in_data)){\n        temp_df[i,]=c(\n            names(in_data)[i],\n            t.test(in_data[,i]~in_response)$estimate[1],\n            t.test(in_data[,i]~in_response)$estimate[2],\n            t.test(in_data[,i]~in_response)$estimate[2]-t.test(data[,i]~response)$estimate[1],\n            t.test(in_data[,i]~in_response)$p.value)\n    }\n    names(temp_df)<-c('metabolite','mean_neg','mean_pos','mean_diff','p.value')\n\n    ## search a variable with the largest difference in mean\n    strong_metabolite<-\n        temp_df%>%\n        mutate(\n            mean_neg=as.numeric(mean_neg),\n            mean_pos=as.numeric(mean_pos),\n            mean_diff=as.numeric(mean_diff),\n            p.value=as.numeric(p.value),\n            abs_mean_diff=abs(mean_diff))%>%\n        filter(abs_mean_diff==max(abs_mean_diff))%>%\n        dplyr::select(metabolite)%>%pull\n    \n    ## generate age data with min max normalization\n    age_data<-\n        data%>%\n        dplyr::select(strong_metabolite)%>%\n        scale_function(vector=.,min=65,max=105,method=\"customized\")%>%\n        rename(age=1)%>%round(0)\n    return(age_data)\n}\n\n\ngenotype_data_generator=function(in_response=response,fun=scale_function){\n    # this function generates a genotype (categorical) data \n    # that are jointly and statistically associated with a continuous data and a binary data \n    # (I am not so sure if I can generate data that are statistically associated \n    # with some fake metabolite data. But, I will give it a try).\n\n    ## Declare the marginal proportions \n    ## for binary (affected vs unaffected) and a genotype (categorical) data, respectively\n\n    ##the simulated proportion for the disease vs non-disease cases\n    binary_proportion<-as.numeric(table(in_response)/sample_size) \n    genotype_proportion<-c(0.084,0.779,0.137) # the known proportion of APOE genotypes from Wiki\n\n    ## Declare the joint proportion predictor matrix\n    joint_proportion<-matrix(\n        c(binary_proportion[1]*genotype_proportion, # for the unaffected cases\n        binary_proportion[2]*genotype_proportion),  # for the affected cases\n        ncol=2, byrow=FALSE)\n\n    # Generate the genotype (catogrical) data\n    genotype_data = numeric(sample_size) # initialize a vector \n    for (i in 1:sample_size) {\n        genotype_data[i] = sample(\n            c('e2','e3','e4'),\n             1, \n             prob=joint_proportion[,ifelse(grepl('neg',in_response[i]),1,2)])\n    }    \n    return(genotype_data)\n}\n\n\n\n\n\n\n\nShow the code\n# generate simulation data using multivariate normal distribution\nfor (i in 1:nrow(group_meta_data)) {\n    \n    group_range <- group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]\n    for (j in group_range){\n        for(k in group_range){\n        covariance_matrix[j, k] <- group_meta_data[i, \"group_correlation\"]\n        }\n    }\n    #covariance_matrix[group_range, group_range]+group_meta_data[i, \"group_correlation\"]    \n    diag(covariance_matrix) <- 1\n    data[, group_range] <- \n        mvrnorm(n = sample_size, \n                mu = rep(0,group_meta_data[i,\"group_n\"]),\n                Sigma = covariance_matrix[group_range, group_range])\n    data=as.data.frame(data)\n    beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] <-\n        beta_coefficients[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]+\n        group_meta_data[i,\"group_effect\"]\n    predictor_names<-paste0(group_meta_data[i,\"group_name\"],\"_\",1:group_meta_data[i,\"group_n\"])\n    names(beta_coefficients)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]] <- predictor_names\n    names(data)[group_meta_data[i, \"first_index\"]:group_meta_data[i, \"last_index\"]]<-predictor_names\n        \n}\nscore=as.matrix(data)%*%beta_coefficients # score of each sample\n\n# logistic function to get a probability, intercept = 0, \n## set probabilities-0.2 to apply noise and negative probabilities into 0\nprobabilities <- \n    ((1/(1+exp(-(0+score))))-rnorm(sample_size,m=0.2,sd=0.05))%>%\n    ifelse(.>1,1,.)%>%\n    ifelse(.<0,0,.)\n\nresponse <-\n    ifelse(rbinom(sample_size, 1, probabilities)==0,'negative','positive')%>%\n    factor(.,levels=c('negative','positive'))\n\n# simulate sex data\nsex_data <- rbinom(sample_size,1,0.5)\n\n# simulate age data\nage_data <- age_data_generator(in_data=data,in_response=response)\n\n# simulate genotype data\ngenotype_data <- genotype_data_generator(in_response=response)\n\n# merge the univariables\nphenotype_data<-\n    data.frame(\n        id=1:sample_size,\n        outcome=response,\n        age=age_data,\n        sex=sex_data,\n        genotype=genotype_data)\n\nall_data=inner_join(\n    phenotype_data,\n    data%>%mutate(id=1:n()),\n    by=\"id\")\n\n#write_rds(all_data,\"./docs/data/llfs_simulated_data.rds\")\n\n\n\n\n\n\n\nShow the code\n# load simulation data\nsimulated_data<-read_rds(datapath)\n\n# simple data pre-processing\nall_data<-\n    simulated_data%>%\n    mutate(\n      outcome=factor(outcome,levels=c(\"negative\",\"positive\")),\n      sex=ifelse(sex==0,\"man\",\"woman\"),\n      sex=factor(sex,levels=c(\"man\",\"woman\")),\n      genotype=factor(genotype,levels=c(\"e3\",\"e2\",\"e4\"))\n      )\n\n# rename metabolite variables\nnames(all_data)[6:ncol(all_data)]<-paste0(\"meta\",1:predictor_size)\n\n\n\n\n\n\n\n\n\nThis data include 500 samples and 1005 variables:\n\nid: sample ID.\noutcome: a disease status (positive, negative), positive is an affected status, negative is an unaffected status, and the reference group is positive.\nage: age\nsex: sex (man, woman) and the reference group is man.\ngenotype: APOE genotypes\n\nthe apolipoprotein \\(\\epsilon\\) (APOE) is a protein produced in the metabolic pathway of fats in mammals, a genotype of which seems to be related to Alzheimer’s disease (AD). APOE is polymorphic and has three major alleles, \\(\\epsilon 2\\) (e2), \\(\\epsilon 3\\)(e3), and \\(\\epsilon 4\\) (e4). The statistics of the polymorphism are 8.4% for e2, 77.9% for e3, and 13.7% for e4 in worldwide allel frequency, respectively. It is known that the e2, e3, and e4 allels are associated with the protective factor, the neutral one, and the risk one with regard to AD. However, this finding has not been replicated in a large population. Therefore, it is known that we do not know their true associations with AD in the true population. (from Wiki)\n\nThere are 6 combinations of the genotypes:\n\ne2/e2\ne2/e3\ne2/e4\ne3/e3\ne3/e4\ne4/e4\n\nIn this study, I generated the 3 major alleles, e3, e4, e2\n\n\nmeta1 ~ meta1000: a list of metabolites that were blood-sampled from the APOE carriers."
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#summary-of-exploratory-data-analysis-eda",
    "href": "docs/projects/LLFS/eda.html#summary-of-exploratory-data-analysis-eda",
    "title": "EDA",
    "section": "1.1 Summary of Exploratory Data Analysis (EDA)",
    "text": "1.1 Summary of Exploratory Data Analysis (EDA)"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#eda",
    "href": "docs/projects/LLFS/eda.html#eda",
    "title": "EDA",
    "section": "2.1 EDA",
    "text": "2.1 EDA\n\n2.1.1 Univariable Analysis\n\n2.1.1.1 Normality Test\n\n\nCode\n# raw data\nnormality_test_result<-multiple_shapiro_test(all_data)%>%\n    filter(column_name!='id')%>%\n    group_by(type)%>%\n    summarise(count=n())%>%\n    mutate(proportion=round(count/sum(count),3),\n    total=sum(count))%>%\n    dplyr::select(type,total,everything())\n    \nnormality_test_result%>%\n    knitr::kable(caption=\"Summary of the Result of Shapiro Wilk Tests on Numeric Variables\")\n\n\n\nSummary of the Result of Shapiro Wilk Tests on Numeric Variables\n\n\ntype\ntotal\ncount\nproportion\n\n\n\n\nnormal\n1001\n1001\n1\n\n\n\n\n\nOut of 1001 numeric variables, the variables following a normal distribution are 1001 (100%) and the ones that do not are NA (NA%).\n\n\n2.1.1.2 16 Variables That Follow Normal Distributions\n\n\nCode\n# 16 numeric variables randomly selected\nnormal_variables<-\n    multiple_shapiro_test(all_data)%>%\n        filter(p_value>0.05,column_name!='id')%>%\n            dplyr::select(column_name)%>%\n            pull%>%sample(16)\n\nnormal_data<-\n    all_data%>%\n        dplyr::select(outcome,normal_variables)%>%\n        gather(key=metabolite,value=value,normal_variables)\n\nnormal_data%>%\n    ggplot(aes(x=value))+\n    geom_histogram(aes(y=..density..))+\n    geom_density(color='red',linewidth=1.5)+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%>%\n    ggplot(aes(x=metabolite,y=value))+\n    geom_boxplot()+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%>%\n    ggplot(aes(x=value,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution Grouped by Disease Status\")    \n\n\n\n\n\nCode\nnormal_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution Grouped by Disease Status\")\n\n\n\n\n\n\n\n2.1.1.3 Variables That Do Not Follow Normal Distributions\nThere is no variable that do not follow a normal distribution.\n\n\n\n2.1.2 Bivariable Analysis\n\n2.1.2.1 AD vs Metabolites\nThrough the exploratory data analysis above, it was confirmed that all variables follow a normal distribution, and t tests were conducted to select metabolites that have significant relationships with the disease status, AD. To minimize type 1 error due to multiple testings, bonferroni correction was used in the EDA\n\nHomoscedasticity Test\n\nLeven’s test is performed to confirm that each variable has equality of variance, one of the assumptions of the t test and ANOVA.\n\n\nCode\nleven_test_result<-\n    main_statistical_test(in_data=all_data,method=\"levene\",categorical_variable=\"outcome\")%>%\n    filter(column_name!='id')\n\nhomo_variable<-leven_test_result%>%\nfilter(p_adjusted>0.05)%>%\ndplyr::select(column_name)%>%\npull()%>%\nunique()\n\nhetero_variable<-leven_test_result%>%\nfilter(p_adjusted<0.05)%>%\ndplyr::select(column_name)%>%\npull()%>%\nunique()\n\n\n\n10 variables randomly selected out of 1001 variables with equal variance: meta408, meta959, meta796, meta957, meta740, meta212, meta171, meta769, meta986, meta178\n0 variables randomly selected with equal variance:\n\n\n\nCode\nhomo_variable_sample<-homo_variable%>%sample(10)\nhetero_variable_sample<-hetero_variable\n\nstratified_levene_data<-all_data%>%\n    dplyr::select(outcome,homo_variable_sample,hetero_variable_sample)%>%\n    gather(key=metabolite,value=value,c(homo_variable_sample,hetero_variable_sample))%>%\n    mutate(levene_test=ifelse(metabolite%in%(homo_variable_sample),\"homoscedasticity\",\"heteroscedasticity\"))\n\nstratified_levene_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(stratified_levene_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    facet_wrap(.~levene_test)+\n    labs(title=\"Boxplot, Metabolites with Heteroscedasticity vs Homoscedasticity\")\n\n\n\n\n\n\nUnpaired Two Sample Mean t Test\n\n\n\nCode\nt_test_result<-\n    main_statistical_test(in_data=all_data,method=\"student\",\n                        categorical_variable=\"outcome\",\n                        homo_variables=homo_variable,\n                        hetero_variables=hetero_variable)\n\nmetabolites_associated_AD<-\n    t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    arrange(p_adjusted)%>%\n    dplyr::select(column_name)%>%pull\n\nmetabolites_associated_AD_data<-\n    all_data%>%\n    dplyr::select(outcome,sex,genotype,metabolites_associated_AD)%>%\n    gather(key=metabolite,value=value,metabolites_associated_AD)        \n\nmetabolites_associated_AD_data%>%\n    group_by(outcome)%>%\n    summarise(mean=mean(value),sd=sd(value))%>%\n    knitr::kable()\n\n\n\n\n\noutcome\nmean\nsd\n\n\n\n\nnegative\n0.2184112\n0.9869302\n\n\npositive\n-0.4186542\n0.9830175\n\n\n\n\n\nCode\ntop_metabolites_associated_AD<-t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    arrange(p_adjusted)%>%head(10)%>%\n    dplyr::select(column_name)%>%pull\n\ntop_metabolites_associated_AD_data<-\n    all_data%>%\n    dplyr::select(outcome,top_metabolites_associated_AD)%>%\n    gather(key=metabolite,value=value,top_metabolites_associated_AD)        \n\nbottom_metabolites_associated_AD<-t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    arrange(p_adjusted)%>%tail(10)%>%\n    dplyr::select(column_name)%>%pull\n\nbottom_metabolites_associated_AD_data<-\n    all_data%>%\n    dplyr::select(outcome,bottom_metabolites_associated_AD)%>%\n    gather(key=metabolite,value=value,bottom_metabolites_associated_AD)        \n\na1<-top_metabolites_associated_AD_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(top_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Strongly Associated with AD\")\n\na2<-bottom_metabolites_associated_AD_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(bottom_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Weakly Associated with AD\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\nCode\nsignificant_metabolites<-\n    t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    dplyr::select(column_name)%>%pull\n\n\nAs a result of the t tests, there are 201 metabolites that are significantly associated with AD status at the 5% significance level. Metabolites with the highest significance were designated as strong metabolites and metabolites with the lowest significance among metabolites significantly related to AD status were designated as weak metabolites, and the expression level of metabolites between the disease status was confirmed through visualization. As a result, a greater difference was observed in strong metabolites than in weak metabolites, but both groups were not clearly separated in terms of AD status.\n\n\n2.1.2.2 AD vs Sex\n\n\nCode\nad_sex_summary<-all_data%>%\n    group_by(outcome,sex)%>%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%>%\n            ungroup%>%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%>%\n    dplyr::select(outcome, sex,count,proportion,mean_age,sd_age)%>%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$sex)$p.value,\n    method=\"chisquare_test\")\n\nad_sex_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\nsex\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\nman\n150\n30%\n85.79333\n5.146572\n0.1049983\nchisquare_test\n\n\nnegative\nwoman\n160\n32%\n84.96875\n5.284868\n0.1049983\nchisquare_test\n\n\npositive\nman\n77\n15.4%\n79.85714\n5.167495\n0.1049983\nchisquare_test\n\n\npositive\nwoman\n113\n22.6%\n81.29204\n4.896576\n0.1049983\nchisquare_test\n\n\n\n\n\n\n\n2.1.2.3 AD vs Genotype\n\n\nCode\nad_genotype_summary<-all_data%>%\n    group_by(outcome,genotype)%>%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%>%\n            ungroup%>%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%>%\n    dplyr::select(outcome, genotype,count,proportion,mean_age,sd_age)%>%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\")\n\ngenotype_ad_summary<-all_data%>%\n    group_by(genotype,outcome)%>%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%>%\n            ungroup%>%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%>%\n    dplyr::select(genotype,outcome,count,proportion,mean_age,sd_age)%>%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\") \n\nad_genotype_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\ngenotype\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\ne3\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\nnegative\ne2\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\nnegative\ne4\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\npositive\ne3\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\npositive\ne2\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\npositive\ne4\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nCode\ngenotype_ad_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenotype\noutcome\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\ne3\nnegative\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\ne3\npositive\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\ne2\nnegative\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\ne2\npositive\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\ne4\nnegative\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\ne4\npositive\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nAs you see the tables above, the count of AD status and that of genotype status are proportionately the same. Thus, there is no relation between AD and genotype in this data at the significant level 5%.\n\n\n2.1.2.4 Age vs AD, Sex, Genotype\nAge is known as a strong risk factor of AD or dementia. Human nerve system gets damaged as people are aged and the nerve fibrosis symptoms progress gradually. For this reason, we need to look into how the sample data are distributed in terms of age.\n\n\nCode\nad_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\")\nsex_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\")\ngenotype_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\")\nage_summary=rbind(\n    ad_age_summary,\n    sex_age_summary,\n    genotype_age_summary)\n\n\n\n\nCode\nage_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\ngroup\nsummary\nn\nproportion\nmean\nsd\nmin\nIQR_min\nQ1\nmedian\nQ3\nIQR_max\nmax\n\n\n\n\noutcome\nnegative\nage\n310\n62%\n85.37\n5.23\n72\n69.625\n81.25\n85\n89\n100.625\n105\n\n\noutcome\npositive\nage\n190\n38%\n80.71\n5.04\n65\n66.500\n77.00\n81\n84\n94.500\n93\n\n\nsex\nman\nage\n227\n45.4%\n83.78\n5.86\n65\n68.000\n80.00\n84\n88\n100.000\n105\n\n\nsex\nwoman\nage\n273\n54.6%\n83.45\n5.43\n67\n69.500\n80.00\n83\n87\n97.500\n100\n\n\ngenotype\ne3\nage\n387\n77.4%\n83.61\n5.64\n65\n69.500\n80.00\n84\n87\n97.500\n102\n\n\ngenotype\ne2\nage\n40\n8%\n85.83\n5.62\n78\n70.875\n81.75\n86\n89\n99.875\n105\n\n\ngenotype\ne4\nage\n73\n14.6%\n82.30\n5.25\n67\n68.500\n79.00\n82\n86\n96.500\n94\n\n\n\n\n\n\n\nCode\nplot<- ggarrange(\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\"),\n    ncol=2, nrow=2,legend=\"bottom\")\nplot\n\n\n\n\n\nThe table above shows the summary statistics of age grouped by the affected status, AD and non AD. The difference of age between the two groups are about -4.66, but their standard deviations are 5.04 and 5.23. Thus, it is hard to say their age in average differ in the affected status because the age variations of the two groups are overlapped. This research has two conflicting characteristics at the population level.\nThe glaring difference of age is also shown below. As you can see, the people with a negative status 4.66 years younger than those with a positive one.\n\n\nCode\na1<-all_data%>%\n    dplyr::select(outcome,age)%>%\n    ggplot(aes(x=age,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(legend.position=\"top\")+\n    labs(title=\"Histogram, Age Distribution Grouped by Disease Status\")   \na2<-all_data%>%\n    dplyr::select(outcome,age)%>%\n    ggplot(aes(x=outcome,y=age,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Age Distribution Grouped by Disease Status\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n2.1.2.5 Age vs Metabolites\n\n\nCode\nage_correlation_data<-\n    main_statistical_test(in_data=all_data,in_numeric_variable = 'age',method = \"cor\")%>%\n    filter(p_adjusted<0.05,column_name!='id')\n\n\n151 metabolites are significantly associated with age.\n\n\n2.1.2.6 Genotype vs Metabolites\n\n\nCode\ngenotype_aov<-main_statistical_test(in_data=all_data,categorical_variable=\"genotype\",method='aov')\ngenotype_aov%>%\nfilter(column_name!='id',p_adjusted<0.05)%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn_name\ngroup_df\nresidual_df\ngroup_ssq\nresidual_ssq\ngroup_msq\nresidual_msq\nF_value\np_value\nmethod\np_adjusted\ntype\n\n\n\n\n\n\n\n0 metabolites are significantly associated with the genotype variable.\n\n\n\n2.1.3 Multivariable Analysis\n\n2.1.3.1 AD vs Sex vs Genotype vs Age\n\n\nCode\nall_data%>%\n    group_by(outcome,sex,genotype)%>%\n    summarise(count=n(),\n    mean_age=mean(age),\n    sd_age=sd(age))%>%ungroup%>%\n    mutate(sum_count=sum(count),\n    proportion=paste0(round(count/sum_count*100,3),'%'))%>%\n    dplyr::select(outcome,sex,genotype,count,proportion, mean_age,sd_age)%>%\n    knitr::kable()\n\n\n\n\n\noutcome\nsex\ngenotype\ncount\nproportion\nmean_age\nsd_age\n\n\n\n\nnegative\nman\ne3\n113\n22.6%\n85.56637\n5.091793\n\n\nnegative\nman\ne2\n14\n2.8%\n88.57143\n6.548215\n\n\nnegative\nman\ne4\n23\n4.6%\n85.21739\n4.067125\n\n\nnegative\nwoman\ne3\n129\n25.8%\n85.07752\n5.370074\n\n\nnegative\nwoman\ne2\n12\n2.4%\n85.75000\n5.446016\n\n\nnegative\nwoman\ne4\n19\n3.8%\n83.73684\n4.628920\n\n\npositive\nman\ne3\n66\n13.2%\n79.74242\n5.384624\n\n\npositive\nman\ne2\n5\n1%\n83.00000\n3.000000\n\n\npositive\nman\ne4\n6\n1.2%\n78.50000\n3.082207\n\n\npositive\nwoman\ne3\n79\n15.8%\n81.65823\n4.784821\n\n\npositive\nwoman\ne2\n9\n1.8%\n83.22222\n3.632416\n\n\npositive\nwoman\ne4\n25\n5%\n79.44000\n5.260545\n\n\n\n\n\nExcept for the disease status variable, it looks like there is no difference of age in average and count in the other categorical variables. Taking into account age varialble is significantly associated with the metabolites that are significantly associated with the disease status, outcome (or AD). There is no need to conduct futher exploratory data analysis in the multivariable analysis senction."
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#summary-of-exploratory-data-analysis-eda-1",
    "href": "docs/projects/LLFS/eda.html#summary-of-exploratory-data-analysis-eda-1",
    "title": "EDA",
    "section": "3.1 Summary of Exploratory Data Analysis (EDA)",
    "text": "3.1 Summary of Exploratory Data Analysis (EDA)"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#summary-of-data-mining",
    "href": "docs/projects/LLFS/eda.html#summary-of-data-mining",
    "title": "EDA",
    "section": "3.2 Summary of Data Mining",
    "text": "3.2 Summary of Data Mining"
  },
  {
    "objectID": "docs/projects/LLFS/eda.html#eda-1",
    "href": "docs/projects/LLFS/eda.html#eda-1",
    "title": "EDA",
    "section": "4.1 EDA",
    "text": "4.1 EDA\n\n4.1.1 Univariable Analysis\n\n\n4.1.2 Normality Test\n\n\nCode\n# raw data\nnormality_test_result<-multiple_shapiro_test(all_data)%>%\n    filter(column_name!='id')%>%\n    group_by(type)%>%\n    summarise(count=n())%>%\n    mutate(proportion=round(count/sum(count),3),\n    total=sum(count))%>%\n    dplyr::select(type,total,everything())\n    \nnormality_test_result%>%\n    knitr::kable(caption=\"Summary of the Result of Shapiro Wilk Tests on Numeric Variables\")\n\n\n\nSummary of the Result of Shapiro Wilk Tests on Numeric Variables\n\n\ntype\ntotal\ncount\nproportion\n\n\n\n\nnormal\n1001\n1001\n1\n\n\n\n\n\nOut of 1001 numeric variables, the variables following a normal distribution are 1001 (100%) and the ones that do not are NA (NA%).\n\n\n4.1.3 Visualization\n\n4.1.3.1 16 Variables That Follow Normal Distributions\n\n\nCode\n# 16 numeric variables randomly selected\nnormal_variables<-\n    multiple_shapiro_test(all_data)%>%\n        filter(p_value>0.05,column_name!='id')%>%\n            dplyr::select(column_name)%>%\n            pull%>%sample(16)\n\nnormal_data<-\n    all_data%>%\n        dplyr::select(outcome,normal_variables)%>%\n        gather(key=metabolite,value=value,normal_variables)\n\nnormal_data%>%\n    ggplot(aes(x=value))+\n    geom_histogram(aes(y=..density..))+\n    geom_density(color='red',linewidth=1.5)+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%>%\n    ggplot(aes(x=metabolite,y=value))+\n    geom_boxplot()+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution\")\n\n\n\n\n\nCode\nnormal_data%>%\n    ggplot(aes(x=value,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    facet_wrap(.~metabolite)+\n    labs(title=\"Histogram, Numeric Variables Following Normal Distribution Grouped by Disease Status\")    \n\n\n\n\n\nCode\nnormal_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(normal_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    labs(title=\"Boxplot, Numeric Variables Following Normal Distribution Grouped by Disease Status\")\n\n\n\n\n\n\n\n4.1.3.2 Variables That Do Not Follow Normal Distributions\nThere is no variable that do not follow a normal distribution.\n\n\n\n4.1.4 Bivariable Analysis\n\n4.1.4.1 AD vs Metabolites\nThrough the exploratory data analysis above, it was confirmed that all variables follow a normal distribution, and t tests were conducted to select metabolites that have significant relationships with the disease status, AD. To minimize type 1 error due to multiple testings, bonferroni correction was used in the EDA\n\nHomoscedasticity Test\n\nLeven’s test is performed to confirm that each variable has equality of variance, one of the assumptions of the t test and ANOVA.\n\n\nCode\nleven_test_result<-\n    main_statistical_test(in_data=all_data,method=\"levene\",categorical_variable=\"outcome\")%>%\n    filter(column_name!='id')\n\nhomo_variable<-leven_test_result%>%\nfilter(p_adjusted>0.05)%>%\ndplyr::select(column_name)%>%\npull()%>%\nunique()\n\nhetero_variable<-leven_test_result%>%\nfilter(p_adjusted<0.05)%>%\ndplyr::select(column_name)%>%\npull()%>%\nunique()\n\n\n\n10 variables randomly selected out of 1001 variables with equal variance: meta408, meta959, meta796, meta957, meta740, meta212, meta171, meta769, meta986, meta178\n0 variables randomly selected with equal variance:\n\n\n\nCode\nhomo_variable_sample<-homo_variable%>%sample(10)\nhetero_variable_sample<-hetero_variable\n\nstratified_levene_data<-all_data%>%\n    dplyr::select(outcome,homo_variable_sample,hetero_variable_sample)%>%\n    gather(key=metabolite,value=value,c(homo_variable_sample,hetero_variable_sample))%>%\n    mutate(levene_test=ifelse(metabolite%in%(homo_variable_sample),\"homoscedasticity\",\"heteroscedasticity\"))\n\nstratified_levene_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(stratified_levene_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+\n    facet_wrap(.~levene_test)+\n    labs(title=\"Boxplot, Metabolites with Heteroscedasticity vs Homoscedasticity\")\n\n\n\n\n\n\nUnpaired Two Sample Mean t Test\n\n\n\nCode\nt_test_result<-\n    main_statistical_test(in_data=all_data,method=\"student\",\n                        categorical_variable=\"outcome\",\n                        homo_variables=homo_variable,\n                        hetero_variables=hetero_variable)\n\nmetabolites_associated_AD<-\n    t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    arrange(p_adjusted)%>%\n    dplyr::select(column_name)%>%pull\n\nmetabolites_associated_AD_data<-\n    all_data%>%\n    dplyr::select(outcome,sex,genotype,metabolites_associated_AD)%>%\n    gather(key=metabolite,value=value,metabolites_associated_AD)        \n\nmetabolites_associated_AD_data%>%\n    group_by(outcome)%>%\n    summarise(mean=mean(value),sd=sd(value))%>%\n    knitr::kable()\n\n\n\n\n\noutcome\nmean\nsd\n\n\n\n\nnegative\n0.2184112\n0.9869302\n\n\npositive\n-0.4186542\n0.9830175\n\n\n\n\n\nCode\ntop_metabolites_associated_AD<-t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    arrange(p_adjusted)%>%head(10)%>%\n    dplyr::select(column_name)%>%pull\n\ntop_metabolites_associated_AD_data<-\n    all_data%>%\n    dplyr::select(outcome,top_metabolites_associated_AD)%>%\n    gather(key=metabolite,value=value,top_metabolites_associated_AD)        \n\nbottom_metabolites_associated_AD<-t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    arrange(p_adjusted)%>%tail(10)%>%\n    dplyr::select(column_name)%>%pull\n\nbottom_metabolites_associated_AD_data<-\n    all_data%>%\n    dplyr::select(outcome,bottom_metabolites_associated_AD)%>%\n    gather(key=metabolite,value=value,bottom_metabolites_associated_AD)        \n\na1<-top_metabolites_associated_AD_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(top_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Strongly Associated with AD\")\n\na2<-bottom_metabolites_associated_AD_data%>%\n    ggplot(aes(x=metabolite,y=value,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(bottom_metabolites_associated_AD_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Metabolites Weakly Associated with AD\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\nCode\nsignificant_metabolites<-\n    t_test_result%>%\n    filter(type==\"significant\"&column_name!='age')%>%\n    dplyr::select(column_name)%>%pull\n# significant_metabolites%>%write_rds(.,file='./docs/data/llfs_fake_significant_metabolites.rds')\n\n\nAs a result of the t tests, there are 201 metabolites that are significantly associated with AD status. Metabolites with the highest significance were designated as strong metabolites and metabolites with the lowest significance among metabolites significantly related to AD status were designated as weak metabolites, and the expression level of metabolites between the disease status was confirmed through visualization. As a result, a greater difference was observed in strong metabolites than in weak metabolites, but both groups were not clearly separated in terms of AD status.\n\n\n4.1.4.2 AD vs Sex\n\n\nCode\nad_sex_summary<-all_data%>%\n    group_by(outcome,sex)%>%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%>%\n            ungroup%>%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%>%\n    dplyr::select(outcome, sex,count,proportion,mean_age,sd_age)%>%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$sex)$p.value,\n    method=\"chisquare_test\")\n\nad_sex_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\nsex\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\nman\n150\n30%\n85.79333\n5.146572\n0.1049983\nchisquare_test\n\n\nnegative\nwoman\n160\n32%\n84.96875\n5.284868\n0.1049983\nchisquare_test\n\n\npositive\nman\n77\n15.4%\n79.85714\n5.167495\n0.1049983\nchisquare_test\n\n\npositive\nwoman\n113\n22.6%\n81.29204\n4.896576\n0.1049983\nchisquare_test\n\n\n\n\n\n\n\n4.1.4.3 AD vs Genotype\n\n\nCode\nad_genotype_summary<-all_data%>%\n    group_by(outcome,genotype)%>%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%>%\n            ungroup%>%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%>%\n    dplyr::select(outcome, genotype,count,proportion,mean_age,sd_age)%>%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\")\n\ngenotype_ad_summary<-all_data%>%\n    group_by(genotype,outcome)%>%\n    summarise(count=n(),\n            mean_age=mean(age),\n            sd_age=sd(age))%>%\n            ungroup%>%\n    mutate(sum=sum(count),\n    proportion=paste0(count/sum*100,\"%\"))%>%\n    dplyr::select(genotype,outcome,count,proportion,mean_age,sd_age)%>%\n    mutate(p_value=chisq.test(all_data$outcome,all_data$genotype)$p.value,\n    method=\"chisquare_test\") \n\nad_genotype_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\ngenotype\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\nnegative\ne3\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\nnegative\ne2\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\nnegative\ne4\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\npositive\ne3\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\npositive\ne2\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\npositive\ne4\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nCode\ngenotype_ad_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenotype\noutcome\ncount\nproportion\nmean_age\nsd_age\np_value\nmethod\n\n\n\n\ne3\nnegative\n242\n48.4%\n85.30579\n5.236867\n0.6645566\nchisquare_test\n\n\ne3\npositive\n145\n29%\n80.78621\n5.138615\n0.6645566\nchisquare_test\n\n\ne2\nnegative\n26\n5.2%\n87.26923\n6.115931\n0.6645566\nchisquare_test\n\n\ne2\npositive\n14\n2.8%\n83.14286\n3.301681\n0.6645566\nchisquare_test\n\n\ne4\nnegative\n42\n8.4%\n84.54762\n4.340408\n0.6645566\nchisquare_test\n\n\ne4\npositive\n31\n6.2%\n79.25806\n4.885132\n0.6645566\nchisquare_test\n\n\n\n\n\nAs you see the tables above, the count of AD status and that of genotype status are proportionately the same. Thus, there is no relation between AD and genotype in this data at the significant level 5%.\n\n\n4.1.4.4 Age vs AD, Sex, Genotype\nAge is known as a strong risk factor of AD or dementia. Human nerve system gets damaged as people are aged and the nerve fibrosis symptoms progress gradually. For this reason, we need to look into how the sample data are distributed in terms of age.\n\n\nCode\nad_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\")\nsex_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\")\ngenotype_age_summary=getNumericSummaryTable(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\")\nage_summary=rbind(\n    ad_age_summary,\n    sex_age_summary,\n    genotype_age_summary)\n\n\n\n\nCode\nage_summary%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\ngroup\nsummary\nn\nproportion\nmean\nsd\nmin\nIQR_min\nQ1\nmedian\nQ3\nIQR_max\nmax\n\n\n\n\noutcome\nnegative\nage\n310\n62%\n85.37\n5.23\n72\n69.625\n81.25\n85\n89\n100.625\n105\n\n\noutcome\npositive\nage\n190\n38%\n80.71\n5.04\n65\n66.500\n77.00\n81\n84\n94.500\n93\n\n\nsex\nman\nage\n227\n45.4%\n83.78\n5.86\n65\n68.000\n80.00\n84\n88\n100.000\n105\n\n\nsex\nwoman\nage\n273\n54.6%\n83.45\n5.43\n67\n69.500\n80.00\n83\n87\n97.500\n100\n\n\ngenotype\ne3\nage\n387\n77.4%\n83.61\n5.64\n65\n69.500\n80.00\n84\n87\n97.500\n102\n\n\ngenotype\ne2\nage\n40\n8%\n85.83\n5.62\n78\n70.875\n81.75\n86\n89\n99.875\n105\n\n\ngenotype\ne4\nage\n73\n14.6%\n82.30\n5.25\n67\n68.500\n79.00\n82\n86\n96.500\n94\n\n\n\n\n\n\n\nCode\nplot<- ggarrange(\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"outcome\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"sex\",summary_variable=\"age\"),\n    getNumericSummaryPlot(in_data=all_data,group_variable=\"genotype\",summary_variable=\"age\"),\n    ncol=2, nrow=2,legend=\"bottom\")\nplot\n\n\n\n\n\nThe table above shows the summary statistics of age grouped by the affected status, AD and non AD. The difference of age between the two groups are about -4.66, but their standard deviations are 5.04 and 5.23. Thus, it is hard to say their age in average differ in the affected status because the age variations of the two groups are overlapped. This research has two conflicting characteristics at the population level.\nThe glaring difference of age is also shown below. As you can see, the people with a negative status 4.66 years younger than those with a positive one.\n\n\nCode\na1<-all_data%>%\n    dplyr::select(outcome,age)%>%\n    ggplot(aes(x=age,fill=outcome))+\n    geom_histogram(aes(y=..density..),alpha=0.5)+\n    geom_density(linewidth=1.5,alpha=0.5)+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(legend.position=\"top\")+\n    labs(title=\"Histogram, Age Distribution Grouped by Disease Status\")   \na2<-all_data%>%\n    dplyr::select(outcome,age)%>%\n    ggplot(aes(x=outcome,y=age,fill=outcome))+\n    geom_boxplot()+\n    scale_fill_manual(values=color_function(length(unique(all_data$outcome))))+\n    theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1), legend.position=\"top\")+\n    labs(title=\"Boxplot, Age Distribution Grouped by Disease Status\")\n\nggarrange(a1, a2, ncol=2, nrow=1)\n\n\n\n\n\n\n\n4.1.4.5 Age vs Metabolites\n\n\nCode\nage_correlation_data<-\n    main_statistical_test(in_data=all_data,in_numeric_variable = 'age',method = \"cor\")%>%\n    filter(p_adjusted<0.05,column_name!='id')\n\n\n151 metabolites are significantly associated with age.\n\n\n4.1.4.6 Genotype vs Metabolites\n\n\nCode\ngenotype_aov<-main_statistical_test(in_data=all_data,categorical_variable=\"genotype\",method='aov')\ngenotype_aov%>%\nfilter(column_name!='id',p_adjusted<0.05)%>%knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn_name\ngroup_df\nresidual_df\ngroup_ssq\nresidual_ssq\ngroup_msq\nresidual_msq\nF_value\np_value\nmethod\np_adjusted\ntype\n\n\n\n\n\n\n\n0 metabolites are significantly associated with the genotype variable.\n\n\n\n4.1.5 Multivariable Analysis\n\n4.1.5.1 AD vs Sex vs Genotype vs Age\n\n\nCode\nall_data%>%\n    group_by(outcome,sex,genotype)%>%\n    summarise(count=n(),\n    mean_age=mean(age),\n    sd_age=sd(age))%>%ungroup%>%\n    mutate(sum_count=sum(count),\n    proportion=paste0(round(count/sum_count*100,3),'%'))%>%\n    dplyr::select(outcome,sex,genotype,count,proportion, mean_age,sd_age)%>%\n    knitr::kable()\n\n\n\n\n\noutcome\nsex\ngenotype\ncount\nproportion\nmean_age\nsd_age\n\n\n\n\nnegative\nman\ne3\n113\n22.6%\n85.56637\n5.091793\n\n\nnegative\nman\ne2\n14\n2.8%\n88.57143\n6.548215\n\n\nnegative\nman\ne4\n23\n4.6%\n85.21739\n4.067125\n\n\nnegative\nwoman\ne3\n129\n25.8%\n85.07752\n5.370074\n\n\nnegative\nwoman\ne2\n12\n2.4%\n85.75000\n5.446016\n\n\nnegative\nwoman\ne4\n19\n3.8%\n83.73684\n4.628920\n\n\npositive\nman\ne3\n66\n13.2%\n79.74242\n5.384624\n\n\npositive\nman\ne2\n5\n1%\n83.00000\n3.000000\n\n\npositive\nman\ne4\n6\n1.2%\n78.50000\n3.082207\n\n\npositive\nwoman\ne3\n79\n15.8%\n81.65823\n4.784821\n\n\npositive\nwoman\ne2\n9\n1.8%\n83.22222\n3.632416\n\n\npositive\nwoman\ne4\n25\n5%\n79.44000\n5.260545\n\n\n\n\n\nExcept for the disease status variable, it looks like there is no difference of age in average and count in the other categorical variables. Taking into account age varialble is significantly associated with the metabolites that are significantly associated with the disease status, outcome (or AD). There is no need to conduct futher exploratory data analysis in the multivariable analysis senction."
  },
  {
    "objectID": "docs/projects/LLFS/mining.html",
    "href": "docs/projects/LLFS/mining.html",
    "title": "Data Mining",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmetabolite_data <- all_data[, -c(1:5)]\noutcome_data <- all_data[, 2]\n\n# normalize the metaoblites\nnormalized_metabolite_data <-\n    as.data.frame(lapply(metabolite_data, function(x) scale_function(vector = x, method = \"min-max\")))\nnormalized_significant_metabolite_data <-\n    normalized_metabolite_data %>%\n    dplyr::select(all_of(significant_metabolites))\n\n# extract the latent variables (PCs: Principal Components)\npc_metabolites <-\n    prcomp(normalized_metabolite_data)\npc_significant_metabolites <-\n    prcomp(normalized_significant_metabolite_data)\n\n# calculate scores\nscores <-\n    as.data.frame(pc_metabolites$x) %>%\n    janitor::clean_names() %>%\n    mutate(row_names = 1:n())\nsignificant_scores <-\n    as.data.frame(pc_significant_metabolites$x) %>%\n    janitor::clean_names() %>%\n    mutate(row_names = 1:n())\n\ntemp <-\n    as.data.frame(pc_metabolites$rotation) %>%\n    janitor::clean_names()\nloadings <- temp %>%\n    mutate(\n        metabolites = rownames(.),\n        arrow_size_normalization = min(\n            (max(scores[, \"pc1\"]) - min(scores[, \"pc1\"]) /\n                (max(temp[, \"pc1\"]) - min(temp[, \"pc1\"]))),\n            (max(scores[, \"pc2\"]) - min(scores[, \"pc2\"]) /\n                (max(temp[, \"pc2\"]) - min(temp[, \"pc2\"]))),\n            (max(scores[, \"pc3\"]) - min(scores[, \"pc3\"]) /\n                (max(temp[, \"pc3\"]) - min(temp[, \"pc3\"])))\n        ),\n        arrow_pc1 = arrow_size_normalization * pc1,\n        arrow_pc2 = arrow_size_normalization * pc2,\n        arrow_pc3 = arrow_size_normalization * pc3\n    )\n\ntemp <-\n    as.data.frame(pc_significant_metabolites$rotation) %>%\n    janitor::clean_names()\nsignificant_loadings <- temp %>%\n    mutate(\n        metabolites = rownames(.),\n        arrow_size_normalization = min(\n            (max(scores[, \"pc1\"]) - min(scores[, \"pc1\"]) /\n                (max(temp[, \"pc1\"]) - min(temp[, \"pc1\"]))),\n            (max(scores[, \"pc2\"]) - min(scores[, \"pc2\"]) /\n                (max(temp[, \"pc2\"]) - min(temp[, \"pc2\"]))),\n            (max(scores[, \"pc3\"]) - min(scores[, \"pc3\"]) /\n                (max(temp[, \"pc3\"]) - min(temp[, \"pc3\"])))\n        ),\n        arrow_pc1 = arrow_size_normalization * pc1,\n        arrow_pc2 = arrow_size_normalization * pc2,\n        arrow_pc3 = arrow_size_normalization * pc3\n    )\n# arrow_size_normalization is a normalization factor that\n# ensures the variable loading arrows are scaled appropriately relative to the data points.\n# The min() function to find the smallest ratio between the range of the data points and\n# the range of the variable loadings along each principal component axis (pc1, pc2, and pc3).\n# The reason why I select the first 3 components is that\n# '3' is the maximum dimension that can visualize the PCA results in 3d.\n\noutcome_scores <-\n    scores %>%\n    mutate(\n        outcome = outcome_data,\n        row_names = 1:n()\n    )\noutcome_significant_scores <-\n    significant_scores %>%\n    mutate(\n        outcome = outcome_data,\n        row_names = 1:n()\n    )\n\n\n# total variance\ntotal_variance <-\n    data.frame(\n        pc = 1:length(pc_metabolites$sdev),\n        pc_variance_proportion = summary(pc_metabolites)[[\"importance\"]][\"Proportion of Variance\", ],\n        cumulative_proportion = summary(pc_metabolites)[[\"importance\"]][\"Cumulative Proportion\", ] * 100\n    )\ntotal_variance_significance <-\n    data.frame(\n        pc = 1:length(pc_significant_metabolites$sdev),\n        pc_variance_proportion = summary(pc_significant_metabolites)[[\"importance\"]][\"Proportion of Variance\", ],\n        cumulative_proportion = summary(pc_significant_metabolites)[[\"importance\"]][\"Cumulative Proportion\", ] * 100\n    )\n\nscree_plot <- function(indata) {\n    scree_plot1 <- ggplot(\n        data = indata,\n        aes(x = pc, y = pc_variance_proportion, group = 1)\n    ) +\n        geom_point() +\n        geom_line() +\n        labs(title = \"\", subtitle = paste0(\n            \"Scree Plot, Total Variance(\",\n            round(tail(indata, 1)[\"cumulative_proportion\"], 3),\n            \"%)Explained by \", nrow(indata), \" PCs\"\n        )) +\n        ylab(\"Total Variance Explained\") +\n        xlab(\"Principal Components\")\n    scree_plot2 <- ggplot(\n        data = indata %>% filter(pc < 13),\n        aes(x = pc, y = pc_variance_proportion, group = 1)\n    ) +\n        geom_point() +\n        geom_line() +\n        labs(title = \"\", subtitle = paste0(\n            \"Scree Plot, Part of Variance(\",\n            round(tail(indata %>% filter(pc < 13), 1)[\"cumulative_proportion\"], 3),\n            \"%)Explained by \", indata %>% filter(pc < 13) %>% nrow(), \" PCs\"\n        )) +\n        ylab(\"Total Variance Explained\") +\n        xlab(\"Principal Components\")\n    return(ggarrange(scree_plot1, scree_plot2, nrow = 1))\n}\n\nggarrange(scree_plot(total_variance), scree_plot(total_variance_significance),\n    labels = c(\n        paste0(\"All \", ncol(metabolite_data), \" Metabolites\"),\n        paste0(length(significant_metabolites), \" Significant Metabolites\")\n    ), nrow = 2\n)\n\n\n\n\n\nCode\n# 2D PCA Scatter Plots with PC1 and PC2\n\nscatter_plot <- function(in_data) {\n    p <- ggplot(\n        data = in_data,\n        aes(x = pc1, y = pc2, color = outcome)\n    ) +\n        geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n        stat_ellipse(type = \"norm\", level = .99) +\n        geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n        geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n        scale_color_manual(values = color_function(length(unique(in_data$outcome)))) +\n        labs(\n            title = \"2D Scatter Plot of the First 2 PCs Grouped by AD status\",\n            subtitle = paste0(ncol(in_data) - 2, \" Metabolites\")\n        )\n    return(p)\n}\n\nggarrange(scatter_plot(outcome_scores),\n    scatter_plot(outcome_significant_scores),\n    nrow = 2\n)\n\n\n\n\n\nCode\n# biplot\nbi_plot <- function(in_data) {\n    p <-\n        ggplot(data = in_data, aes(x = pc1, y = pc2, color = outcome)) +\n        geom_text(alpha = .75, size = 3, aes(label = row_names)) +\n        geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n        geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n        coord_equal() +\n        scale_color_manual(values = color_function(length(unique(in_data$outcome)))) +\n        stat_ellipse(type = \"norm\", level = .99) +\n        geom_text(\n            data = loadings, aes(x = arrow_pc1, y = arrow_pc2, label = metabolites),\n            alpha = 0.5, size = 5, vjust = 1, color = \"red\"\n        ) +\n        geom_segment(\n            data = loadings, aes(x = 0, y = 0, xend = arrow_pc1, yend = arrow_pc2),\n            arrow = arrow(length = unit(0.5, \"cm\")), alpha = 0.5, color = \"red\"\n        ) +\n        labs(\n            title = \"Biplot, the Effect of Metabolites on Samples with Disease Status\",\n            subtitle = paste0(ncol(in_data) - 2, \" Metabolites\")\n        ) +\n        ylab(\"PC2\") +\n        xlab(\"PC1\")\n    return(p)\n}\nggarrange(bi_plot(outcome_scores), bi_plot(outcome_significant_scores), nrow = 2)\n\n\n\n\n\nCode\nplot_ly(\n    data = outcome_scores,\n    x = ~pc1, y = ~pc2, z = ~pc3,\n    type = \"scatter3d\", mode = \"markers\", color = ~ outcome_scores$outcome,\n    colors = color_function(2),\n    size = 2\n) %>%\n    layout(\n        title = \"Effect of 500 Metabolites on Samples with Disease Status in 3d\",\n        scene = list(\n            bgcolor = \"#e5ecf6\",\n            xaxis = list(title = \"PC1\"),\n            yaxis = list(title = \"PC2\"),\n            zaxis = list(title = \"PC3\")\n        ),\n        legend = list(title = list(text = \"Disease(AD) Status\"))\n    )\n\n\n\n\n\n\nCode\nplot_ly(\n    data = outcome_significant_scores,\n    x = ~pc1, y = ~pc2, z = ~pc3,\n    type = \"scatter3d\", mode = \"markers\", color = ~ outcome_scores$outcome,\n    colors = color_function(2),\n    size = 2\n) %>%\n    layout(\n        title = \"Effect of 201 Metabolites on Samples with Disease Status in 3d\",\n        scene = list(\n            bgcolor = \"#e5ecf6\",\n            xaxis = list(title = \"PC1\"),\n            yaxis = list(title = \"PC2\"),\n            zaxis = list(title = \"PC3\")\n        ),\n        legend = list(title = list(text = \"Disease(AD) Status\"))\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nkmean_result_list <- list(\n    \"mse_list\" = list(\n        \"all_metabolites\" = matrix(nrow = 20),\n        \"significant_metabolites\" = matrix(nrow = 20)\n    ),\n    \"cluster_sse_list\" = list(\n        \"all_metabolites\" = matrix(nrow = 20, ncol = 2),\n        \"significant_metabolites\" = matrix(nrow = 20, ncol = 2)\n    ),\n    \"Variance_Explained\" = list(\n        \"all_metabolites\" = matrix(nrow = 20),\n        \"significant_metabolites\" = matrix(nrow = 20)\n    )\n)\n\nfor (j in c(\"all_metabolites\", \"significant_metabolites\")) {\n    for (i in (1:20)) {\n        if (j == \"all_metabolites\") {\n            kmean_fit <- kmeans(normalized_metabolite_data, centers = i, iter.max = 300)\n        } else {\n            kmean_fit <- kmeans(normalized_significant_metabolite_data, centers = i, iter.max = 300)\n        }\n        kmean_result_list[[\"mse_list\"]][[j]][i] <- mean(kmean_fit$withinss) %>% round(3)\n        kmean_result_list[[\"cluster_sse_list\"]][[j]][i, 1] <- i\n        kmean_result_list[[\"cluster_sse_list\"]][[j]][i, 2] <- kmean_result_list[[\"mse_list\"]][[j]][i]\n        kmean_result_list[[\"Variance_Explained\"]][[j]][i] <- kmean_fit$betweenss / kmean_fit$totss\n        cat(\n            \"For \", j, \", K: \", i,\n            \"within-cluster MSE: \", kmean_result_list[[\"mse_list\"]][[j]][i],\n            \"Variance_Explained: \", kmean_result_list[[\"Variance_Explained\"]][[j]][i], \"\\n\"\n        )\n    }\n}\n\n\nFor  all_metabolites , K:  1 within-cluster MSE:  13619.32 Variance_Explained:  4.941702e-15 \nFor  all_metabolites , K:  2 within-cluster MSE:  6487.104 Variance_Explained:  0.04736709 \nFor  all_metabolites , K:  3 within-cluster MSE:  4205.437 Variance_Explained:  0.0736459 \nFor  all_metabolites , K:  4 within-cluster MSE:  3107.38 Variance_Explained:  0.08736085 \nFor  all_metabolites , K:  5 within-cluster MSE:  2443.924 Variance_Explained:  0.1027728 \nFor  all_metabolites , K:  6 within-cluster MSE:  2010.74 Variance_Explained:  0.114167 \nFor  all_metabolites , K:  7 within-cluster MSE:  1705.235 Variance_Explained:  0.1235504 \nFor  all_metabolites , K:  8 within-cluster MSE:  1481.469 Variance_Explained:  0.1297835 \nFor  all_metabolites , K:  9 within-cluster MSE:  1306.354 Variance_Explained:  0.1367274 \nFor  all_metabolites , K:  10 within-cluster MSE:  1166.08 Variance_Explained:  0.1438044 \nFor  all_metabolites , K:  11 within-cluster MSE:  1051.345 Variance_Explained:  0.1508531 \nFor  all_metabolites , K:  12 within-cluster MSE:  957.817 Variance_Explained:  0.1560656 \nFor  all_metabolites , K:  13 within-cluster MSE:  880.831 Variance_Explained:  0.1592231 \nFor  all_metabolites , K:  14 within-cluster MSE:  812.71 Variance_Explained:  0.1645731 \nFor  all_metabolites , K:  15 within-cluster MSE:  755.508 Variance_Explained:  0.1679013 \nFor  all_metabolites , K:  16 within-cluster MSE:  702.891 Variance_Explained:  0.1742422 \nFor  all_metabolites , K:  17 within-cluster MSE:  657.791 Variance_Explained:  0.1789275 \nFor  all_metabolites , K:  18 within-cluster MSE:  617.96 Variance_Explained:  0.1832713 \nFor  all_metabolites , K:  19 within-cluster MSE:  582.219 Variance_Explained:  0.1877591 \nFor  all_metabolites , K:  20 within-cluster MSE:  550.074 Variance_Explained:  0.1922144 \nFor  significant_metabolites , K:  1 within-cluster MSE:  2660.227 Variance_Explained:  -8.034324e-15 \nFor  significant_metabolites , K:  2 within-cluster MSE:  1022.505 Variance_Explained:  0.2312645 \nFor  significant_metabolites , K:  3 within-cluster MSE:  632.194 Variance_Explained:  0.28706 \nFor  significant_metabolites , K:  4 within-cluster MSE:  452.85 Variance_Explained:  0.3190806 \nFor  significant_metabolites , K:  5 within-cluster MSE:  354.801 Variance_Explained:  0.3331372 \nFor  significant_metabolites , K:  6 within-cluster MSE:  290.621 Variance_Explained:  0.3445191 \nFor  significant_metabolites , K:  7 within-cluster MSE:  245.829 Variance_Explained:  0.3531377 \nFor  significant_metabolites , K:  8 within-cluster MSE:  212.9 Variance_Explained:  0.3597537 \nFor  significant_metabolites , K:  9 within-cluster MSE:  187.706 Variance_Explained:  0.3649574 \nFor  significant_metabolites , K:  10 within-cluster MSE:  167.108 Variance_Explained:  0.3718263 \nFor  significant_metabolites , K:  11 within-cluster MSE:  150.928 Variance_Explained:  0.3759169 \nFor  significant_metabolites , K:  12 within-cluster MSE:  137.487 Variance_Explained:  0.3798103 \nFor  significant_metabolites , K:  13 within-cluster MSE:  126.018 Variance_Explained:  0.3841728 \nFor  significant_metabolites , K:  14 within-cluster MSE:  116.658 Variance_Explained:  0.3860626 \nFor  significant_metabolites , K:  15 within-cluster MSE:  107.843 Variance_Explained:  0.3919131 \nFor  significant_metabolites , K:  16 within-cluster MSE:  100.885 Variance_Explained:  0.393227 \nFor  significant_metabolites , K:  17 within-cluster MSE:  94.421 Variance_Explained:  0.3966092 \nFor  significant_metabolites , K:  18 within-cluster MSE:  88.62 Variance_Explained:  0.4003684 \nFor  significant_metabolites , K:  19 within-cluster MSE:  83.8 Variance_Explained:  0.4014796 \nFor  significant_metabolites , K:  20 within-cluster MSE:  79.155 Variance_Explained:  0.4049023 \n\n\nCode\nkmean_mse_data <-\n    rbind(\n        data.frame(\n            metabolites = \"all_metabolites\",\n            k = kmean_result_list[[\"cluster_sse_list\"]][[\"all_metabolites\"]][, 1],\n            mse = kmean_result_list[[\"cluster_sse_list\"]][[\"all_metabolites\"]][, 2],\n            variance_exaplained = kmean_result_list[[\"Variance_Explained\"]][[\"all_metabolites\"]]\n        ),\n        data.frame(\n            metabolites = \"significant_metabolites\",\n            k = kmean_result_list[[\"cluster_sse_list\"]][[\"significant_metabolites\"]][, 1],\n            mse = kmean_result_list[[\"cluster_sse_list\"]][[\"significant_metabolites\"]][, 2],\n            variance_exaplained = kmean_result_list[[\"Variance_Explained\"]][[\"significant_metabolites\"]]\n        )\n    )\n\nggarrange(\n    ggplot(\n        data = kmean_mse_data,\n        aes(x = k, y = mse, group = metabolites, color = metabolites)\n    ) +\n        geom_line() +\n        geom_point() +\n        scale_color_manual(values = color_function(2)) +\n        labs(title = \"K Mean Clustering Result: MSE for All Metabolites vs Significant Ones\") +\n        xlab(\"Number of Clusters\") +\n        ylab(\"Mean Squared Error\"),\n    ggplot(\n        data = kmean_mse_data,\n        aes(x = k, y = variance_exaplained, group = metabolites, color = metabolites)\n    ) +\n        geom_line() +\n        geom_point() +\n        scale_color_manual(values = color_function(2)) +\n        labs(title = \"K Mean Clustering Result: Variance Explained for All Metabolites vs Significant Ones\") +\n        xlab(\"Number of Clusters\") +\n        ylab(\"Variance Exaplained\"),\n    ncol = 1\n)\n\n\n\n\n\nCode\n# K means\n\n\nkm_clustering <- kmeans(normalized_metabolite_data, centers = 2, iter.max = 300)\nkm_significant_clustering <- kmeans(normalized_significant_metabolite_data, centers = 2, iter.max = 300)\n\nconfusionMatrix(table(all_data[, 2], ifelse(km_clustering$cluster == 1, \"negative\", \"positive\")))\n\n\nConfusion Matrix and Statistics\n\n          \n           negative positive\n  negative       96      214\n  positive      147       43\n                                          \n               Accuracy : 0.278           \n                 95% CI : (0.2391, 0.3195)\n    No Information Rate : 0.514           \n    P-Value [Acc > NIR] : 1.0000000       \n                                          \n                  Kappa : -0.4344         \n                                          \n Mcnemar's Test P-Value : 0.0005134       \n                                          \n            Sensitivity : 0.3951          \n            Specificity : 0.1673          \n         Pos Pred Value : 0.3097          \n         Neg Pred Value : 0.2263          \n             Prevalence : 0.4860          \n         Detection Rate : 0.1920          \n   Detection Prevalence : 0.6200          \n      Balanced Accuracy : 0.2812          \n                                          \n       'Positive' Class : negative        \n                                          \n\n\nCode\nconfusionMatrix(table(all_data[, 2], ifelse(km_significant_clustering$cluster == 1, \"negative\", \"positive\")))\n\n\nConfusion Matrix and Statistics\n\n          \n           negative positive\n  negative      101      209\n  positive      143       47\n                                          \n               Accuracy : 0.296           \n                 95% CI : (0.2563, 0.3381)\n    No Information Rate : 0.512           \n    P-Value [Acc > NIR] : 1.0000000       \n                                          \n                  Kappa : -0.3999         \n                                          \n Mcnemar's Test P-Value : 0.0005312       \n                                          \n            Sensitivity : 0.4139          \n            Specificity : 0.1836          \n         Pos Pred Value : 0.3258          \n         Neg Pred Value : 0.2474          \n             Prevalence : 0.4880          \n         Detection Rate : 0.2020          \n   Detection Prevalence : 0.6200          \n      Balanced Accuracy : 0.2988          \n                                          \n       'Positive' Class : negative        \n                                          \n\n\nCode\noutcome_pca_km <- outcome_scores %>%\n    mutate(\n        km_clusters = km_clustering$cluster,\n        km_clusters = factor(km_clusters, levels = c(1, 2)),\n        km_significant_clusters = km_significant_clustering$cluster,\n        km_significant_clusters = factor(km_significant_clusters, levels = c(1, 2))\n    )\n\n\nggplot(\n    data = outcome_pca_km,\n    aes(x = pc1, y = pc2, color = km_clusters)\n) +\n    geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n    stat_ellipse(type = \"norm\", level = .99) +\n    geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n    geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n    scale_color_manual(values = color_function(2)) +\n    labs(\n        title = \"2D Scatter Plot of the First 2 PCs Grouped by K Mean Clusters, AD Status\",\n        subtitle = paste0(ncol(outcome_scores) - 2, \" Metabolites\")\n    )\n\n\n\n\n\nCode\nggplot(\n    data = outcome_pca_km,\n    aes(x = pc1, y = pc2, color = km_significant_clusters)\n) +\n    geom_text(alpha = .5, size = 3, aes(label = row_names)) +\n    stat_ellipse(type = \"norm\", level = .99) +\n    geom_hline(aes(yintercept = 0), alpha = 0.5, size = .1) +\n    geom_vline(aes(xintercept = 0), alpha = 0.5, size = .1) +\n    scale_color_manual(values = color_function(2)) +\n    labs(\n        title = \"2D Scatter Plot of the First 2 PCs Grouped by K Mean Clusters, AD Status\",\n        subtitle = paste0(ncol(outcome_significant_scores) - 2, \" Metabolites\")\n    )\n\n\n\n\n\n비지도 학습 방법인 PCA와 K-means clustering를 이용하여 차원 축소와 군집화를 시도하였으나, 이 방법을 사용하는 모든 대사체에 대해 AD 상태가 명확하게 분류되지 않는 것으로 보인다. PCA와 K-means는 EDA에서 선별된 대사산물로 군집화를 수행했을 때 전체 metabolites 보다 선별된 metabolites 에서 AD 상태에 대한 정보가 조금 더 많이 설명되는 것을 PCA를 통해 관찰할 수 있었다. K means clustering도 선별된 metabolites에 대해서 성능 향상을 보여준다. 그러나 전반적인 정확도가 매우 낮기 때문에 지도 학습을 통해 AD 상태를 잘 설명하는 대사체를 선택할 것이다.\nDimensionality reduction and clustering were attempted using PCA and K-means clustering, which are unsupervised learning methods, but AD status seems to not be clearly classified for all metabolites using the methods. When PCA and K means clustering were performed with the metabolites selected from EDA, it was observed through PCA that a little more information about AD status was explained with the selected metabolites than with the entire set of metaboliotes. K means clustering also showed an improvement in performance with the selected metabolites. However, the overall accuracy is very low, so we will select metabolites that explain AD status well through supervised learning."
  },
  {
    "objectID": "docs/projects/LLFS/ml_approach.html",
    "href": "docs/projects/LLFS/ml_approach.html",
    "title": "ML Approach",
    "section": "",
    "text": "alpha      mse fit.name\n1    0.0 2217.331   alpha0\n2    0.1 2217.331 alpha0.1\n3    0.2 2217.331 alpha0.2\n4    0.3 2217.331 alpha0.3\n5    0.4 2217.331 alpha0.4\n6    0.5 2217.331 alpha0.5\n7    0.6 2217.331 alpha0.6\n8    0.7 2217.331 alpha0.7\n9    0.8 2217.331 alpha0.8\n10   0.9 2217.331 alpha0.9\n11   1.0 2217.331   alpha1"
  },
  {
    "objectID": "docs/projects/LLFS/project_description.html",
    "href": "docs/projects/LLFS/project_description.html",
    "title": "Description",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n알츠하이머병(Alzheimer’s Disease, AD)은 수백만 명의 미국인에게 영향을 미치는 가장 흔한 형태의 치매이다. 알츠하이머병은 기억력, 사고력 및 행동에 영향을 주지만 증상이 나타나기까지 거의 20년에 걸쳐 진행이 된다. 따라서 전 임상 단계에서 생리학을 이해하는 것이 필수적이다. 유전적 요인이 AD에 거의 50% 기여하는 것으로 추정된다. 유전자가 세포 환경을 변경하여 알츠하이머병 위험에 어떻게 기여하는지 더 잘 이해하기 위해 AD와 연관이 있는 유전자인 APOE를 보유한 사람들의 대사체(Metabolome)를 조사했다. 대사체는 유전체(Genome)과 단백질체(Proteome)에서 생성된 산물을 의미한다. 이러한 생화학 부산물은 유전적 요인과 환경적 요인 모두의 영향을 받는다. 모집단은 장수마을에 사는 Caucasian (백인) 참여자들이다.\n\n\n\nLLFS(Long Life Family Study) 프로젝트의 목적은 유전체, 전사체, 단백질체 및 대사체 단계를 통해 유전체에서 대사체 단계에 이르는 여러 단계에서 통계 및 기계 학습을 사용하여 분석 파이프라인을 구축하고 알츠하이머병에 대한 중요한 바이오마커를 식별하는 것이다.\n\n\n\n읽기의 편의성을 위해 LLFS project에 대한 설명을 project와 self project 와 같이 2개의 section으로 나누었다. project는 내가 실제로 프로젝트를 수행했던 과정을 기술했고 self project는 그 방법론을 대략적으로 간소화된 형태로 기술했다.\n\nProject Description (Current)\nSelf Project\n\nSelf Project Description\nData Preparation\nEDA (Exploratory Data Analysis)\nMethod1: statistical approach\nMethod2: ML Approach\nConclusion\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    subgraph Data_Collection\n        direction TB\n        Multi_Centerd_Blood_Sampling---\n        Mass_Spectrometry---\n        Data_Transfer\n    end\n    subgraph Quality_Control\n        direction TB\n        Identify_Anomaly_Data---\n        Identify_Missing_Values\n    end\n    subgraph Analytics\n        direction TB \n        EDA---\n        Data_Mining---\n        Statistical_Analysis---\n        Machine_Learning\n    end\n    subgraph Reporting_and_Conclusion\n        direction TB \n        Share_with_Faculty\n    end\n\nData_Collection-->Quality_Control-->Analytics-->Reporting_and_Conclusion\n\n\n\n\n\n\n\n\nData는 장수 마을에 거주하는 백인을 대상으로 New York, Bonston, Pittsburgh 및 Denmark에 있는 여러 medical centers에서 sampled blood를 MS Spectromtetry로 Digitalization을 했다. 여러 과정을 통해 data를 csv형태로 받아 data의 QC(Quality Control)를 진행한뒤 Data 분석 업무를 수행했다. EDA (Exploratory Data Analysis) 와 Data Mining을 통해 data에 대한 이해도를 높였고 이를 토대로 통계 분석과 machine learning을 이용하여 이 data에 적합한 모형을 찾았다. 모든 결과물은 The Taub Institute for Research on Alzheimer’s Disease and the Aging Brain 의 biostaticians, medical doctors, biologists, neurologists, bioinformaticians 및 epidemiologists와 공유를 했다.\n\n\n\n\n\n\n\nflowchart LR\n    subgraph Data_Quality_Control\n        direction TB\n        identify_anomaly_data---\n        identify_missing_values\n        subgraph Missing_Value_Analysis\n            direction LR\n            MCAR---\n            MAR---\n            MNAR\n        end\n        either_imputation_or_omission---\n        communication_with_labs---\n        set_data_inclusion/exclusion_criteria\n    end\n    identify_missing_values---Missing_Value_Analysis---either_imputation_or_omission\n    subgraph Data_Preprocessing\n        direction TB\n        data_transformation---\n        log_transformation---\n        standardization\n    end\n\nData_Quality_Control-->Data_Preprocessing\n\n\n\n\n\n\n\n\n\nData의 품질 관리를 위해 data를 생성한 biochemists와 소통하여 실험실 기준에 따라 결측치와 이상치를 구분하여 labeling을 수행했고 missing value analysis를 통해 결과에 따라 medical doctors를 포함한 다른 faculty members와 상의하여 결측치 처리를 했다. data QC criteria는 rowwise 와 columwise sum의 합이 sample size에 대하여 missing values의 비율이 5%가 넘는 환자와 변수는 분석 대상에서 제외 됐다. 모든 metabolites data는 log transoformation 과 standardization을 통해 data의 단위를 표준화 했다.\n\n\n\n\n\n\n\nflowchart TB\n    subgraph Data_Analytics\n        direction TB\n        Exploratory_Data_Analysis---\n        Data_Minig---\n        Statistical_Analysis---\n        Machine_Learning\n    end\n\n\n\n\n\n\n\n\nData 분석은 크게 EDA (Exploratory Data Analysis), Statistical Analysis 및 Machine Learning과 같이 3 단계로 수행했다. 각 단계에서 나온 결과가 각 각의 단계에서 일관되게 나오는 metabolites를 선별했다.\n\n\nstudent t tests, Wilcoxon Man Whiteney tests, \\(\\chi^2\\) tests, Fisher Exact Tests, ANOVAs, Kruskal Wallis Tests 및 regression analysis이 수행됐고 visualization을 통해 검정 결과를 재확인하는 작업을 수행했다. 고차원 데이터를 시각화하여 data의 pattern을 관찰하기 위해 KNN, PCA, K means clustering 및 DB Scan을 이용했다.\n\n\n\nmultivariable linear regression, logistic regression 및 Cox PH(Proportional Hazards) regression anayses 가 수행됐고 질병과 유의한 metabolites를 선별했다. multiple testing으로 인한 1 종 오류를 범하는 것을 줄이기 위해 permuted p-values를 계산하여 유의성을 한번 더 확인했다.\n\n\n\nLasso, ridge regression, elastic net, decision tree, random rorests, ada boosting, gradient descent boosting, SVM (support vector machine), partial least square 및 sparse partial least square가 사용됐다. 질병을 가장 잘 예측하는 classifier를 평가하여 최적의 classifier를 선택했다.\n\n\n\n\n146개의 관측치와 약 3,000여개의 변수로 구성된 data에서 약 60개 내외의 대사물질이 질병과 5% 유의수준으로 유의한 관계가 있는 것으로 관찰됐고 partial least suare 가 가장 성능이 좋은 것으로 관찰됐다.\n\n\n\n\n\n\nAlzheimer Disease (AD) is the most common form of dementia that affects millions of Americans. AD affects memory, thinking and behavior, but its progression is slow, spanning nearly two decades before the symptoms appear. Thus, it is imperative to understand the physiology at the pre-clinical stage. It is estimated that genetic factors contribute nearly 50% to AD. To better understand how genes contribute to the risk of AD by altering cellular milieu, I have examined the metabolome of individuals with the AD-related genotype, APOE. The metabolome represents the products that were generated from the genome and proteome. These biochemical products represent influences of both genetic and environmental factors. The population is Caucasian participants living in longevity village.\n\n\n\nThe objective of the Long Life Family Study (LLFS) project was to build an analysis pipeline of identifying significant biomarkers for AD using statistics and machine learning at the multi-stages from the genomic to the metabolomic stage through the transcriptomic and proteomic stage.\n\n\n\nFor the convenience of reading, the LLFS project is divided into the two sections: project and self-project. The project section roughly described the process of the project I actually carried out, and the self project one described the methodology in a roughly simplified form.\n\nProject Description (Current)\nSelf Project\n\nSelf Project Description\nData Preparation\nEDA (Exploratory Data Analysis)\nMethod1: statistical approach\nMethod2: ML Approach\nConclusion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster0\n\n Data Collection  \n\ncluster1\n\n Quality Control  \n\ncluster2\n\n Analytics  \n\ncluster3\n\n Reporting and Conclusion   \n\nMulti_Centered_Blood_Sampling\n\n Multi_Centered_Blood_Sampling   \n\nMass_Spectrometry\n\n Mass_Spectrometry   \n\nData_Transfer\n\n Data_Transfer   \n\nIdentify_Anomaly_Data\n\n Identify_Anomaly_Data   \n\nData_Transfer->Identify_Anomaly_Data\n\n    \n\nIdentify_Missing_Values\n\n Identify_Missing_Values   \n\nEDA\n\n EDA   \n\nIdentify_Missing_Values->EDA\n\n    \n\nData_Mining\n\n Data_Mining   \n\nStatistical_Analysis\n\n Statistical_Analysis   \n\nMachine_Learning\n\n Machine_Learning   \n\nShare_with_Faculty\n\n Share_with_Faculty   \n\nMachine_Learning->Share_with_Faculty\n\n   \n\n\n\n\n\nData were obtained by digitization through MS Spectromtetry of blood samples from multiple medical centers in New York, Bonston, Pittsburgh, and Denmark for Caucasians residing in longevity villages. After receiving the data in a csv format through various processes, QC (Quality Control) of the data and data analysis were performed. To better understand data, exploratory data analysis (EDA) and data mining were conducted. Based on the analysis findings on data, the machine learning model to explain the data most was selcted. All findings were shared with biostatisticians, medical doctors, biologists, neurologists and epidemiologists at the neurology department and the Taub Institute for Research on Alzheimer’s Disease and the Aging Brain in the Columbia University Irving Medical Center.\n\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster2\n\n Data Preprocessing  \n\ncluster0\n\n Data Quality Control  \n\ncluster1\n\n Missing Value Analysis   \n\nidentify_anomaly_data\n\n identify_anomaly_data   \n\nidentify_missing_values\n\n identify_missing_values   \n\nidentify_anomaly_data->identify_missing_values\n\n    \n\nMissing_Completely_At_Random\n\n Missing_Completely_At_Random   \n\nidentify_missing_values->Missing_Completely_At_Random\n\n    \n\nMissing_At_Random\n\n Missing_At_Random   \n\nMissing_Completely_At_Random->Missing_At_Random\n\n    \n\nMissing_Not_at_Random\n\n Missing_Not_at_Random   \n\nMissing_At_Random->Missing_Not_at_Random\n\n    \n\neither_imputation_or_omission\n\n either_imputation_or_omission   \n\nMissing_Not_at_Random->either_imputation_or_omission\n\n    \n\ncommunication_with_labs\n\n communication_with_labs   \n\neither_imputation_or_omission->communication_with_labs\n\n    \n\nset_data_inclusion_exclusion_criteria\n\n set_data_inclusion_exclusion_criteria   \n\ncommunication_with_labs->set_data_inclusion_exclusion_criteria\n\n    \n\nData_Transformation\n\n Data_Transformation   \n\nset_data_inclusion_exclusion_criteria->Data_Transformation\n\n    \n\nLog_Transformation\n\n Log_Transformation   \n\nData_Transformation->Log_Transformation\n\n    \n\nStandardization\n\n Standardization   \n\nLog_Transformation->Standardization\n\n   \n\n\n\n\n\nFor data quality control, I communicated with whom generated the data, classified missing values ​​and outliers according to laboratory standards, and labeled them. Based on the results through missing value analysis, I processed the missing values through consultation with the faculty members several times. For the data QC criteria, patients and variables whose ratio of missing values ​​for the sum of the rowwise and columnwise sums exceeded 5% for the sample size were excluded from the analysis. All metabolites data were standardized through log transformation and standardization.\n\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster2\n\n Data Analytics   \n\nExploratory_Data_Analysis\n\n Exploratory_Data_Analysis   \n\nData_Minig\n\n Data_Minig   \n\nExploratory_Data_Analysis->Data_Minig\n\n    \n\nStatistical_Analysis\n\n Statistical_Analysis   \n\nData_Minig->Statistical_Analysis\n\n    \n\nMachine_Learning\n\n Machine_Learning   \n\nStatistical_Analysis->Machine_Learning\n\n   \n\n\n\n\n\nData analysis was performed in three stages: Exploratory Data Analysis (EDA), Statistical Analysis, and Machine Learning. In each stage, metabolites commonly associated with diseases were selected.\n\n\nStudent t tests, Wilcoxon Man Whiteney tests, \\(\\chi^2\\) tests, Fisher Exact Tests, ANOVAs, Kruskal Wallis Tests, and regression testing were performed, and I visualizaed data to reconfirm the test results. To visualize high-dimensional data and observe data patterns, KNN, PCA, K means, Clustering, and DB Scan were used.\n\n\n\nMultivariable linear regression, logistic regression, and Cox PH (Proportional Hazards) regression analyses were conducted and the metabolites that are signficantly associated with the disease status were selected. In order to reduce the possibility of making a type 1 error due to multiple testing, the significance was checked once more by calculating permuted p-values.\n\n\n\nLasso, ridge regression, elastic net, decision tree, random rorests, ada boosting, gradient descent boosting, support vector machine (SVM), partial least square, and sparse partial least square were used. The optimal classifier was selected by evaluating the classifier that best predicted the disease status.\n\n\n\n\nIn the data consisting of 146 observations and about 3,000 variables, about 60 metabolites were observed to have a significant relationship with the disease at the 5% significance level, and partial least suare was observed to perform the best."
  },
  {
    "objectID": "docs/projects/LLFS/self_description.html",
    "href": "docs/projects/LLFS/self_description.html",
    "title": "Project Description",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n보안상의 이유로 프로젝트에서 사용됐던 실제 data를 사용하지 못하기 때문에 분석을 위해 사용됐던 방법론을 구체적으로 보여주기 어렵다. 이에 따라 대략적인 분석 방식을 고차원의 서로 상관 관계가 있는 data를 simulation을 통해 만들어 보여주려고 한다.\n\n\n\n이 시뮬레이션 연구의 목표는 AD와 None AD와 관련된 biomarkers를 구별할 수 있는 일련의 예측인자(또는 대사물질 또는 생화학물질)를 식별하는데 사용됐던 방법론을 소개하는 것이다.\n\n\n\n\n보안 문제로 인해 이 프로젝트에 사용된 실제 데이터와 전체 분석 파이프라인을 보여주기 어렵다.\n이 시뮬레이션 연구에서는 다변량 정규분포 하에서 대사 물질 데이터를 생성하여 대사 단계에서 가상의 데이터를 생성하고 분석 방법론을 기술하는 데에만 집중할 것이다.\n시뮬레이션 경험이 많지 않아 시뮬레이션이 수학적으로 통계적으로 틀린 부분이 있을 수 있다.\n시뮬레이션은 내가 수행했던 분석 방법론을 간단히 재현하는 용도로 사용하는 것이기 때문에 시뮬레이션 자체에 많은 시간을 할애하진 않았다.\n시뮬레이션 데이터는 실제 연구를 위해 표본으로 쓰인 sample 데이터의 분포를 전혀 반영하지 않았다.\n이 시뮬레이션 연구에서, 실제 데이터의 분포를 반영하지 않았고 범주형 변수 및 연속형 변수와 종속 변수를 통계적으로 잘 연관시키지 못했기 때문에 분석 결과가 생물학적인 사실과 많이 다를 수 있다.\n\n\n\n\n\nOperating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 used for coding in the Korean section\nPython 3.11 used for coding in the English section\n\n\n\n\n\n\n\nIt is difficult to show the methodology used for analysis in detail because the actual data used in the project cannot be used for security reasons. Accordingly, I am going to show a rough analysis method through simulation of high-dimensional, mutually correlated data.\n\n\n\nThe aim of this simulation study is to identify a set of predictors (or metabolites or bio-chemicals) that will enable to differentiate bio-markers that are associated with AD vs. non-AD.\n\n\n\n\nIn this article, due to security concerns, it is difficult to display the real data and the entire analysis pipeline used in this project.\nIn this simulation study, I will focus only on generating fake data at the metabolomic stage by generating data under multivariate normal distributions.\nSince I don’t have much experience in simulation, there may be mathematically and statistically incorrect parts in the simulation.\nI did not put a lot of effort into the simulation itself because the simulation was used to simply reproduce the analysis methodology I had performed.\nThe simulated data does not reflect the distribution of the truely sampled data used in the LLFS at all.\nIn this simulation, since the categorical and continuous variables and the dependent variable could not be statistically associated properly, the analysis result for the discrete variables could be very different from the biological or medical fact.\n\n\n\n\n\nOperating System\n\nWindow\nUbuntu 20.04\n\nSoftware\n\nQuarto for dynamic documentation\nVS code\nR studio 2022.07.2+576\nR base 4.2.2 used for coding in the Korean section\nPython 3.11 used for coding in the English section"
  },
  {
    "objectID": "docs/projects/LLFS/statistical_approach.html",
    "href": "docs/projects/LLFS/statistical_approach.html",
    "title": "Statistical Approach",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 Go to Project Content List\nProject Content List\n\n\n2 Go to Blog Content List\nBlog Content List"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\nBrief Introduction\n저는 R과 Python같은 open-source tool을 사용하여 통계, machine learning 및 deep learning을 독학하는 열정 가득한 data scientist입니다. 약 7년 여 동안 data modeling, 통계적 모델링, machine learning 모델링 및 시각화를 통하여 data 관련 업무 경험을 쌓았습니다.\n\n\nExperience\n저는 한국에서 학부과정으로 생화학과 미국에서 학부 과정으로 수학과 석사 과정으로 생물통계학을 전공했습니다. 저의 생화학 전공을 살려 Bio와 의료분야에서 커리어를 시작했고 그 과정에서 많은 data science 비전문가들과 협업하면서 통계 및 data science에 대하여 그들과 소통하는 법을 익혔습니다.\n\n\nReason for Blogging\nData science에 대하여 비전문가들과 SW 개발자들과 협업을 하면서 그들과 효율적이고 효과적으로 소통하는것이 중요하다는 것을 깨달았습니다. 그 효과적인 소통이 수학, 통계 및 IT에 대한 지식으로부터 온다고 생각해서 새로운 기술에 대하여 세부적이고 체계적인 지식을 쌓기위해 블로깅을 시작했습니다.\n\n\n\n\nBrief Introduction\nI am passionate and self-taught in statistics, machine learning, deep learning, and programming using open-source tools such as Python, R, and SQL. It has been 7 years since I dealt with data research from data modeling to data visualization through modeling.\n\n\nExperience\nMy educational background is a Bachelor of Science in biochemistry in South Korea, Bachelor of Arts in mathematics in the USA, and Master of Sceince in biostatistics in the USA. I started my career in the medical area of analytics because I have a strong background in biology. My work experience exposed me to an environment where I had to interact with non-experts to make them understand data science.\n\n\nReason for Blogging\n(Pursuing Goal in Data Science)\nAfter dealing with data and collaborating with SW developers and non-professional colleagues, I realized that the importance of communicating with them efficiently and effectively. This requires more knowledge in mathematics, statistics, and IT to master the art of explaining easily, clearly, and concisely. So, I started blogging to have an opportunity to have a detailed and systematic understanding of new techaniques of machine learning, as well as fundamental statistics."
  },
  {
    "objectID": "template.html",
    "href": "template.html",
    "title": "Template",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe radius of the circle is 10.\n\n\n\n1 Go to Project Content List\nProject Content List\n\n\n2 Go to Blog Content List\nBlog Content List"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "docs/blog/posts/cSense/common_sense.html",
    "href": "docs/blog/posts/cSense/common_sense.html",
    "title": "Common Sense in Data Sense",
    "section": "",
    "text": "The central limit theorem is a fundamental concept in probability theory and statistics. It states that if a large enough sample size is taken from a population, the distribution of the sample means will be approximately normal, regardless of the shape of the original population distribution.\nIn simpler terms, this means that as the sample size increases, the sample mean will become a more accurate representation of the population mean, and the distribution of the sample means will become more bell-shaped and symmetrical, resembling a normal distribution.\nThe central limit theorem is important because it allows us to make statistical inferences about a population based on a sample, even if we don’t know the exact shape of the population distribution. It also helps us to understand the behavior of the sample mean and standard deviation, which are key parameters used in many statistical tests and models.\n\n\nSuppose a factory produces a large number of identical products, and the weight of each product follows a normal distribution with a mean of 500 grams and a standard deviation of 10 grams. The factory wants to ensure that the average weight of a batch of 100 products is within a certain range, say between 498 and 502 grams, with a confidence level of 95%.\nUsing the central limit theorem, we can take random samples of 100 products from the population, calculate the mean weight of each sample, and create a sampling distribution of the means. Since the sample size is sufficiently large (n > 30), the central limit theorem tells us that the sampling distribution of the means will be approximately normal, with a mean of 500 grams and a standard deviation of 1 gram (calculated as the population standard deviation divided by the square root of the sample size).\nWe can then use this information to calculate the probability that the average weight of a batch of 100 products will fall within the desired range. Using a standard normal distribution table or software, we can find the z-score for each endpoint of the range (498 and 502), and calculate the probability that a randomly selected sample of 100 products will have a mean weight within this range. If the probability is at least 95%, we can conclude that the factory’s quality control standards are met.\n\n\n\n\nThe purpose of sampling is to obtain a representative subset of the population that can be used to make inferences or draw conclusions about the larger population.\nThere are several sampling methods, including:\n\n\n\nSimple Random Sampling: In this method, each individual or unit in the population has an equal chance of being selected for the sample. This is often done using random number generators or by using a table of random numbers.\nStratified Sampling: This method involves dividing the population into subgroups, or strata, based on some characteristic (such as age, gender, or geographic location), and then selecting a random sample from each stratum. This ensures that the sample is representative of the population with respect to the characteristic of interest.\nCluster Sampling: This method involves dividing the population into clusters, such as households or schools, and then selecting a random sample of clusters to survey. All individuals or units within each selected cluster are then surveyed.\nSystematic Sampling: In this method, a random starting point is chosen from the population, and then every nth individual or unit is selected for the sample, where n is a predetermined interval.\n\n\n\n\n\nConvenience Sampling: This method involves selecting individuals or units from the population based on convenience or availability, rather than using a formal sampling technique. While convenient, this method can lead to biased results and is generally not recommended for research purposes.\nSnowball Sampling: This method is often used when studying hard-to-reach populations or groups with limited accessibility. It involves starting with a small group of individuals who meet certain criteria, and then having them refer other individuals who meet the same criteria. This process continues until the desired sample size is reached.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTruth Table\nReject \\(H_{null}\\)\nFail to Reject \\(H_{null}\\)\n\n\n\n\n\\(H_{null}\\) = True\nType I Error, \\(\\alpha\\), FP\nCorrect Decision (1 - \\(\\alpha\\)), TN\n\n\n\\(H_{null}\\) = False\nCorrect Decision (Power, 1-\\(\\beta\\)) TP\nType II Error, \\(\\beta\\), FN\n\n\n\nIn hypothesis testing, a Type I error occurs when we reject a null hypothesis that is actually true. In other words, we conclude that there is a significant effect or difference when there really isn’t one. This error is also known as a false positive.\nOn the other hand, a Type II error occurs when we fail to reject a null hypothesis that is actually false. In other words, we conclude that there is no significant effect or difference when there actually is one. This error is also known as a false negative.\n\n\n\nLinear regression is a statistical technique used to model the relationship between a dependent variable (also known as the response variable) and one or more independent variables (also known as predictors or explanatory variables). The goal of linear regression is to find the best linear relationship between the dependent variable and the independent variable(s) that can be used to predict the values of the dependent variable.\nIn linear regression, the p-value, coefficient, and R-squared value are important components that can help to evaluate the statistical significance and predictive power of the model:\nP-value: The p-value measures the probability of obtaining a result as extreme as the one observed, assuming that the null hypothesis is true. In the context of linear regression, the null hypothesis is that the coefficient of the independent variable is zero, indicating no linear relationship between the independent variable and the dependent variable. A low p-value (typically less than 0.05) indicates that the relationship between the independent variable and the dependent variable is statistically significant, and that the null hypothesis can be rejected.\nCoefficient: The coefficient is the slope of the line that represents the linear relationship between the independent variable and the dependent variable. It measures the amount of change in the dependent variable that can be attributed to a one-unit change in the independent variable, while holding all other variables constant. A positive coefficient indicates a positive relationship between the independent variable and the dependent variable, while a negative coefficient indicates a negative relationship.\nR-squared value: The R-squared value measures the proportion of the variation in the dependent variable that is explained by the variation in the independent variable(s). It is a measure of the goodness-of-fit of the model, and it ranges from 0 to 1. A high R-squared value indicates that the model explains a large proportion of the variation in the dependent variable, and that it is a good predictor of the dependent variable.\nThe significance of each of these components in linear regression is as follows:\nP-value: It helps to determine whether the relationship between the independent variable and the dependent variable is statistically significant, and whether the model can be used to make valid predictions.\nCoefficient: It helps to determine the direction and strength of the linear relationship between the independent variable and the dependent variable, and to make predictions about the dependent variable based on changes in the independent variable.\nR-squared value: It helps to evaluate the overall fit of the model, and to compare different models to determine which one is the best predictor of the dependent variable.\n\n\n\nSelection bias is a type of bias that occurs when the process of selecting individuals or groups to participate in a study or analysis is not random or representative of the population of interest. Selection bias can lead to biased estimates of the population parameters and affect the internal and external validity of the study.\nSelection bias can occur in different stages of a study or analysis, such as:\nSampling bias: Occurs when the sample is not representative of the population of interest. For example, if a study on the prevalence of a disease only includes participants from a certain demographic or geographic area, the results may not be generalizable to the entire population.\nNon-response bias: Occurs when the response rate is low, or participants who decline to participate are different from those who agree to participate. This can lead to an unrepresentative sample and biased estimates.\nSurvivorship bias: Occurs when the sample only includes individuals or groups that have survived or persisted through a certain selection process, such as a follow-up study. This can lead to an overestimation of the survival rates or other outcomes.\nBerkson’s bias: Occurs when the sample is selected based on a condition that is related to the exposure or outcome of interest, such as hospital-based samples. This can lead to an underestimation of the association between the exposure and outcome.\nSelection bias can be minimized by using random sampling techniques and ensuring that the sample is representative of the population of interest. It is also important to analyze and report any potential biases and limitations of the study to ensure that the results are interpreted correctly."
  },
  {
    "objectID": "docs/blog/posts/cSense/common_sense.html#data-science",
    "href": "docs/blog/posts/cSense/common_sense.html#data-science",
    "title": "Common Sense in Data Sense",
    "section": "2 Data Science",
    "text": "2 Data Science\n\n2.1 WHAT IS DATA SCIENCE?\nData science is an interdisciplinary field that involves the use of statistical, computational, and domain-specific knowledge and techniques to extract insights and knowledge from data. It combines methods and concepts from statistics, computer science, mathematics, and domain-specific knowledge to solve complex data-related problems.\nStatistics and computer science are two important components of data science. Statistics provides the foundational methods for analyzing and modeling data, while computer science provides the computational infrastructure and tools for data storage, processing, and analysis. In addition, computer science provides techniques for data visualization, machine learning, and artificial intelligence that can be used to extract insights and knowledge from large datasets.\nData science involves a wide range of activities, including data acquisition, cleaning and preprocessing, exploratory data analysis, modeling and prediction, data visualization, and communication of results. It requires a strong foundation in statistics, programming, and domain-specific knowledge, as well as expertise in data management, data visualization, and machine learning.\nIn summary, data science is a multidisciplinary field that combines statistical and computational methods to extract insights and knowledge from data. It draws on a wide range of disciplines, including statistics, computer science, mathematics, and domain-specific knowledge, and requires a diverse set of skills and expertise.\n\n\n2.2 \n\n\n2.3 \n\n\n2.4 \n\n\n2.5 \n\n\n2.6 \n\n\n2.7 \n\n\n2.8 \n\n\n2.9 \n\n\n2.10 \n\n\n2.11 \n\n\n2.12 \n\n\n2.13 \n\n\n2.14 \n\n\n2.15 \n\n\n2.16 \n\n\n2.17 \n\n\n2.18 \n\n\n2.19 \n\n\n2.20 \n\n\n2.21"
  },
  {
    "objectID": "docs/projects/qc_platform/index.html",
    "href": "docs/projects/qc_platform/index.html",
    "title": "DevOps Project: Diagnostic Device QC Platform Construction",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n진단 장비의 품질 관리는 의료 장비와 연관된 제품의 특성상 Global Market 진출시 각 나라의 정부에서 요구하는 규제사항 중 하나이다.\n\n시약의 안정성 검증 요구\n장비의 안정성 검증 요구\nSoftware의 안정성 검증 요구\nDiagnostic Algorithm의 안정성 검증 요구\n\n현재 Seegene이 사용하고 있는 진단 장비는 자사 제품이 아니기 때문에 진단 장비의 품질 관리 방식에 어려움이 있다.\nPCR 기기의 noise test는 의료 장비의 QC process 중 하나로서, Seegene의 시약 제품의 성능 안정성과 직접적으로 영향을 주는 품질 검증 시스템이다.\n회사의 규모가 커지고 잦은 조직 개편으로 수동 방식의 noise tests가 여러 부서로 이관됨에 따라 noise test 수행자의 이해도와 숙련도가 떨어져 noise test가 올바르게 수행되지 않고 있다.\n씨젠의 장기 목표인 전사 자동화를 위해 noise test를 자동화하는 프로젝트가 발탁됐다.\n\n\n\n\n\n2020.12.19~2021.03.06에 입고된 PCR기기 2201대를 2552번의 실험에서 만들어진 61,248개의 신호에서 얻은 data-driven parameters로 장비의 성능을 평가하여 합격/불합격 뿐만 아니라 장비에 등급을 차등 부여하여 시간에 따른 장비의 성능을 지속적으로 분석 가능하게 한다.\n다음의 주요 문제점을 개선한다.\n\n신호의 증폭에 크기에 따라 noise test 결과에 영향을 크게 주어 잘못된 결과를 산출해주는 metric 개선\n단순한 휴먼 에러 신호에 무조건적으로 장비의 불합격처리가 결정 되는 문제 보완하여 robust한 평가체계로 현업부서의 부담을 덜어준다.\n장비 고유에서 발생하는 pattern을 찾아 장비 error 신호를 labeling 한다.\n\n수동 계산과정에 소요되는 시간을 20번의 test당 30분에서 웹 기반의 자동화로 약 2~3분내로 단축시킬 수 있다.\n씨젠의 full automation의 best practice로 만들어 IT 부분과 제조 부문 및 BT부문과의 협력체계 구축 및 활성화 한다.\n시각화와 noise test result history를 제공하여 추적 및 VOC 대응 system을 구축한다.\n\n\n\n\n\n\n\n현업 부서와의 긴밀한 소통으로 QC process를 세분화하여 앞 단계 QC에서 발생하는 data를 활용하여 뒷 단계 QC인 noise test의 결과를 예측한다.\nnoise가 적다고 확실시 되는 기기에 한해서 noise test 생략\n\n\n\n\n\n전체 QC 프로세스를 자동화 또는 반 자동화\n기존의 noise 측정 metric 분석 및 새로운 metric 생성하여 검사 결과의 정확도를 향상\n시각화와 noise test result history를 제공하여 실무자의 이해도를 높이고 관리가능하게 한다.\n\n\n\n\n\n\n현업 업무 기술서 부재\n실무자의 백업 실수\n부서마다 산재된 데이터\n높은 난이도의 data cleansing\n\n실무자의 데이터로부터 분석 가능한 데이터 선별\n실무자의 데이터의 오입력\ndata 및 문서의 DRM 수동해제\n\nreverse engineering 필요\n잦은 조직 개편으로 관련 인원 및 부서 연락체계 부재\n\n\n\n\n\nBack-end Engineering for DevOps Pipeline Construction\nData Engineering for data cleansing and reverse engineering\nData Modeling for a RDB system construction\nStatistical Analysis for the noise test performance verification\nMachine Learning for pattern analysis\nFront-end Engineering for UI/UX construction\nBiologics, Biophysics, Physics Knowledge\n\n\n\n\n\n1 data scientist (me) - project owner\n5 mechanical engineers\n4 biologists\n2 patent attoneys\n2 data engineers\n3 full stack developers\n1 advisor (a professor of Computer Science at Seoul National University)\n\n\n\n\n\n\n\n\n\n\n\n\nNoise Test\nAs-Is\nTo-Be\n\n\n\n\nSample Size\nn=100\n2552번의 실험에서 얻은 신호 n=61,248\n\n\nEvaluation Metrics\n2 metrics\n10 metrics ( the exsting 2 metrics + new 8 metrics)\n\n\nInput Process\nmanually copy & paste data of excel files extracted from a certain program\nWeb-Based Automation, upload multiple experiment files\n\n\nOutput Process\nBatch Evaluation method, if even one of the signals from the equipment fails, the equipment fails. (Blind Spot: If there is one human error signal, the equipment is unconditionally disqualified.)\nDifferential Evaluation method, The signals from the equipment are scored, the average value is obtained, and the equipment is graded A+, A, B, and F. Failed if F. Robust on the error signal as it excludes the error signal from evaluation\n\n\nOutput 1\npass: 92.58%, fail: 7.42%\nA+ (pass): 7.01%, A (pass): 12.91%, B (pass): 75.72%, F (fail): 4.36%\n\n\nOutput 2\nNA\nVisualized Plots and Tables.\n\n\nOutput 3\nNA\nNormal Signals, Human Errors, Device Errors, Manufacturing Errors\n\n\nTime Consumed\nAbout 30 minutes per 20 experiments\nAbout 25 minutes per 2552 experiments\n\n\nData Management\nNon-standard management method (stored in NAS directory in a different form as an Excel file in a different way for each worker)\nAutomatic loading in DB in standardized form by a scheduler (equipment failure tracing analysis becomes possible)\n\n\n\n\n\n\nThe noise test was abolished in the whole QC Process because I statistically proved that the noise test result does not have a significant impact on the difference of the mean of the final result in the whole diagnostic process at the 1% significance level.\nIt turned out to be an opportunity for the Data Science department to increase their understanding of experiments and data generated from equipment.\nIt served as the basis for building a company-wide DB and building a platform architecture.\n\n\n\n\n\n\n\n\nQuality Control of diagnostic equipment is one of the necessities for the regulations required by the government of each country when entering the global market due to the nature of products related to medical equipment.\n\nReagent stability verification & validation required\nEquipment stability verification & validation request\nSoftware stability verification & validation request\nStability Verification & validation Request of Diagnostic Algorithm\n\nSince the diagnostic equipment currently being used by Seegene is not its own product, it is difficult to manage the quality of the diagnostic equipment.\nThe noise test of PCR equipment is one of the QC processes of medical equipments, and it is a quality verification system that directly affects the performance stability of Seegene’s reagent products.\nAs the size of the company grows and frequent organizational reshuffles result in manual noise tests being transferred to various departments, the noise test performers’ understanding and skills are low, resulting in noise tests not being performed correctly.\nA project to automate the noise test was selected for Seegene’s long-term goal of enterprise automation.\n\n\n\n\n\nData-driven parameters obtained from 61,248 signals from 2552 experiments were used to evaluate 2201 PCR devices, which were stocked between 2020.12.19 and 2021.03.06. yield Enables continuous analysis of equipment performance over time.\nImprove the following major problems:\n\nImproved metrics that produces erroneous results by greatly affecting the noise test result depending on the size of the signal amplification.\nIt relieves the burden on the field department with a robust evaluation system by supplementing the problem of unconditionally determining equipment rejection in response to a simple human error signal.\nEquipment error signals are labeled by finding patterns that occur in equipment.\n\nThe time required for the manual calculation process can be reduced from 30 minutes per 20 tests to about 2 to 3 minutes with web-based automation.\nBy making it the best practice of Seegene’s full automation, establish and vitalize the cooperation system between the IT, manufacturing and BT sectors.\nVisualization and noise test result history are provided to build a tracking and VOC response system.\n\n\n\n\n\n\n\nThe QC process is subdivided through close communication with the field departments, and the results of the noise test, which is the next stage QC, are predicted by utilizing the data generated in the previous stage QC.\nNoise test omitted only for devices that are certain to have low noise.\n\n\n\n\n\nAutomate or semi-automate the entire QC process\nImprove the accuracy of inspection results by analyzing the existing noise measurement metric and creating a new metric\nVisualization and noise test result history are provided to increase the understanding of practitioners and enable management\n\n\n\n\n\n\nAbsence of job description\nBackup Mistakes by Practitioners\nData scattered across departments\nHigh level of data cleansing\n\nSelect data that can be analyzed from practitioner data\nIncorrect input of practitioner data\nManual release of DRM for data and documents\n\nreverse engineering required\nAbsence of contact system for related personnel and departments due to frequent organizational reshuffle\n\n\n\n\n\nBack-end Engineering for DevOps Pipeline Construction\nData Engineering for data cleansing and reverse engineering\nData Modeling for a RDB system construction\nStatistical Analysis for the noise test performance verification\nMachine Learning for pattern analysis\nFront-end Engineering for UI/UX construction\nBiologics, Biophysics, Physics Knowledge\n\n\n\n\n\n1 data scientist (me) - project owner\n5 mechanical engineers\n4 biologists\n2 patent attoneys\n2 data engineers\n3 full stack developers\n1 advisor (a professor of Computer Science at Seoul National University)\n\n\n\n\n\n\n\n\n\n\n\n\nNoise Test\nAs-Is\nTo-Be\n\n\n\n\nSample Size\nn=100\nSignals from 2552 experiments, n=61,248\n\n\nEvaluation Metrics\n2 metrics\n10 metrics ( the exsting 2 metrics + new 8 metrics)\n\n\nInput Process\nmanually copy & paste data of excel files extracted from a certain program\nWeb-Based Automation, upload multiple experiment files\n\n\nOutput Process\nBatch Evaluation method, if even one of the signals from the equipment fails, the equipment fails. (Blind Spot: If there is one human error signal, the equipment is unconditionally disqualified.)\nDifferential Evaluation method, The signals from the equipment are scored, the average value is obtained, and the equipment is graded A+, A, B, and F. Failed if F. Robust on the error signal as it excludes the error signal from evaluation\n\n\nOutput 1\npass: 92.58%, fail: 7.42%\nA+ (pass): 7.01%, A (pass): 12.91%, B (pass): 75.72%, F (fail): 4.36%\n\n\nOutput 2\nNA\nVisualized Plots and Tables.\n\n\nOutput 3\nNA\nNormal Signals, Human Errors, Device Errors, Manufacturing Errors\n\n\nTime Consumed\nAbout 30 minutes per 20 experiments\nAbout 25 minutes per 2552 experiments\n\n\nData Management\nNon-standard management method (stored in NAS directory in a different form as an Excel file in a different way for each worker)\nAutomatic loading in DB in standardized form by a scheduler (equipment failure tracing analysis becomes possible)\n\n\n\n\n\n\nThe noise test was abolished in the whole QC Process because I statistically proved that the noise test result does not have a significant impact on the difference of the mean of the final result in the whole diagnostic process at the 1% significance level.\nIt turned out to be an opportunity for the Data Science department to increase their understanding of experiments and data generated from equipment.\nIt served as the basis for building a company-wide DB and building a platform architecture."
  },
  {
    "objectID": "docs/projects/dsp_validation/index.html",
    "href": "docs/projects/dsp_validation/index.html",
    "title": "Project (ongoing): Diagnostic Signal Processing Algorithm Validation for Surveilance",
    "section": "",
    "text": "Korean\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n의료 장비와 연관된 시약 제품의 특성상 Global Market 진출시 각 나라의 정부에서 자국민의 건강 및 생명의 안전을 위해 요구하는 규제사항들이 있다.\n\n시약의 안정성 검증 요구\n장비의 안정성 검증 요구\nSoftware의 안정성 검증 요구\nDiagnostic Algorithm의 안정성 검증 요구\n\nCOVID19 특수 시기 해제 후 Global market으로 진입 및 유지를 위해 제품의 안정성 검증이 각 국의 정부로 부터 요구된다. 그 중 가장 엄격한 기준을 요구하는 미국의 FDA와 캐나다의 Health Canada를 기준으로 진단 알고리즘의 안정성 검증 문서를 기획하고 작성해야만 한다.\n시간이 지날 수록 각 국에서 Software 및 algorithms에 대한 규제가 강화되고 있기때문에 기존의 Software Engineering에 의한 안전성 검증보다 더 엄격한 Advanced Testing이 요구되고 있다.\n\n\n\n\n\nalgorithm이 안전한 성능을 발휘한다는 것을 증명한다\nalgorithm이 risk 관리가 가능하다는 것을 증명한다.\n\n\n\n\n\n세계에서 가장 엄격한 검열 인증서를 발급 및 교육을 제공하는 기업인 SGS의 guidance를 참고한다.\nSGS는 FDA를 target으로 guidance를 제공한다.\nSoftware의 안전성 검증을 위해 FDA에서 제공하는 General Principles of Software Validation 문서를 정독 후 이 문서를 기반으로 기획 및 작성한다.\nStructural Testing와 Advanced Testing 모두 포함한다.\nalgorithm 안전성에 대한 정의와 논리를 확립한다.\nalgorithm 안전성에 대한 지표를 확립한다.\nSoftware engineering은 General Principles of Software Validation 문서를 기반하여 수행한다.\nAdvanced Testing인 Statistical Testing은 data scientist의 창의성이 요구되는 작업으로 Testing Model을 기획하여 statistical analysis design을 한다.\n기획된 Testing model에 적합한 statistical model을 찾고 biological experiment design과 engineering design을 한다.\n\n\n\n\n\n선례 및 template를 찾을 수 없을 정도로 매우 드물다.\n\n\n\n\n\nFDA software validation\nStatistics\nDynamic documentation\nBiology\nClinical study design\n\n\n\n\n\nproject owner (me)\n7 data scientists\n1 data enineer\n27 biologists\n2 patent attoneys\n\n\n\n\n\n\n\n\n\n\n\n\nNoise Test\nAs-Is\nTo-Be\n\n\n\n\nSample Size\nn=100\n2552번의 실험에서 얻은 신호 n=61,248\n\n\nEvaluation Metrics\n2 metrics\n10 metrics ( the exsting 2 metrics + new 8 metrics)\n\n\nInput Process\nmanually copy & paste data of excel files extracted from a certain program\nWeb-Based Automation, upload multiple experiment files\n\n\nOutput Process\nBatch Evaluation method, if even one of the signals from the equipment fails, the equipment fails. (Blind Spot: If there is one human error signal, the equipment is unconditionally disqualified.)\nDifferential Evaluation method, The signals from the equipment are scored, the average value is obtained, and the equipment is graded A+, A, B, and F. Failed if F. Robust on the error signal as it excludes the error signal from evaluation\n\n\nOutput 1\npass: 92.58%, fail: 7.42%\nA+ (pass): 7.01%, A (pass): 12.91%, B (pass): 75.72%, F (fail): 4.36%\n\n\nOutput 2\nNA\nVisualized Plots and Tables.\n\n\nOutput 3\nNA\nNormal Signals, Human Errors, Device Errors, Manufacturing Errors\n\n\nTime Consumed\nAbout 30 minutes per 20 experiments\nAbout 25 minutes per 2552 experiments\n\n\nData Management\nNon-standard management method (stored in NAS directory in a different form as an Excel file in a different way for each worker)\nAutomatic loading in DB in standardized form by a scheduler (equipment failure tracing analysis becomes possible)\n\n\n\n\n\n\n\n\n\n\n\n\nQuality Control of diagnostic equipment is one of the necessities for the regulations required by the government of each country when entering the global market due to the nature of products related to medical equipment.\n\nReagent stability verification & validation required\nEquipment stability verification & validation request\nSoftware stability verification & validation request\nStability Verification & validation Request of Diagnostic Algorithm\n\nSince the diagnostic equipment currently being used by Seegene is not its own product, it is difficult to manage the quality of the diagnostic equipment.\nThe noise test of PCR equipment is one of the QC processes of medical equipments, and it is a quality verification system that directly affects the performance stability of Seegene’s reagent products.\nAs the size of the company grows and frequent organizational reshuffles result in manual noise tests being transferred to various departments, the noise test performers’ understanding and skills are low, resulting in noise tests not being performed correctly.\nA project to automate the noise test was selected for Seegene’s long-term goal of enterprise automation.\n\n\n\n\n\nData-driven parameters obtained from 61,248 signals from 2552 experiments were used to evaluate 2201 PCR devices, which were stocked between 2020.12.19 and 2021.03.06. yield Enables continuous analysis of equipment performance over time.\nImprove the following major problems:\n\nImproved metrics that produces erroneous results by greatly affecting the noise test result depending on the size of the signal amplification.\nIt relieves the burden on the field department with a robust evaluation system by supplementing the problem of unconditionally determining equipment rejection in response to a simple human error signal.\nEquipment error signals are labeled by finding patterns that occur in equipment.\n\nThe time required for the manual calculation process can be reduced from 30 minutes per 20 tests to about 2 to 3 minutes with web-based automation.\nBy making it the best practice of Seegene’s full automation, establish and vitalize the cooperation system between the IT, manufacturing and BT sectors.\nVisualization and noise test result history are provided to build a tracking and VOC response system.\n\n\n\n\n\n\n\nThe QC process is subdivided through close communication with the field departments, and the results of the noise test, which is the next stage QC, are predicted by utilizing the data generated in the previous stage QC.\nNoise test omitted only for devices that are certain to have low noise.\n\n\n\n\n\nAutomate or semi-automate the entire QC process\nImprove the accuracy of inspection results by analyzing the existing noise measurement metric and creating a new metric\nVisualization and noise test result history are provided to increase the understanding of practitioners and enable management\n\n\n\n\n\n\nAbsence of job description\nBackup Mistakes by Practitioners\nData scattered across departments\nHigh level of data cleansing\n\nSelect data that can be analyzed from practitioner data\nIncorrect input of practitioner data\nManual release of DRM for data and documents\n\nreverse engineering required\nAbsence of contact system for related personnel and departments due to frequent organizational reshuffle\n\n\n\n\n\nBack-end Engineering for DevOps Pipeline Construction\nData Engineering for data cleansing and reverse engineering\nData Modeling for a RDB system construction\nStatistical Analysis for the noise test performance verification\nMachine Learning for pattern analysis\nFront-end Engineering for UI/UX construction\nBiologics, Biophysics, Physics Knowledge\n\n\n\n\n\n1 data scientist (me) - project owner\n5 mechanical engineers\n4 biologists\n2 patent attoneys\n2 data engineers\n3 full stack developers\n1 advisor (a professor of Computer Science at Seoul National University)\n\n\n\n\n\n\n\n\n\n\n\n\nNoise Test\nAs-Is\nTo-Be\n\n\n\n\nSample Size\nn=100\nSignals from 2552 experiments, n=61,248\n\n\nEvaluation Metrics\n2 metrics\n10 metrics ( the exsting 2 metrics + new 8 metrics)\n\n\nInput Process\nmanually copy & paste data of excel files extracted from a certain program\nWeb-Based Automation, upload multiple experiment files\n\n\nOutput Process\nBatch Evaluation method, if even one of the signals from the equipment fails, the equipment fails. (Blind Spot: If there is one human error signal, the equipment is unconditionally disqualified.)\nDifferential Evaluation method, The signals from the equipment are scored, the average value is obtained, and the equipment is graded A+, A, B, and F. Failed if F. Robust on the error signal as it excludes the error signal from evaluation\n\n\nOutput 1\npass: 92.58%, fail: 7.42%\nA+ (pass): 7.01%, A (pass): 12.91%, B (pass): 75.72%, F (fail): 4.36%\n\n\nOutput 2\nNA\nVisualized Plots and Tables.\n\n\nOutput 3\nNA\nNormal Signals, Human Errors, Device Errors, Manufacturing Errors\n\n\nTime Consumed\nAbout 30 minutes per 20 experiments\nAbout 25 minutes per 2552 experiments\n\n\nData Management\nNon-standard management method (stored in NAS directory in a different form as an Excel file in a different way for each worker)\nAutomatic loading in DB in standardized form by a scheduler (equipment failure tracing analysis becomes possible)\n\n\n\n\n\n\nThe noise test was abolished in the whole QC Process because I statistically proved that the noise test result does not have a significant impact on the difference of the mean of the final result in the whole diagnostic process at the 1% significance level.\nIt turned out to be an opportunity for the Data Science department to increase their understanding of experiments and data generated from equipment.\nIt served as the basis for building a company-wide DB and building a platform architecture."
  }
]