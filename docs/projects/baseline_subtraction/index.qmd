---
title: "Baseline Fitting Algorithm Improvement"
subtitle: "Diagnostic Algorithms Optimization"
description: "This project is to improve the baseline-fitting performance of the diagnostic signal processing algorithms."
author: Kwangmin Kim
date: 2024-07-08
format: 
  html:
    toc: true  
    #page-layout: article
    code-fold: true
    code-copy: true
    code-overflow: wrap
    number-sections: true
    number-depth: 3
    grid:
      sidebar-width: 200px
      body-width: 1200px
      margin-width: 100px
execute:
  eval: false
---

<ul class="nav nav-pills" id="language-tab" role="tablist">
  <li class="nav-item" role="presentation">
    <button class="nav-link active" id="Korean-tab" data-bs-toggle="tab" data-bs-target="#Korean" type="button" role="tab" aria-controls="Korean" aria-selected="true">Korean</button>
  </li>
  <li class="nav-item" role="presentation">
    <button class="nav-link" id="English-tab" data-bs-toggle="tab" data-bs-target="#English" type="button" role="tab" aria-controls="knitr" aria-selected="false">English</button>
  </li>

<div class="tab-content" id="language-tabcontent">

<div class="tab-pane fade  show active" id="Korean" role="tabpanel" aria-labelledby="Korean-tab">

## Background

### Real Time PCR의 원리

Real Time PCR 장비는 핵산(Nucleic Acid) 검체의 존재 여부를 확인하기 위해 각 사이클마다 핵산 검체를 2배씩 증폭하면서 신호를 감지한다.

* Lag Phase (지연 단계): 증폭된 검체의 신호가 일정 농도 이하일 경우, PCR 장비는 신호를 감지하지 못하고, 장비 내부 기본값과 시약 및 소모품의 값이 합산된 신호로 반환한다. 이는 일정 시간 동안 지연되는 구간이다.
* Log Phase (로그 단계): Exponential Phase라고도 불리며 증폭된 검체의 신호가 일정 농도 이상일 때, PCR 장비에서는 신호의 변화가 나타나며, 포화 상태에 이를 때까지 신호가 2의 지수승으로 증가한다.
* Plateau Phase (플라토 단계): Stationary Phase 효소의 활성도 감소 및 반응물의 고갈 등으로 인해 포화 상태에 도달하면, 신호는 더 이상 증가하지 않고 일정하게 유지된다.

이러한 방식으로 Real Time PCR 장비의 신호는 이론적으로 **sigmoid curve**의 패턴을 나타낸다.

* 양/음성 결과
  * 양성: 아래 그림과 같이 Real Time PCR 결과가 명확한 sigmoid curve가 나올 경우 검체가 발견된 **양성** 상태로 판독한다.
  * 음성: 반응이 없는 평평한 curve가 나오거나 명확한 sigmoid curve가 나오지 않을 경우 검체가 발견되지 않은 **음성** 상태로 판독한다.
* Ct
  * Ct 값은 **Cycle threshold**의 약자로, 특정한 양의 증폭된 DNA가 처음으로 검출 가능한 수준에 도달했을 때의 PCR 사이클 수를 의미한다
  * 초기 DNA 양: Ct 값은 초기 DNA 양과 반비례한다.
  * 증폭 효율성: Ct 값은 증폭 효율성과 관련 있다. 이상적으로는 각 사이클에서 두 배로 증폭되지만, 실제 실험에서는 다양한 요인에 의해 증폭 효율성이 100% 미만일 수 있다.
  
![Sigmoid Curve Example](../../../images/projects/baseline_subtraction/sigmoid%20curve.PNG)

### Risk

PCR 신호는 시험관 내에서 일어나는 미시적 화학 반응, 광학 반응 및 기계적 반응의 결과이다. 이 과정에서 인간이 명확히 측정하거나 설명하기 어려운 원인에 의한 노이즈가 섞일 수 있다. 이로 인해 Ct 값의 편향과 결과적으로 위양성 또는 위음성 결과가 발생할 위험이 증가할 수 있다.

### PCR 신호처리 및 최종 출력물

따라서, 이러한 노이즈를 보정하기 위한 전처리 방법이 필요하다. 전처리 후, 진단 알고리즘은 검체의 존재 여부와 Ct 값을 시그모이드 곡선 적합을 통해 최종 출력한다.

## Objective

noise를 보정하는 대표적인 전처리 방식 중 하나인 baseline 차감 알고리즘을 최적화한다. Baseline의 차감은 baseline fitting을 수행하여 fitted value를 raw data에서 차감하여 순수 신호를 분리한다. 

$$
\text{RFU (or PCR Signal)} =  \text{Pure Signal} + \text{Noise} + \text{Error}
$$

* Pure Signal: 시약에 의한 순수 증폭 및 미증폭 신호
* Noise: 장비의 결함, 소모품 및 시약의 기대하지 않은 반응에 의한 신호
* Error: 다른 설명할 수 없는 요인으로 인하여 발생한 신호

## Rationale

Baseline 데이터에 노이즈가 많을 경우, 최적화 과정에서 과적합이나 local minima와 같은 문제로 인해 파라미터 추정의 복잡성이 증가한다. 따라서, sigmoid curve fitting 전의 전처리 단계인 baseline subtraction의 정확도가 매우 중요하다.

## Motivation

회사에 처음 입사했을 때, 진단 알고리즘의 수많은 if문과 설정값, 그리고 해석하기 어려운 코드에 압도되었다. rule-based 알고리즘이 data-driven 알고리즘에 비해 성능이 떨어진다고는 할 수 없지만, 변화무쌍한 PCR 신호를 조건문으로 처리하기에는 한계가 있다고 판단했다. 따라서, 점진적으로 진단 알고리즘을 data-driven 방식으로 개선할 필요가 있다고 생각했다.  

### Considerations

* 보수적인 biologists와 전문적 지식이 없는 임원진을 이해시킬 수 있는 설명력있는 간단한 알고리즘이어야 한다.
* 현재의 진단 알고리즘은 수 많은 조건문으로 이루어진 rule-based 알고리즘이기 때문에 전체적인 알고리즘 수정 시 정합성 및 규제 문제가 발생할 수 있다. 따라서, baseline fitting 알고리즘만 개선하여 대체해야 한다.
* 통계를 전공하지 않은 알고리즘 관리자1 (Computer Science 전공), 관리자2 (Physics 전공)가 이해할 수 있도록 알고리즘 평가 metric을 선정 및 설명할 수 있어야한다. 
* Python으로 작성된 알고리즘을 C++로 porting을 해야만하기 때문에 최소한의 package를 사용하여 알고리즘을 low level로 coding해야한다.
* Lag Phase에서의 baseline data points 수가 적을 수 있으므로, 적은 데이터로도 fitting할 수 있는 알고리즘이 필요하다.
* baseline의 noise 패턴은 매우 다양하지만 기존의 legacy baseline fitting 알고리즘은 1차 곡선과 2차 곡선으로 fitting 한다. 따라서, 다양한 noise 패턴에 대응할 수 있는 알고리즘이 필요하다.

### Goal

* 기존 및 새로운 baseline fitting 알고리즘들의 성능을 평가 및 비교한다.
    * 알고리즘 1 (rule-based): 자사의 기존 legacy 알고리즘 named 'Legacy'
    * 알고리즘 2 (rule-based): 자사의 biologists들이 실험 설정값을 산출하기 위해 사용하는 알고리즘 named 'Bio'
    * 알고리즘 3 (black box): 업계 1위 타사의 black box인 알고리즘 named 'Black Box'
    * 알고리즘 4 (rule-based): 자사의 신제품에 특이적인 증폭신호에 최적화된 알고리즘 named '신제품'
    * 알고리즘 5 (rule-based): 알고리즘4 를 개선한 새로운 조건문이 추가된 rule-based 알고리즘 named '신제품 +'
    * 알고리즘 6 (data-driven): machine learning을 이용한 필자의 data-driven 알고리즘 named 'ML'
* **음성 신호에 한하여** 위양성 결과를 최소화하기 위해 baseline 차감 알고리즘을 최적화하여 여러 baseline fitting 알고리즘을 단일화한다.
* 알고리즘의 성능을 평가할 수 있는 평가 metrics을 찾거나 개발한다.

## Strategies

### Inspiration

baseline data를 살펴 보고 다양한 신호 패턴의 존재를 인식하고, 차감 결과가 white noise에 가까워야 성공적인 것으로 간주했다. 초월 함수를 다항식의 조합으로 근사할 수 있다는 점에서 착안하여, 다항식 기저 함수를 사용한 선형 회귀로 데이터를 적합하는 방법을 선택했다. 이 접근법은 데이터의 feature space를 확장하고, 데이터 내 복잡한 비선형 관계를 모델링할 수 있게 한다. 그러나 다항식의 차수가 증가함에 따라 과적합의 위험이 있으므로, 적절한 차수 선택과 정규화를 통해 신호에 적합한 모델을 선택할 수 있도록 해야 한다.

## Methodology

## Analysis Pipeline

![Analysis Pipeline](../../../images/projects/baseline_subtraction/pipeline.PNG)

## Result

### Import Packages

```{python}
# import packages
import polars as pl

# Analysis Preparation
import pandas as pd
import numpy as np

# PreProcessing
import pprint
import pyarrow as pa
import os

import subprocess
from pathlib import Path
from source.preprocess import (main_function,
                                load_and_prepare_parquet,
                                get_column_percentiles,
                                process_column_for_outliers,
                                compute_lm_slope,
                                check_memory_status,
                                get_disk_usage,
                                get_package_details)
from source.signal_filter import (detect_noise_naively,
                                    detect_noise_naively_ver2,
                                    detect_noise_naively_ywj1,
                                    detect_noise_naively_pbg,
                                    detect_noise_naively_kkm,
                                    compute_autocorrelation,
                                    test_white_noise)

# Visualization
import plotly.express as px
import matplotlib.pyplot as plt
from source.visualization import (find_sub_extremes,
                                    find_global_extremes,
                                    get_comparison_metrics,
                                    compute_bpn,
                                    plot_baseline_subtractions,
                                    plot_single_well,
                                    plot_signal_patterns)

# pandas 출력 옵션 설정
#pd.set_option('display.max_rows', None)  # 모든 행 출력
#pd.set_option('display.max_columns', None)  # 모든 열 출력
#pd.set_option('display.width', 1000)  # 셀 너비 설정
#pd.set_option('display.max_colwidth', None)  # 열 내용 전체 출력

import warnings

warnings.filterwarnings("ignore")
```

### Data Preparation

```{python}
mudt=False
outlier_naive_metric =1.65
result_data = main_function(outlier_naive_metric,mudt)
merged_data = result_data[0]
filtered_data = result_data[1]
```

```{python}
merged_data = merged_data.query("`final_ct`<0")
channels=merged_data['channel'].unique()
temperatures=merged_data['temperature'].unique()
plate_names=merged_data['name'].unique()
well_names=merged_data['well'].unique()
colors = {'Low':'blue','High':'red'}
pcrd_name = merged_data['name'].unique()[0]
channel_name = merged_data['channel'].unique()[0]
temperature_name = merged_data['temperature'].unique()[0]
```


```{python}
plot_baseline_subtractions(merged_data,pcrd_name,channel_name,temperature_name, mudt=mudt)
```

```{python}
plot_single_well(filtered_data,pcrd_name,channel_name, temperature_name, 'H12',mudt=mudt)
```
```{python}
ascending_data = merged_data.query("`channel` == @channel_name & `temperature` == @temperature_name & `final_ct` <0 & `linear_slope` >0") #`analysis_dataprocnum` == 13 & 
```

```{python}
plot_signal_patterns(ascending_data,channel_name, temperature_name, mudt = mudt)
```

```{python}
temp=ascending_data[ascending_data['original_rfu_cfx'].apply(lambda x: x[30]>1000)]
```

```{python}
pcrd_nm = temp.loc[17632]['name']
channel_nm =temp.loc[17632]['channel']
temperature_nm=temp.loc[17632]['temperature']
well_nm=temp.loc[17632]['well']
```

```{python}
plot_single_well(ascending_data,pcrd_nm,channel_nm,temperature_nm,well_nm,mudt=mudt)
```

```{python}
temp=ascending_data[ascending_data['strep_plus2_analysis_absd'].apply(lambda x: x[41]>200)]
pcrd_nm = temp.loc[18048]['name']
channel_nm =temp.loc[18048]['channel']
temperature_nm=temp.loc[18048]['temperature']
well_nm=temp.loc[18048]['well']
```
```{python}
plot_single_well(ascending_data,pcrd_nm,channel_nm,temperature_nm,well_nm,mudt=mudt)
```

```{python}
descending_data = merged_data.query("`analysis_dataprocnum` == 13 & `channel` == @channel_name & `temperature` == @temperature_name & `final_ct` <0 & `linear_slope` <0")
descending_data=descending_data.iloc[:len(descending_data) // 2]
```

```{python}
plot_signal_patterns(descending_data,channel_name, temperature_name, mudt = mudt)
```

```{python}
temp=descending_data[descending_data['strep_plus1_analysis_absd'].apply(lambda x: x[12]>200)]
pcrd_nm = temp.loc[5272]['name']
channel_nm =temp.loc[5272]['channel']
temperature_nm=temp.loc[5272]['temperature']
well_nm=temp.loc[5272]['well']
```

```{python}
plot_single_well(descending_data,pcrd_nm,channel_nm,temperature_nm,well_nm,mudt=mudt)
```

```{python}
temp=descending_data[descending_data['strep_plus2_analysis_absd'].apply(lambda x: x[31]>50)]
pcrd_nm = temp.loc[18528]['name']
channel_nm =temp.loc[18528]['channel']
temperature_nm=temp.loc[18528]['temperature']
well_nm=temp.loc[18528]['well']
```

```{python}
plot_single_well(descending_data,pcrd_nm,channel_nm,temperature_nm,well_nm,mudt=mudt)
```

```{python}
mudt=True
outlier_naive_metric =1.65
result_data = main_function(outlier_naive_metric,mudt)
merged_data = result_data[0]
filtered_data = result_data[1]

channels=filtered_data['channel'].unique()
temperatures=filtered_data['temperature'].unique()
plate_names=filtered_data['name'].unique()
well_names=filtered_data['well'].unique()
colors = {'Low':'blue','High':'red'}
pcrd_name = filtered_data['name'].unique()[0]
channel_name = filtered_data['channel'].unique()[0]
temperature_name = filtered_data['temperature'].unique()[0]
```

```{python}
plot_baseline_subtractions(filtered_data,pcrd_name,channel_name,temperature_name, mudt=mudt)
```

```{python}
temp=filtered_data[filtered_data['strep_plus2_analysis_absd'].apply(lambda x: x[20]>100)]
temp
```

```{python}
pcrd_nm = temp.loc[640]['name']
channel_nm =temp.loc[640]['channel']
temperature_nm=temp.loc[640]['temperature']
well_nm=temp.loc[640]['well']
print(pcrd_nm)
print(channel_nm)
print(temperature_nm)
print(well_nm)
```
```{python}
plot_single_well(filtered_data,pcrd_nm,channel_nm, temperature_nm, well_nm,mudt=mudt)
```
```{python}
ascending_data = merged_data.query("`analysis_dataprocnum` == 13 & `channel` == @channel_name & `temperature` == @temperature_name & `final_ct` <0 & `linear_slope` >0")
ascending_data = ascending_data.iloc[:len(ascending_data) // 2]
```
```{python}
plot_signal_patterns(ascending_data,channel_name, temperature_name, mudt = mudt)
```
```{python}
temp=ascending_data[ascending_data['strep_plus1_analysis_absd'].apply(lambda x: x[10]<-200)]
pcrd_nm = temp.loc[1528]['name']
channel_nm =temp.loc[1528]['channel']
temperature_nm=temp.loc[1528]['temperature']
well_nm=temp.loc[1528]['well']
plot_single_well(ascending_data,pcrd_nm,channel_nm, temperature_nm, well_nm,mudt=mudt)
```
```{python}
temp=ascending_data[ascending_data['strep_plus2_analysis_absd'].apply(lambda x: x[10]<-300)]
pcrd_nm = temp.loc[36200]['name']
channel_nm =temp.loc[36200]['channel']
temperature_nm=temp.loc[36200]['temperature']
well_nm=temp.loc[36200]['well']
plot_single_well(ascending_data,pcrd_nm,channel_nm, temperature_nm, well_nm,mudt=mudt)
```
```{python}
descending_data = merged_data.query("`channel` == @channel_name & `temperature` == @temperature_name & `final_ct` <0 & `linear_slope` <0")
descending_data = descending_data.iloc[:len(descending_data) // 2]
plot_signal_patterns(descending_data,channel_name, temperature_name, mudt = True)
```
```{python}
descending_data = merged_data.query("`analysis_dataprocnum` == 13 & `channel` == @channel_name & `temperature` == @temperature_name & `final_ct` <0 & `linear_slope` <0")
plot_signal_patterns(descending_data,channel_name, temperature_name, mudt = True)
```
```{python}
temp=descending_data[descending_data['strep_plus2_analysis_absd'].apply(lambda x: x[35]<-50)]
pcrd_nm = temp.loc[26112]['name']
channel_nm =temp.loc[26112]['channel']
temperature_nm=temp.loc[26112]['temperature']
well_nm=temp.loc[26112]['well']
plot_single_well(descending_data,pcrd_nm,channel_nm, temperature_nm, well_nm,mudt=mudt)
```
```{python}
merged_data['mudt_distortion_combo_key']=merged_data[['name', 'channel', 'well']].apply(lambda x: ' '.join(x), axis=1)
mudt_high_risk_list = merged_data.query("`temperature`=='High' & `final_ct`<40 & `final_ct`>0")['mudt_distortion_combo_key'].unique()
mudt_risk_data=merged_data.query("`mudt_distortion_combo_key` in @mudt_high_risk_list & `temperature`=='Low' & `final_ct`<0")
plot_signal_patterns(mudt_risk_data, channel_name, temperature_name, mudt=mudt)
```

```{python}
temp=mudt_risk_data[mudt_risk_data['strep_plus2_analysis_absd'].apply(lambda x: x[20]>1000)]
temp

```

```{python}
pcrd_nm = temp.loc[54384]['name']
channel_nm =temp.loc[54384]['channel']
temperature_nm=temp.loc[54384]['temperature']
well_nm=temp.loc[54384]['well']
plot_single_well(mudt_risk_data,pcrd_nm,channel_nm, temperature_nm, well_nm,mudt=mudt)
```
```{python}
temp=mudt_risk_data[mudt_risk_data['strep_plus1_analysis_absd'].apply(lambda x: x[20]>1000)]
pcrd_nm = temp.loc[24912]['name']
channel_nm =temp.loc[24912]['channel']
temperature_nm=temp.loc[24912]['temperature']
well_nm=temp.loc[24912]['well']
plot_single_well(mudt_risk_data,pcrd_nm,channel_nm, temperature_nm, well_nm,mudt=mudt)
```
```{python}
temp=mudt_risk_data[mudt_risk_data['strep_plus2_analysis_absd'].apply(lambda x: x[35]<-1800)]
pcrd_nm = temp.loc[54304]['name']
channel_nm =temp.loc[54304]['channel']
temperature_nm=temp.loc[54304]['temperature']
well_nm=temp.loc[54304]['well']
plot_single_well(mudt_risk_data,pcrd_nm,channel_nm, temperature_nm, well_nm,mudt=mudt)
```
```{python}

```
```{python}

```
```{python}

```

## Conclusion

## Issues

## Solutions to Issues

## Required Skills

* Statistics
* Python

## Colaborators

* 3 data scientists (I am a project manager.)
* 2 data enineers
* 2 biologists
 
## Acheivements

|Output| Description |
|:---:|:---|
|  |  |
|  |  |
|  |  |
|  |  |
|  |  |
|  |  |
|  |  |
|  |  |

## Long Term Project

* 알고리즘 Verification and Validation



</div>

<div class="tab-pane fade" id="English" role="tabpanel" aria-labelledby="English-tab">

## Background

* Due to the nature of reagent products related to medical device, there are regulations required by each country's government for the health and life safety of its citizens when entering the global market.
  * Reagent stability verification and validation required
  * Equipment stability verification and validation request
  * Software stability verification and validation request
  * **Stability verification and validation Request of Diagnostic Algorithm**
* In order to enter and manage the global market after the COVID19 special period is lifted, product safety verification and regulatory requirements required by each country's government must be met.
* In the case of the EU (European Union), IVDR (In Vitro Diagnostics Regulation) is required
* In order to enter the North American market, it is necessary to plan and write a document verifying the stability of the diagnostic algorithm based on the surveilance standards of the US FDA and Canada's Health Canada, which require the world's most stringent standards.
* As time goes by, regulations on software and algorithms are being strengthened in each country, so advanced testing that is more stringent than the existing safety verification method by software engineering is required.
* Therefore, the stability verification and validation of the diagnostic algorithm includes software engineering testing and advanced testing. Here, advanced testing means statistical testing based on statistical analysis, and building a stable software engineering system is the prerequisite.
* Since the stability verification method of Seegene's Diagnostic Signal Process (DSP) Algorithm is directly related to the business performance of the company, it is classified as a first-class security matter within the company, so specific and detailed planning and implementation details cannot be shared.

## Objective

* Design a system to statistically prove that the algorithm shows safe performance.
* Establish a Statistical Validation System to prove that the algorithm shows safe performance through statistical analysis.
  * Here, Establishment is defined as Definition, Documentation, and Implementation.
* Define the risk of the algorithm in detail and quantitatively analyze the effect of the risk on the algorithm.
* It is proved through statistical simulation that the algorithm is capable of risk management.
* In the case of code changes according to algorithm implementations and operations, a new validation report must be submitted, so an automation system is built.

## Methodology

* Refer to the guidance of SGS, a company that issues and provides training for the world's most stringent inspection certificates.
* SGS provides guidance to the FDA as a target.
* After thoroughly reading the General Principles of Software Validation document provided by the FDA for software safety verification, establish a validation system based on this document.
  * [Copy: General Principles of Software Validation](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/general-principles-software-validation)
  * [Summary: General Principles of Software Validation](../../blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.qmd)
  * [Diagram: General Principles of Software Validation](../../blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.qmd)
* Software engineering is performed based on the General Principles of Software Validation document.
* The stability verification of Diagnostic Algorithm includes both Structural Testing and Advanced Testing. Structural Testing means code-based Software Engineering Testing and Advanced Testing means Statistical Testing based on Statistical Analysis. Advanced Testing is based on the establishment of a stable Software Engineering System.
* Establish a definition and logic for algorithm safety.
* Establish metrics or indicators for algorithm safety.
* Statistical Testing, which is Advanced Testing, is a task that requires the creativity of a data scientist, and a testing model is planned to materialize and document statistical analysis design.
* Cooperation between the BT (Biotechnology) sector and the IT (Information technology) sector must be a premise, and engineering design and statistical design should be established considering the BT department's experimental design and limitation factors at a experimental level.
* Find a statistical model suitable for the planned testing model and calculate the minimum reuirement sample size.
* As per the above strategy, the BT department conducts experiments and the IT department (Data Science team) conducts analysis.
* Establish a document automation system in case of code changes in algorithm implementation and operation and the obligation to submit a new validation report for new products.

## Issues & Solutions

### Issues

* Absence of a system that can input data generated by BT departments
* Difficulties in communication due to lack of job description in BT departments.
* Absence of a system that preprocesses input data.
* Difficulties in communication within the team due to lack of job description within the Data Science team.
* It is so rare that no precedent or template for validation report can be found.

### Solutions

* Building a system that can input data generated by BT departments
  * digitalization: experimental design file, raw data generated from medical device, data extracted from medical device
* Work documented through communication with the BT department to establish the standard for the expected correct answer of the experiment results, and to establish independent and dependent variables
* Building an engineering system that preprocesses input data and merges the results of diagnostic algorithms
* Strengthen Data Quality Control Process
  * Step 1 typo correction
  * Step 2 missing value processing
  * Step 3 anomaly data processing
  * Step 4 algorithm data conformity 1st Test: Preprocessed algorithm for FDA validation vs Original algorithm
  * Step 5 algorithm data conformity 2nd test: Data Science team’s preprocessed algorithm for FDA validation vs algorithm published by BT department
* Realization of code centralization, data centralization, and documentation of specific matters by writing job descriptions within the Data Science team
* Plan and conduct statistical analysis after planning and establishing Seegene's own software testing & advanced testing model

## Required Skills

* FDA software validation knowledge
* Statistics
* Dynamic documentation
* Biology
* Clinical study design
* R, Python, Matlab
* Apache Airflow 

## Colaborators

* 5 data scientists (I am a project manager.)
* 3 data enineers
* 27 biologists
* 2 patent attorneys
 
## Acheivements

|DSP Algorithm Output| Description |
|:---:|:---|
| FDA Validation 1st Draft | the 1st draft of verification & validation report for FDA submission |
| Data Input System | It is a temporary data input system that develops into a platform that calculates a large amount of data in large quantities. |
| Documentation System | Establishment of previously absent documentation and document automation systems $\rightarrow$ Necessary for business communication and establishment of Relational Database System |
| Data Management System | data quality control system |
| FDA Validation Model |Establishment of validation model for DSP algorithm|
| Patent Invention |  Inventing the FDA Validation Model |
| In-house first Performance evaluation of algorithms and reagent products | Comprehensive performance evaluation of algorithms and reagent products that did not previously exist in-house |
| Statistical analysis related to algorithmic risk management | Risk management-related statistical analysis is performed on noise and anomaly data that may occur due to reagent and equipment-specific random effects and other confounders. |

## Long Term Project

* Collaboratively write BT's job description and establish RDB system
* Building a DevOps Platform for reagents, equipment, software and algorithm validation

</div>