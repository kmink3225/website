---
title: "Baseline Fitting Algorithm Improvement"
subtitle: "Diagnostic Algorithms Optimization"
description: "This project is to improve the baseline-fitting performance of the diagnostic signal processing algorithms."
author: Kwangmin Kim
date: 2024-07-08
format: 
  html:
    toc: true  
    #page-layout: article
    code-fold: true
    code-copy: true
    code-overflow: wrap
    number-sections: true
    number-depth: 3
    grid:
      sidebar-width: 200px
      body-width: 1150px
      margin-width: 100px
---

<ul class="nav nav-pills" id="language-tab" role="tablist">
  <li class="nav-item" role="presentation">
    <button class="nav-link active" id="Korean-tab" data-bs-toggle="tab" data-bs-target="#Korean" type="button" role="tab" aria-controls="Korean" aria-selected="true">Korean</button>
  </li>
  <li class="nav-item" role="presentation">
    <button class="nav-link" id="English-tab" data-bs-toggle="tab" data-bs-target="#English" type="button" role="tab" aria-controls="knitr" aria-selected="false">English</button>
  </li>

<div class="tab-content" id="language-tabcontent">

<div class="tab-pane fade  show active" id="Korean" role="tabpanel" aria-labelledby="Korean-tab">

## 개요

* 목적: Real-Time PCR 신호의 baseline fitting 알고리즘 최적화
* 기간: 2024.01 - 2024.07 (6 months)
* 역할: Project Manager & Data Scientist
* 참여 인원: 3명의 데이터 사이언티스트, 2명의 데이터 엔지니어, 2명의 생물학자

## 동기

* 회사에 처음 입사했을 때, legacy 진단 알고리즘의 수많은 if문과 설정값, 그리고 낮은 가독성의 코드에 압도되었다. 
* rule-based 알고리즘이 data-driven 알고리즘에 비해 성능이 떨어진다고는 할 수 없지만, 다양한 PCR 신호를 조건문으로 처리하기에는 한계가 있다고 판단했다. 
* 따라서, 점진적으로 진단 알고리즘을 data-driven 방식으로 개선할 필요가 있다고 생각했다.  

## 배경

* Real-Time PCR 검사의 정확도 향상을 위한 신호 처리 알고리즘 개선 필요성 대두
* 기존 rule-based 알고리즘의 한계 극복 및 data-driven 접근 방식 도입

### Real Time PCR의 원리

Real Time PCR 장비는 핵산(Nucleic Acid) 검체의 존재 여부를 확인하기 위해 각 사이클마다 핵산 검체를 2배씩 증폭하면서 신호를 감지한다.

* Lag Phase (지연 단계): 증폭된 검체의 신호가 일정 농도 이하일 경우, PCR 장비는 신호를 감지하지 못하고,  
  장비 내부 기본값과 시약 및 소모품의 값이 합산된 신호로 반환한다. 이는 일정 시간 동안 지연되는 구간이다.
* Log Phase (로그 단계): Exponential Phase라고도 불리며 증폭된 검체의 신호가 일정 농도 이상일 때,  
  PCR 장비에서는 신호의 변화가 나타나며, 포화 상태에 이를 때까지 신호가 2의 지수승으로 증가한다.
* Plateau Phase (플라토 단계): Stationary Phase 효소의 활성도 감소 및 반응물의 고갈 등으로 인해 포화 상태에 도달하면,   
신호는 더 이상 증가하지 않고 일정하게 유지된다.

이러한 방식으로 Real Time PCR 장비의 신호는 이론적으로 **sigmoid curve**의 패턴을 나타낸다.

#### 주요 결과 지표

* 양/음성 결과 (정성적 지표)
  * 양성: 아래 그림과 같이 Real Time PCR 결과가 명확한 sigmoid curve가 나올 경우 검체가 발견된 **양성** 상태로 판독한다.
  * 음성: 반응이 없는 평평한 curve가 나오거나 명확한 sigmoid curve가 나오지 않을 경우 검체가 발견되지 않은 **음성** 상태로 판독한다.
* Ct (정량적 지표)
  * Ct (=**Cycle threshold**)의 약자로, 특정한 양의 증폭된 DNA가 처음으로 검출 가능한 수준에 도달했을 때의 PCR 사이클 수를 의미한다
  * 초기 DNA 양: Ct 값은 초기 DNA 양과 반비례한다.
  * 증폭 효율성: Ct 값은 증폭 효율성과 관련 있다. 이상적으로는 각 사이클에서 두 배로 증폭되지만, 실제 실험에서는 다양한 요인에 의해 증폭 효율성이 100% 미만일 수 있다.
  
![Sigmoid Curve Example](../../../images/projects/baseline_subtraction/sigmoid%20curve.PNG){width=30%}

### Risk

* PCR 신호는 시험관 내에서 일어나는 미시적 화학 반응, 광학 반응 및 기계적 반응의 결과이다. 이 과정에서 인간이 명확히 측정하거나 설명하기 어려운 원인에 의한 노이즈가 섞일 수 있다. 
* 이로 인해, Ct 값의 편향과 결과적으로 위양성 또는 위음성 결과가 발생할 위험이 증가할 수 있다.

## 필요성

따라서, 이러한 노이즈를 보정하기 위한 전처리 방법이 필요하다.

* sigmoid curve fitting 전의 전처리 단계인 baseline subtraction의 정확도가 매우 중요하다.
  * Baseline 데이터에 노이즈가 많을 경우, 최적화 과정에서 과적합이나 local minima와 같은 문제로 인해 파라미터 추정의 복잡성이 증가한다. 
  * Baseline subtraction의 정확도가 떨어질 경우 위양성/위음성의 위험이 있다. 
* 사내에 표준화된 baseline fitting 알고리즘이 없어 여러 baseline fitting 알고리즘이 존재한다. 


## 고려 사항

* **제한적 알고리즘 수정**
  * 정합성의 문제로 전반적인 rule-based 알고리즘의 변경 없이 baseline fitting 알고리즘만 개선
* **이해도 고려**
  * 다양한 배경(생물학, 비전문가 임원)의 이해관계자들이 이해할 수 있는 간단한 알고리즘 설계
* **기술적 제약**
  * Python에서 C++로의 용이한 포팅을 위한 최소한의 패키지 사용
  * 저수준(low-level) 코딩 필요
* **데이터 특성 고려**
  * 적은 수의 baseline 데이터 포인트에도 적용 가능한 fitting 알고리즘 필요
  * 다양한 noise 패턴에 대응 가능한 유연한 알고리즘 개발 (기존 1차, 2차 곡선 fitting의 한계 극복)

### 이슈

* 신호 데이터 선정 기준 부재
  * 기존 방식: 1년 동안 기존 담당자가 주먹구구식으로 파일을 열어가며 주관적 느낌으로 타사 SW를 통해 특이한 신호를 육안 및 수동으로 찾음. 
  * 기존 방식 고수: 기존 방식으로 프로젝트 진행할 경우 너무 많은 소요 시간이 발생하여 프로젝트의 종결이 보이지 않음
* 데이터 파이프라인 부재
* 성능 평가 기준과 방법 부재
* 비전문가가 알고리즘 성능을 쉽게 이해할 수 있도록 직관적인 시각화가 요구됨 (기획자가 부재).

## 주요 과제 및 해결 방안

### 데이터 파이프라인 구축

* 다양한 PCR 신호 데이터 수집 및 전처리 파이프라인 구축
* 성능 평가를 위한 데이터 처리 파이프라인 구현

### 알고리즘 개발 및 최적화

* 기존 rule-based 알고리즘 분석 및 개선
* 새로운 ML 기반 data-driven 알고리즘 개발

### 성능 평가 체계 수립

* 객관적인 성능 평가 메트릭 설정
* 다양한 신호 패턴에 대한 알고리즘 성능 비교 분석

### 시각화 및 결과 해석

* 복수 신호 및 단일 신호에 대한 성능 시각화 도구 개발
* 비전문가도 이해할 수 있는 직관적인 시각화 고안

## 기술 스택

* 프로그래밍 언어: Python
* 주요 라이브러리: NumPy, Pandas
* 머신러닝/딥러닝: Characteristic Equations & Neural Networks using NumPy, Pandas (low level로 구현)
* 데이터 시각화: matplotlib

## Objective

* noise를 보정하는 대표적인 전처리 방식 중 하나인 baseline 차감 알고리즘을 최적화한다. 
  * Baseline의 차감은 baseline fitting을 수행하여 fitted value를 raw data에서 차감하여 순수 신호를 분리한다. 
* 우수한 성능을 보이는 basline fitting 알고리즘을 정하고 사내의 여러 baseline fitting 알고리즘을 단일화를 한다.

$$
\text{RFU (or PCR Signal)} =  \text{Pure Signal} + \text{Noise} + \text{Error}
$$

* Pure Signal: 시약에 의한 순수 증폭 및 미증폭 신호
* Noise: 장비, 장비의 결함, 소모품 및 시약에서 방출하는 형광신호와 같은 기대하지 않은 반응에 의한 신호
* Error: 다른 설명할 수 없는 요인으로 인하여 발생한 신호

### Goals

* Baseline 알고리즘 별 PCR 판독 결과 Data 마련을 위한 Data Engineering
* data 처리 pipeline 구축
* 분석을 위한 비교 metric 고안 및 구현
* 분석을 위한 신호 selection metric 고안 및 구현
* 분석이 가능한 복수 신호 시각화 기획 및 구현
* 분석을 위한 신호 처리 세부 분석 시각화 기획 및 구현
* 기존 및 새로운 baseline fitting 알고리즘들의 성능을 평가 및 비교
* **음성 신호에 한하여** 위양성 결과를 최소화하기 위해 baseline 차감 알고리즘을 최적화하여 여러 baseline fitting 알고리즘의 단일화

## Method

### Data Pipeline

![Data Pipeline](../../../images/projects/baseline_subtraction/pipeline.PNG){width=70%}

### 알고리즘 종류

비전공자인 생물학자, 컴퓨터 공학출신들과 임원들의 이해도를 높이기 위해 직관적으로 이해할 수 있는 시각화를 통해 각 알고리즘을 비교 분석한다.

* MuDT는 SG 고유의 BT 기술에 의해 태생적으로 거쳐야 하는 전처리 과정이다. MuDT를 하지 않았을 때와 했을때를 구별하여 시각화를 한다.

* MuDT Off 한 데이터에 대하여,
  * 1st Pannel [After BPN]: normalized **Raw Data**를 보여준다. 
  * 2nd Pannel [CFX]: (대조군1)업계 1위 타사의 알고리즘으로 기기전용 SW에 내재된 **Black Box** 알고리즘 
  * 3rd Pannel [DSP]: (대조군2)현재 Data Science팀에서 사용되고있는 공식적인 **Legacy Rule-Based** 알고리즘 
  * 4th Pannel [Auto]: (대조군3)생물 실험자들이 제품을 최적화하기 위해 현재 사용하고있는 **Bio Legacy Rule-Based** 알고리즘  
  * 5th Pannel [Strep+N]: (실험군1)팀원(물리학자)이 [DSP]를 보완하기 위해 만든 N+1 번째 **Rule-Based** 알고리즘 
  * 6th Pannel [ML]: (실험군2)본인이 **data driven ML**알고리즘 
    * Characteristic Equation ([코드 링크](https://github.com/kmink3225/website/blob/28864068ed97b7285e8873ecefde95590ea17ebf/docs/projects/baseline_subtraction/source/signal_estimation.py#L415)): Talyer Series에서 초월 함수를 다항식의 조합으로 근사할 수 있다는 점에서 착안하여,   
    다항식 기저 함수를 사용한 선형 회귀로 데이터를 적합하는 방법을 시도했다. 이 접근법은 데이터의   
    feature space를 확장하고, 데이터 내 복잡한 비선형 관계를 모델링할 수 있게 한다.   
    적절한 차수 선택과 정규화를 통해 baseline 신호에 적합시킨다.
    
      1. 로그 정규화
      
          Let $X$ be the input data.  
          $X_{normalized} = \frac{\log(X - \min(X) + \epsilon) - \min(\log(X - \min(X) + \epsilon))}{\max(\log(X - \min(X) + \epsilon)) - \min(\log(X - \min(X) + \epsilon))}$ 
      
      2. 기저 함수

          $\phi_1(x) = 1$   
          $\phi_2(x) = x$  
          $\phi_3(x) = x^2$  
          $\phi_4(x) = x^3$  
          $\phi_5(x) = e^x - 1$  
          $\phi_6(x) = e^{-x}$  
          $\phi_7(x) = \log(x + 1)$  
          $\phi_8(x) = \frac{1}{x + 1}$  
          $\phi_9(x) = \max(0, x - 0.5)^2$  

      3. 특성 방정식 (Characteristic Equation)
        
          $y(x) = \sum_{i=1}^{9} w_i \phi_i(x)$  
          where $w_i$ are the weights to be optimized.

      4. 비용 함수 (Cost Function)
        
          $J(w) = \frac{1}{2} \sum_{j=1}^{n} (y_j - \sum_{i=1}^{9} w_i \phi_i(x_j))^2$  
          where $n$ is the number of data points.

      5. 그래디언트

          $\nabla J(w) = \sum_{j=1}^{n} (\sum_{i=1}^{9} w_i \phi_i(x_j) - y_j) \phi_i(x_j)$

      6. Momentum (Conjugate Gradient Method)

          $w_{k+1} = w_k + \alpha_k d_k$  
          $d_{k+1} = -\nabla J(w_{k+1}) + \beta_k d_k$  
          $\beta_k = \frac{\|\nabla J(w_{k+1})\|^2}{\|\nabla J(w_k)\|^2}$  
          where $\alpha_k$ is determined by line search.

      7. 예측  

         $y_{pred} = \sum_{i=1}^{9} w_i^* \phi_i(x)$  
         where $w_i^*$ are the optimized weights.

      8. 역정규화

         $X_{restored} = \exp(X_{normalized} \cdot (\max(\log(X_{original})) - \min(\log(X_{original}))) + \min(\log(X_{original}))) + \min(X_{original}) - \epsilon$
          
    * Neural Network: Characteristic Equation (특성 방정식)은 기저함수를 분석가가 선정해줘야 하는 위험 부담이 있다.   
    Baseline 신호가 실험 단계에서 흔하게 보이는 패턴에 맞게 기저함수를 설정할 수 있지만 임상신호나   
    실제 시장에서 만들어지는 신호는 그 패턴이 예상과 다를 수 있기 때문에 Data에 맞게   
    적합가능한 간단한 신경망을 구현하여 적합시킨다.  
* MuDT On 한 데이터에 대하여,
  * 1st Pannel [After BPN]: normalized **Raw Data**를 보여준다.
  * 2nd Pannel [After BPN]: normalized raw data를 MuDT에 의해 **Preprocessed Data**를 보여준다.
  * 3rd Pannel [DSP]: (대조군1)현재 Data Science팀에서 사용되고있는 공식적인 **Legacy Rule-Based** 알고리즘 
  * 4th Pannel [Auto]: (대조군2)생물 실험자들이 제품을 최적화하기 위해 현재 사용하고있는 **Bio Legacy Rule-Based** 알고리즘 
  * 5th Pannel [Strep+N]: (실험군1)팀원(물리학자)이 [DSP]를 보완하기 위해 만든 N+1 번째 **Improved Rule-Based** 알고리즘 
  * 6th Pannel [ML]: (실험군2)본인이 **data driven ML**알고리즘 
    * Characteristic Equation
    * Neural Network

### 시각화에 의한 직관적 분석

baseline data를 관찰 및 기존의 경험으로 알고있는 다양한 신호 패턴에 각 알고리즘들이 어떤 성능을 보이는지 비교한다. 

* **복수 신호**에 대한 성능 평가: 알고리즘의 총체적인 성능평가를 관찰 및 분석
* **단일 신호**에 대한 성능 평가: 특이 신호에 대한 알고리즘이 어떤 성능을 보이는지 세부적으로 관찰 및 분석
* **신호 유형**별 성능 평가
  * Baseline **증가 신호**에 대한 성능 평가
  * Baseline **감소 신호**에 대한 성능 평가
  * **제품 특이적인 신호**에 대한 성능 평가: MuDT(씨젠의 제품과 BT기술)에 의한 신호처리 후 신호 패턴에 대하여 어떤 성능을 보이는지 관찰 및 분석 

### 평가 Metric

* MSE: mean squared error
* MAE: mean absolute error
* 차감 결과가 white noise에 가까워야 성공적인 fitting인 것으로 간주 

## Result

#### Import Packages

```{python}
# Analysis Preparation
import pandas as pd
import numpy as np

# PreProcessing
# from source.preprocess import (main_function,
#                                 load_and_prepare_parquet,
#                                 get_column_percentiles,
#                                 process_column_for_outliers,
#                                 compute_lm_slope,
#                                 check_memory_status,
#                                 get_disk_usage,
#                                 get_package_details)

from source.cp_preprocess import prepare_baseline_data


from source.signal_filter import (detect_noise_naively,
                                    detect_noise_naively_ver2,
                                    detect_noise_naively_ywj1,
                                    detect_noise_naively_pbg,
                                    detect_noise_naively_kkm,
                                    compute_autocorrelation,
                                    test_white_noise)

# Visualization
import plotly.express as px
import matplotlib.pyplot as plt
from source.visualization import (find_sub_extremes,
                                    find_global_extremes,
                                    get_comparison_metrics,
                                    compute_bpn,
                                    plot_baseline_subtractions,
                                    plot_single_well,
                                    plot_signal_patterns)

import warnings

warnings.filterwarnings("ignore")
```

#### Data Preparation

#test
```{python}
"""
PCR 신호 데이터 전처리 모듈

- 이 모듈은 Real-Time PCR 신호의 baseline fitting 알고리즘 최적화를 위한 
  데이터 전처리 기능을 제공
- 다양한 출처의 PCR 데이터를 로드하고, 병합하며, 이상치 탐지 및 전처리를 수행
- Data Engineer들이 C++로 porting 해야하기 때문에 pytorch, scikit-learn, 
  scipy 등의 라이브러리 사용 금지

주요 기능:
- PCR 데이터 파일 로드 및 병합
- 신호 이상치 탐지 및 처리
- 신호 특성(선형 기울기 등) 계산
- 시스템 리소스 모니터링

작성자: Kwangmin Kim
날짜: 2024-07
"""

# Analysis Preparation: 데이터 분석을 위한 기본 라이브러리 
import polars as pl
import numpy as np
import os
import pathlib
from typing import Dict, List, Tuple, Optional

# Signal Processing: 신호 처리 및 노이즈 검출을 위한 커스텀 함수들
from source.signal_filter import detect_noise_naively


# -------------------------------------------------------------------
# 데이터 로드 및 경로 관련 함수
# -------------------------------------------------------------------

def setup_data_paths(mudt: bool = True) -> Dict[str, str]:
    """
    MuDT 처리 여부에 따른 데이터 파일 경로 설정
    
    입력 파라미터에 따라 MuDT 전처리가 적용된 또는 적용되지 않은 
    데이터셋에 대한 경로를 생성합
    
    매개변수:
        mudt (bool): MuDT 전처리 적용 여부 (기본값: True)
        
    반환값:
        dict: 각 데이터셋 유형에 대한 파일 경로가 포함된 사전
            - 'raw': 원시 데이터 파일 경로
            - 'auto': Auto baseline 데이터 파일 경로
            - 'cfx': CFX 데이터 파일 경로
            - 'strep_plus1': Strep+1 데이터 파일 경로
            - 'strep_plus2': Strep+2 데이터 파일 경로
            - 'ml': ML 데이터 파일 경로
    """
    rootpath = os.getcwd()
    # 공통 경로 부분과 사용자별 경로 부분 분리
        
    if 'Administrator' in rootpath:       
        user_path = 'C:/Users/Administrator'
    else:
        user_path = 'C:/Users/kmkim'
    
    common_path = 'Desktop/projects/website/docs/data/baseline_optimization/GI-B-I'

    base_path = os.path.join(user_path, common_path)
    paths = {}
    base_suffix = 'mudt_' if mudt else 'no_mudt_'
    
    paths['raw'] = os.path.join(base_path, 'raw_data', f'{base_suffix}raw_data.parquet')
    paths['auto'] = os.path.join(base_path, 'auto_baseline_data', 
                                f'{base_suffix}auto_baseline_data.parquet')
    paths['cfx'] = os.path.join(base_path, 'cfx_data', f'{base_suffix}cfx_data.parquet')
    paths['strep_plus1'] = os.path.join(base_path, 'strep_plus1_data', 
                                        f'{base_suffix}strep_plus1_data.parquet')
    paths['strep_plus2'] = os.path.join(base_path, 'strep_plus2_data', 
                                        f'{base_suffix}strep_plus2_data.parquet')
    paths['ml'] = os.path.join(base_path, 'ml_data', f'{base_suffix}ml_data.parquet')
    
    return paths

def load_all_datasets(data_paths: Dict[str, str]) -> Dict[str, pl.DataFrame]:
    """
    여러 소스에서 데이터셋 로드
    
    각 데이터 파일 경로에서 parquet 파일을 읽어들여 Polars DataFrame으로 변환
    
    매개변수:
        data_paths (dict): setup_data_paths() 함수에서 생성된 데이터 경로 사전
        
    반환값:
        dict: 각 데이터셋 유형에 대한 Polars DataFrame이 포함된 사전
    """
    datasets = {}
    
    for dataset_type in ["raw", "cfx", "auto", "strep_plus1", "strep_plus2", "ml"]:
        try:
            datasets[dataset_type] = pl.scan_parquet(data_paths[dataset_type]).collect()
            print(f"{dataset_type} columns: {datasets[dataset_type].columns}")
        except Exception as e:
            print(f"Error loading {dataset_type} dataset: {e}")
            datasets[dataset_type] = pl.DataFrame()
    
    return datasets


def load_and_prepare_parquet(
    i_file_path: str, 
    i_selected_columns: Optional[List[str]] = None, 
    i_combo_key_columns: Optional[List[str]] = None, 
    i_rename_columns: Optional[Dict[str, str]] = None
) -> pl.DataFrame:
    """
    parquet 파일을 로드하고 데이터프레임 준비
    
    파일 경로에서 parquet 파일을 로드하고 선택적으로 열 선택, 
    조합 키 생성, 열 이름 변경 등의 작업을 수행
    
    매개변수:
        i_file_path: parquet 파일이 있는 파일 경로
        i_selected_columns: 데이터 분석에 사용할 열 이름 목록
        i_combo_key_columns: 조합 키 생성에 사용할 열 이름 목록
        i_rename_columns: 열 이름 변경을 위한 매핑 딕셔너리
    
    반환값:
        DataFrame: 처리된 데이터프레임
    """
    o_df = pl.scan_parquet(i_file_path).collect()
    
    if i_selected_columns:
        o_df = o_df.select(i_selected_columns)
    
    if i_combo_key_columns:
        o_df = o_df.with_columns(
            pl.concat_str(
                [pl.col(col).cast(pl.Utf8) for col in i_combo_key_columns],
                separator=" "
            ).alias("combo_key")
        )
    
    if i_rename_columns:
        for old_name, new_name in i_rename_columns.items():
            o_df = o_df.rename({old_name: new_name})
    
    return o_df


# -------------------------------------------------------------------
# 데이터 처리 및 계산 관련 함수
# -------------------------------------------------------------------


def get_column_percentiles(i_data: pl.DataFrame, i_column_name: str) -> pl.Series:
    """
    데이터프레임의 특정 열에 대한 백분위 순위 계산
    
    데이터프레임의 지정된 열에 대해 각 값의 백분위 순위를 계산
    
    매개변수:
        i_data: 지표 점수가 포함된 데이터프레임
        i_column_name: 백분위 순위를 계산할 지표 열의 이름
    
    반환값:
        Series: 지정된 열의 점수에 대한 백분위 순위를 포함하는 시리즈
    """
    def get_percentile(i_sorted_data: np.ndarray, i_value: float) -> float:
        """
        정렬된 점수들에 대한 특정 지표 점수의 백분위 순위를 계산
        
        매개변수:
            i_sorted_data: 정렬된 점수들의 numpy 배열
            i_value: 백분위 순위를 계산할 점수
        
        반환값:
            float: 해당 점수의 백분위 순위
        """
        if not i_sorted_data.size:  # Check if sorted data is empty
            return np.nan  # Return NaN for empty data
        count = np.sum(i_sorted_data <= i_value)
        o_percent = 100.0 * count / len(i_sorted_data)
        return np.round(o_percent, 1)
    
    # Convert to pandas for easy manipulation
    pandas_df = i_data.to_pandas()
    non_empty_values = pandas_df[i_column_name].apply(
        lambda x: x if x else np.nan
    ).dropna()
    i_sorted_data = np.sort(non_empty_values.values)
    o_percentiles = non_empty_values.apply(
        lambda x: get_percentile(i_sorted_data, x)
    )
    
    # Convert back to polars
    return pl.Series(o_percentiles.values)


def process_column_for_outliers(
    i_data: pl.DataFrame, 
    i_column: str, 
    i_groupby_columns: List[str], 
    i_function: callable
) -> pl.DataFrame:
    """
    이상치 탐지 및 관련 지표 계산을 위한 열 처리
    
    지정된 함수를 사용하여 이상치를 탐지하고, 이상치 지표를 계산하여
    그룹별 백분위수와 임계값을 계산합니다.
    
    매개변수:
        i_data: 처리할 데이터프레임
        i_column: 이상치 처리를 위한 열 이름
        i_groupby_columns: 백분위 계산을 위한 그룹화 열 이름 목록
        i_function: 이상치 탐지에 사용할 함수
    
    반환값:
        DataFrame: 이상치 관련 열이 추가된 업데이트된 데이터프레임
    """
    # Convert to pandas for processing since some operations are easier there
    pandas_df = i_data.to_pandas()
    
    # Detect outliers and calculate outlierness metric
    pandas_df[f'outlier_naive_residuals_{i_column}'] = pandas_df[i_column].apply(
        lambda cell: i_function(cell)[1]
    )
    pandas_df[f'outlier_naive_metric_{i_column}'] = pandas_df[i_column].apply(
        lambda cell: i_function(cell)[0]
    )
    
    # Calculate 95th percentile cutoff for outlierness metric
    metric_col = f'outlier_naive_metric_{i_column}'
    pandas_df[f'outlier_naive_cutoff_{i_column}'] = pandas_df.groupby(
        i_groupby_columns
    )[metric_col].transform(lambda x: np.percentile(x, 95))
    
    # Calculate metric percentiles within each group
    for name, group in pandas_df.groupby(i_groupby_columns):
        percentiles = get_column_percentiles(
            pl.DataFrame(group), f'outlier_naive_metric_{i_column}'
        )
        percentile_col = f'outlier_naive_metric_percentile_{i_column}'
        pandas_df.loc[group.index, percentile_col] = percentiles.to_numpy()
    
    # Convert back to polars
    return pl.DataFrame(pandas_df)


def compute_lm_slope(i_signal: np.ndarray) -> List[float]:
    """
    신호 데이터의 선형 기울기와 절편 계산
    
    PCR 신호 데이터의 선형 추세를 파악하기 위해 
    공분산 방법으로 기울기와 절편을 계산
    
    매개변수:
        i_signal: 기울기를 계산할 신호 데이터 배열
    
    반환값:
        list: [slope, intercept] 형태의 기울기와 절편 값
    """
    cycle_length = np.size(i_signal)
    cycle = np.arange(1, cycle_length + 1)
    i_signal_mean = np.mean(i_signal)
    cycle_mean = np.mean(cycle)
    
    # Compute covariance matrix
    cov_matrix = np.cov(i_signal, cycle)
    slope = cov_matrix[0, 1] / cov_matrix[0, 0] if cov_matrix[0, 0] != 0 else 0
    intercept = i_signal_mean - slope * cycle_mean
    
    return [slope, intercept]


# -------------------------------------------------------------------
# 시스템 리소스 모니터링 관련 함수
# -------------------------------------------------------------------


def check_memory_status() -> None:
    """
    현재 시스템의 메모리 사용 상태 확인 및 출력
    
    실행 중인 프로세스의 메모리 사용량과 전체 시스템 메모리 상태를 MB 단위로 확인
    
    반환값:
        None: 메모리 정보를 콘솔에 출력만 하고 반환값은 없음
    """
    import psutil  # memory checking
    memory = psutil.virtual_memory()
    total_memory = memory.total / (1024 ** 2)  # 메가바이트 단위로 변환
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    memory_usage = mem_info.rss / (1024 ** 2)  # rss는 실제 메모리 사용량을 나타냄
    available_memory = memory.available / (1024 ** 2)  # 메가바이트 단위로 변환
    
    print(f"Memory used: {memory_usage:.2f} MB")
    print(f"Total memory: {total_memory:.2f} MB")
    print(f"Available memory: {available_memory:.2f} MB")


def get_disk_usage(path: str = "/") -> None:
    """
    디스크 저장 공간 사용 현황 확인 및 출력
    
    지정된 경로의, 또는 기본 경로("/")의 디스크 용량과 
    사용량, 여유 공간을 GB 단위로 출력
    
    매개변수:
        path: 디스크 사용량을 확인할 경로 (기본값: "/")
    
    반환값:
        None: 디스크 사용 정보를 콘솔에 출력만 하고 반환값은 없음
    """
    import shutil
    usage = shutil.disk_usage(path)
    print(f"Total Disk Capacity: {usage.total / (1024**3):.2f} GB")
    print(f"Used Disk Space: {usage.used / (1024**3):.2f} GB")
    print(f"Free Disk Space: {usage.free / (1024**3):.2f} GB")


def get_package_details() -> Dict[str, str]:
    """
    설치된 Python 패키지 목록과 버전 정보 수집
    
    현재 Python 환경에 설치된 모든 패키지의 이름과 
    버전 정보를 사전 형태로 반환
    
    반환값:
        dict: {패키지명: 버전} 형태의 사전
    """
    import subprocess
    import sys
    
    result = subprocess.run(
        [sys.executable, '-m', 'pip', 'list'], 
        stdout=subprocess.PIPE, 
        text=True
    )
    lines = result.stdout.split('\n')
    
    package_dict = {}
    for line in lines[2:]:  # 첫 두 줄은 헤더 정보이므로 제외
        parts = line.split()
        if len(parts) >= 2:
            package_name = parts[0]
            version = parts[1]
            package_dict[package_name] = version
    return package_dict


# -------------------------------------------------------------------
# 데이터 병합 및 전처리 함수
# -------------------------------------------------------------------


def merge_datasets(datasets: Dict[str, pl.DataFrame]) -> pl.DataFrame:
    """
    여러 데이터셋을 병합하여 하나의 통합 데이터프레임 생성
    
    각 알고리즘의 결과 데이터셋을 'combo_key' 기준으로 병합하여
    베이스라인 fitting 알고리즘 간 비교 분석이 가능한 형태로 만듦
    
    매개변수:
        datasets: 각 알고리즘별 데이터프레임이 포함된 딕셔너리
            - 'raw': 원시 신호 데이터
            - 'cfx': CFX 알고리즘 처리 결과
            - 'auto': Auto 알고리즘 처리 결과
            - 'strep_plus1': Strep+1 알고리즘 처리 결과
            - 'strep_plus2': Strep+2 알고리즘 처리 결과
            - 'ml': ML 알고리즘 처리 결과
    
    반환값:
        DataFrame: 모든 알고리즘의 결과가 병합된 통합 데이터프레임
    """
    # Extract relevant columns from each dataset
    cfx_df = datasets['cfx'].select(['original_rfu_cfx', 'combo_key'])
    
    auto_df = datasets['auto']
    auto_cols = auto_df.columns[6:]
    auto_df = auto_df.select(auto_cols)
    
    strep_plus1_df = datasets['strep_plus1']
    strep_plus1_cols = strep_plus1_df.columns[6:]
    strep_plus1_df = strep_plus1_df.select(strep_plus1_cols)
    
    strep_plus2_df = datasets['strep_plus2']
    strep_plus2_cols = strep_plus2_df.columns[6:]
    strep_plus2_df = strep_plus2_df.select(strep_plus2_cols)
    
    ml_df = datasets['ml'].select([
        'ml_baseline_fit', 'ml_analysis_absd', 'combo_key'
    ])
    
    # Merge all datasets
    merged_data = (ml_df
        .join(datasets['raw'], on='combo_key')
        .join(auto_df, on='combo_key')
        .join(strep_plus1_df, on='combo_key')
        .join(strep_plus2_df, on='combo_key')
        .join(cfx_df, on='combo_key'))
    
    return merged_data


def preprocess_merged_data(
    merged_data: pl.DataFrame, 
    outlier_naive_metric: float, 
    mudt: bool = True
) -> pl.DataFrame:
    """
    병합된 데이터에 전처리 작업 적용
    
    병합된 데이터에 이상치 탐지 및 선형 기울기 계산 등의 전처리 작업을 수행
    
    매개변수:
        merged_data: 병합된 통합 데이터프레임
        outlier_naive_metric: 이상치 탐지를 위한 임계값
        mudt: MuDT 전처리 적용 여부 (기본값: True)
            True일 경우 'preproc_rfu', False일 경우 'original_rfu' 열 사용
    
    반환값:
        DataFrame: 전처리가 적용된 데이터프레임
    """
    # 이상치 탐지
    columns_to_process = ['original_rfu']
    groupby_columns = ['channel', 'temperature']
    
    for column in columns_to_process:
        merged_data = process_column_for_outliers(
            merged_data, column, groupby_columns, detect_noise_naively
        )
    
    # 선형 기울기 계산
    source_column = 'preproc_rfu' if mudt else 'original_rfu'
    
    # Convert to pandas for this operation
    pandas_df = merged_data.to_pandas()
    pandas_df['linear_slope'] = pandas_df[source_column].apply(
        lambda x: compute_lm_slope(x)[0]
    )
    merged_data = pl.DataFrame(pandas_df)
    
    return merged_data


def filter_data_for_analysis(
    merged_data: pl.DataFrame, 
    outlier_naive_metric: float
) -> pl.DataFrame:
    """
    분석용 데이터 필터링
    
    특정 플레이트, 채널, 온도 조건 및 음성 결과('final_ct < 0')와
    이상치 기준('outlier_naive_metric > threshold')을 만족하는 데이터만 필터링
    
    매개변수:
        merged_data: 병합 및 전처리된 데이터프레임
        outlier_naive_metric: 이상치 필터링 임계값
    
    반환값:
        DataFrame: 분석 조건에 맞게 필터링된 데이터프레임
    """
    # 필터링 기준 설정
    filtered_data = merged_data.filter(
        (pl.col('final_ct') < 0) &
        (pl.col(f'outlier_naive_metric_original_rfu') > outlier_naive_metric)
    )
    
    return filtered_data


# -------------------------------------------------------------------
# 통합 함수 및 메인 함수
# -------------------------------------------------------------------


def prepare_baseline_data(
    outlier_naive_metric: float = 1.65, 
    mudt: bool = True
) -> Tuple[pl.DataFrame, pl.DataFrame]:
    """
    베이스라인 최적화 분석을 위한 데이터 준비
    
    여러 알고리즘 결과 데이터를 로드, 병합, 전처리하여 
    baseline fitting 알고리즘 비교 분석을 위한 데이터셋을 구성
    
    매개변수:
        outlier_naive_metric: 이상치 탐지를 위한 임계값 (기본값: 1.65)
        mudt: MuDT 전처리 적용 여부 (기본값: True)
    
    반환값:
        tuple: (merged_data, filtered_data)
            - merged_data: 모든 알고리즘 결과가 병합된 전체 데이터셋
            - filtered_data: 분석 조건에 맞게 필터링된 데이터셋
    """
    # 데이터 경로 설정
    data_paths = setup_data_paths(mudt)
    
    # 모든 데이터셋 로드
    datasets = load_all_datasets(data_paths)
    
    # 데이터셋 병합
    merged_data = merge_datasets(datasets)
    
    # 데이터 전처리
    merged_data = preprocess_merged_data(merged_data, outlier_naive_metric, mudt)
    
    # 분석용 데이터 필터링
    filtered_data = filter_data_for_analysis(merged_data, outlier_naive_metric)
    
    return merged_data, filtered_data
 
```

```{python}
mudt=False
outlier_naive_metric =1.2
result_data = prepare_baseline_data(outlier_naive_metric,mudt)
merged_data = result_data[0]
filtered_data = result_data[1]


```

* [main_function() 코드 링크](https://github.com/kmink3225/website/blob/28864068ed97b7285e8873ecefde95590ea17ebf/docs/projects/baseline_subtraction/source/preprocess.py#L40)

```{python}
negative_data = merged_data.query("`final_ct`<0")
channels=merged_data['channel'].unique()
temperatures=merged_data['temperature'].unique()
plate_names=merged_data['name'].unique()
well_names=merged_data['well'].unique()
colors = {'Low':'blue','High':'red'}
pcrd_name = merged_data['name'].unique()[0]
channel_name = merged_data['channel'].unique()[0]
temperature_name = merged_data['temperature'].unique()[0]
```


### MuDT 전처리가 없는 경우

#### 복수 신호에 대한 성능 평가

```{python}
temp = merged_data
temp['has_non_zero'] = merged_data['analysis_rd_diff'].apply(lambda x: np.any(x != 0))
non_zero_rows = merged_data[merged_data['has_non_zero']]
pcrd_name = non_zero_rows['name'].unique()[0]
channel_name = non_zero_rows['channel'].unique()[0]
temperature_name = non_zero_rows['temperature'].unique()[0]
well_name = non_zero_rows['well'].unique()[3]

plot_baseline_subtractions(merged_data,pcrd_name,channel_name,temperature_name, mudt=mudt)
```

* [plot_baseline_subtractions() code link](https://github.com/kmink3225/website/blob/28864068ed97b7285e8873ecefde95590ea17ebf/docs/projects/baseline_subtraction/source/visualization.py#L186)

복수 신호에 대하여 알고리즘의 전반적인 fitting 및 차감 성능을 확인한다.

* 1st Pannel [After BPN]: Raw Data를 Normalizing하여 신호를 RFU value 6636 근처로 모은 결과 신호의 양 끝단에서 variance가 높은 경향을 가진것을 볼 수 있다.
* 2nd Pannel [CFX]: (대조군1) **Black Box** 알고리즘이 신호의 앞쪽 data points를 버리고 선형적으로 fitting하여 차감한 것으로 추측된다. 
* 3rd Pannel [DSP]: (대조군2) Data Science팀의 **Legacy Rule-Based** 알고리즘의 결과로 음성으로 판단된 신호에 조건적으로 차감결과를 산출하지 않는다. 
* 4th Pannel [Auto]: (대조군3)생물 실험자들의 **Bio Legacy Rule-Based** 알고리즘의 결과로 전반적으로 [CFX]와 비슷한 신호 차감 성능을 보인다.
* 5th Pannel [Strep+2]: (실험군1)팀원(물리학자)이 [DSP]를 3번 보완한 **Improved Rule-Based** 알고리즘의 결과로 조건에 따라 특정 그룹은 선형적으로 fitting하여 차감하고 다른 그룹엔 평균을 차감하고 신호를 평균값 0으로 평행이동 시킨것을 볼 수 있다. 
* 6th Pannel [ML]: (실험군2)본인의 **data driven ML**알고리즘의 결과로 조건문 없이 신호 자체에 정확한 fitting을 하여 차감한 결과 white noise에 가까운 패턴을 보인다.

#### 단일 신호에 대한 성능 평가

```{python}
temp2 = merged_data.query('`name` == @pcrd_name')
plot_single_well(temp2,pcrd_name,channel_name, temperature_name, well_name,mudt=mudt)
```

* [plot_single_well()](https://github.com/kmink3225/website/blob/28864068ed97b7285e8873ecefde95590ea17ebf/docs/projects/baseline_subtraction/source/visualization.py#L304)

단일 신호에 대하여 알고리즘별로 신호를 fitting, 차감 및 보정 처리하는 성능을 한눈에 확인한다.

* Plot의 첫 번째 행은 보정 처리 및 fitting 결과 (Red)를 한눈에 확인가능하다
* Plot의 두 번째 행은 보정 처리 (Blue)와 fitting 결과 (Red)를 분리하여 확인한다. 다시 말해서, 파란색 선에 대한 fitting 결과 (Red) 를 확인할 수 있다.
* Plot의 세 번째 행은 차감 결과를 확인할 수 있고 white noise에 가까울 수록 보정과 fitting의 성능이 우수한 것으로 평가한다.

성능 비교

* 1st Pannel [DSP]: (대조군2) Data Science팀의 **Legacy Rule-Based** 알고리즘의 조건상 아래로 jumping 신호를 위로 보정하여 신호의 증폭 유무를 판별한다. 음성신호로 판독되어 차감결과를 0으로 반환한다.
* 2nd Pannel [Auto]: (대조군3) (대조군3)생물 실험자들의 **Bio Legacy Rule-Based** 알고리즘은 jumping 신호를 위로 보정한 후 1차 회귀곡선으로 fitting하여 차감한 결과를 관찰할 수 있다.  
* 3rd Pannel [CFX]: **Black Box** 알고리즘은 어떠한 보정없이 1차 회귀곡선으로 차감한 것을 관찰할 수 있다.
* 4th Pannel [Strep+2]: (실험군1) **Improved Rule-Based** 알고리즘은 특정 조건하에 신호를 fitting하지 않고 평균값으로 평행이동한 것을 관찰할 수 있다.
* 5th Pannel [ML]: (실험군2) **data driven ML**알고리즘은 신호의 jumping을 보정한 결과를 사용하지 않고 raw data에 직접 fitting하여 차감한 것을 관찰할 수있다.


## Conclusion

본 프로젝트는 분자진단 시스템의 신호 분석 알고리즘 개선을 목표로 수행되었으며, 다음과 같은 주요 성과와 향후 과제를 도출하였다.

### 기존 문제점 해결

#### 문제 인식

  * 시약 성능 및 환경적 요인으로 인한 'gray zone' 이상 신호에 대한 판독 문제
  * 위음성/위양성 패턴에 대한 대응 부족
  * Ct값 산출의 불안정성

#### 접근 방식

  * Rule-based 알고리즘 개선: 추가 조건 반영으로 이상신호 관리 강화
  * Machine Learning 기반 data-driven 알고리즘 도입

#### 결과

  * 타사 **black box** 알고리즘 성능 확인
  * 개선된 rule-based 알고리즘 [strep+2]의 성능 향상 확인
  * Machine Learning 기반 알고리즘 [ML]의 탁월한 성능 입증
  * [ML]에서 Neural Network는 연산의 복잡도로 인하여 연산시간이 많아 실험군에서 제외

### 알고리즘 성능 평가

#### 평가 제한 사항

* 기존 legacy 알고리즘의 Matlab에서 C++로의 Refactoring 작업으로 인한 정량적 측정의 한계

#### 향후 과제

* System level에서의 전체 알고리즘 성능 비교를 위한 대규모 데이터 기반 정량적 평가 필요

### 시각화 도구 개선 및 표준화

#### 기존 문제

* 다양한 알고리즘의 비표준화된 사용으로 인한 시약 개발 프로세스의 비일관성

#### 개선 사항

* 알고리즘별 성능 시각화 도구 제공
* 알고리즘 단일화에 대한 설득력 향상 및 개발 프로세스 표준화 촉진


### 향후 발전 방향

#### 단기 과제

* 신규 개발 알고리즘에 대한 정밀 검증 및 추가 연구

#### 장기 과제

* 알고리즘의 Verification and Validation 프로세스 확립

## Achievements

* 알고리즘 성능 비교 분석 시각화 도구 개발
* Data-driven 방식의 baseline fitting 알고리즘 구현
* Rule-based 및 Machine Learning 기반 알고리즘의 성능 개선을 시각화로 입증 

## Future Work

* 대규모 데이터셋을 활용한 알고리즘 성능의 정량적 평가 실시
* Data governance 프로젝트와 연계한 알고리즘 고도화
* 알고리즘의 Verification and Validation 프로세스 수립 및 실행
* 개선된 알고리즘의 실제 진단 환경 적용 및 성능 모니터링

## Supplement

### 증가 신호에 대한 성능 평가

이 Sampled Data엔 증가신호가 없음

```{python}
#| eval: false

# ascending_data = merged_data.query("`channel` == @channel_name & `temperature` == @temperature_name & `final_ct` <0 & `linear_slope` >0") #


# plot_signal_patterns(ascending_data,channel_name, temperature_name, mudt = mudt)


# temp=ascending_data[ascending_data['original_rfu_cfx'].apply(lambda x: x[30]>1000)]


# pcrd_nm = temp.loc[17632]['name']
# channel_nm =temp.loc[17632]['channel']
# temperature_nm=temp.loc[17632]['temperature']
# well_nm=temp.loc[17632]['well']

# #### 단일 신호에 대한 성능 평가


# plot_single_well(ascending_data,pcrd_nm,channel_nm,temperature_nm,well_nm,mudt=mudt)

# temp=ascending_data[ascending_data['strep_plus2_analysis_absd'].apply(lambda x: x[41]>200)]
# pcrd_nm = temp.loc[18048]['name']
# channel_nm =temp.loc[18048]['channel']
# temperature_nm=temp.loc[18048]['temperature']
# well_nm=temp.loc[18048]['well']

# plot_single_well(ascending_data,pcrd_nm,channel_nm,temperature_nm,well_nm,mudt=mudt)
```

### 감소 신호에 대한 성능 평가

```{python}
descending_data = merged_data.query("`analysis_dataprocnum` == 13 & `channel` == @channel_name & `temperature` == @temperature_name & `final_ct` <0 & `linear_slope` <0")
descending_data=descending_data.iloc[:len(descending_data) // 2]
```

```{python}
plot_signal_patterns(descending_data,channel_name, temperature_name, mudt = mudt)
```

#### 단일 신호에 대한 성능 평가

```{python}
temp = descending_data[descending_data['strep_plus2_analysis_absd'].apply(lambda x: x[42]<(-30))]
pcrd_nm = temp.tail(1)['name'].iloc[0]
channel_nm = temp.tail(1)['channel'].iloc[0]
temperature_nm = temp.tail(1)['temperature'].iloc[0]
well_nm = temp.tail(1)['well'].iloc[0]

plot_single_well(descending_data, pcrd_nm, channel_nm, temperature_nm, well_nm, mudt=mudt)
```

### 제품 특이적인 신호처리에 대한 성능 평가

```{python}
mudt=True
outlier_naive_metric =1.1
result_data = main_function(outlier_naive_metric,mudt)
merged_data = result_data[0]
filtered_data = result_data[1]

channels=filtered_data['channel'].unique()
temperatures=filtered_data['temperature'].unique()
plate_names=filtered_data['name'].unique()
well_names=filtered_data['well'].unique()
colors = {'Low':'blue','High':'red'}
pcrd_name = filtered_data['name'].unique()[0]
channel_name = filtered_data['channel'].unique()[0]
temperature_name = filtered_data['temperature'].unique()[0]
```

#### 복수 신호에 대한 성능 평가
```{python}
plot_baseline_subtractions(filtered_data,pcrd_name,channel_name,temperature_name, mudt=mudt)
```

##### 단일 신호에 대한 성능 평가

```{python}
temp=filtered_data[filtered_data['strep_plus2_analysis_absd'].apply(lambda x: x[40]>100)]
```

```{python}
pcrd_nm = temp.tail(1).iloc[0]['name']
channel_nm = temp.tail(1).iloc[0]['channel']
temperature_nm= temp.tail(1).iloc[0]['temperature']
well_nm= temp.tail(1).iloc[0]['well']

plot_single_well(filtered_data,pcrd_nm,channel_nm, temperature_nm, well_nm,mudt=mudt)
```

#### 증가 신호에 대한 성능 평가 

```{python}
ascending_data = merged_data.query("`analysis_dataprocnum` == 13 & `channel` == @channel_name & `temperature` == @temperature_name & `final_ct` <0 & `linear_slope` >0")
plot_signal_patterns(ascending_data,channel_name, temperature_name, mudt = mudt)
```

##### 단일 신호에 대한 성능 평가

```{python}
temp=ascending_data[ascending_data['strep_plus2_analysis_absd'].apply(lambda x: x[40]>200)]
pcrd_name = temp.tail(1).iloc[0]['name']
channel_name =temp.tail(1).iloc[0]['channel']
temperature_name=temp.tail(1).iloc[0]['temperature']
well_name=temp.tail(1).iloc[0]['well']
plot_single_well(ascending_data,pcrd_name,channel_name, temperature_name, well_name,mudt=mudt)
```

#### 감소 신호에 대한 성능 평가

```{python}
descending_data = merged_data.query("`channel` == @channel_name & `temperature` == @temperature_name & `linear_slope`<0 & `final_ct` <0")

plot_signal_patterns(descending_data,channel_name, temperature_name, mudt = True)
```

##### 단일 신호에 대한 성능 평가

```{python}
temp = descending_data[descending_data['strep_plus2_analysis_absd'].apply(lambda x: x[30]<(-20))]
pcrd_name = temp.tail(1)['name'].iloc[0]
channel_name = temp.tail(1)['channel'].iloc[0]
temperature_name = temp.tail(1)['temperature'].iloc[0]
well_name = temp.tail(1)['well'].iloc[0]

plot_single_well(descending_data, pcrd_name, channel_name, temperature_name, well_name, mudt=mudt)

```

#### 제품 고유 Risk 신호

```{python}
merged_data['mudt_distortion_combo_key']=merged_data[['name', 'channel', 'well']].apply(lambda x: ' '.join(x), axis=1)
mudt_high_risk_list = merged_data.query("`temperature`=='High' & `final_ct`<40 & `final_ct`>0")['mudt_distortion_combo_key'].unique()
mudt_risk_data=merged_data.query("`mudt_distortion_combo_key` in @mudt_high_risk_list & `temperature`=='Low' & `final_ct`<0")

pcrd_name = mudt_risk_data.tail(1)['name'].iloc[0]
channel_name = mudt_risk_data.tail(1)['channel'].iloc[0]
temperature_name = mudt_risk_data.tail(1)['temperature'].iloc[0]

plot_signal_patterns(mudt_risk_data, channel_name, temperature_name, mudt=mudt)
```

```{python}
temp=mudt_risk_data[mudt_risk_data['strep_plus2_analysis_absd'].apply(lambda x: x[40]< -200)]
pcrd_name = temp.tail(1)['name'].iloc[0]
channel_name = temp.tail(1)['channel'].iloc[0]
temperature_name = temp.tail(1)['temperature'].iloc[0]
well_name = temp.tail(1)['well'].iloc[0]

plot_single_well(mudt_risk_data,pcrd_name,channel_name, temperature_name, well_name,mudt=mudt)
```


</div>

<div class="tab-pane fade" id="English" role="tabpanel" aria-labelledby="English-tab">

## Background

* Due to the nature of reagent products related to medical device, there are regulations required by each country's government for the health and life safety of its citizens when entering the global market.
  * Reagent stability verification and validation required
  * Equipment stability verification and validation request
  * Software stability verification and validation request
  * **Stability verification and validation Request of Diagnostic Algorithm**
* In order to enter and manage the global market after the COVID19 special period is lifted, product safety verification and regulatory requirements required by each country's government must be met.
* In the case of the EU (European Union), IVDR (In Vitro Diagnostics Regulation) is required
* In order to enter the North American market, it is necessary to plan and write a document verifying the stability of the diagnostic algorithm based on the surveilance standards of the US FDA and Canada's Health Canada, which require the world's most stringent standards.
* As time goes by, regulations on software and algorithms are being strengthened in each country, so advanced testing that is more stringent than the existing safety verification method by software engineering is required.
* Therefore, the stability verification and validation of the diagnostic algorithm includes software engineering testing and advanced testing. Here, advanced testing means statistical testing based on statistical analysis, and building a stable software engineering system is the prerequisite.
* Since the stability verification method of Seegene's Diagnostic Signal Process (DSP) Algorithm is directly related to the business performance of the company, it is classified as a first-class security matter within the company, so specific and detailed planning and implementation details cannot be shared.

## Objective

* Design a system to statistically prove that the algorithm shows safe performance.
* Establish a Statistical Validation System to prove that the algorithm shows safe performance through statistical analysis.
  * Here, Establishment is defined as Definition, Documentation, and Implementation.
* Define the risk of the algorithm in detail and quantitatively analyze the effect of the risk on the algorithm.
* It is proved through statistical simulation that the algorithm is capable of risk management.
* In the case of code changes according to algorithm implementations and operations, a new validation report must be submitted, so an automation system is built.

## Methodology

* Refer to the guidance of SGS, a company that issues and provides training for the world's most stringent inspection certificates.
* SGS provides guidance to the FDA as a target.
* After thoroughly reading the General Principles of Software Validation document provided by the FDA for software safety verification, establish a validation system based on this document.
  * [Copy: General Principles of Software Validation](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/general-principles-software-validation)
  * [Summary: General Principles of Software Validation](../../blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.qmd)
  * [Diagram: General Principles of Software Validation](../../blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.qmd)
* Software engineering is performed based on the General Principles of Software Validation document.
* The stability verification of Diagnostic Algorithm includes both Structural Testing and Advanced Testing. Structural Testing means code-based Software Engineering Testing and Advanced Testing means Statistical Testing based on Statistical Analysis. Advanced Testing is based on the establishment of a stable Software Engineering System.
* Establish a definition and logic for algorithm safety.
* Establish metrics or indicators for algorithm safety.
* Statistical Testing, which is Advanced Testing, is a task that requires the creativity of a data scientist, and a testing model is planned to materialize and document statistical analysis design.
* Cooperation between the BT (Biotechnology) sector and the IT (Information technology) sector must be a premise, and engineering design and statistical design should be established considering the BT department's experimental design and limitation factors at a experimental level.
* Find a statistical model suitable for the planned testing model and calculate the minimum reuirement sample size.
* As per the above strategy, the BT department conducts experiments and the IT department (Data Science team) conducts analysis.
* Establish a document automation system in case of code changes in algorithm implementation and operation and the obligation to submit a new validation report for new products.

## Issues & Solutions

### Issues

* Absence of a system that can input data generated by BT departments
* Difficulties in communication due to lack of job description in BT departments.
* Absence of a system that preprocesses input data.
* Difficulties in communication within the team due to lack of job description within the Data Science team.
* It is so rare that no precedent or template for validation report can be found.

### Solutions

* Building a system that can input data generated by BT departments
  * digitalization: experimental design file, raw data generated from medical device, data extracted from medical device
* Work documented through communication with the BT department to establish the standard for the expected correct answer of the experiment results, and to establish independent and dependent variables
* Building an engineering system that preprocesses input data and merges the results of diagnostic algorithms
* Strengthen Data Quality Control Process
  * Step 1 typo correction
  * Step 2 missing value processing
  * Step 3 anomaly data processing
  * Step 4 algorithm data conformity 1st Test: Preprocessed algorithm for FDA validation vs Original algorithm
  * Step 5 algorithm data conformity 2nd test: Data Science team's preprocessed algorithm for FDA validation vs algorithm published by BT department
* Realization of code centralization, data centralization, and documentation of specific matters by writing job descriptions within the Data Science team
* Plan and conduct statistical analysis after planning and establishing Seegene's own software testing & advanced testing model

## Required Skills

* FDA software validation knowledge
* Statistics
* Dynamic documentation
* Biology
* Clinical study design
* R, Python, Matlab
* Apache Airflow 

## Colaborators

* 5 data scientists (I am a project manager.)
* 3 data enineers
* 27 biologists
* 2 patent attorneys
 
## Acheivements

|DSP Algorithm Output| Description |
|:---:|:---|
| FDA Validation 1st Draft | the 1st draft of verification & validation report for FDA submission |
| Data Input System | It is a temporary data input system that develops into a platform that calculates a large amount of data in large quantities. |
| Documentation System | Establishment of previously absent documentation and document automation systems $\rightarrow$ Necessary for business communication and establishment of Relational Database System |
| Data Management System | data quality control system |
| FDA Validation Model |Establishment of validation model for DSP algorithm|
| Patent Invention |  Inventing the FDA Validation Model |
| In-house first Performance evaluation of algorithms and reagent products | Comprehensive performance evaluation of algorithms and reagent products that did not previously exist in-house |
| Statistical analysis related to algorithmic risk management | Risk management-related statistical analysis is performed on noise and anomaly data that may occur due to reagent and equipment-specific random effects and other confounders. |

## Long Term Project

* Collaboratively write BT's job description and establish RDB system
* Building a DevOps Platform for reagents, equipment, software and algorithm validation

</div>