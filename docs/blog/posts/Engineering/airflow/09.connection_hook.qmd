---
title: "Template Variabler"
subtitle: DAG Creation, Bash Operator, Task Performance Subject, 
description: |
  template
categories:
  - Engineering
author: Kwangmin Kim
date: 05/01/2023
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
comments: 
  utterances: 
    repo: ./docs/comments
draft: False
---

<ul class="nav nav-pills" id="language-tab" role="tablist">
  <li class="nav-item" role="presentation">
    <button class="nav-link active" id="Korean-tab" data-bs-toggle="tab" data-bs-target="#Korean" type="button" role="tab" aria-controls="Korean" aria-selected="true">Korean</button>
  </li>
  <li class="nav-item" role="presentation">
    <button class="nav-link" id="English-tab" data-bs-toggle="tab" data-bs-target="#English" type="button" role="tab" aria-controls="knitr" aria-selected="false">English</button>
  </li>

<div class="tab-content" id="language-tabcontent">

<div class="tab-pane fade  show active" id="Korean" role="tabpanel" aria-labelledby="Korean-tab">

# Goal

* posgres DB를 container로 띄우기
* airflow의 connection & hook 설정

# Docker Compose Interpretation

* 목적
    * 1 개 이상 도커 컨테이너 생성시 스크립트 하나로 컨테이너들의 설정을 관리할 수 있도록 해주는 Docker 의 기능 중 하나
    * 각 각의 container 설정 관리 뿐만 아니라 containers간의 연관관계 및 dependency까지 설정할 수 있다.
    * 일부 containers를 같은 network에서 띄워지는지도 설정할 수 있는 network 설정도 할 수 있다.
* 작성방법
    * 모든 설정은 docker compose.yaml 파일에 컨테이너들의 설정 내용을 입력
* 도커컴포즈 서비스 시작
    * yaml 파일이 있는 위치에서 sudo docker compose up 명령 입력하면 docker compose.yaml에 있는 모든 설정이 적용된다.
    * 기본적으로 Docker 서비스가 설치되어 있어야 함
* docker compose.yaml 파일의 구성
    * yaml 파일은 json 이나 xml 과 같이 **key, value 로 구성되며 계층적인 구조**를 가지며 파이썬처럼 들여쓰기 문법을 사용함
    * 다시 말해서, json 이나 xml은 파이썬의 dictionary 같이 nested {key:value} structure로 작성할 수 있다.
* docker compose.yaml 파일의 1 Level 내용

```markdown
version: '3.8' # yaml 파일의버전 정보 옵션
x-airflow-common: # 'x-': Extention Fields(각 서비스 항목에 또는 container에  공통 적용될 항목들 정의)
services: # 컨테이너로 실행할 서비스 정의로 가장 신경써서 적어야할 부분
volumes:  # 컨테이너에 할당할 volume 정의
networks: # 컨테이너에 연결할 network 정의
```
위의 내용에서 key값은  version, x-airflow-common, services, volumes, networks

* x-airflow-common: 공통 지정할 항목을 를 붙여서 지정

```markdown
x-airflow-common
    &airflow-common # 공통 지정 parameters 정의: image, environment, depends_on
    image: ${AIRFLOW_IMAGE:-apache/airflow:2.5.2}
    environment
        &airflow-common-env
        AIRFLOW__CORE__DEFAULT_TIMEZONE: 'Asia/Seoul'
        AIRFLOW__CORE__EXECUTOR: CeleryExecutor
        AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    depends_on # containers 실행 순서를 결정
        &airflow-common-depends-on # 공통 지정 parameters 정의: redis, postgres
        redis:
            condition: service_healthy
        postgres:
            condition: service_healthy
```
* 공통 지정 항목1: &airflow-common
    * 변수 또는 parameter: image, environment, depends_on
* 공통 지정 항목2: &airflow-common-env
    * 변수 또는 parameter: AIRFLOW__CORE__DEFAULT_TIMEZONE, AIRFLOW__CORE__EXECUTOR, AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
* 공통 지정 항목3: &airflow-common-depends-on
    * 변수 또는 parameter: redis, postgres

* services: 컨테이너로 올릴 서비스 지정
    ```markdown
    services:
       postgres:
         image: postgres:13
         environment:
           POSTGRES_USER: airflow
           POSTGRES_PASSWORD: airflow
           POSTGRES_DB: airflow
         volumes:
           - postgres-db-volume:/var/lib/postgresql/data
         healthcheck:
           test: ["CMD", "pg_isready", "-U", "airflow"]
           interval: 10s
           retries: 5
           start_period: 5s
        ports
            - 5432:5432
         restart: always
    ```
    * `image: postgres:13` 은 image는 postgre:13 버전의 image를 쓴다는 것이고 이 image가 local에 있으면 그대로 쓰고 없으면 인터넷에서 download됨. 
    * `environment:` 는 postgres OS에 설정할 환경 변수들
    * `volumes:` container와 연결할 local file system 경로를 의미
        * `postgres-db-volume:/var/lib/postgresql/data` `:`을 기준으로 왼쪽이 local file system의 경로 오른쪽이 연결할 container의 directory. 이 과정을 **mount** 시켰다라고 말함
        * `postgres-db-volume` 문서 제일 하단에 미리 만들어져 있음
        * container가 실행되었다가 (띄어졌다가) 꺼지면 (내려지면) 안에 있는 data들이 모두 사라지기 때문에 mount시키는 것이 필요함. 특히, DB container는 mount가 잘 됐는지 확인해야함
        * `postgres-db-volume:/var/lib/postgresql/data`는 postgresql의 data가 저장되는 directory를 local file system으로 연결시켜 놓은 것  
    * `healthcheck:`  container가 상태 꺼졌는지 켜졌는지 확인
    * `ports`: container에 접속하기 위해 공개할 port를 명시
        * `5432:5432` `:` 을 기준으로 왼쪽이 local에서 web에 접속할 port 번호고 오른쪽이 service가 갖고 있는 port번호. 다시 말해서, wsl 시스템안에 여러 컨테이너들이 있고 그 중 postgres 이미지가 깔려 있다면 postgres는 5432 port를 가지고 있는 상태이다. postgres 이미지에 접근하려면 wsl의 port를 통해서 접근해야하는데 그 wsl의 port가 5432로 지정된 것을 의미한다.
    * `restart: always` container가 죽으면 언제 새로 띄워주겠냐는 것이고 always니까 항상 새로 띄워줌 

    ```markdown
     redis:
       image: redis:latest
       expose:
         - 6379
       healthcheck:
         test: ["CMD", "redis-cli", "ping"]
         interval: 10s
         timeout: 30s
         retries: 50
         start_period: 30s
       restart: always

    ```
    * `expose: 6379` 이것 역시 port번호인데 외부와 연결될 때 사용되는 게 아니라 내부 다른 containers와 연결시 사용되는 port번호로 expose parameter로 공개 설정한다.

    ```markdown
    services
      airflow-webserver:
        <<: *airflow-common # 공통 지정 parameters 호출: image, environment, depends_on
        command: webserver # container를 띄울 때 실행할 명령어
        ports:
          - "8080:8080"
        healthcheck:
          test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
          interval: 30s
          timeout: 10s
          retries: 5
          start_period: 30s
        restart: always
        depends_on: # 2번째 depends_on 선언으로 x-airflow-common의 depends_on 의 내용은 무시된다.
          <<: *airflow-common-depends-on # 공통 지정 parameters 호출: redis, postgres
          airflow-init:
            condition: service_completed_successfully
    ```
    * services: 1-level (x-airflow-common 같은 level)
        * airflow-webserver, airflow-scheduler, redis, postgres 등이 같은 level의 서비스 항목으로 열거 된다.
    * depends_on: containers를 띄우는 순서를 설정하는 부분으로 위의 예시는
        * redis, postgres, airflow-init을 띄우고 나서 airflow-webserver를 띄우겠다는 것.
        * [redis, postgres, airflow-init]>>airflow-webserver

    ```markdown

    airflow-scheduler: # 1-level
        <<: *airflow-common
        command: scheduler
        healthcheck:
          test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
          interval: 30s
          timeout: 10s
          retries: 5
          start_period: 30s
        restart: always
        depends_on:
          <<: *airflow-common-depends-on
          airflow-init:
            condition: service_completed_successfully
    ```
* volumes: 컨테이너와 연결하기 위한 볼륨 (공간) 정보
    ```markdown
    volumns:
        postgree-db-volume: #새로 만들 볼륨 이름

    ```
    * 볼륨에 대한 정보 확인하기
        1.볼륨 리스트 보기 : sudo docker volume ls (현재 만들어진 volumes 리스트와 volumne_id가 보임)
        2.볼륨 상세 보기 : sudo docker volume inspect {volume_id}
* networks: 컨테이너의 network 정보 구성
    ```markdown
    networks:
        network_custom: # 새로 만들 네트워크 이름
            driver: bridge
            ipam:
                driver: default
                config:
                    - subnet : 172.18.0.0/16 # 네트워크 IP의 주소값이 2^16 개, host가 가질 IP의 주소값은 2^16-2 만큼을 할당할 수 있다.
                      gateway: 172.18.0.1
    ```
    * config:
        - subnet : 172.18.0.0/16
        - gateway: 172.18.0.1

    주어진 서브넷은 172.18.0.0/16으로 주어졌는데 이 경우, 16비트의 네트워크 부분이 주소에 할당되어 있다. 네트워크 부분은 호스트를 식별하는데 사용되는 부분이 아닌 네트워크를 식별하는데 사용되는 부분이다. 이러한 설정에서는 네트워크 IP의 주소값이 2^16 (또는 65536) 개로 할당된다. 호스트는 네트워크에서 실제로 사용되는 장치 또는 시스템을 의미한다. 이 설정에서는 게이트웨이 주소를 172.18.0.1로 지정하여 해당 IP 주소를 게이트웨이로 사용한다. 따라서 호스트가 할당받을 수 있는 IP 주소값의 개수는 네트워크 IP 주소값에서 게이트웨이 주소를 제외한 개수이다. 즉, 2^16-2 (또는 65534) 개의 주소를 호스트에 할당할 수 있다.
    * 네트워크에 대한 정보 확인하기
        1. 네트워크 리스트 보기 : sudo docker network ls
        2. 네트워크 상세 보기 : sudo docker network inspect {network_id}

# Postgres 컨테이너 올리기

## Postgress 컨테이너 추가하기

* postgres_custom 이라는 이름의 컨테이너 서비스 추가하기

## 컨테이너 고정 IP 할당하기

* 기본적으로 컨테이너들은 유동 IP를 지니며 (재기동시 IP 변경 가능)
* 고정 IP를 할당하려면 networks를 만들어 할당해야 함.
* networks 를 지정하지 않은 컨테이너들(airflow를 설치하면서 기본적으로 설치되는 containers)은 default network에 묶이게 됨
* 따라서 동일 네트워크에 두고 싶은 컨테이너들은 모두 동일 netwworks 할당 필요
  ```markdown
  services:
    postgres_custom:
      image: postgres:13
      environment:
        POSTGRES_USER: kmkim
        POSTGRES_PASSWORD: kmkim
        POSTGRES_DB: kmkim
        TZ: Asia/Seoul
      volumes:
        - postgres-custom-db-volume:/var/lib/postgresql/data
      ports:
        - 5432:5432
      networks:
        network_custom: # 밑에서 정의한 network_custom 을 쓰겠다는 의미
          ipv4_address: 172.28.0.3 # 할당된 IP
  
  networks:
    network_custom: 
      driver: bridge
      ipam:
          driver: default
          config:
              - subnet: 172.28.0.0/16 # 네트 워크 ID 주소 밑에 16개/host id 주소 밑에 16개를 할당하겠다는 의미
                gateway: 172.28.0.1 # default 네트워크 (172.18.0.0)가 쓰고 있지 않은 서브넷으로 구성
  volumes:
    postgres-db-volume:
    postgres-custom-db-volume:
  ```
* Postgres_custom 컨테이너 뿐만 아니라 다른 컨테이너에도 network_custom 할당하고 IP 부여
  ```markdown
  postgres_custom:  # 172.28.0.3
  postgres:  # 172.28.0.3 +  포트 노출 설정:(5431:5432)
  redis: # 172.28.0.5
  airflow-webserver: # 172.28.0.6
  airflow-scheduler: # 172.28.0.7
  airflow-worker: # 172.28.0.8
  airflow-triggerer: # 172.28.0.9
  airflow-init: # 172.28.0.10
  ```
    * postgres:  # airflow가 기본 메타DB로 쓰고 있는 postgress 컨테이너에 포트 번호 5431로 노출

## DB 접속하기

* Dbeaver: 설치 https://dbeaver.io/

# Postgres에 데이터 insert

```markdown
from airflow import DAG
import pendulum
from airflow.operators.python import PythonOperator

with DAG(
    dag_id='dags_python_with_postgres',
    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),
    schedule=None,
    catchup=False
) as dag:

    
    def insrt_postgres(ip, port, dbname, user, passwd, **kwargs):
        import psycopg2
        from contextlib import closing

        with closing(psycopg2.connect(host=ip, dbname=dbname, user=user, password=passwd, port=int(port))) as conn:
            with closing(conn.cursor()) as cursor:
                dag_id = kwargs.get('ti').dag_id
                task_id = kwargs.get('ti').task_id
                run_id = kwargs.get('ti').run_id
                msg = 'insrt 수행'
                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);'
                cursor.execute(sql,(dag_id,task_id,run_id,msg))
                conn.commit()

    insrt_postgres = PythonOperator(
        task_id='insrt_postgres',
        python_callable=insrt_postgres,
        op_args=['172.28.0.3', '5432', 'hjkim', 'hjkim', 'hjkim']
    )
        
    insrt_postgres
```

## 문제점 및 해결방법
* 문제점
  * 접속정보 노출: postgres DB에 대한 User, Password 등
  * 접속정보 변경시 대응 어려움
    * 만약 직접 접속하는 DAG이 수백개라면?
* 해결 방법
  * Variable 이용 (User, Password 등을 Variable에 등록하고 꺼내오기)
  * Hook 이용 (Variable 등록 필요없음)

## Connection과 Hook

* Connection
  * Airflow UI 화면에서 등록한 커넥션 정보
* Hook의 개념
  * Airflow에서 외부 솔루션의 기능을 사용할 수 있도록 미리 구현된 메서드를 가진 클래스 
* Hook의 특징
  * Connection 정보를 통해 생성되는 객체
    * 접속정보를 Connection을 통해 받아오므로 접속정보가 코드상 노출되지 않음
  * 특정 솔루션을 다룰 수 있는 메서드가 구현되어 있음.
  * 오퍼레이터나 센서와는 달리 Hook은 task를 만들어내지 못하므로 Custom 오퍼레이터 안에서나 Python 오퍼레이터 내 함수에서 사용됨

## Connection 등록

| Connection_id   | conn-db-postgres-custom |
|-----------------|:------------------------|
| Connection_type | postgres                |
| Host            | 172.28.0.3              |
| Schema          | kmkim                   |
| Login           | kmkim                   |
| Password        | kmkim                   |
| Port            | 5432                    |

## Postgres Hook 명세 보기

* [airflow web service >> Providers >> apache.airflow.providers.postgres >> Python API >> airflow.providers.postgres.hooks.postgres >> get_conn() >> [source] >> def get_conn(self)](https://airflow.apache.org/docs/apache-airflow-providers-postgres/5.4.0/_modules/airflow/providers/postgres/hooks/postgres.html#PostgresHook.get_conn)

```markdown
def get_conn(self) -> connection:
        """Establishes a connection to a postgres database."""
        conn_id = getattr(self, self.conn_name_attr)
        conn = deepcopy(self.connection or self.get_connection(conn_id))

        # check for authentication via AWS IAM
        if conn.extra_dejson.get("iam", False):
            conn.login, conn.password, conn.port = self.get_iam_token(conn)

        conn_args = dict(
            host=conn.host,
            user=conn.login,
            password=conn.password,
            dbname=self.database or conn.schema,
            port=conn.port,
        )
        raw_cursor = conn.extra_dejson.get("cursor", False)
        if raw_cursor:
            conn_args["cursor_factory"] = self._get_cursor(raw_cursor)

        for arg_name, arg_val in conn.extra_dejson.items():
            if arg_name not in [
                "iam",
                "redshift",
                "cursor",
                "cluster-identifier",
                "aws_conn_id",
            ]:
                conn_args[arg_name] = arg_val

        self.conn = psycopg2.connect(**conn_args)
        return self.conn


```

## Hook 이용하여 Postgres Insert

:::: {.columns}

::: {.column width="50%"}

```markdown
from airflow import DAG
import pendulum
from airflow.operators.python import PythonOperator

with DAG(
    dag_id='dags_python_with_postgres',
    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),
    schedule=None,
    catchup=False
) as dag:

    
    def insrt_postgres(ip, port, dbname, user, passwd, **kwargs):
        import psycopg2
        from contextlib import closing

        with closing(psycopg2.connect(host=ip, dbname=dbname, user=user, password=passwd, port=int(port))) as conn:
            with closing(conn.cursor()) as cursor:
                dag_id = kwargs.get('ti').dag_id
                task_id = kwargs.get('ti').task_id
                run_id = kwargs.get('ti').run_id
                msg = 'insrt 수행'
                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);'
                cursor.execute(sql,(dag_id,task_id,run_id,msg))
                conn.commit()

    insrt_postgres = PythonOperator(
        task_id='insrt_postgres',
        python_callable=insrt_postgres,
        op_args=['172.28.0.3', '5432', 'hjkim', 'hjkim', 'hjkim']
    )
        
    insrt_postgres
```

:::

::: {.column width="50%"}

```markdown
from airflow import DAG
import pendulum
from airflow.operators.python import PythonOperator

with DAG(
        dag_id='dags_python_with_postgres_hook',
        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),
        schedule=None,
        catchup=False
) as dag:
    def insrt_postgres(postgres_conn_id, **kwargs):
        from airflow.providers.postgres.hooks.postgres import PostgresHook
        from contextlib import closing
        
        postgres_hook = PostgresHook(postgres_conn_id)
        with closing(postgres_hook.get_conn()) as conn:
            with closing(conn.cursor()) as cursor:
                dag_id = kwargs.get('ti').dag_id
                task_id = kwargs.get('ti').task_id
                run_id = kwargs.get('ti').run_id
                msg = 'hook insrt 수행'
                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);'
                cursor.execute(sql, (dag_id, task_id, run_id, msg))
                conn.commit()

    insrt_postgres_with_hook = PythonOperator(
        task_id='insrt_postgres_with_hook',
        python_callable=insrt_postgres,
        op_kwargs={'postgres_conn_id':'conn-db-postgres-custom'}
    )
    insrt_postgres_with_hook

```
:::

::::

# Postgres Hook 으로 bulk_load 하기

## Postgres Hook 명세 보기

* [airflow web service >> Providers >> apache.airflow.providers.postgres >> Python API >> airflow.providers.postgres.hooks.postgres >> get_conn() >> [source] >> def bulg_load(self, temp_file)[source],def bulk_dump(self, table: str, tmp_file: str),def copy_expert(self, sql: str, filename: str)](https://airflow.apache.org/docs/apache-airflow-providers-postgres/5.4.0/_modules/airflow/providers/postgres/hooks/postgres.html#PostgresHook.get_conn)

```markdown
[docs]    def bulk_load(self, table: str, tmp_file: str) -> None:
        """Loads a tab-delimited file into a database table"""
        self.copy_expert(f"COPY {table} FROM STDIN", tmp_file)

[docs]    def bulk_dump(self, table: str, tmp_file: str) -> None:
        """Dumps a database table into a tab-delimited file"""
        self.copy_expert(f"COPY {table} TO STDOUT", tmp_file)

[docs]    def copy_expert(self, sql: str, filename: str) -> None:
        """
        Executes SQL using psycopg2 copy_expert method.
        Necessary to execute COPY command without access to a superuser.

        Note: if this method is called with a "COPY FROM" statement and
        the specified input file does not exist, it creates an empty
        file and no data is loaded, but the operation succeeds.
        So if users want to be aware when the input file does not exist,
        they have to check its existence by themselves.
        """
        self.log.info("Running copy expert: %s, filename: %s", sql, filename)
        if not os.path.isfile(filename):
            with open(filename, "w"):
                pass

        with open(filename, "r+") as file:
            with closing(self.get_conn()) as conn:
                with closing(conn.cursor()) as cur:
                    cur.copy_expert(sql, file)
                    file.truncate(file.tell())
                    conn.commit()
```

```markdown
from airflow import DAG
import pendulum
from airflow.operators.python import PythonOperator
from hooks.custom_postgres_hook import CustomPostgresHook

with DAG(
        dag_id='dags_python_with_custom_hook_bulk_load',
        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),
        schedule='0 7 * * *',
        catchup=False
) as dag:
    def insrt_postgres(postgres_conn_id, tbl_nm, file_nm, **kwargs):
        custom_postgres_hook = CustomPostgresHook(postgres_conn_id=postgres_conn_id)
        custom_postgres_hook.bulk_load(table_name=tbl_nm, file_name=file_nm, delimiter=',', is_header=True, is_replace=True)

    insrt_postgres = PythonOperator(
        task_id='insrt_postgres',
        python_callable=insrt_postgres,
        op_kwargs={'postgres_conn_id': 'conn-db-postgres-custom',
                   'tbl_nm':'TbCorona19CountStatus_bulk2',
                   'file_nm':'/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone("Asia/Seoul") | ds_nodash}}/TbCorona19CountStatus.csv'}
    )
```

## Bulk_load 문제점 & 개선방안

* 문제점
  * Load 가능한 Delimiter는 Tab으로 고정되어 있음
  * Header까지 포함해서 업로드됨
  * 특수문자로 인해 파싱이 안될 경우 에러 발생
* 개선방안
  * Custom Hook 을 만들어서 Delimiter 유형을 입력받게 하고
  * Header 여부를 선택하게끔 하며
  * 특수문자를 제거하는 로직을 추가 후
  * sqlalchemy를 이용하여 Load 한다면? 그리고 테이블을 생성하면서 업로드할 수 있다면?

# Custom Hook 으로 bulk_load 하기

## BaseHook 명세 보기

* [Airflow Docs](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/hooks/base/index.html#airflow.hooks.base.BaseHook.get_connection_form_widgets)

## Custom Hook 개발

* 해야할 일
  * get_conn 메서드 구현하기
    * DB 와의 연결 세션 객체인 conn 을 리턴하도록 구현
      * (Airflow 에서 등록한 Connection 정보를 담은 conn 이 아님)
      * BaseHook 의 추상 메서드 , 자식 클래스에서 구현 필요
  * bulk_load 메서드 구현하기
    * 입맛대로 만들기

## Providers 더 보기

* 현재 설치되어 있는 Providers 패키지 확인
  * 웹의 Admin Providers 에서 확인 가능
* [설치 가능한 Providers 더 보기](https://airflow.apache.org/docs/apache-airflow-providers/)

## Connection Type 추가하기

* 만약 HDFS, Hive 에 대한 커넥션을 추가하고 싶다면
* 커넥션 타입 목록에 있는 대상은 이미 패키지 설치가 된 Providers 이며 Admin>>Providers 목록에서 설치된 대상 확인 가능
* Provider 검색 , py 라이브러리 설치 목록 확인
* [Airflow 이미지 Extend 방법 확인](https://airflow.apache.org/docs/docker-stack/build.html)
  * 이미지 Extend vs Custom 이미지 생성

  | Comparison          | image extend    | Custom Image Creation   |
  |---------------------|-----------------|:------------------------|
  | 간단히 생성 가능     | O               | X                       |
  | 빌드 시간            | 짧음            | 상대적으로 긺            |
  | 크기 최적화된 이미지 |  X              | O (약 20% 정도 사이즈 감소) |
  | 폐쇄망에서 구성 가능 |  X              | O                       |
  
* Dockerfile 만들기  

:::: {.columns}

::: {.column width="50%"}

```markdown
#> cd; cd airflow
#> mkdir -p custom_image/airflow
#> cd custom_image/airflow
#> vi Dockerfile
```

:::

::: {.column width="50%"}

```markdown

FROM apache/airflow:2.5.1
USER root
RUN apt get update \
  && apt-get install -y --no-install-recommends \
    gcc \
    heimdal-dev \ 
    g++ \
    libsasl2-dev \
  && apt-get autoremove -yqq -purge \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*
USER airflow
RUN pip install \
  apache airflow providers apache hdfs \
  apache airflow providers apache hive
RUN pip uninstall y argparse
```
:::

::::


</div>

<div class="tab-pane fade" id="English" role="tabpanel" aria-labelledby="English-tab">




</div>


# Go to Blog Content List

[Blog Content List](../../content_list.qmd)