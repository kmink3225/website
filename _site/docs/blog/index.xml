<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Kwangmin Kim</title>
<link>kmink3225.netlify.app/docs/blog/index.html</link>
<atom:link href="kmink3225.netlify.app/docs/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>blog</description>
<generator>quarto-1.2.335</generator>
<lastBuildDate>Tue, 31 Dec 2999 15:00:00 GMT</lastBuildDate>
<item>
  <title>Blog Content List</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/content_list.html</link>
  <description><![CDATA[ 



<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Scalars are denoted with a lower-case letter (ex a ) or a non-bolded lower-case Greek letter (ex <img src="https://latex.codecogs.com/png.latex?%5Calpha"> ).</li>
<li>Vectors are denoted using a bold-faced lower-case letter (ex <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20a">).</li>
<li>Matrices are denoted using a bold-faced upper-case letter (ex <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20%5Cphi">) or a bold-faced upper-case Greek letter (ex <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20%5CPhi">).</li>
<li>Tensors are denoted using a bold-faced upper-case letter with multiple subscripts or superscripts, indicating the number of indices and the dimensions of the tensor along each axis.
<ul>
<li>A second-order tensor (also known as a matrix) <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> with dimensions <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m"> can be represented as: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A_%7Bij%7D"> where <img src="https://latex.codecogs.com/png.latex?i%20=%201,%5Cdots,m"> and <img src="https://latex.codecogs.com/png.latex?j%20=%201,%5Cdots,n">, which are the indices that run over the rows and columns of the matrix, respectively.</li>
<li>A third-order tensor <img src="https://latex.codecogs.com/png.latex?T"> with dimensions <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m%20%5Ctimes%20p"> can be represented as: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A_%7Bijk%7D"> where <img src="https://latex.codecogs.com/png.latex?i%20=%201,%5Cdots,m">, <img src="https://latex.codecogs.com/png.latex?j%20=%201,%5Cdots,n">, which are <img src="https://latex.codecogs.com/png.latex?i">, and <img src="https://latex.codecogs.com/png.latex?k%20=%201,%5Cdots,p"> <img src="https://latex.codecogs.com/png.latex?j">, and <img src="https://latex.codecogs.com/png.latex?k">, which are the indices that run over the three dimensions of the tensor.</li>
</ul></li>
</ul>
</div>
</div>
<section id="contents" class="level1">
<h1>Contents</h1>
<ul>
<li><a href="../../../docs/blog/posts/DL/guide_map/index.html">Deep Learning</a></li>
<li><a href="../../../docs/blog/posts/ML/guide_map/index.html">Machine Learning</a></li>
<li><a href="../../../docs/blog/posts/Mathmatics/guide_map/index.html">Mathematics</a></li>
<li><a href="../../../docs/blog/posts/statistics/guide_map/index.html">Statistics</a></li>
<li><a href="../../../docs/blog/posts/Engineering/guide_map/index.html">Engineering</a></li>
<li><a href="../../../docs/blog/posts/Patent/guide_map/index.html">Patent</a></li>
<li><a href="../../../docs/blog/posts/Language/guide_map/index.html">Language</a></li>
<li><a href="../../../docs/blog/posts/Surveilance/guide_map/index.html">Surveilance</a></li>
</ul>
</section>
<section id="reference" class="level1">
<h1>Reference</h1>
<ul>
<li>Statistics
<ul>
<li>George Casella &amp; Rogeer L. Berger - Statistcal Inference, 2nd Edition</li>
<li>Dobson and Barnett (2008) An Introduction to Generalized Linear Model. 3rd Ed. Chapman &amp; Hall.</li>
<li>Fitzmaurice, Laird and Ware (2011) Applied Longitudinal Analysis. 2nd Ed. Wiley.</li>
<li>Hosmer, Lemeshow and May (2008) Applied Survival Analysis. 2nd Ed. Wiley.</li>
<li>슬기로운 통계생활 - https://www.youtube.com/<span class="citation" data-cites="statisticsplaybook">@statisticsplaybook</span></li>
<li>슬기로운 통계생활 - https://github.com/statisticsplaybook</li>
<li>Fast Campus, Coursera, Inflearn</li>
<li>그 외 다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Mathematics
<ul>
<li>James Stewart - Calculus Early Transcedentals, 7th Eidition &amp; any James Stewart series</li>
<li>GILBERT STRANG - Introduction to Linear Algebra, 4th Edition.</li>
<li>임장환 - 머신러닝, 인공지능, 컴퓨터 비전 전공자를 위한 최적화 이론</li>
<li>Fast Campus, Coursera, Inflearn</li>
<li><a href="https://www.youtube.com/playlist?list=PLaqQvlCBe8vIkIEb4GX2ZZ1A4tFYeXR5W">8일간의 선형대수학 기초(이상준 경희대 교수)</a></li>
<li><a href="https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/">Linear Algebra(Prof.&nbsp;Gilbert Strang, MIT Open Courseware)</a></li>
<li><a href="https://github.com/fastai/numerical-linear-algebra/blob/master/README.md">Computational Linear Algebra for Coders</a></li>
<li><a href="http://immersivemath.com/ila/">Immersive linear Algebra</a></li>
<li><a href="https://www.3blue1brown.com/topics/linear-algebra">3blue1brown</a></li>
<li>그 외 다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Machine Learning
<ul>
<li>Gareth M. James, Daniela Witten, Trevor Hastie, Robert Tibshirani - An Introduction to Statistical Learning: With Applications in R 2nd Edition</li>
<li>Trevor Hastie, Robert Tibshirani, Jerome H. Friedman - The Elements of Statistical Learning 2nd Edition</li>
<li>Fast Campus, Coursera, Inflearn</li>
<li>그 외 다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Deep Learning
<ul>
<li>Saito Koki - Deep Learning from Scratch 1,2,3 (밑바닥부터 시작하는 딥러닝 1,2,3)</li>
<li>조준우 - 머신러닝·딥러닝에 필요한 기초 수학 with 파이썬</li>
<li>조준우 - https://github.com/metamath1/noviceml</li>
<li>동빈나 - https://www.youtube.com/c/dongbinna</li>
<li>혁펜하임 - https://www.youtube.com/channel/UCcbPAIfCa4q0x7x8yFXmBag</li>
<li>Fast Campus, Coursera, Inflearn</li>
<li>다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Engineering
<ul>
<li>Fast Campus, Coursera, Inflearn</li>
<li>그 외 다수의 Youtube, and Documents from Googling</li>
</ul></li>
</ul>


</section>

 ]]></description>
  <category>All List</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/content_list.html</guid>
  <pubDate>Tue, 31 Dec 2999 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Statistics</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/statistics/guide_map/index.html</link>
  <description><![CDATA[ 



<p><strong>(Draft)</strong></p>
<section id="contents-list" class="level1">
<h1>Contents List</h1>
<section id="basic" class="level2">
<h2 class="anchored" data-anchor-id="basic">Basic</h2>
<section id="probability-theory" class="level3">
<h3 class="anchored" data-anchor-id="probability-theory">Probability Theory</h3>
<ul>
<li>2023-02-05, Set Theory</li>
<li>2023-02-05, [Basics of Probability Theory - Axiomatic Foundations]</li>
<li>2023-02-05, [Basics of Probability Theory - Calculus of Probabilities]</li>
<li>2023-02-05, <a href="../../../../../docs/blog/posts/statistics/2023-02-05_probability/index.html">Basics of Probability Theory - Probability</a></li>
<li>2023-02-05, <a href="../../../../../docs/blog/posts/statistics/2023-02-05_conditional_probability/index.html">Conditional Probability</a></li>
<li>2023-02-05, [Independence]</li>
<li>2023-02-05, <a href="../../../../../docs/blog/posts/statistics/2023-02-05_bayes_rule/index.html">Bayes’ Rule</a></li>
<li>2023-02-05, Random Variable</li>
<li>1111-11-11, Probability Distribution</li>
</ul>
</section>
<section id="transformations-and-expectations" class="level3">
<h3 class="anchored" data-anchor-id="transformations-and-expectations">Transformations and Expectations</h3>
<ul>
<li>2023-02-21, <a href="../../../../../docs/blog/posts/statistics/2023-02-21_transformation/index.html">Transformation of Random Variables</a></li>
<li>1111-11-11, Expected Value vs Realizaed Value</li>
<li>1111-11-11, Variance</li>
<li>1111-11-11, Covariance and Correlation</li>
<li>2023-02-28, <a href="../../../../../docs/blog/posts/statistics/2023-02-28_mgf/index.html">Moment Generating Function, MGF</a></li>
</ul>
</section>
<section id="exponential-family-distributions" class="level3">
<h3 class="anchored" data-anchor-id="exponential-family-distributions">Exponential Family Distributions</h3>
<ul>
<li>Discrete Random Variable
<ul>
<li>2023-02-27,<a href="../../../../../docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_bernoulli.html">Bernoulli Distribution</a></li>
<li>2023-02-28,<a href="../../../../../docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_binomial.html">Binomial Distribution</a></li>
<li>2023-03-01,<a href="../../../../../docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_poisson.html">Poisson Distribution</a></li>
<li>2023-03-01,<a href="../../../../../docs/blog/posts/statistics/2023-02-27_exponential_family/discrete_geometric.html">Geometric Distribution</a></li>
<li>1111-11-11, Hypergeometric Distribution</li>
</ul></li>
<li>Continuous Random Variable
<ul>
<li>1111-11-11, Normal Distribution</li>
<li>1111-11-11, Exponential Distribution</li>
<li>1111-11-11, Beta Distribution</li>
<li>1111-11-11, Chi-squared Distribution</li>
</ul></li>
<li>1111-11-11,</li>
</ul>
</section>
<section id="multiple-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="multiple-random-variables">Multiple Random Variables</h3>
<ul>
<li>1111-11-11, Joint Distribution and Marginal Distribution</li>
</ul>
</section>
<section id="point-estimation" class="level3">
<h3 class="anchored" data-anchor-id="point-estimation">Point Estimation</h3>
<ul>
<li>1111-11-11, Estimation Methods - Method of Moments</li>
<li>2023-03-29, Estimation Methods - <a href="../../../../../docs/blog/posts/statistics/2023-03-25_MLE/index.html">Maximum Likelihood Estimation &amp; Statistical Bias</a></li>
<li>1111-11-11, Estimation Methods - Bayesian Estimation</li>
<li>1111-11-11, Estimation Methods - The EM Algorithm</li>
<li>1111-11-11, Evaluation Methods of Estimators - Mean Squared Error</li>
<li>1111-11-11, Evaluation Methods of Estimators - Best Unbiased Estimators</li>
<li>1111-11-11, Evaluation Methods of Estimators - Sufficiency and Unbiasedness</li>
<li>1111-11-11, Evaluation Methods of Estimators - Loss Function Optimality</li>
</ul>
</section>
</section>
<section id="hypothesis-testing" class="level2">
<h2 class="anchored" data-anchor-id="hypothesis-testing">Hypothesis Testing</h2>
<ul>
<li>1111-11-11, Hypothesis Testing</li>
<li>1111-11-11, Permutation Test</li>
</ul>
<section id="methods-of-finding-tests" class="level3">
<h3 class="anchored" data-anchor-id="methods-of-finding-tests">Methods of Finding Tests</h3>
<ul>
<li>1111-11-11, Likelihood Ratio Tests</li>
<li>1111-11-11, Bayesian Tests</li>
<li>1111-11-11, Union-Intersection and Intersection-Union Tets</li>
</ul>
</section>
<section id="methods-of-evaluating-tests" class="level3">
<h3 class="anchored" data-anchor-id="methods-of-evaluating-tests">Methods of Evaluating Tests</h3>
<ul>
<li>1111-11-11, Power</li>
<li>1111-11-11, Error Proabilities and the Power Function</li>
<li>1111-11-11, Most Powerful Tests</li>
<li>2022-12-28, <a href="../../../../../docs/blog/posts/statistics/2022-12-08-P-value/index.html">p-values</a></li>
<li>1111-11-11, Loss Function Optimality</li>
<li>1111-11-11, Multiple Testing</li>
<li>1111-11-11, Sample Size Calculation</li>
<li>1111-11-11, A/B Testing</li>
<li>2023-01-07, <a href="../../../../../docs/blog/posts/statistics/2023-01-07-anova/index.html">ANOVA</a>
<ul>
<li>2023-01-27, <a href="../2023-01-27_ANCOVA/">ANCOVA</a></li>
<li>2023-01-27, <a href="../2023-01-27_rmANOVA/">repeated measures ANOVA</a></li>
<li>2023-01-28, <a href="../2023-01-28_MANOVA/">MANOVA</a></li>
</ul></li>
</ul>
</section>
</section>
<section id="categorical-data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="categorical-data-analysis">Categorical Data Analysis</h2>
<ul>
<li>1111-11-11, Introduction</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>2022-12-28,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>2023-01-07,</li>
<li>2023-01-27,</li>
<li>2023-01-27,</li>
<li>2023-01-28,</li>
</ul>
</section>
<section id="regression" class="level2">
<h2 class="anchored" data-anchor-id="regression">Regression</h2>
<ul>
<li>1111-11-11, Least Square and Simple Linear Regression</li>
<li>1111-11-11, Multiple Linear Regression</li>
</ul>
<section id="generalized-linear-models" class="level3">
<h3 class="anchored" data-anchor-id="generalized-linear-models">Generalized Linear Models</h3>
<ul>
<li>1111-11-11, Logistic Regression</li>
<li>1111-11-11, Multinomial Regression</li>
<li>1111-11-11, Poisson Regression</li>
<li>1111-11-11, Poisson Regression</li>
<li>1111-11-11, Poisson Regression</li>
</ul>
</section>
</section>
<section id="longitudinal-data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="longitudinal-data-analysis">Longitudinal Data Analysis</h2>
<ul>
<li>2023-03-23, <a href="../../../../../docs/blog/posts/statistics/LDA/1_intro.html">LDA (1) - Intro</a></li>
<li>2023-03-23, <a href="../../../../../docs/blog/posts/statistics/LDA/2_covariance_model.html">LDA (2) - Concepts &amp; Covariance Models</a></li>
<li>2023-03-25, <a href="../LDA/intro.qmd">LDA (3) - WLS &amp; REML</a></li>
<li>2023-03-25, <a href="../LDA/intro.qmd">LDA (4) - Respiratory Infection Data Example</a></li>
<li>2023-03-28, <a href="../LDA/intro.qmd">LDA (5) - Epileptic Seizures Data Example</a></li>
</ul>
<section id="mixed-models" class="level3">
<h3 class="anchored" data-anchor-id="mixed-models">Mixed Models</h3>
<ul>
<li>1111-11-11, Linear Mixed Models</li>
</ul>
</section>
</section>
<section id="generalized-additive-models" class="level2">
<h2 class="anchored" data-anchor-id="generalized-additive-models">Generalized Additive Models</h2>
</section>
<section id="survival-analysis" class="level2">
<h2 class="anchored" data-anchor-id="survival-analysis">Survival Analysis</h2>
<ul>
<li>1111-11-11, Cox-Hazard Model</li>
</ul>


</section>
</section>

 ]]></description>
  <category>Statistics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/statistics/guide_map/index.html</guid>
  <pubDate>Fri, 30 Apr 2100 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Machine Learning</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/ML/guide_map/index.html</link>
  <description><![CDATA[ 



<p><strong>(Draft)</strong></p>
<section id="contents-list" class="level1">
<h1>Contents List</h1>
<section id="basic" class="level2">
<h2 class="anchored" data-anchor-id="basic">Basic</h2>
<section id="r" class="level3">
<h3 class="anchored" data-anchor-id="r">R</h3>
</section>
<section id="python" class="level3">
<h3 class="anchored" data-anchor-id="python">Python</h3>
<section id="tensor-flow-framework" class="level4">
<h4 class="anchored" data-anchor-id="tensor-flow-framework">Tensor Flow Framework</h4>
<ul>
<li>2023-02-03, <a href="../../../../../docs/blog/posts/ML/2023-02-03_tf_introduction/index.html">Tensor Flow Introduction</a></li>
</ul>
</section>
<section id="pytorch-framework" class="level4">
<h4 class="anchored" data-anchor-id="pytorch-framework">Pytorch Framework</h4>
<ul>
<li>2023-02-03, <a href="../../../../../docs/blog/posts/ML/2023-02-03_pytorch_introduction/index.html">Pytorch Introduction</a></li>
</ul>
</section>
</section>
</section>
<section id="machine-learning-methods" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning-methods">Machine Learning Methods</h2>
<section id="supervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="supervised-learning">Supervised Learning</h3>
<ul>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Logistic Regression]</li>
<li>0000-00=00, [Generative Models]
<ul>
<li>0000-00=00, [Linear Discriminant Analysis]</li>
<li>0000-00=00, [Quadratic Discriminant Analysis]</li>
<li>0000-00=00, [Naive Bayes]</li>
</ul></li>
<li>0000-00=00, [Resampling Methods]</li>
<li>0000-00=00, [Regularization]</li>
<li>0000-00=00, [Smoothing]</li>
<li>0000-00=00, [Tree Based Methods]</li>
<li>0000-00=00, [Support Vector Machine]</li>
<li>0000-00=00, [PCR]</li>
<li>0000-00=00, [PLS]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
</ul>
</section>
<section id="unupervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="unupervised-learning">Unupervised Learning</h3>
<ul>
<li>0000-00=00, [PCA]</li>
<li>0000-00=00, [K means clustering]</li>
<li>0000-00=00, [Hierarchical Clustering]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
<li>0000-00=00, [Linear Regression]</li>
</ul>


</section>
</section>
</section>

 ]]></description>
  <category>ML</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/ML/guide_map/index.html</guid>
  <pubDate>Wed, 31 Mar 2100 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Mathematics</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/guide_map/index.html</link>
  <description><![CDATA[ 



<section id="contents" class="level1">
<h1>Contents</h1>
<section id="basic" class="level2">
<h2 class="anchored" data-anchor-id="basic">Basic</h2>
<ul>
<li>2023-03-24, <a href="../../../../../docs/blog/posts/Mathmatics/variables/index.html">Variable types</a></li>
<li>1111-11-11, <a href="../../../../../docs/blog/posts/Mathmatics/function/index.html">Function</a>
<ul>
<li>2023-01-31, <a href="../../../../../docs/blog/posts/Mathmatics/function/univariable_scalar_function.html">Function (1) - Univariable Scalar Function (One to One)</a></li>
<li>2023-01-31, <a href="../../../../../docs/blog/posts/Mathmatics/function/multivariable_scalar_function.html">Function (2) - Multi-variable Scalar Function (Many to One)</a></li>
<li>2023-01-31, <a href="../../../../../docs/blog/posts/Mathmatics/function/univariable_vector_function.html">Function (3) - Univariable Vector Function (One to Many)</a></li>
<li>2023-01-31, <a href="../../../../../docs/blog/posts/Mathmatics/function/mutivariable_vector_function.html">Function (4) - Multi-variable Vector Function (Many to Many)</a></li>
<li>2023-02-18, <a href="../../../../../docs/blog/posts/Mathmatics/function/composite_function.html">Function (5) - Composite Function</a></li>
</ul></li>
<li>2023-02-18, <a href="../../../../../docs/blog/posts/Mathmatics/transformation/index.html">Transformations of Functions</a></li>
<li>1111-11-11, Vector &amp; Matrix</li>
<li>2023-03-15, <a href="../epsilon_delta/">Limit, <img src="https://latex.codecogs.com/png.latex?%5Cepsilon-%5Cdelta"> Method</a></li>
<li>Differentiation
<ul>
<li>2023-02-04, <a href="../../../../../docs/blog/posts/Mathmatics/differentiation/2023-02-04_uni_derivative.html">Derivative (1) - Univariable Scalar Funtion</a></li>
<li>1111-11-11, <a href="../../../../../docs/blog/posts/Mathmatics/differentiation/2023-02-10_composite_partial_derivative.html">Derivative (2) - Chain Rule &amp; Partial Derivative</a></li>
<li>1111-11-11, Derivative (3) - Higher Order Derivative</li>
<li>1111-11-11, Derivative (4) - Mean Value Theorem</li>
<li>1111-11-11, Derivative (5) - Gradient</li>
</ul></li>
<li>2023-03-15, <a href="../../../../../docs/blog/posts/Mathmatics/taylor_series/index.html">Talyer’s Series</a></li>
<li>1111-11-11, Gradient Direction</li>
<li>1111-11-11, Random Variable</li>
<li>1111-11-11, Probability Distribution</li>
<li>1111-11-11, Information Theory - Entropy</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
</ul>
</section>
<section id="linear-algebra" class="level2">
<h2 class="anchored" data-anchor-id="linear-algebra">Linear Algebra</h2>
<ul>
<li>2023-03-30, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/01.basic_vector.html">Basics (1) - Vector Operations</a></li>
<li>2023-03-30, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/02.basic_matrix.html">Basics (2) - Matrix Operations</a></li>
<li>2023-03-30, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/03.basic_special_matrix.html">Basics (3) - Special Matrix</a></li>
<li>1111-11-11, Inner Product</li>
<li>1111-11-11, Linear Combination</li>
<li>1111-11-11,</li>
<li>1111-11-11, Linear Independence</li>
<li>1111-11-11, Basis, Dimension, &amp; Rank</li>
<li>1111-11-11, Outer Product</li>
<li>1111-11-11, Eigen Value &amp; Eigen Vector</li>
<li>1111-11-11, Eigen Decomposition</li>
<li>1111-11-11, Singular Value Decomposition (SVD)</li>
<li>1111-11-11, Gram-Schmidt</li>
<li>1111-11-11, Group</li>
<li>1111-11-11, Orthogonal Matrix</li>
<li>1111-11-11, Rotation &amp; Group</li>
<li>2023-04-02, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/quadratic_form.html">Matrix Transformation (5) - Quadratic Form</a></li>
<li>2023-04-02, <a href="../../../../../docs/blog/posts/Mathmatics/linear_algebra/derivative_matrix_vector.html">Matrix Calculus (1) - Quadratic Form</a></li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
</ul>
</section>
<section id="optimization" class="level2">
<h2 class="anchored" data-anchor-id="optimization">Optimization</h2>
<ul>
<li>2023-03-23, <a href="../../../../../docs/blog/posts/Mathmatics/optimization/minimizer.html">Minimizer &amp; Minimum</a></li>
<li>1111-11-11, Convex Set</li>
<li>1111-11-11, Convex Function</li>
<li>1111-11-11, Unconstrained Optimization</li>
<li>1111-11-11, Non-linear Least Square</li>
<li>1111-11-11, Largrange Multiplier Method
<ul>
<li>1111-11-11, Largrange Primal Function</li>
<li>1111-11-11, Largrange Dual Function</li>
<li>1111-11-11, KKT conditions</li>
</ul></li>
<li>1111-11-11, Gradient Descent Optimizers</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
</ul>
</section>
<section id="reference" class="level2">
<h2 class="anchored" data-anchor-id="reference">Reference</h2>
<ul>
<li>Statistics
<ul>
<li>George Casella &amp; Rogeer L. Berger - Statistcal Inference, 2nd Edition</li>
<li>슬기로운 통계생활 - https://www.youtube.com/<span class="citation" data-cites="statisticsplaybook">@statisticsplaybook</span></li>
<li>슬기로운 통계생활 - https://github.com/statisticsplaybook</li>
<li>다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Mathematics
<ul>
<li>James Stewart - Calculus Early Transcedentals, 7th Eidition</li>
<li>any James Stewart series</li>
<li>임장환 - 머신러닝, 인공지능, 컴퓨터 비전 전공자를 위한 최적화 이론</li>
<li>다수의 Youtube, and Documents from Googling</li>
</ul></li>
<li>Deep Learning
<ul>
<li>조준우 - 머신러닝·딥러닝에 필요한 기초 수학 with 파이썬</li>
<li>조준우 - https://github.com/metamath1/noviceml</li>
<li>동빈나 - https://www.youtube.com/c/dongbinna</li>
<li>혁펜하임 - https://www.youtube.com/channel/UCcbPAIfCa4q0x7x8yFXmBag</li>
<li>다수의 Youtube, and Documents from Googling</li>
</ul></li>
</ul>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/guide_map/index.html</guid>
  <pubDate>Sun, 28 Feb 2100 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Deep Learning</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/DL/guide_map/index.html</link>
  <description><![CDATA[ 



<p><strong>(Draft)</strong></p>
<section id="contents-list" class="level1">
<h1>Contents List</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<ul>
<li>1111-11-11, Artificial Intelligence</li>
<li>1111-11-11, Perceptron</li>
<li>1111-11-11, Artificial Neural Netwroks (ANN)
<ul>
<li>1111-11-11, activation functions</li>
<li>1111-11-11, output layer design</li>
</ul></li>
<li>1111-11-11, loss function</li>
<li>1111-11-11, numerical differentiation</li>
<li>1111-11-11, gradient descent</li>
<li>1111-11-11, backpropagation</li>
<li>1111-11-11, optimizer
<ul>
<li>1111-11-11, stochastic gradient descent</li>
<li>1111-11-11, momentum</li>
<li>1111-11-11, adaGrad</li>
<li>1111-11-11, adam</li>
<li>1111-11-11, weight initalization</li>
</ul></li>
<li>1111-11-11, batch normalization</li>
<li>1111-11-11, dropout</li>
<li>1111-11-11, tuning parameter</li>
<li>1111-11-11, auto-encoder</li>
<li>1111-11-11, stacked auto-encoder</li>
<li>1111-11-11, denoising auto-encoder(DAE)</li>
</ul>
<section id="convolutional-neural-network-cnn" class="level3">
<h3 class="anchored" data-anchor-id="convolutional-neural-network-cnn">Convolutional Neural Network (CNN)</h3>
<ul>
<li>2023-03-10, <a href="../../../../../docs/blog/posts/DL/2023-03-10_cnn/index.html">CNN (1) - Concept</a></li>
<li>2023-03-10, <a href="">CNN (2) - Practice</a></li>
</ul>
</section>
<section id="natural-language-process-nlp" class="level3">
<h3 class="anchored" data-anchor-id="natural-language-process-nlp">Natural Language Process (NLP)</h3>
<ul>
<li>1111-11-11, word2vec</li>
<li>1111-11-11, improved word2vec</li>
</ul>
</section>
<section id="recurrent-neural-network-rnn" class="level3">
<h3 class="anchored" data-anchor-id="recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</h3>
</section>
<section id="gate-recurrent-unit-gru" class="level3">
<h3 class="anchored" data-anchor-id="gate-recurrent-unit-gru">Gate Recurrent Unit (GRU)</h3>
</section>
<section id="long-short-term-memory-lstm" class="level3">
<h3 class="anchored" data-anchor-id="long-short-term-memory-lstm">Long Short-Term Memory (LSTM)</h3>
</section>
<section id="attention-transformer" class="level3">
<h3 class="anchored" data-anchor-id="attention-transformer">Attention (Transformer)</h3>
</section>
<section id="bidirectional-encoder-representations-from-transformers-bert" class="level3">
<h3 class="anchored" data-anchor-id="bidirectional-encoder-representations-from-transformers-bert">Bidirectional Encoder Representations from Transformers (BERT)</h3>
</section>
<section id="generative-pre-training-transformer-gpt" class="level3">
<h3 class="anchored" data-anchor-id="generative-pre-training-transformer-gpt">Generative Pre-training Transformer (GPT)</h3>


</section>
</section>
</section>

 ]]></description>
  <category>DL</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/DL/guide_map/index.html</guid>
  <pubDate>Thu, 31 Dec 2099 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Engineering</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Engineering/guide_map/index.html</link>
  <description><![CDATA[ 



<section id="it-terminology" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="it-terminology"><span class="header-section-number">1</span> IT Terminology</h2>
<ul>
<li>0000-00-00, Terminology</li>
</ul>
</section>
<section id="data-structure" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="data-structure"><span class="header-section-number">2</span> Data Structure</h2>
<ul>
<li>2023-01-17, <a href="../2023-01-17_data_structure_overview/">Overview</a></li>
<li>2023-01-18, <a href="../2023-01-18_array/">Array</a></li>
<li>2023-01-18, <a href="../2023-01-18_linked_list/">Linked List</a></li>
<li>2023-01-18, <a href="../2023-01-18_python_list/">Python List</a></li>
<li>2023-01-19, <a href="../2023-01-19_stack/">Stack</a></li>
<li>2023-01-19, <a href="../2023-01-19_queue/">Queue</a></li>
<li>2023-01-26, <a href="../2023-01-19_deque/">Deque</a></li>
<li>2023-01-26, <a href="../2023-01-20_binary_search_tree/">Binary Search Tree</a></li>
<li>2023-01-20, <a href="../2023-01-20_priority_queue/">Priority Queue</a></li>
<li>2023-01-20, <a href="../2023-01-20_graph/">Graph</a></li>
</ul>
</section>
<section id="conda" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="conda"><span class="header-section-number">3</span> Conda</h2>
</section>
<section id="docker" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="docker"><span class="header-section-number">4</span> Docker</h2>
<ul>
<li>2023-01-30, Docker Install</li>
<li>2023-01-31, Docker Compose</li>
<li>2023-02-01, Docker Container</li>
</ul>
</section>
<section id="dynamic-documentation" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="dynamic-documentation"><span class="header-section-number">5</span> Dynamic Documentation</h2>
<ul>
<li>2023-01-19, <a href="https://quarto.org/docs/get-started/">Quarto</a></li>
<li>2023-01-19, <a href="https://github.com/yihui/xaringan">xaringan[R]</a></li>
<li>2023-01-19, <a href="https://bookdown.org/yihui/bookdown/get-started.html">Bookdown[R]</a></li>
<li>2023-01-19, <a href="https://decile-team-distil.readthedocs.io/en/latest/index.html">DISTL</a></li>
<li>2023-01-26, <a href="https://www.sphinx-doc.org/en/master/">Sphinx[Python]</a></li>
</ul>
</section>
<section id="aws-cloud" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="aws-cloud"><span class="header-section-number">6</span> AWS Cloud</h2>
<p>Coursera Course: AWS Fundamentals</p>
<ul>
<li>2023-03-09, <a href="../../../../../docs/blog/posts/Engineering/2023-03-02_aws/computing_networking.html">Computing and Networking</a></li>
<li>2023-03-12, <a href="../../../../../docs/blog/posts/Engineering/2023-03-02_aws/storage_database.html">Storage and Database</a></li>
<li>2023-03-26, <a href="../../../../../docs/blog/posts/Engineering/2023-03-02_aws/monitoring_sharedresponsibility.html">Monitoring and SharedResponsibility</a></li>
<li>2023-04-05, <a href="../../../../../docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html">Infrastructure Security</a></li>
</ul>
</section>
<section id="azure-cloud" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="azure-cloud"><span class="header-section-number">7</span> Azure Cloud</h2>
</section>
<section id="data-modeling" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="data-modeling"><span class="header-section-number">8</span> Data Modeling</h2>
</section>
<section id="apache-airflow" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="apache-airflow"><span class="header-section-number">9</span> Apache Airflow</h2>
</section>
<section id="apache-spark" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="apache-spark"><span class="header-section-number">10</span> Apache Spark</h2>
</section>
<section id="front-end" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="front-end"><span class="header-section-number">11</span> Front End</h2>
</section>
<section id="back-end" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="back-end"><span class="header-section-number">12</span> Back End</h2>


</section>

 ]]></description>
  <category>Engineering</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Engineering/guide_map/index.html</guid>
  <pubDate>Thu, 31 Dec 2099 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Language</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Language/guide_map/index.html</link>
  <description><![CDATA[ 



<p><strong>(Draft)</strong></p>
<section id="contents-list" class="level1">
<h1>Contents List</h1>
<section id="r" class="level2">
<h2 class="anchored" data-anchor-id="r">R</h2>
<ul>
<li>1111-11-11, tidyverse
<ul>
<li>1111-11-11, dplyr</li>
<li>1111-11-11, ggplot2</li>
<li>1111-11-11, tidyr</li>
<li>1111-11-11, readr</li>
<li>1111-11-11, purrr</li>
<li>1111-11-11, tibble</li>
<li>1111-11-11, stringr</li>
<li>1111-11-11, forcats</li>
</ul></li>
<li>1111-11-11, tidymodels</li>
<li>1111-11-11, R shiny</li>
</ul>
</section>
<section id="python" class="level2">
<h2 class="anchored" data-anchor-id="python">Python</h2>
<ul>
<li>1111-11-11, numpy</li>
<li>1111-11-11, pandas</li>
<li>1111-11-11, matplotlib</li>
<li>1111-11-11, seaborn</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
<li>1111-11-11,</li>
</ul>
</section>
<section id="sql" class="level2">
<h2 class="anchored" data-anchor-id="sql">SQL</h2>
<section id="sqlite" class="level3">
<h3 class="anchored" data-anchor-id="sqlite">SQLite</h3>
</section>
<section id="oracle-sql" class="level3">
<h3 class="anchored" data-anchor-id="oracle-sql">Oracle SQL</h3>
</section>
<section id="ms-sql" class="level3">
<h3 class="anchored" data-anchor-id="ms-sql">MS-SQL</h3>
</section>
<section id="postgre-sql" class="level3">
<h3 class="anchored" data-anchor-id="postgre-sql">Postgre SQL</h3>
</section>
</section>
<section id="linux" class="level2">
<h2 class="anchored" data-anchor-id="linux">Linux</h2>
</section>
<section id="powershell" class="level2">
<h2 class="anchored" data-anchor-id="powershell">Powershell</h2>
</section>
<section id="c" class="level2">
<h2 class="anchored" data-anchor-id="c">C++</h2>
</section>
<section id="javascript" class="level2">
<h2 class="anchored" data-anchor-id="javascript">Javascript</h2>


</section>
</section>

 ]]></description>
  <category>Language</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Language/guide_map/index.html</guid>
  <pubDate>Thu, 31 Dec 2099 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Patent</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Patent/guide_map/index.html</link>
  <description><![CDATA[ 



<p><strong>(Draft)</strong></p>
<section id="contents-list" class="level1">
<h1>Contents List</h1>
<section id="basic" class="level2">
<h2 class="anchored" data-anchor-id="basic">Basic</h2>


</section>
</section>

 ]]></description>
  <category>Patent</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Patent/guide_map/index.html</guid>
  <pubDate>Thu, 31 Dec 2099 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Content List, Validation</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Surveilance/guide_map/index.html</link>
  <description><![CDATA[ 



<section id="sgs" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="sgs"><span class="header-section-number">1</span> SGS</h2>
<ul>
<li>0000-00-00, EN62304</li>
</ul>
</section>
<section id="fda" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="fda"><span class="header-section-number">2</span> FDA</h2>
<ul>
<li>2023-01-27, <a href="../../../../../docs/blog/posts/Surveilance/2022-12-10_FDA_sw_general_guidance/index.html">General Principles of SW Validation</a></li>
<li>2023-01-27, <a href="../../../../../docs/blog/posts/Surveilance/2023-01-27_FDA_sw_general_guidance_presentation/index.html">General Principles of SW Validation - Diagram Summary</a></li>
<li>1111-11-11, Guidance for the Content of Premarket Submissions for Software Contained in Medical Devices</li>
</ul>
</section>
<section id="dhf" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="dhf"><span class="header-section-number">3</span> DHF</h2>
</section>
<section id="public-health" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="public-health"><span class="header-section-number">4</span> Public Health</h2>
</section>
<section id="wet-lab" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="wet-lab"><span class="header-section-number">5</span> Wet Lab</h2>
<ul>
<li>0000-00-00, PCR (Polymerase Chain Reaction) Experiment</li>
</ul>


</section>

 ]]></description>
  <category>Surveilance</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Surveilance/guide_map/index.html</guid>
  <pubDate>Sat, 31 Dec 2089 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Infrastructure Security</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html</link>
  <description><![CDATA[ 



<ul class="nav nav-pills" id="language-tab">
<li class="nav-item">
<button class="nav-link active" id="Korean-tab" data-bs-toggle="tab" data-bs-target="#Korean" type="button" aria-controls="Korean" aria-selected="true">
Korean
</button>
</li>
<li class="nav-item">
<button class="nav-link" id="English-tab" data-bs-toggle="tab" data-bs-target="#English" type="button" aria-controls="knitr" aria-selected="false">
English
</button>
</li>
<div class="tab-content" id="language-tabcontent">

<div id="Korean" class="tab-pane fade show active" aria-labelledby="Korean-tab">
<div id="Korean" class="tab-pane fade show active" aria-labelledby="Korean-tab">
<section id="network-isolation" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="network-isolation"><span class="header-section-number">1</span> Network Isolation</h2>
<p>AWS has implemented network isolation through a <strong>limited number of access points</strong> to the cloud, allowing for comprehensive monitoring of inbound and outbound communications and network traffic.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>inbound and outbound communications involve observing and analyzing the data and traffic that is entering or leaving the AWS network to ensure security and compliance.</p>
</div>
</div>
<ul>
<li>Endpoints are URLs that serve as entry points for web services.</li>
<li>Some services do not support regions (like IAM), so their endpoints do not include a region. But, some services (like US-West-2) do support regions.</li>
<li>AWS offers Amazon Virtual Private Cloud (VPC) as a private network within the AWS Cloud that provides isolation from other customers and from the internet
<ul>
<li>VPC allows you to allocate IP address spaces and build a private infrastructure with networks isolated from the internet.</li>
<li>You can also connect your on-premises environment or other VPN infrastructures to VPC using IPSec tunnels and AWS Direct Connect.</li>
<li>VPC allows resources to communicate with the internet if desired.</li>
</ul></li>
<li>Building a fort on a barren planet to protect themselves and the bees, and further isolating hives inside the fort itself is similar to the concept of isolating resources within a secure environment, such as Amazon VPC, to protect them from potential external threats.</li>
<li>Network Isolation VPC
<ul>
<li>the concept of Virtual Private Cloud (VPC) in AWS is a way to logically separate your AWS infrastructure from other customers.</li>
<li>VPC is like creating a fort around your AWS account and isolating resources into hives, using subnets or logical subdivision of IPs.
<ul>
<li>ex) EC2 instances are able to access the internet and be accessed from by putting them in a public subnet via Network Access Control Lists (NACLs), which are used to control inbound and outbound traffic at the subnet level.</li>
</ul></li>
<li>security groups
<ul>
<li>act as firewalls for EC2 instances by controlling both inbound and outbound traffic at the instance level.</li>
<li>This fine-grained access is defined by allow rules and looks</li>
</ul></li>
</ul></li>
<li>how to secure traffic between VPCs in AWS using VPC endpoints and route tables?
<ul>
<li>further secure communication between Virtual Private Clouds (VPCs) in AWS
<ul>
<li>It compares the traditional method of sending traffic between VPCs through the internet with the use of private links, which allow for direct communication between VPCs within the AWS infrastructure, resulting in a safer path of travel for data.</li>
<li>the concept of route tables in VPCs
<ul>
<li>route tables contain <strong>rules or routes</strong> used to determine where network traffic is directed, and the option to create custom route tables for routing traffic according to specific requirements.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="detective-controls" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="detective-controls"><span class="header-section-number">2</span> Detective Controls</h2>
<ul>
<li>Detective controls: 감사(auditing), 자동화된 분석, 경보를 가능하게 하는 사건 모니터링 및 블록 처리의 지속적인 개선
<ul>
<li>잠재적인 보안 위협 또는 사건을 식별할 수 있는 능력 향상</li>
<li>governance frameworks에 필수적</li>
<li>법이나 compliance 준수 의무, 보안 작업 등의 개선을 지원</li>
</ul></li>
<li>Different types of detective controls
<ul>
<li>자산 인벤토리의 작성(conducting an inventory of AWS resources)</li>
<li>내부 감사(internal auditing)</li>
<li>정보 시스템과 관련된 제어가 정책 및 요구사항 충족하는지 검사</li>
</ul></li>
<li>이상 활동 범위를 식별하고 이해하는데 도움이 됨</li>
</ul>
</section>
<section id="auditing" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="auditing"><span class="header-section-number">3</span> Auditing</h2>
<ul>
<li>AWS infrastructure의 보안 및 compliance를 향상시키는 AWS services (일부는 무료, 일부는 유료)
<ul>
<li>AWS CloudTrail: AWS infrastructure와 interact 하는 사람 추적 가능 (잘못된 변경/데이터 유출 추적에 도움)</li>
<li>AWS Config: configuration 관리 및 변경 기록, 모든 실제 config 세부 사항의 inventory 제공</li>
<li>AWS Inspector: 자동 보안 평가 실행
<ul>
<li>deploy된 applications의 보안 및 compliance를 향상시키기 위해, best practies와의 차이, EC2 instances의 노출, 취약점 등을 체크</li>
</ul></li>
<li>Trusted Advisor
<ul>
<li>AWS resources의 프로비저닝 보조 - best practices를 사용해서 리포트 제공
<ul>
<li>리포트 항목: 비용 최적화, 성능, 보안, 장애 허용 정도, 서비스 제한</li>
<li>조사 또는 실행을 위해, 심각한 수준(녹색,주황,적색)에 따라 권장 사항 제공</li>
</ul></li>
<li>Security section: S3 bucket의 권한, 보안 그룹, IAM 사용, root 계정의 MFA, 노출된 access keys, IAM 비밀번호 정책 등을 스캔</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="monitoring-cloudwatch-and-cloudwatch-log" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="monitoring-cloudwatch-and-cloudwatch-log"><span class="header-section-number">4</span> Monitoring CloudWatch and CloudWatch Log</h2>
<ul>
<li>Monitoring: Infrastructure와 관련된 데이터와 통계를 수집, 추적, 표시하는 과정</li>
<li>AWS의 CloudWatch: metrics repository역할 - repository에 넣은 metrics 기반으로 통계 검색
<ul>
<li>사용자가 정한 threshold를 넘었을때 경보 생성 가능</li>
<li>특정 기간 동안 하나의 metric을 감시 → threshold와 비교한 metric의 상대적인 값에 따라 하나 이상의 특정 action 수행 가능</li>
</ul></li>
<li>CloudWatch Logs: 여러 resources의 log files을 모니터링, 저장, 접근 가능한 tool
<ul>
<li>application, 서버 OS의 로그 수집 및 저장</li>
<li>CloudTrail 사용해서 API activity 수집</li>
<li>Amazon Route 53(Amazon의 DNS 웹 서비스)의 DNS queries를 기록</li>
<li>S3의 로그 데이터 저장</li>
</ul></li>
<li>CloudWatch Logs Insights: 로그 데이터를 interactive하게 검색하고 분석</li>
</ul>
</section>
<section id="monitoring-guard-duty-and-security-hub" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="monitoring-guard-duty-and-security-hub"><span class="header-section-number">5</span> Monitoring Guard Duty and Security Hub</h2>
<ul>
<li>Amazon GuardDuty: 위협 감지 서비스
<ul>
<li>AWS 계정 및 리소스에 대한 허가되지 않거나 악성인 행동들을 계속 모니터링</li>
<li>머신 러닝, 이상 감지, integrated threat intelligence를 사용해서 잠재적인 위협을 식별하고 우선 순위를 정함</li>
<li>여러 AWS data resources에서 수백억건의 사건 분석</li>
<li>잠재적인 위협을 세 단계(low, medium, high) 심각 수준으로 나눠서 대응 우선순위 결정</li>
<li>HTTPs API, CLI tools, Amazon CloudWatch events를 제공해서 보안 관련 발견에 대한 자동화된 보안 제공 지원</li>
</ul></li>
<li>Security Hub: 여러 AWS service의 보안 경고나 발견을 모으고, 정리하고, 우선 순위를 정함 → 통합 dashboards에서 시각화 요약 제공
<ul>
<li>AWS best practies 및 업계 표준을 기반으로, compliance check 자동화를 통해 환경을 지속적으로 모니터링할 수 있도록 함</li>
</ul></li>
</ul>
</section>
</div>
</div>
<div id="English" class="tab-pane fade" aria-labelledby="English-tab">
<div id="English" class="tab-pane fade" aria-labelledby="English-tab">

</div>
</div>
<section id="back-to-content-list" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="back-to-content-list"><span class="header-section-number">6</span> Back to Content List</h2>
<ul>
<li><a href="../../../../../docs/blog/posts/content_list.html">Global Blog Content List</a></li>
</ul>


</section>

</div></ul> ]]></description>
  <category>Engineering</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Engineering/2023-03-02_aws/infra_security.html</guid>
  <pubDate>Tue, 04 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Matrix Transformation (4) - Biinear Form</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.bilinear_form.html</link>
  <description><![CDATA[ 



<section id="binear-form" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="binear-form"><span class="header-section-number">1</span> Binear Form</h2>
<p>A bilinear form of a matrix is a function that extends the linear form and takes two vectors as inputs and produces a scalar as output. It is linear in both of its arguments, meaning that it satisfies the following properties:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AB(%5Cmathbf%20u+%5Cmathbf%20v)&amp;=B(%5Cmathbf%20u+%5Cmathbf%20w)+B(%5Cmathbf%20v+%5Cmathbf%20w)%5C%5C%0AB(%5Cmathbf%20u,%5Calpha%20%5Cmathbf%20v)&amp;=%5Calpha%20B(%5Cmathbf%20u,%5Cmathbf%20v)%5C%5C%0AB(%5Calpha%5Cmathbf%20u,%5Cmathbf%20v)&amp;=%5Calpha%20B(%5Cmathbf%20u,%5Cmathbf%20v)%0A%5Cend%7Baligned%7D%0A"></p>
<p>for all vectors <img src="https://latex.codecogs.com/png.latex?u">, <img src="https://latex.codecogs.com/png.latex?v">, <img src="https://latex.codecogs.com/png.latex?w"> and scalars <img src="https://latex.codecogs.com/png.latex?%5Calpha">.</p>
<p>A bilinear form can be represented by a matrix <img src="https://latex.codecogs.com/png.latex?B"> such that <img src="https://latex.codecogs.com/png.latex?B_%7Bi,j%7D"> is the coefficient of the product <img src="https://latex.codecogs.com/png.latex?u_i%20v_j"> in the expansion of <img src="https://latex.codecogs.com/png.latex?B(u,v)">. The bilinear form can then be written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AB(%5Cmathbf%20u,%5Cmathbf%20v)=%5Cmathbf%20u%5ET%20B%20%5Cmathbf%20v%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20u"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20v"> are column vectors and <img src="https://latex.codecogs.com/png.latex?B"> is a matrix.</p>
<p>For example, consider the bilinear form <img src="https://latex.codecogs.com/png.latex?B(%5Cmathbf%20u,%5Cmathbf%20v)%20=%20u_1%20v_1%20+%20u_2%20v_2">. This bilinear form can be represented by the matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AB=%5Cbegin%7Bbmatrix%7D1&amp;0%5C%5C0&amp;1%5Cend%7Bbmatrix%7D%0A"></p>
<p>and written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AB(%5Cmathbf%20u,%5Cmathbf%20v)=%5Cbegin%7Bbmatrix%7Du_1&amp;%20u_2%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D1&amp;0%5C%5C0&amp;1%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dv_1%5C%5Cv_2%5Cend%7Bbmatrix%7D=u_1v_1+u_2v_2%0A"></p>
<p>This bilinear form computes the dot product of <img src="https://latex.codecogs.com/png.latex?u"> and <img src="https://latex.codecogs.com/png.latex?v">, which measures the similarity between the two vectors. Bilinear forms are commonly used in applications such as optimization, geometry, and physics, where they capture the interaction between two quantities or variables.</p>
<p>The covariance matrix can be represented as a bilinear form using matrix multiplication. Let’s say we have a random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Bx_1,%20x_2,%20%5Cldots,%20x_n%5D%5ET"> with mean vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cmu%7D%20=%20%5B%5Cmu_1,%20%5Cmu_2,%20%5Cldots,%20%5Cmu_n%5D%5ET"> and covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CSigma%7D">. Then, we can represent the covariance matrix as a bilinear form in the following way:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5CSigma&amp;=%5Coperatorname%7BE%7D%5B(%5Cmathbf%20x-%5Cmathbf%20%5Cmu)(%5Cmathbf%20x-%5Cmathbf%20%5Cmu)%5ET%5D%5C%5C%0A&amp;=%5Cfrac%7B1%7D%7Bn-1%7D%5Csum_%7Bi=1%7D%5E%7Bn%7D(x_i-%5Cbar%7Bx%7D)(x_i-%5Cbar%7Bx%7D)%5ET%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BE%7D"> denotes the expectation operator. We can expand this expression as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5CSigma&amp;=%5Coperatorname%7BE%7D%5B%5Cmathbf%20x%5Cmathbf%20x%5ET%5D-%5Cmathbf%20%5Cmu%5Cmathbf%20%5Cmu%5ET%0A%5Cend%7Baligned%7D%0A"></p>
<p>Now, we can represent the covariance matrix as a bilinear form using matrix multiplication as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5CSigma&amp;=%5Coperatorname%7BE%7D%5B(%5Cmathbf%20x-%5Cmathbf%20%5Cmu)(%5Cmathbf%20x-%5Cmathbf%20%5Cmu)%5ET%5D%5C%5C%0A&amp;=%5Csum_%7Bi=1%7D%5E%7Bn%7D%5Csum_%7Bj=1%7D%5E%7Bn%7D(%5Cmathbf%20x_i-%5Cmathbf%5Cmu_i)(%5Cmathbf%20x_i-%5Cmathbf%20%5Cmu_j)%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bx%7D-%5Cmathbf%7B%5Cmu%7D%5D"> is the deviation of the random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> from its mean vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cmu%7D">.</p>
<p>covariance matrix, and correlation matrix</p>
<p>One of the most famous examples is the use of bilinear forms in convolutional neural networks (CNNs), which are a type of deep learning model used for image and video recognition tasks.</p>
<p>In a CNN, a bilinear form is used to compute the similarity between a filter and a local region of an input image. This similarity measure is used to determine how much the filter “matches” the local region of the image, and is used to produce an output feature map.</p>
<p>More specifically, the bilinear form used in a CNN takes the form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Az_%7Bi,j%7D%20=%20%5Csum_%7Bm=1%7D%5E%7BM%7D%5Csum_%7Bn=1%7D%5E%7BN%7D%20w_%7Bm,n%7Dx_%7Bi+m-1,j+n-1%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?z_%7Bi,j%7D"> is the output feature map at location <img src="https://latex.codecogs.com/png.latex?(i,j)">, <img src="https://latex.codecogs.com/png.latex?x_%7Bi+m-1,j+n-1%7D"> is the input image pixel at location <img src="https://latex.codecogs.com/png.latex?(i+m-1,j+n-1)">, and <img src="https://latex.codecogs.com/png.latex?w_%7Bm,n%7D"> is the weight of the filter at position <img src="https://latex.codecogs.com/png.latex?(m,n)">. This computation is performed for each location <img src="https://latex.codecogs.com/png.latex?(i,j)"> in the output feature map.</p>
<p>The bilinear form used in CNNs is a type of convolution operation, and is used to learn features such as edges, corners, and other patterns in the input image. CNNs with bilinear forms have achieved state-of-the-art performance on many image recognition tasks, including object detection, face recognition, and scene classification.</p>
<p>Bilinear forms also have applications in other areas of machine learning, such as natural language processing (NLP). In NLP, bilinear forms can be used to compute the similarity between two word embeddings, which are vector representations of words. This similarity measure can be used for tasks such as sentiment analysis, text classification, and machine translation.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AB(%5Cmathbf%7Bu%7D,%5Cmathbf%7Bv%7D)=%5Cmathbf%7Bu%7D%5ET%20%5Cmathbf%7BW%7D%5Cmathbf%7Bv%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> are word embeddings, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BW%7D"> is a weight matrix, and <img src="https://latex.codecogs.com/png.latex?B(%5Cmathbf%7Bu%7D,%5Cmathbf%7Bv%7D)"> represents the bilinear form used to compute the similarity between the two embeddings.</p>
<p>Overall, bilinear forms are a powerful tool for learning features from complex data such as images and text, and have many applications in deep learning and machine learning.</p>


</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.bilinear_form.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Matrix Transformation (3) - Linear Form</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.linear_form.html</link>
  <description><![CDATA[ 



<section id="linear-form" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="linear-form"><span class="header-section-number">1</span> Linear Form</h2>
<p>A linear form is a linear function that maps a vector space to its underlying field. Let <img src="https://latex.codecogs.com/png.latex?V"> be a vector space over a field <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BF%7D">, and let <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(V,%5Cmathbb%7BF%7D)"> denote the set of all linear functions from <img src="https://latex.codecogs.com/png.latex?V"> to <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BF%7D">. A linear form on <img src="https://latex.codecogs.com/png.latex?V"> is an element of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(V,%5Cmathbb%7BF%7D)">.</p>
<p>A linear form <img src="https://latex.codecogs.com/png.latex?%5Cvarphi"> can be represented by a row vector of dimension <img src="https://latex.codecogs.com/png.latex?1%5Ctimes%20n">, where <img src="https://latex.codecogs.com/png.latex?n"> is the dimension of <img src="https://latex.codecogs.com/png.latex?V">. Let <img src="https://latex.codecogs.com/png.latex?%7B%5Cmathbf%7Be%7D_1,%20%5Cmathbf%7Be%7D_2,%20%5Cdots,%20%5Cmathbf%7Be%7D_n%7D"> be a basis for <img src="https://latex.codecogs.com/png.latex?V">, and let <img src="https://latex.codecogs.com/png.latex?%7B%5Calpha_1,%20%5Calpha_2,%20%5Cdots,%20%5Calpha_n%7D"> be the corresponding dual basis for <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(V,%5Cmathbb%7BF%7D)">, such that <img src="https://latex.codecogs.com/png.latex?%5Calpha_i(%5Cmathbf%7Be%7Dj)%20=%20%5Cdelta%7Bij%7D"> (the Kronecker delta). Then, any linear form <img src="https://latex.codecogs.com/png.latex?%5Cvarphi%5Cin%5Cmathcal%7BL%7D(V,%5Cmathbb%7BF%7D)"> can be written as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cvarphi(x)=%5Csum_%7Bi=1%7D%5E%7Bn%7Da_ix_i=%5Cmathbf%20a%20%5Cmathbf%20x%5ET=%5Cmathbf%20x%20%5Cmathbf%20a%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5Cin%20V"> is a column vector of dimension <img src="https://latex.codecogs.com/png.latex?n%5Ctimes%201">, <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Ba%7D%5D"> is the row vector representing <img src="https://latex.codecogs.com/png.latex?%5Cvarphi">, and <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bx%7D%5D"> is the column vector representing <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?V%20=%20%5Cmathbb%7BR%7D%5E2"> be the vector space of 2-dimensional column vectors, and let <img src="https://latex.codecogs.com/png.latex?%5Cvarphi%5Cin%5Cmathcal%7BL%7D(V,%5Cmathbb%7BR%7D)"> be the linear form defined by <img src="https://latex.codecogs.com/png.latex?%5Cvarphi(%5Cbegin%7Bbmatrix%7Dx%5Cy%5Cend%7Bbmatrix%7D)%20=%203x%20-%202y">. Then, we can represent <img src="https://latex.codecogs.com/png.latex?%5Cvarphi"> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5B%5Cmathbf%20a%5D=%5Cbegin%7Bbmatrix%7D%203%20&amp;%20-2%5Cend%7Bbmatrix%7D%20%5B%5Cmathbf%20x%5D=%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D%20%5Cvarphi(x)=%5Csum_%7Bi=1%7D%5E%7Bn%7D%5Cmathbf%20a%5Cmathbf%20x%5ET=3x_1-2x_2%0A"></p>
<p>which shows that <img src="https://latex.codecogs.com/png.latex?%5Cvarphi"> is a linear form on <img src="https://latex.codecogs.com/png.latex?V">.</p>
<p>consider a linear regression model that predicts the price of a house based on its size and location. The model can be represented by the linear form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cvarphi(x)=%5Cmathbf%20w%5Cmathbf%20x%5ET=%5Csum_%7Bi=1%7D%5E%7Bn%7Dw_ix_i=w_0+w_1x_1+w_2x_2%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cvarphi(%5Cmathbf%7Bx%7D)"> is the predicted price, <img src="https://latex.codecogs.com/png.latex?x_1"> is the size of the house, <img src="https://latex.codecogs.com/png.latex?x_2"> is a measure of the location (such as the distance from the city center), and <img src="https://latex.codecogs.com/png.latex?w_0">, <img src="https://latex.codecogs.com/png.latex?w_1">, and <img src="https://latex.codecogs.com/png.latex?w_2"> are the model parameters that control the intercept and the weights of the features. This linear form can be written in matrix form as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cvarphi(x)=%5Cmathbf%20x%5Cmathbf%20w=%5Cmathbf%20w%20%5Cmathbf%20x%5ET%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bw%7D%5D"> is a row vector of the model parameters and <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bx%7D%5D"> is a row vector of the features.</p>
<p>Linear forms can also be used in deep learning and machine learning models that involve linear transformations, such as fully connected layers in neural networks or linear classifiers. For example, consider a simple linear classifier that classifies images of digits into one of 10 classes. The classifier can be represented by the linear form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cvarphi(x)=%5Cmathbf%20x%5Cmathbf%20w%20+%20b%20=%5Cmathbf%20w%20%5Cmathbf%20x%5ET%20+b%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cvarphi(%5Cmathbf%7Bx%7D)"> is the predicted class score, <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bx%7D%5D"> is a row vector of the pixel values of the image, <img src="https://latex.codecogs.com/png.latex?%5B%5Cmathbf%7Bw%7D%5D"> is a row vector of the weights of the classifier, and <img src="https://latex.codecogs.com/png.latex?b"> is the bias term. This linear form can be used to classify the image by selecting the class with the highest score.</p>
<p>In both of these examples, linear forms are used to represent linear relationships between variables or features, and the model parameters are learned through training on a set of labeled examples.</p>


</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.linear_form.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Matrix Transformation (5) - Quadratic Form</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.matrix_transformation.html</link>
  <description><![CDATA[ 



<section id="matrix" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="matrix"><span class="header-section-number">1</span> Matrix</h2>
<p>A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix, it can be represented as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%20%20a_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7B1n%7D%20%5C%5C%0A%20%20a_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7B2n%7D%20%5C%5C%0A%20%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A%20%20a_%7Bm1%7D%20&amp;%20a_%7Bm2%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7Bmn%7D%0A%5Cend%7Bbmatrix%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> is the element in the <img src="https://latex.codecogs.com/png.latex?i">-th row and <img src="https://latex.codecogs.com/png.latex?j">-th column of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
</section>
<section id="basic-matrix-operations" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="basic-matrix-operations"><span class="header-section-number">2</span> Basic Matrix Operations</h2>
<section id="matrix-addition" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="matrix-addition"><span class="header-section-number">2.1</span> Matrix addition</h3>
<p>The sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.</p>
<p>Given two <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D">, their sum <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D%20=%20%5Cmathbf%7BA%7D%20+%20%5Cmathbf%7BB%7D"> is defined by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ac_%7Bi,j%7D=a_%7Bi,j%7D+b_%7Bi,j%7D%E2%80%8B%0A"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%20m"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20j%20%5Cleq%20n">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%20%201%20&amp;%202%20%5C%5C%0A%20%203%20&amp;%204%20%5C%5C%0A%20%205%20&amp;%206%0A%5Cend%7Bbmatrix%7D%20+%0A%5Cbegin%7Bbmatrix%7D%0A%20%20-1%20&amp;%200%20%5C%5C%0A%20%202%20&amp;%20-3%20%5C%5C%0A%20%20-5%20&amp;%204%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A%20%200%20&amp;%202%20%5C%5C%0A%20%205%20&amp;%201%20%5C%5C%0A%20%200%20&amp;%2010%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="scalar-multiplication" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="scalar-multiplication"><span class="header-section-number">2.2</span> Scalar multiplication</h3>
<p>The product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.</p>
<p>Given a scalar <img src="https://latex.codecogs.com/png.latex?k"> and an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, their product <img src="https://latex.codecogs.com/png.latex?k%5Cmathbf%7BA%7D"> is defined by: <img src="https://latex.codecogs.com/png.latex?%0A(k%5Cmathbf%7BA%7D)_%7Bi,j%7D%20=%20k(a_%7Bi,j%7D)%0A"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%20m"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20j%20%5Cleq%20n">.</p>
<p>Example: <img src="https://latex.codecogs.com/png.latex?%0A2%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%20%5C%5C%0A5%20&amp;%206%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A2%20&amp;%204%20%5C%5C%0A6%20&amp;%208%20%5C%5C%0A10%20&amp;%2012%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="matrix-multiplication" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="matrix-multiplication"><span class="header-section-number">2.3</span> Matrix multiplication</h3>
<p>The product of two matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D"> is a matrix obtained by multiplying the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> by the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D">.</p>
<p>Given two matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D"> with dimensions <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> and <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20p">, respectively, their product <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D%20=%20%5Cmathbf%7BAB%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20p"> matrix defined by: <img src="https://latex.codecogs.com/png.latex?%0Ac_%7Bi,j%7D%20=%20%5Csum_%7Bk=1%7D%5En%20a_%7Bi,k%7Db_%7Bk,j%7D%0A"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%20m"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20j%20%5Cleq%20p">.</p>
<p>Example: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%20%5C%5C%0A5%20&amp;%206%0A%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%0A-1%20&amp;%200%20&amp;%202%20%5C%5C%0A2%20&amp;%20-3%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A3%20&amp;%20-6%20&amp;%204%20%5C%5C%0A5%20&amp;%20-12%20&amp;%2010%20%5C%5C%0A7%20&amp;%20-18%20&amp;%2016%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="transpose" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="transpose"><span class="header-section-number">2.4</span> Transpose</h3>
<p>The transpose of an <img src="https://latex.codecogs.com/png.latex?m%20x%20n"> matrix A, denoted by <img src="https://latex.codecogs.com/png.latex?A%5ET">, is the <img src="https://latex.codecogs.com/png.latex?n%20x%20m"> matrix obtained by interchanging the rows and columns of A. Formally, if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Ba_%7Bij%7D%5D"> is an m x n matrix, then its transpose <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET%20=%20%5Bb_%7Bij%7D%5D"> is an <img src="https://latex.codecogs.com/png.latex?n%20x%20m"> matrix where <img src="https://latex.codecogs.com/png.latex?b_%7Bij%7D"> = <img src="https://latex.codecogs.com/png.latex?a_%7Bji%7D"> for all <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j">. In other words, the element in the <img src="https://latex.codecogs.com/png.latex?i"> th row and <img src="https://latex.codecogs.com/png.latex?j"> th column of <img src="https://latex.codecogs.com/png.latex?A%5ET"> is equal to the element in the <img src="https://latex.codecogs.com/png.latex?j"> th row and ith column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>Given an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, its transpose <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m"> matrix defined by: $$ <em>{i,j}^T = </em>{j,i}</p>
<p>$$</p>
<ul>
<li>When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is transposed, diagnoal entries(<img src="https://latex.codecogs.com/png.latex?a_%7Bii%7D">) do not change but off-diagnoal elements(<img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D%20%5C;%20i%20%5Cneq%20j">) change.</li>
<li>A column vector is tranposed into a row vector, and vice versa.</li>
<li>symmetric matrix: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BA%7D%5ET"></li>
</ul>
<p>Example:</p>
<p>Let A be the matrix <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%5C%5C%0A4%20&amp;%205%20&amp;%206%0A%5Cend%7Bbmatrix%7D%0A"> The transpose of A, denoted by A^T, is the matrix <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5ET%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%204%5C%5C%0A2%20&amp;%205%5C%5C%0A3%20&amp;%206%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="determinant" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="determinant"><span class="header-section-number">2.5</span> Determinant</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> square matrix. The determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted by <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7BA%7D%7C"> or <img src="https://latex.codecogs.com/png.latex?%5Cdet(%5Cmathbf%7BA%7D)">, is a scalar value calculated as the sum of the products of the elements in any row or column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> with their corresponding cofactors, that is,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%5Cmathbf%7BA%7D%7C=%5Csum_%7Bi=1%7D%5E%7Bn%7Da_%7Bij%7DC_%7Bij%7D=%5Csum_%7Bj=1%7D%5E%7Bn%7Da_%7Bij%7DC_%7Bij%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> is the element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> in the <img src="https://latex.codecogs.com/png.latex?i">-th row and <img src="https://latex.codecogs.com/png.latex?j">-th column, and <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D"> is the cofactor of <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D">. The cofactor of <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D">, denoted by <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D">, is given by <img src="https://latex.codecogs.com/png.latex?(-1)%5E%7Bi+j%7D"> times the determinant of the <img src="https://latex.codecogs.com/png.latex?(n-1)%20%5Ctimes%20(n-1)"> matrix obtained by deleting the <img src="https://latex.codecogs.com/png.latex?i">-th row and <img src="https://latex.codecogs.com/png.latex?j">-th column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>The determinant of an <img src="https://latex.codecogs.com/png.latex?n%20x%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a scalar value denoted as <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7BA%7D%7C">. It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a <img src="https://latex.codecogs.com/png.latex?3%20x%203"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20="> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20a_%7B13%7D%20%5C%5C%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B32%7D%20&amp;%20a_%7B33%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%5Cmathbf%7BA%7D%7C%20=%20a_%7B11%7D%0A%5Cbegin%7Bvmatrix%7D%0Aa_%7B22%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B32%7D%20&amp;%20a_%7B33%7D%0A%5Cend%7Bvmatrix%7D%0A-%20a_%7B12%7D%0A%5Cbegin%7Bvmatrix%7D%0Aa_%7B21%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B33%7D%0A%5Cend%7Bvmatrix%7D%0A+%20a_%7B13%7D%0A%5Cbegin%7Bvmatrix%7D%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B32%7D%0A%5Cend%7Bvmatrix%7D%0A"></p>
<p>For example, consider the <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20&amp;%203%20%5C%5C%204%20&amp;%205%20&amp;%206%20%5C%5C%207%20&amp;%208%20&amp;%209%20%5Cend%7Bbmatrix%7D">. We can calculate the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> using any row or column. Let’s use the first column:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%5Cmathbf%7BA%7D%7C%20=%201%0A%5Cbegin%7Bvmatrix%7D%0A5%20&amp;%206%20%5C%5C%0A8%20&amp;%209%0A%5Cend%7Bvmatrix%7D%0A-%204%0A%5Cbegin%7Bvmatrix%7D%0A2%20&amp;%203%5C%5C%0A8%20&amp;%209%0A%5Cend%7Bvmatrix%7D%0A+%207%0A%5Cbegin%7Bvmatrix%7D%0A2%20&amp;%205%20%5C%5C%0A3%20&amp;%206%0A%5Cend%7Bvmatrix%7D%20=%200%0A"></p>
<p>Therefore, the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is zero.</p>
</section>
<section id="inverse" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="inverse"><span class="header-section-number">2.6</span> Inverse</h3>
<p>The inverse of a square matrix <img src="https://latex.codecogs.com/png.latex?A"> of size <img src="https://latex.codecogs.com/png.latex?n"> is a matrix <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> such that the product of <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> is the identity matrix <img src="https://latex.codecogs.com/png.latex?I_n">, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?A%20%5Ctimes%20A%5E%7B-1%7D%20=%20I_n">. If such a matrix exists, then <img src="https://latex.codecogs.com/png.latex?A"> is said to be invertible or non-singular.</p>
<p>The inverse of a square matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E%7B-1%7D"> and is defined as the unique matrix that satisfies the following equation: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BI%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D"> is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.</p>
<p>For example, consider the <img src="https://latex.codecogs.com/png.latex?2%5Ctimes%202"> matrix <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%0A%5Cend%7Bbmatrix%7D.%0A"> The inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%0A%5Cfrac%7B1%7D%7B-2%7D%0A%5Cbegin%7Bbmatrix%7D%0A4%20&amp;%20-2%20%5C%5C%0A-3%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A-2%20&amp;%201%20%5C%5C%0A%5Cfrac%7B3%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>We can verify that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BI%7D"> by computing:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A-2%20&amp;%201%20%5C%5C%0A%5Cfrac%7B3%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20%5C%5C%0A0%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7BI%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%0A%5Cbegin%7Bbmatrix%7D%20-2%20&amp;%201%20%5C%5C%0A%5Cfrac%7B3%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20%5C%5C%0A0%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7BI%7D%0A"></p>
<p>Let me give another example and A be a <img src="https://latex.codecogs.com/png.latex?3x3"> square matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20a_%7B13%7D%20%5C%5C%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B32%7D%20&amp;%20a_%7B33%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"> Then, the inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E%7B-1%7D">, is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cfrac%7B1%7D%7B%5Ctext%7Bdet%7D(%5Cmathbf%7BA%7D)%7D%5Cbegin%7Bbmatrix%7D%0Aa_%7B22%7Da_%7B33%7D-a_%7B23%7Da_%7B32%7D%20&amp;%20a_%7B13%7Da_%7B32%7D-a_%7B12%7Da_%7B33%7D%20&amp;%20a_%7B12%7Da_%7B23%7D-a_%7B13%7Da_%7B22%7D%20%5C%5C%0Aa_%7B23%7Da_%7B31%7D-a_%7B21%7Da_%7B33%7D%20&amp;%20a_%7B11%7Da_%7B33%7D-a_%7B13%7Da_%7B31%7D%20&amp;%20a_%7B13%7Da_%7B21%7D-a_%7B11%7Da_%7B23%7D%20%5C%5C%0Aa_%7B21%7Da_%7B32%7D-a_%7B22%7Da_%7B31%7D%20&amp;%20a_%7B12%7Da_%7B31%7D-a_%7B11%7Da_%7B32%7D%20&amp;%20a_%7B11%7Da_%7B22%7D-a_%7B12%7Da_%7B21%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?det(%5Cmathbf%7BA%7D)"> is the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>For example, let:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%20%5C%5C%0A0%20&amp;%201%20&amp;%204%20%5C%5C%0A5%20&amp;%206%20&amp;%200%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"> Then, <img src="https://latex.codecogs.com/png.latex?det(A)"> = -57, and the inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cfrac%7B1%7D%7B-57%7D%5Cbegin%7Bbmatrix%7D%0A-24%20&amp;%2018%20&amp;%205%20%5C%5C%0A20%20&amp;%20-15%20&amp;%20-4%20%5C%5C%0A-3%20&amp;%202%20&amp;%201%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>There are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.</p>
</section>
<section id="rank" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="rank"><span class="header-section-number">2.7</span> Rank</h3>
<p>The rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Brank%7D(%5Cmathbf%7BA%7D)">.</p>
<p>For example, consider the following matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%20%20%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%201%20&amp;%202%20&amp;%203%5C%5C%0A%20%20%20%204%20&amp;%205%20&amp;%206%20%5C%5C%0A%20%20%20%207%20&amp;%208%20&amp;%209%5C%5C%0A%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>The columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is 2.</p>
</section>
<section id="trace" class="level3" data-number="2.8">
<h3 data-number="2.8" class="anchored" data-anchor-id="trace"><span class="header-section-number">2.8</span> Trace</h3>
<p>The trace of a square matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Btr%7D(%5Cmathbf%7BA%7D)">, is defined as the sum of the diagonal elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. In other words, if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> matrix, then its trace is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathrm%7Btr%7D(%5Cmathbf%7BA%7D)=%5Csum_%7Bi=1%7D%5E%7Bn%7Da_%7Bij%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?a_%7Bii%7D"> denotes the <img src="https://latex.codecogs.com/png.latex?i"> th diagonal element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%20%202%20&amp;%203%20&amp;%201%20%5C%5C%0A%20%200%20&amp;%205%20&amp;%202%20%5C%5C%0A%20%201%20&amp;%201%20&amp;%204%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>Then, the trace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Btr%7D(%5Cmathbf%7BA%7D)%20=%202%20+%205%20+%204%20=%2011"></p>
</section>
<section id="eigenvalues-and-eigenvectors" class="level3" data-number="2.9">
<h3 data-number="2.9" class="anchored" data-anchor-id="eigenvalues-and-eigenvectors"><span class="header-section-number">2.9</span> Eigenvalues and Eigenvectors</h3>
<p>Let A be an <img src="https://latex.codecogs.com/png.latex?n%20%C3%97%20n"> square matrix. A scalar <img src="https://latex.codecogs.com/png.latex?%5Clambda"> is called an eigenvalue of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> if there exists a non-zero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> such that <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BAv%7D=%5Clambda%5Cmathbf%7Bv%7D%0A"></p>
<p>Such a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> is called an eigenvector corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda">.</p>
<p>Example:</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be the matrix</p>
<p>To find the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, we solve the characteristic equation <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdet%7D(%5Cmathbf%20A%20-%20%5Clambda%20%5Cmathbf%20I%20)%20=%200">, where I is the n × n identity matrix.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%5Ctext%7Bdet%7D(%5Cmathbf%20A%20-%20%5Clambda%20%5Cmathbf%20I%20)%0A%20%20&amp;=%0A%20%20%20%20%5Cbegin%7Bvmatrix%7D%0A%20%20%20%203%20-%20%5Clambda%20&amp;%201%20%5C%5C%0A%20%20%20%201%20&amp;%203%20-%20%5Clambda%0A%20%20%20%20%5Cend%7Bvmatrix%7D%20%5C%5C%0A%20%20&amp;=%0A%20%20(3%20-%20%5Clambda)(3%20-%20%5Clambda)%20-%201%20%5C%5C%0A%20%20&amp;=%20%5Clambda%5E2%20-%206%5Clambda%20+%208%20=%200%0A%5Cend%7Balign*%7D%0A"></p>
<p>Solving this quadratic equation gives us the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">: <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202"> and <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">.</p>
<p>To find the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202">, we solve the equation <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%20A%20-%202%20%5Cmathbf%20I)%5Cmathbf%7Bv%7D%20=%20%5Cmathbf%7B0%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20I"> is the <img src="https://latex.codecogs.com/png.latex?2%20%5Ctimes%202"> identity matrix.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20(%5Cmathbf%20A%20-%202%20%5Cmathbf%20I)%5Cmathbf%7Bv%7D%20=%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%201%20&amp;%201%20%5C%5C%0A%20%20%20%20%20%201%20&amp;%201%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20x%20%5C%5C%0A%20%20%20%20y%0A%20%20%5Cend%7Bbmatrix%7D%20=%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%200%20%5C%5C%0A%20%20%20%200%0A%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p>Solving this system of equations gives us the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202">: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_1%7D%20=%20%5Cbegin%7Bbmatrix%7D%20-1%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"></p>
<p>Similarly, for <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">, we solve the equation <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%20A%20-%204%5Cmathbf%20I)%5Cmathbf%7Bv%7D"> = <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D"> to get the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_2%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"></p>
</section>
<section id="singular-value-and-singluar-vectors" class="level3" data-number="2.10">
<h3 data-number="2.10" class="anchored" data-anchor-id="singular-value-and-singluar-vectors"><span class="header-section-number">2.10</span> Singular value and Singluar Vectors</h3>
<p>The singular value decomposition (SVD) of a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is a factorization of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> into the product of three matrices as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BU%7D%20%5Cmathbf%7B%5CSigma%7D%20%5Cmathbf%7BV%7D%5ET%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BU%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20m"> orthogonal matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CSigma%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> rectangular diagonal matrix with non-negative real numbers on the diagonal, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> orthogonal matrix.</p>
<p>The diagonal entries of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CSigma%7D"> are called the singular values of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Csigma_1,%20%5Csigma_2,%20%5Cldots,%20%5Csigma_r"> (where <img src="https://latex.codecogs.com/png.latex?r"> is the rank of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">), and are arranged in descending order. The columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BU%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D"> are called the left and right singular vectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, respectively, and are orthonormal vectors.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be a 3 by 2 matrix given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%201%20&amp;%202%5C%5C%0A%20%20%20%203%20&amp;%204%5C%5C%0A%20%20%20%205%20&amp;%206%0A%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>The SVD of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BU%7D%20%5Cmathbf%7B%5CSigma%7D%20%5Cmathbf%7BV%7D%5ET%20=%0A%5Cbegin%7Bbmatrix%7D%0A%20%20-0.23%20&amp;%20-0.53%20&amp;%20-0.81%5C%5C%0A%20%20-0.53%20&amp;%20-0.72%20&amp;%200.45%5C%5C%0A%20%20-0.81%20&amp;%200.45%20&amp;%20-0.38%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%209.53%20&amp;%200%5C%5C%0A%20%200%20&amp;%200.90%5C%5C%0A%20%200%20&amp;%200%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20-0.62%20&amp;%20-0.78%5C%5C%0A%20%20-0.78%20&amp;%200.62%0A%5Cend%7Bbmatrix%7D%5ET%0A%5Cend%7Bequation*%7D%0A"></p>
<p>where the left singular vectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BU%7D">, the right singular vectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D">, and the singular values of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are the diagonal entries of <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%7B%5CSigma%7D">.</p>
<ul>
<li>연립 방정식을 행렬의 곱으로 나타내보기 <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bmatrix%7Dx_1+2y_1=4%5C%5C2x_1+5y_1=9%5Cend%7Bmatrix%7D%20%5Cquad%20%5Cquad%20%5Cquad%20%5Cbegin%7Bmatrix%7Dx_2+2y_2=3%5C%5C2x_2+5y_2=7%5Cend%7Bmatrix%7D"> <img src="https://latex.codecogs.com/png.latex?%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%202%20&amp;%205%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5C%5C%20y_1%20&amp;%20y_2%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%204%20&amp;%209%20%5C%5C%203%20&amp;%207%20%5Cend%7Bbmatrix%7D"></li>
<li>중요한 사실(….당연한 사실?)
<ul>
<li>곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bm%20%5Ctimes%20n%7D%20%5Ctimes%20B_%7Bo%20%5Ctimes%20p%7D"> 에서 <img src="https://latex.codecogs.com/png.latex?n%20=%20o"> 여야 곱셈 성립</li>
</ul></li>
<li>곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 <img src="https://latex.codecogs.com/png.latex?%5Ctimes"> 곱셈의 오른쪽 행렬의 열 수
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bm%20%5Ctimes%20n%7D%20%5Ctimes%20B_%7Bo%20%5Ctimes%20p%7D%20=%20C_%7Bm%20%5Ctimes%20p%7D"></li>
</ul></li>
<li>교환법칙(Commutative property)이 성립하지 않음
<ul>
<li><img src="https://latex.codecogs.com/png.latex?AB%20%5Cneq%20BA"></li>
</ul></li>
</ul></li>
<li>행렬 곱셈의 여러가지 관점
<ul>
<li>내적으로 바라보기 <img src="https://latex.codecogs.com/png.latex?%20A%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%5ET%7D%20%5C%5C%20%5Cmathbf%7Ba_2%5ET%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cmathbf%7Ba_m%5ET%7D%20%5Cend%7Bbmatrix%7D%20"> <img src="https://latex.codecogs.com/png.latex?%20AB%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%5ET%7D%20%5C%5C%20%5Cmathbf%7Ba_2%5ET%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cmathbf%7Ba_m%5ET%7D%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Bb_1%7D%20&amp;%20%5Cmathbf%7Bb_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Bb_m%7D%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%5ET%20b_1%7D%20&amp;%20%5Cmathbf%7Ba_1%5ET%20b_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Ba_1%5ET%20b_m%7D%20%5C%5C%20%5Cmathbf%7Ba_2%5ET%20b_1%7D%20&amp;%20%5Cmathbf%7Ba_2%5ET%20b_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Ba_2%5ET%20b_m%7D%20%5C%5C%20%5Cvdots%20%20&amp;%20%5Cvdots%20%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%20%5Cmathbf%7Ba_m%5ET%20b_1%7D%20&amp;%20%5Cmathbf%7Ba_m%5ET%20b_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Ba_m%5ET%20b_m%7D%20%5Cend%7Bbmatrix%7D"></li>
<li>rank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) <img src="https://latex.codecogs.com/png.latex?AB%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%7D%20&amp;%20%5Cmathbf%7Ba_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Ba_m%7D%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Bb_1%5ET%7D%20%5C%5C%20%5Cmathbf%7Bb_2%5ET%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cmathbf%7Bb_m%5ET%7D%20%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7Ba_1%20b_1%5ET%7D%20+%20%5Cmathbf%7Ba_2%20b_2%5ET%7D%20+%20%5Ccdots%20+%20%5Cmathbf%7Ba_m%20b_m%5ET%7D"></li>
<li>column space로 바라보기 <img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%7D%20&amp;%20%5Cmathbf%7Ba_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Ba_m%7D%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5C%5C%20%5Cvdots%20%5C%5C%20x_m%20%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7Ba_1%7D%20x_1%20+%20%5Cmathbf%7Ba_2%7D%20x_2%20+%20%5Ccdots%20+%20%5Cmathbf%7Ba_m%7D%20x_m%20"> (스칼라배의 합)
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%200%20%5C%5C%200%20&amp;%201%20%5Cend%7Bbmatrix%7D"> 는 2차원 좌표평면의 모든 점을, <img src="https://latex.codecogs.com/png.latex?A=%5Cbegin%7Bbmatrix%7D%201%20&amp;%200%20&amp;%200%20%5C%5C%200%20&amp;%201%20&amp;%200%20%5C%5C%200%20&amp;%200%20&amp;%201%20%5Cend%7Bbmatrix%7D">은 3차원 좌표평면의 모든 점 표현 가능</li>
<li><img src="https://latex.codecogs.com/png.latex?AB%20=%20A%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Bb_1%7D%20&amp;%20%5Cmathbf%7Bb_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Bb_m%7D%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20A%20%5Cmathbf%7Bb_1%7D%20&amp;%20A%20%5Cmathbf%7Bb_2%7D%20&amp;%20%5Ccdots%20&amp;%20A%20%5Cmathbf%7Bb_m%7D%20%5Cend%7Bbmatrix%7D"></li>
<li>column space: A의 column vector로 만들 수 있는 부분 공간</li>
</ul></li>
<li>row space로 바라보기 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%5ET%7DA%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20&amp;%20%5Ccdots%20&amp;%20x_m%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%5ET%7D%20%5C%5C%20%5Cmathbf%7Ba_2%5ET%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cmathbf%7Ba_m%5ET%7D%20%5Cend%7Bbmatrix%7D%20=%20x_1%20%5Cmathbf%7Ba_1%5ET%7D%20+%20x_2%20%5Cmathbf%7Ba_2%5ET%7D%20+%20%5Ccdots%20+%20x_m%20%5Cmathbf%7Ba_m%5ET%7D%20"></li>
</ul></li>
</ul>
</section>
</section>
<section id="열공간column-space" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="열공간column-space"><span class="header-section-number">3</span> 열공간(Column Space)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/g0eaDeVRdZk" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li>column space: column vector 들이 span 하는 space
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A">의 column space = <img src="https://latex.codecogs.com/png.latex?C(A)"> 또는 <img src="https://latex.codecogs.com/png.latex?range(A)"></li>
</ul></li>
<li>span: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합
<ul>
<li>vector에 따라, 점일수도 선일수도 평면일 수도 있음</li>
<li>vector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space</li>
</ul></li>
<li>vector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_1%7D"> 과 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_2%7D">의 linear combination으로 2차원 좌표평면 나타내기 <img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/images/chap02_09.PNG" class="img-fluid"></li>
</ul></li>
</ul>
</section>
<section id="선형-독립linear-independent" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="선형-독립linear-independent"><span class="header-section-number">4</span> 선형 독립(Linear Independent)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/mOOI4-BfjGQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>…and also see</p>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/9F4PZ_1orF0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li>선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌</li>
<li>orthogonal 하면 independent
<ul>
<li>but independent해도 항상 orthogonal하지는 않음 (Independent &gt; Orthogonal)</li>
</ul></li>
<li>definition: <img src="https://latex.codecogs.com/png.latex?a_1%20%5Cmathbf%7Bv_1%7D%20+%20a_2%20%5Cmathbf%7Bv_2%7D%20+%20a_3%20%5Cmathbf%7Bv_3%7D%20%5Ccdots%20a_n%20%5Cmathbf%7Bv_n%7D%20=%20%5Cmathbf%7B0%7D"> 를 만족하는 <img src="https://latex.codecogs.com/png.latex?a_1,%20a_2,%20a_3,%20%5Ccdots%20a_n"> 이 <img src="https://latex.codecogs.com/png.latex?a_1%20=%20a_2%20=%20a_3%20=%20%5Ccdots%20=%20a_n%20=%200"> 밖에 없을때
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D">는 모든 elements가 <img src="https://latex.codecogs.com/png.latex?0">인 벡터</li>
<li>예: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%202%20%5C%5C%202%20%5Cend%7Bbmatrix%7D"> 는 <img src="https://latex.codecogs.com/png.latex?-2%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%20+%201%20%5Cbegin%7Bbmatrix%7D%202%20%5C%5C%202%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%200%20%5Cend%7Bbmatrix%7D"> 이 되므로, linearly independent 하지 않음</li>
<li>independent한 vector 들의 수 = 표현할 수 있는 차원의 dimension</li>
</ul></li>
</ul>
</section>
<section id="기저basis" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="기저basis"><span class="header-section-number">5</span> 기저(basis)</h2>
<ul>
<li>주어진 vector space를 span하는 linearly independent한 vectors</li>
<li>어떤 공간을 이루는 필수적인 구성요소</li>
<li>orthogonal 하면 orthogonal basis</li>
<li>예: 2차원 좌표평면에 대해
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5Cend%7Bbmatrix%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"> : orthogonal basis</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5Cend%7Bbmatrix%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"> : orthogonal 하지 않은 basis</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5Cend%7Bbmatrix%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%202%20%5C%5C%200%20%5Cend%7Bbmatrix%7D"> : linearly independent 하지 않으므로 basis 아님</li>
</ul></li>
</ul>
</section>
<section id="항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix"><span class="header-section-number">6</span> 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/XqOvyfMUAwA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="identity-matrix항등행렬" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="identity-matrix항등행렬"><span class="header-section-number">6.1</span> Identity matrix(항등행렬)</h3>
<ul>
<li>항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)
<ul>
<li>실수에서 곱셈의 항등원은 1</li>
</ul></li>
<li>행렬의 항등원: 항등행렬(<img src="https://latex.codecogs.com/png.latex?I">) <img src="https://latex.codecogs.com/png.latex?I%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%200%20&amp;%201%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%20%5Cvdots%20%20&amp;%20%5Cvdots%20%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%200%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%201%20%5Cend%7Bbmatrix%7D">
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bm%20%5Ctimes%20n%7D%20%5Ctimes%20I_%7Bn%20%5Ctimes%20n%20=%20n%7D%20=%20A_%7Bm%20%5Ctimes%20n%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?I_%7Bm%20%5Ctimes%20m%20=%20m%7D%20%5Ctimes%20A_%7Bm%20%5Ctimes%20n%7D%20=%20A_%7Bm%20%5Ctimes%20n%7D"></li>
</ul></li>
</ul>
</section>
<section id="sec-inv" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="sec-inv"><span class="header-section-number">6.2</span> Inverse matrix(역행렬)</h3>
<ul>
<li>역원: 연산 결과 항등원이 나오게 하는 연소
<ul>
<li>실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): <img src="https://latex.codecogs.com/png.latex?a%20%5Ctimes%20a%5E%7B-1%7D%20=%201"></li>
</ul></li>
<li>행렬의 역원: 역행렬(<img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D">) <img src="https://latex.codecogs.com/png.latex?A%20%5Ctimes%20A%5E%7B-1%7D%20=%20I%20,%20A%5E%7B-1%7D%20%5Ctimes%20A%20=%20I">
<ul>
<li>존재하지 않는 경우도 있음</li>
<li>존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림
<ul>
<li>존재하지 않으면 singular, degenerate라고 불림</li>
</ul></li>
<li>square matrix(정사각행렬, <img src="https://latex.codecogs.com/png.latex?m%20=%20n">)은 특수한 경우를 제외하면 역행렬이 항상 존재
<ul>
<li>역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우</li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?m%20%5Cneq%20n">인 행렬의 경우에는 역행렬이 존재하지 않음
<ul>
<li>다만, 경우에 따라 <img src="https://latex.codecogs.com/png.latex?A%20%5Ctimes%20A%5E%7B-1%7D%20=%20I"> 를 만족하거나(right inverse), <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D%20%5Ctimes%20A%20=%20I">를 만족하는(left inverse)는 <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D">이 존재함</li>
</ul></li>
<li>연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 <img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D%20=%20%5Cmathbf%7Bb%7D%20%5CRightarrow%20A%5E%7B-1%7DA%5Cmathbf%7Bx%7D%20=%20A%5E%7B-1%7D%5Cmathbf%7Bb%7D%20%5CRightarrow%20I%5Cmathbf%7Bx%7D%20=%20A%5E%7B-1%7D%5Cmathbf%7Bb%7D%20%5CRightarrow%20%5Cmathbf%7Bx%7D%20=%20A%5E%7B-1%7D%5Cmathbf%7Bb%7D"></li>
</ul></li>
</ul>
</section>
<section id="diagonal-matrix대각행렬" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="diagonal-matrix대각행렬"><span class="header-section-number">6.3</span> Diagonal Matrix(대각행렬)</h3>
<ul>
<li>diagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix <img src="https://latex.codecogs.com/png.latex?%20D%20=%20Diag(%5Cmathbf%7Ba%7D)%20=%20%5Cbegin%7Bbmatrix%7D%20a_%7B1,1%7D%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%200%20&amp;%20a_%7B2,2%7D%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%20%5Cvdots%20%20&amp;%20%5Cvdots%20%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%200%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%20a_%7Bn,n%7D%20%5Cend%7Bbmatrix%7D">
<ul>
<li>identity matrix는 diagonal matrix</li>
<li>diagnomal matrix는 symmetric matrix 이기도 함</li>
<li>보통은 square matrix에서 주로 사용됨
<ul>
<li>square matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="orthogonal-matrix직교행렬-orthonomal-matrix" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="orthogonal-matrix직교행렬-orthonomal-matrix"><span class="header-section-number">6.4</span> Orthogonal matrix(직교행렬, orthonomal matrix)</h3>
<ul>
<li>행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) <img src="https://latex.codecogs.com/png.latex?A%20A%5ET%20=%20A%5ET%20A%20=%20I"></li>
<li>identity matrix는 orthogonal matrix</li>
<li>square matrix에서만 정의됨</li>
<li>Orthogonal matrix인 <img src="https://latex.codecogs.com/png.latex?A">이면 <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D%20=%20A%5E%7BT%7D">
<ul>
<li>각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임</li>
</ul></li>
<li>complex matrix(복소수 행렬)에서는 unitary matrix라고 부름</li>
</ul>
</section>
</section>
<section id="계수rank" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="계수rank"><span class="header-section-number">7</span> 계수(Rank)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/HMST0Yc7EXE" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li>rank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension</li>
<li>independent한 column의 수 = independent한 행의 수: <img src="https://latex.codecogs.com/png.latex?rank(A)%20=%20rank(A%5ET)">
<ul>
<li>proof: <a href="https://en.wikipedia.org/wiki/Rank_%28linear_algebra%29#Proofs_that_column_rank_=_row_rank">Wikipedia</a></li>
</ul></li>
<li>예: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20&amp;%203%20%5C%5C%200%20&amp;%200%20&amp;%200%20%5Cend%7Bbmatrix%7D%20%5CRightarrow%20rank=1"> <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20&amp;%200%20&amp;%201%20%5C%5C%200%20&amp;%201%20&amp;%201%20%5Cend%7Bbmatrix%7D%20%5CRightarrow%20rank=2"></li>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bm%20%5Ctimes%20n%7D"> 의 최대 랭크는 <img src="https://latex.codecogs.com/png.latex?min%5C%7Bm,n%5C%7D">
<ul>
<li><img src="https://latex.codecogs.com/png.latex?rank(A)%20%3C%20min%5C%7Bm,n%5C%7D"> 면 rank-deficient, <img src="https://latex.codecogs.com/png.latex?rank(A)%20=%20min%5C%7Bm,n%5C%7D">면 full (row/column) rank</li>
</ul></li>
</ul>
</section>
<section id="영공간null-space" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="영공간null-space"><span class="header-section-number">8</span> 영공간(Null space)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/Eizc9TSRYMQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D=%20%5Cmathbf%7B0%7D"> 을 만족하는 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">의 집합
<ul>
<li>column space 관점에서 보기: <img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D%20=%20x_1%20%5Cmathbf%7Ba_1%7D%20+%20x_2%20%5Cmathbf%7Ba_2%7D%20+%20%5Ccdots%20+%20x_n%20%5Cmathbf%7Ba_n%7D%20=%20%5Cmathbf%7B0%7D"></li>
<li>null space에 항상 들어가는 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%200%20%5C%5C%20%5Cvdots%20%5C%5C%200%20%5Cend%7Bbmatrix%7D"> : trivial solution
<ul>
<li>모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D=%5Cmathbf%7B0%7D">하나 밖에 없음</li>
</ul></li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D=%5Cmathbf%7B0%7D"> 가 아닌 vector가 null space에 있으면, 스칼라배(constant <img src="https://latex.codecogs.com/png.latex?c">에 대해 <img src="https://latex.codecogs.com/png.latex?c%20%5Cmathbf%7Bx%7D">) 역시 null space에 포함됨</li>
<li>혼동 주의! null space는 column space의 일부가 아님
<ul>
<li>row vector의 차원이 null space가 존재하는 공간</li>
</ul></li>
<li>rank와 null space의 dimension의 합은 항상 matrix의 column의 수
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bm%20%5Ctimes%20n%7D">에 대해, <img src="https://latex.codecogs.com/png.latex?dim(N(A))%20=%20n%20-%20r">
<ul>
<li>모든 columns이 다 lienarly independent 하면 null space는 0차원(점)</li>
</ul></li>
<li>null space는 row space와 수직한 space
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D=%20%5Cmathbf%7B0%7D"> : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0</li>
<li>rank는 row space의 dimension → row space의 dimension(<img src="https://latex.codecogs.com/png.latex?dim(R(A))">)과 null space의 dimension(<img src="https://latex.codecogs.com/png.latex?dim(N(A))">)의 합이 <img src="https://latex.codecogs.com/png.latex?n"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%5En%7D"> 공간에 표현: <img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/images/chap02_10.PNG" class="img-fluid">
<ul>
<li>겹친 점: 영벡터</li>
</ul></li>
</ul></li>
</ul></li>
<li>left null space: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%5ET%7D%20A%20=%20%5Cmathbf%7B0%5ET%7D"> 인 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">
<ul>
<li>위의 성질을 row에 대해 적용
<ul>
<li>m 차원에 놓인 벡터</li>
<li>dimension: <img src="https://latex.codecogs.com/png.latex?dim(N_L(A))%20=%20m%20-%20r"></li>
<li>column space와 수직: <img src="https://latex.codecogs.com/png.latex?dim(N_L(A))%20+dim(C(A))%20=%20m"></li>
</ul></li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?R(A)">에 있는 vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_r%7D"> 와 <img src="https://latex.codecogs.com/png.latex?N(A)">에 있는 vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_n%7D">에 대해:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_r%7D">에 <img src="https://latex.codecogs.com/png.latex?A">를 곱하면 column space로 감</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_n%7D">에 <img src="https://latex.codecogs.com/png.latex?A">를 곱하면 $</li>
<li><img src="https://latex.codecogs.com/png.latex?A(%5Cmathbf%7Bx_r%7D%20+%5Cmathbf%7Bx_n%7D)%20=%20A%5Cmathbf%7Bx_r%7D%20+%20A%5Cmathbf%7Bx_n%7D%20=%20A%5Cmathbf%7Bx_r%7D%20=%20%5Cmathbf%7Bb%7D"></li>
</ul></li>
</ul>
</section>
<section id="ax-b의-해의-수" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="ax-b의-해의-수"><span class="header-section-number">9</span> Ax = b의 해의 수</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/nNI2TlD598c" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li><p>full column rank 일때</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">가 column space(<img src="https://latex.codecogs.com/png.latex?C(A)">)안에 있으면 해가 하나</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">가 column space(<img src="https://latex.codecogs.com/png.latex?C(A)">)안에 없으면 해가 없음 <img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/images/chap02_11.PNG" class="img-fluid"></li>
</ul></li>
<li><p>full row rank 일때</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">는 항상 column space 안에 있음: 무한의 해를 가짐</li>
<li>임의의 특정한 해(particular solution) <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_p%7D">와 null space의 vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_n%7D">에 대해, <img src="https://latex.codecogs.com/png.latex?A(%5Cmathbf%7Bx_p%7D%20+%5Cmathbf%7Bx_n%7D)=%5Cmathbf%7Bb%7D">
<ul>
<li>즉, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_p%7D%20+%5Cmathbf%7Bx_n%7D"> 도 해가 됨: complete solution
<ul>
<li>null space는 무한하므로, 해도 무한함</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>full rank 일때(square matrix): 해가 하나 존재 (<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20A%5E%7B-1%7D">$)</p></li>
<li><p>rank-deficient 일때</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">가 column space(<img src="https://latex.codecogs.com/png.latex?C(A)">)안에 있으면 무한한 해를 가짐</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">가 column space(<img src="https://latex.codecogs.com/png.latex?C(A)">)안에 없으면 해가 없음</li>
</ul></li>
</ul>


</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/11.matrix_transformation.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
  <media:content url="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/images/chap02_09.PNG" medium="image"/>
</item>
<item>
  <title>Matrix Calculus (1) - Matrix to Vector Derivatives</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/derivative_matrix_vector.html</link>
  <description><![CDATA[ 



<section id="matrix-to-vector-derivatives" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="matrix-to-vector-derivatives"><span class="header-section-number">1</span> Matrix to Vector Derivatives</h2>
<p>Matrix-to-vector derivatives refer to the derivatives of a matrix function with respect to a vector argument. Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)"> be a matrix-valued function of a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En">. The matrix-to-vector derivative is denoted as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20x_1%7D%20&amp;%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20x_2%7D%20&amp;%20%5Ccdots%20&amp;%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20x_n%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>Here, the matrix-to-vector derivative is a matrix whose <img src="https://latex.codecogs.com/png.latex?i">th column is the partial derivative of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bf%7D"> with respect to the <img src="https://latex.codecogs.com/png.latex?i">th component of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> be matrices in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%20n%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%201%7D">, respectively. Consider the function <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%20=%20%5Cmathbf%7BAx%7D">, which is a matrix-vector product. The matrix-to-vector derivative of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is given by: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bf%7D(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D%20=%20%5Cmathbf%7BA%7D%0A"> Here, the derivative is a matrix whose rows are the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<section id="differentiation-of-quadratic-form" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="differentiation-of-quadratic-form"><span class="header-section-number">1.1</span> Differentiation of Quadratic Form</h3>
<p>This is because the output of this differentiation is a vector (with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">), rather than a scalar.</p>
<p>The differentiation of a quadratic form is the process of finding the gradient of a quadratic form with respect to its input vector.</p>
<p>Given a quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is an <img src="https://latex.codecogs.com/png.latex?n">-dimensional column vector, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En"> is a vector, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> symmetric matrix, the derivative of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%7B%5Cmathbf%20x%7D%20f(%5Cmathbf%20x)%20=%20(A%20+%20A%5ET)%5Cmathbf%20x%20+%20b%0A"></p>
<p>In this expression, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET"> is the transpose of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A+%5Cmathbf%20A%5ET"> is written instead of <img src="https://latex.codecogs.com/png.latex?2%5Cmathbf%20A"> when calculating the gradient of a quadratic form. It is because in general, the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> might not be symmetric, so <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5Cneq%20%5Cmathbf%20A%5ET">. However, for any matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, we have <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A+%5Cmathbf%20A%5ET%20=%20(%5Cmathbf%20A+%5Cmathbf%20A%5ET)%5ET">, which is a symmetric matrix. Therefore, by writing the gradient as <img src="https://latex.codecogs.com/png.latex?%5Cnabla_x%20f(x)%20=%20(%5Cmathbf%20A+%5Cmathbf%20A%5ET)x">, we ensure that the gradient is always a symmetric matrix, even if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is not symmetric. This is useful in many applications where symmetric matrices are preferred. But if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is constrained to be a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?2%5Cmathbf%20A"> can be written.</p>
</section>
<section id="when-mathbfa-is-symmetric" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="when-mathbfa-is-symmetric"><span class="header-section-number">1.2</span> When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is Symmetric</h3>
<p>As an example, consider the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=x_1%5E2+2x_1x_2+3x_2%5E2">, which can be written in the form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D">, where:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20x=%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%20x_2%20%5Cend%7Bbmatrix%7D,%20%5Cmathbf%20A=%5Cbegin%7Bbmatrix%7D%201%20&amp;%201%20%5C%5C%201%20&amp;%203%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>The derivative of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is then:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D=(%5Cmathbf%7BA%7D+%5Cmathbf%7BA%7D%5ET)%5Cmathbf%7Bx%7D=%5Cbegin%7Bbmatrix%7D2%20&amp;%202%5C%5C2%20&amp;%206%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D=%5Cbegin%7Bbmatrix%7D2x_1+2x_2%5C%5C2x_1+6x_2%5Cend%7Bbmatrix%7D%0A"></p>
<p>This represents the gradient vector of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> at any point <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
</section>
<section id="when-mathbfa-is-not-symmetric" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="when-mathbfa-is-not-symmetric"><span class="header-section-number">1.3</span> When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is Not Symmetric</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%203%20&amp;%204%20%5Cend%7Bbmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D">.</p>
<p>Then, we have <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%203%20&amp;%204%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20+%202x_2%20%5C%5C%203x_1%20+%204x_2%20%5Cend%7Bbmatrix%7D%20=%20x_1%5E2%20+%205x_1x_2%20+%204x_2%5E2">.</p>
<p>To find the gradient of this quadratic form, we can take the partial derivatives of <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2"> with respect to each variable:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20x_1%7D%20(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D)%20=%202x_1%20+%205x_2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20x_2%7D%20(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D)%20=%205x_1%20+%208x_2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D=(%5Cmathbf%7BA%7D+%5Cmathbf%7BA%7D%5ET)%5Cmathbf%7Bx%7D=%5Cbegin%7Bbmatrix%7D1+1%20&amp;%202+3%5C%5C3+2%20&amp;%204+4%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D=%5Cbegin%7Bbmatrix%7D2%20&amp;%205%5C%5C5%20&amp;%208%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%202x_1%20+%205x_2%20%5C%5C%205x_1%20+%208x_2%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>So the gradient of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cnabla_x%20f(x)%20=%20%5Cbegin%7Bbmatrix%7D%202x_1%20+%205x_2%20%5C%5C%205x_1%20+%208x_2%20%5Cend%7Bbmatrix%7D">.</p>
</section>
<section id="ordinary-least-square" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="ordinary-least-square"><span class="header-section-number">1.4</span> Ordinary Least Square</h3>
<p>For the <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%201"> vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D">, the <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20k"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">, the <img src="https://latex.codecogs.com/png.latex?k%20%5Ctimes%201"> vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cbeta%7D">, when <img src="https://latex.codecogs.com/png.latex?L=(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)">, what is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Cmathbf%7B%5Cbeta%7D%7D">?</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AL%20&amp;=%20(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%20%5C%5C%0A&amp;=%20%5Cmathbf%7By%7D%5ET%5Cmathbf%7By%7D%20-%20%5Cmathbf%7By%7D%5ET%5Cmathbf%7BA%7D%5Cmathbf%7B%5Cbeta%7D%20-%20%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%20%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D%20%5C%5C%0A&amp;=%20%5Cmathbf%7By%7D%5ET%5Cmathbf%7By%7D%20-%202%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%20%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>Now, we can take the derivative of <img src="https://latex.codecogs.com/png.latex?L"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cbeta%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Cmathbf%7B%5Cbeta%7D%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Cmathbf%7B%5Cbeta%7D%7D%20(%5Cmathbf%7By%7D%5ET%5Cmathbf%7By%7D%20-%202%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%20%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%20%5C%5C%0A&amp;=%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Cmathbf%7B%5Cbeta%7D%7D%20(-%202%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%20%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%20%5C%5C%0A&amp;=%20-%202%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%202%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D"> is the solution to the optimization problem by taking its derivative with respect to and setting it equal to zero.</p>
<p>Starting with the expression for <img src="https://latex.codecogs.com/png.latex?L">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL=(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D)%0A"></p>
<p>Expanding the quadratic term gives:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL=%5Cmathbf%7By%7D%5ET%5Cmathbf%7By%7D-%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D-%5Cmathbf%7By%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D+%5Cmathbf%7B%5Cbeta%7D%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D%0A"></p>
<p>Taking the derivative of <img src="https://latex.codecogs.com/png.latex?L"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cbeta%7D"> and setting it to zero gives:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Cmathbf%7B%5Cbeta%7D%7D%20=%20-2%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20+%202%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D%20=%200%0A"></p>
<p>Solving for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cbeta%7D"> gives:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cmathbf%7B%5Cbeta%7D=%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7B%5Cbeta%7D=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%0A"> Multiplying both sides of the equation by <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)"> gives:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5Cmathbf%7B%5Cbeta%7D=%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%0A"></p>
<p>Therefore, we have verified that <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Cbeta%7D=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%0A"> is the solution to the optimization problem of OLS (Ordinary Least Square).</p>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/derivative_matrix_vector.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Matrix Calculus (1) - Matrix to Vector Derivatives</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/derivative_vector_matrix.html</link>
  <description><![CDATA[ 



<section id="matrix-to-vector-derivatives" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="matrix-to-vector-derivatives"><span class="header-section-number">1</span> Matrix to Vector Derivatives</h2>
<p>The matrix-to-vector derivative is a derivative where a function <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7BX%7D)"> maps an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> to a <img src="https://latex.codecogs.com/png.latex?p">-dimensional vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D">, and we want to find the derivative of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">. It is denoted by <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20%5Cmathbf%7BX%7D%7D">.</p>
<p>Formally, let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D%20=%20f(%5Cmathbf%7BX%7D)%20%5Cin%20%5Cmathbb%7BR%7D%5Ep"> be a function that maps an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix to a <img src="https://latex.codecogs.com/png.latex?p">-dimensional vector. Then, the matrix-to-vector derivative is defined as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B11%7D%7D%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B12%7D%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B1n%7D%7D%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B21%7D%7D%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B22%7D%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B2n%7D%7D%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7Bm1%7D%7D%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7Bm2%7D%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7Bmn%7D%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>where each element of the matrix is the derivative of the corresponding element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D"> with respect to the corresponding element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7BX%7D)%20=%20%5Cmathbf%7BA%7D%5Cmathbf%7BX%7D+%5Cmathbf%7Bb%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bp%20%5Ctimes%20m%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D%20%5Cin%20%5Cmathbb%7BR%7D%5Ep">. Then, the matrix-to-vector derivative of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D%20=%20f(%5Cmathbf%7BX%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> is: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20%5Cmathbf%7BX%7D%7D%20=%20%5Cmathbf%7BA%7D%0A"></p>
<section id="differentiation-of-quadratic-form" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="differentiation-of-quadratic-form"><span class="header-section-number">1.1</span> Differentiation of Quadratic Form</h3>
<p>This is because the output of this differentiation is a vector (with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">), rather than a scalar.</p>
<p>The differentiation of a quadratic form is the process of finding the gradient of a quadratic form with respect to its input vector.</p>
<p>Given a quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is an <img src="https://latex.codecogs.com/png.latex?n">-dimensional column vector, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En"> is a vector, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> symmetric matrix, the derivative of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%7B%5Cmathbf%20x%7D%20f(%5Cmathbf%20x)%20=%20(A%20+%20A%5ET)%5Cmathbf%20x%20+%20b%0A"></p>
<p>In this expression, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET"> is the transpose of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A+%5Cmathbf%20A%5ET"> is written instead of <img src="https://latex.codecogs.com/png.latex?2%5Cmathbf%20A"> when calculating the gradient of a quadratic form. It is because in general, the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> might not be symmetric, so <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5Cneq%20%5Cmathbf%20A%5ET">. However, for any matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, we have <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A+%5Cmathbf%20A%5ET%20=%20(%5Cmathbf%20A+%5Cmathbf%20A%5ET)%5ET">, which is a symmetric matrix. Therefore, by writing the gradient as <img src="https://latex.codecogs.com/png.latex?%5Cnabla_x%20f(x)%20=%20(%5Cmathbf%20A+%5Cmathbf%20A%5ET)x">, we ensure that the gradient is always a symmetric matrix, even if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is not symmetric. This is useful in many applications where symmetric matrices are preferred. But if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is constrained to be a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?2%5Cmathbf%20A"> can be written.</p>
</section>
<section id="when-mathbfa-is-symmetric" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="when-mathbfa-is-symmetric"><span class="header-section-number">1.2</span> When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is Symmetric</h3>
<p>As an example, consider the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=x_1%5E2+2x_1x_2+3x_2%5E2">, which can be written in the form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D">, where:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20x=%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%20x_2%20%5Cend%7Bbmatrix%7D,%20%5Cmathbf%20A=%5Cbegin%7Bbmatrix%7D%201%20&amp;%201%20%5C%5C%201%20&amp;%203%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>The derivative of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is then:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D=(%5Cmathbf%7BA%7D+%5Cmathbf%7BA%7D%5ET)%5Cmathbf%7Bx%7D=%5Cbegin%7Bbmatrix%7D2%20&amp;%202%5C%5C2%20&amp;%206%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D=%5Cbegin%7Bbmatrix%7D2x_1+2x_2%5C%5C2x_1+6x_2%5Cend%7Bbmatrix%7D%0A"></p>
<p>This represents the gradient vector of <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> at any point <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
</section>
<section id="when-mathbfa-is-not-symmetric" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="when-mathbfa-is-not-symmetric"><span class="header-section-number">1.3</span> When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is Not Symmetric</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%203%20&amp;%204%20%5Cend%7Bbmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D">.</p>
<p>Then, we have <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%203%20&amp;%204%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20+%202x_2%20%5C%5C%203x_1%20+%204x_2%20%5Cend%7Bbmatrix%7D%20=%20x_1%5E2%20+%205x_1x_2%20+%204x_2%5E2">.</p>
<p>To find the gradient of this quadratic form, we can take the partial derivatives of <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2"> with respect to each variable:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20x_1%7D%20(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D)%20=%202x_1%20+%205x_2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20x_2%7D%20(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D)%20=%205x_1%20+%208x_2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D=(%5Cmathbf%7BA%7D+%5Cmathbf%7BA%7D%5ET)%5Cmathbf%7Bx%7D=%5Cbegin%7Bbmatrix%7D1+1%20&amp;%202+3%5C%5C3+2%20&amp;%204+4%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D=%5Cbegin%7Bbmatrix%7D2%20&amp;%205%5C%5C5%20&amp;%208%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%20%5C%5C%20x_2%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%202x_1%20+%205x_2%20%5C%5C%205x_1%20+%208x_2%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>So the gradient of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cnabla_x%20f(x)%20=%20%5Cbegin%7Bbmatrix%7D%202x_1%20+%205x_2%20%5C%5C%205x_1%20+%208x_2%20%5Cend%7Bbmatrix%7D">.</p>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/derivative_vector_matrix.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Matrix Transformation (5) - Quadratic Form</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/quadratic_form.html</link>
  <description><![CDATA[ 



<section id="quadratic-form" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="quadratic-form"><span class="header-section-number">1</span> Quadratic Form</h2>
<p>For a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Bx_1,x_2,%5Cldots,x_n%5D%5ET">, the quadratic form is defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)%20=%20%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> symmetric matrix.</p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET"> represents the transpose of the vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"> represents the dot product of the vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> with itself after the transformation by the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Bx_1,x_2%5D%5ET"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be a <img src="https://latex.codecogs.com/png.latex?2%20%5Ctimes%202"> symmetric matrix given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A%20=%20%5Cbegin%7Bbmatrix%7D%202&amp;1%20%5C%5C%201&amp;3%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>Then, the quadratic form <img src="https://latex.codecogs.com/png.latex?Q(%5Cmathbf%7Bx%7D)"> can be written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20Q(%5Cmathbf%20x)%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D%202&amp;1%20%5C%5C%201&amp;3%20%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5Cend%7Bbmatrix%7D=2x_1%5E2+4x_1x_2+3x_2%5E2%0A"></p>
<p>Here, we can see that the quadratic form can be represented as a polynomial function of degree 2 in the variables <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2"> with the coefficients given by the entries of the symmetric matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>a quadratic form can be expressed as a bilinear form. In other words, a quadratic form can be written in terms of a bilinear form by defining a new matrix that is the sum of the matrix representing the quadratic form and its transpose.</p>
<p>More formally, suppose we have a quadratic form defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?q(%5Cmathbf%7Bx%7D)%20=%20%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a symmetric matrix. Then, we can define a bilinear form as:</p>
<p><img src="https://latex.codecogs.com/png.latex?b(%5Cmathbf%7Bx%7D,%20%5Cmathbf%7By%7D)%20=%20%5Cfrac%7B1%7D%7B2%7D(%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7By%7D%20+%20%5Cmathbf%7By%7D%5ET%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D)"></p>
<p>Note that the factor of <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D"> is introduced to avoid double-counting. It can be shown that the two forms are equivalent, in the sense that for any <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, <img src="https://latex.codecogs.com/png.latex?q(%5Cmathbf%7Bx%7D)%20=%20b(%5Cmathbf%7Bx%7D,%20%5Cmathbf%7Bx%7D)">.</p>
<p>In other words, every quadratic form can be expressed as a bilinear form, and every symmetric bilinear form can be expressed as a quadratic form.</p>
</section>
<section id="examples" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="examples"><span class="header-section-number">2</span> Examples</h2>
<section id="sum-of-squares" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="sum-of-squares"><span class="header-section-number">2.1</span> Sum of Squares</h3>
<p>The sum of squares of a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Bx_1,%20x_2,%20%5Cldots,%20x_n%5D%5ET"> can be represented as a quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%5Cmathbf%7Bx%7D">. To see this, consider the sum of squares:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bi=1%7D%5E%7Bn%7D%20x_i%5E2%20=%20x_1%5E2%20+%20x_2%5E2%20+%20%5Cdots%20+x_n%5E2%0A"></p>
<p>Now, we can write this in vector form as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20x%5ET%20%5Cmathbf%20x%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20&amp;%20%5Cdots%20&amp;%20x_n%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5C%5C%20%5Cvdots%20%5C%5C%20x_n%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>Therefore, the sum of squares can be represented as a quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%5Cmathbf%7Bx%7D">.</p>
</section>
<section id="covariance-matrix" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="covariance-matrix"><span class="header-section-number">2.2</span> Covariance Matrix</h3>
<p>The covariance matrix of a random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> can be represented as a quadratic form in terms of the vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> and the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20x%5ET%20%5Cmathbf%20C%20%5Cmathbf%20x%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> is the covariance matrix. This expression is a quadratic form because it involves a quadratic polynomial in the elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>In this representation, the diagonal elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> correspond to the variances of the individual components of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, and the off-diagonal elements correspond to the covariances between the components. The expression <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BC%7D%20%5Cmathbf%7Bx%7D"> measures the variability of the random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> in all possible directions, weighted by the covariances between the components.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> measures the covariance between each pair of components of the random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">. It is a matrix that summarizes the <strong>pairwise covariances</strong> between the components of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>On the other hand, the quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BC%7D%20%5Cmathbf%7Bx%7D"> measures the total variability of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, taking into account the covariances between <strong>all possible pairs of components</strong>.</p>
<p>It does this by weighting the contribution of each component to the overall variability by its covariance with every other component. So, while the covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> captures the pairwise covariances between the components of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, the quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BC%7D%20%5Cmathbf%7Bx%7D"> captures the total variability of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> in all directions.</p>
</div>
</div>
<p>Let’s take a simple example with a 2-dimensional random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D=%5Bx_1,%20x_2%5D%5ET">. We can think of this random vector as representing data points in a 2D space. The covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> will capture the covariances between <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2">. Let’s say that the covariance matrix is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20C%20=%5Cbegin%7Bbmatrix%7D%20%5Csigma_%7Bx_1%7D%20&amp;%20%5Coperatorname%7BCov%7D(x_1,x_2)%20%5C%5C%20%5Coperatorname%7BCov%7D(x_2,x_1)%20&amp;%20%5Csigma_%7Bx_2%7D%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Csigma_%7Bx_1%7D%5E2"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma_%7Bx_2%7D%5E2"> are the variances of <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2">, respectively, and <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BCov%7D(x_1,x_2)"> is their covariance.</p>
<p>Now, let’s consider the quadratic form <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20%5Cmathbf%7BC%7D%20%5Cmathbf%7Bx%7D">. This expression gives us a scalar value that measures the variability of the random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> in all possible directions, weighted by the covariances between the components. We can see this geometrically by plotting the data points in the 2D space and drawing an ellipse that captures the variability of the data. The shape of the ellipse is determined by the eigenvalues and eigenvectors of the covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D">.</p>
<p>To see this, let’s first rewrite the quadratic form as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20x%5ET%20%5Cmathbf%20C%20%5Cmathbf%20x%20=%20%5Csigma_%7Bx_1%7D%5E2x_1%5E2%20+2%5Coperatorname%7BCov%7D(x_1,x_2)x_1x_2+%5Csigma_%7Bx_2%7D%5E2%0A"></p>
<p>This is a quadratic equation in <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2"> and can be thought of as the equation of an ellipse centered at the origin by the determinant of the conic equation. The shape of the ellipse is determined by the coefficients of the quadratic terms, which are the variances and covariances in the covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D">.</p>
<p>Now, let’s find the eigenvectors and eigenvalues of the covariance matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D">. The eigenvectors are the directions along which the data has the most variance, and the corresponding eigenvalues are the variances of the data along those directions.</p>
<p>Let’s assume that the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D"> are ordered such that <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20%5Cgeq%20%5Clambda_2">. Then, the eigenvectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_2"> satisfy:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20C%20%5Cmathbf%20v_1%20=%5Clambda_1%5Cmathbf%20v_1%20%5Ctext%7B%20%20%7D%0A%5Cmathbf%20C%20%5Cmathbf%20v_2%20=%5Clambda_2%5Cmathbf%20v_2%0A"></p>
<p>These equations can be rewritten as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%5Csigma_%7Bx_1%7D%5E2%20&amp;%20%5Ctext%7BCov%7D(x_1,x_2)%5C%5C%0A%20%20%5Ctext%7BCov%7D(x_1,x_2)%20&amp;%20%5Csigma_%7Bx_2%7D%5E2%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20v_%7B11%7D%5C%5C%0A%20%20v_%7B21%7D%0A%5Cend%7Bbmatrix%7D%0A=%20%5Clambda_1%0A%5Cbegin%7Bbmatrix%7D%0Av_%7B11%7D%5C%5C%0Av_%7B21%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation%7D%0A"></p>
<p>This equation can be expanded as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Csigma_%7Bx_1%7D%5E2%20v_%7B11%7D%20+%20%5Ctext%7BCov%7D(x_1,x_2)%20v_%7B21%7D%20=%20%5Clambda_1%20v_%7B11%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BCov%7D(x_1,x_2)%20v_%7B11%7D%20+%20%5Csigma_%7Bx_2%7D%5E2%20v_%7B21%7D%20=%20%5Clambda_1%20v_%7B21%7D"></p>
<p>Now, let’s multiply the first equation by <img src="https://latex.codecogs.com/png.latex?v_%7B11%7D"> and the second equation by <img src="https://latex.codecogs.com/png.latex?v_%7B21%7D">, and then subtract the second equation from the first:</p>
<p><img src="https://latex.codecogs.com/png.latex?(%5Csigma_%7Bx_1%7D%5E2%20-%20%5Clambda_1)v_%7B11%7Dv_%7B21%7D%20+%20%5Ctext%7BCov%7D(x_1,x_2)(v_%7B21%7D%5E2%20-%20v_%7B11%7D%5E2)%20=%200"></p>
<p>This can be rewritten as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bv_%7B21%7D%7D%7Bv_%7B11%7D%7D%20=%20%5Cfrac%7B%5Csigma_%7Bx_1%7D%5E2%20-%20%5Clambda_1%7D%7B%5Ctext%7BCov%7D(x_1,x_2)%7D%20-%20%5Cfrac%7Bv_%7B11%7D%7D%7Bv_%7B21%7D%7D"></p>
<p>Let <img src="https://latex.codecogs.com/png.latex?t%20=%20%5Cfrac%7Bv_%7B21%7D%7D%7Bv_%7B11%7D%7D">. Then, we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?t%5E2%20-%20%5Cleft(%5Cfrac%7B%5Csigma_%7Bx_1%7D%5E2%20+%20%5Csigma_%7Bx_2%7D%5E2%7D%7B%5Ctext%7BCov%7D(x_1,x_2)%7D%5Cright)t%20+%20%5Cfrac%7B%5Clambda_1%7D%7B%5Ctext%7BCov%7D(x_1,x_2)%7D%20=%200"></p>
<p>This is a quadratic equation in <img src="https://latex.codecogs.com/png.latex?t">, and its roots can be solved using the quadratic formula. The roots are:</p>
<p><img src="https://latex.codecogs.com/png.latex?t_1%20=%20%5Cfrac%7B%5Csigma_%7Bx_1%7D%5E2%20-%20%5Csigma_%7Bx_2%7D%5E2%20+%20%5Csqrt%7B(%5Csigma_%7Bx_1%7D%5E2%20-%20%5Csigma_%7Bx_2%7D%5E2)%5E2%20+%204%5Ctext%7BCov%7D(x_1,x_2)%5E2%7D%7D%7B2%5Ctext%7BCov%7D(x_1,x_2)%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?t_2%20=%20%5Cfrac%7B%5Csigma_%7Bx_1%7D%5E2%20-%20%5Csigma_%7Bx_2%7D%5E2%20-%20%5Csqrt%7B(%5Csigma_%7Bx_1%7D%5E2%20-%20%5Csigma_%7Bx_2%7D%5E2)%5E2%20+%204%5Ctext%7BCov%7D(x_1,x_2)%5E2%7D%7D%7B2%5Ctext%7BCov%7D(x_1,x_2)%7D"></p>
<p>Finally, the eigenvectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_2"> are given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bv%7D_1%20=%20%5Cbegin%7Bbmatrix%7D1%20%5C%5C%20t_1%20%5Cend%7Bbmatrix%7D%20%5Ctext%7B%20%20%7D%0A%5Cmathbf%7Bv%7D_2%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%20t_2%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>These eigenvectors define the principal components of the data, which are the orthogonal directions in the feature space along which the data varies the most.</p>
<p>Let’s apply this difference between <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20C=%20%5Coperatorname%7BCov(%5Cmathbf%20X)%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x%5ET%20%5Cmathbf%7BC%7D%20%5Cmathbf%20x"> or PCA to Iris dataset:</p>
<div class="cell" data-execution_count="1">
<div class="cell-output cell-output-stdout">
<pre><code>C=Cov(X) =
[[ 1.00671141 -0.11835884  0.87760447  0.82343066]
 [-0.11835884  1.00671141 -0.43131554 -0.36858315]
 [ 0.87760447 -0.43131554  1.00671141  0.96932762]
 [ 0.82343066 -0.36858315  0.96932762  1.00671141]]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/quadratic_form_files/figure-html/cell-2-output-2.png" width="587" height="449"></p>
</div>
</div>
<p>This representation is useful in many statistical and machine learning applications, where the covariance matrix provides information about the variability and dependencies between different features or variables. For example, in principal component analysis (PCA), the covariance matrix is used to identify the directions of maximum variability in a dataset, which can be used to reduce the dimensionality of the data while retaining as much information as possible.</p>
</section>
<section id="pca" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="pca"><span class="header-section-number">2.3</span> PCA</h3>
<p>The principal components of a dataset can be obtained by finding the eigenvectors of the covariance matrix. In other words, we can express the covariance matrix as a quadratic form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20C%20=%20%5Cmathbf%20x%5ET%20%5Cmathbf%20A%20%5Cmathbf%20x%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is a column vector of centered data, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a symmetric positive semi-definite matrix (the covariance matrix). Diagonalizing <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> gives us the eigenvalues and eigenvectors, which are used to transform the original data into a new coordinate system, where the first axis (the first principal component) corresponds to the direction of greatest variance, the second axis (the second principal component) corresponds to the direction of second greatest variance, and so on. This new coordinate system is called the principal component space.</p>
<p>In summary, PCA can be seen as a method for finding the principal components of a dataset by diagonalizing the covariance matrix, which can be expressed as a quadratic form.</p>
</section>
<section id="positive-definit-matrix" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="positive-definit-matrix"><span class="header-section-number">2.4</span> Positive Definit Matrix</h3>
<p>a symmetric matrix <img src="https://latex.codecogs.com/png.latex?A"> is positive definite if and only if the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> is positive for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>To see why this is true, consider the eigenvalue decomposition of <img src="https://latex.codecogs.com/png.latex?A">, which can be written as <img src="https://latex.codecogs.com/png.latex?A%20=%20Q%20%5CLambda%20Q%5ET">, where <img src="https://latex.codecogs.com/png.latex?Q"> is an orthogonal matrix and <img src="https://latex.codecogs.com/png.latex?%5CLambda"> is a diagonal matrix containing the eigenvalues of <img src="https://latex.codecogs.com/png.latex?A">. Then, for any nonzero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">,</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Diagonalization
</div>
</div>
<div class="callout-body-container callout-body">
<p>Diagonalization is a process of finding a diagonal matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20D"> and an invertible matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20P"> such that <img src="https://latex.codecogs.com/png.latex?P%5E%7B-1%7DAP%20=%20D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is a square matrix. In other words, diagonalization is a way of representing a matrix as a diagonal matrix, which is a matrix with non-zero values only on its main diagonal.</p>
</div>
</div>
<p>we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbf%20x%5ET%20%5Cmathbf%20A%20%5Cmathbf%20x&amp;=%5Cmathbf%20x%5ET%20%5Cmathbf%20Q%20%5Cmathbf%20%5CLambda%20%5Cmathbf%20Q%5ET%20%5Cmathbf%20x%5C%5C%0A&amp;=(%5Cmathbf%20x%5ET%20%5Cmathbf%20Q)%5Cmathbf%20%5CLambda%20(%20%5Cmathbf%20Q%5ET%5Cmathbf%20x)%5C%5C%0A&amp;=%5Csum_%7Bi=1%7D%5E%7Bn%7D%20%5Clambda_iy_i%5E2%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?y_i%20=%20(%5Cmathbf%7Bx%7D%5ET%20Q)_i"> is the <img src="https://latex.codecogs.com/png.latex?i">th coordinate (i.e., a scalar value that represents the position of a point or a vector relative to a chosen basis) of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20Q"> and <img src="https://latex.codecogs.com/png.latex?n"> is the dimension of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?A">. Note that since <img src="https://latex.codecogs.com/png.latex?Q"> is orthogonal, we have <img src="https://latex.codecogs.com/png.latex?Q%5ET%20Q%20=%20I">, so <img src="https://latex.codecogs.com/png.latex?y_i%20=%20%5Cmathbf%7Bq%7D_i%5ET%20%5Cmathbf%7Bx%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bq%7D_i"> is the <img src="https://latex.codecogs.com/png.latex?i">th column of <img src="https://latex.codecogs.com/png.latex?Q">. Therefore, the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)%20=%20%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> can be written in terms of the eigenvalues of <img src="https://latex.codecogs.com/png.latex?A"> and the coordinates of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> with respect to the eigenvectors of <img src="https://latex.codecogs.com/png.latex?A">.</p>
<p>Since <img src="https://latex.codecogs.com/png.latex?A"> is positive definite, we have <img src="https://latex.codecogs.com/png.latex?%5Clambda_i%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?i">, and so <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5En%20%5Clambda_i%20y_i%5E2%20%3E%200"> for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">. Therefore, the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> is positive for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, which implies that <img src="https://latex.codecogs.com/png.latex?A"> is positive definite.</p>
<p>In other words, the positive definiteness of a symmetric matrix <img src="https://latex.codecogs.com/png.latex?A"> is equivalent to the positivity of the associated quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>Therefore, a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is said to be positive definite if all of its eigenvalues are positive or equivalently, a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is positive definite if left-multiplying and right-multiplying it by the same vector, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x"> always gives a positive number if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x%5ET%20%5Cmathbf%20A%20%5Cmathbf%20x"></p>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/quadratic_form.html</guid>
  <pubDate>Sat, 01 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Basics (2) - Matrix Operations</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/02.basic_matrix.html</link>
  <description><![CDATA[ 



<section id="matrix" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="matrix"><span class="header-section-number">1</span> Matrix</h2>
<p>A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix, it can be represented as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%20%20a_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7B1n%7D%20%5C%5C%0A%20%20a_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7B2n%7D%20%5C%5C%0A%20%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A%20%20a_%7Bm1%7D%20&amp;%20a_%7Bm2%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7Bmn%7D%0A%5Cend%7Bbmatrix%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> is the element in the <img src="https://latex.codecogs.com/png.latex?i">-th row and <img src="https://latex.codecogs.com/png.latex?j">-th column of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
</section>
<section id="basic-matrix-operations" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="basic-matrix-operations"><span class="header-section-number">2</span> Basic Matrix Operations</h2>
<section id="matrix-with-combinations-of-vectors" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="matrix-with-combinations-of-vectors"><span class="header-section-number">2.1</span> Matrix with Combinations of Vectors</h3>
<p>A matrix with combinations of vectors is a matrix that can be written as a linear combination of column vectors. Given column vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cdots,%20%5Cmathbf%7Bv%7D_n%20%5Cin%20%5Cmathbb%7BR%7D%5Em"> and scalars <img src="https://latex.codecogs.com/png.latex?a_1,%20a_2,%20%5Cdots,%20a_n%20%5Cin%20%5Cmathbb%7BR%7D">, the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> can be written as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A%20=%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Bv_1%7D&amp;%5Cmathbf%7Bv_2%7D&amp;%20%5Cdots%20&amp;%5Cmathbf%7Bv_n%7D%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20a_1%20%5C%5C%20a_2%20%5C%5C%20%5Cvdots%20%5C%5C%20a_n%20%5Cend%7Bbmatrix%7D%20=a_1%5Cmathbf%20v_1%20+a_2%5Cmathbf%20v_2%20+%5Cdots+a_n%20%5Cmathbf%20v_n%0A"></p>
<p>In other words, the columns of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are linear combinations of the column vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1,%20%5Cmathbf%7Bv%7D_2,%20%5Cdots,%20%5Cmathbf%7Bv%7D_n">. This can be written more compactly as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A%20=%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Bv_1%7D&amp;%5Cmathbf%7Bv_2%7D&amp;%20%5Cdots%20&amp;%5Cmathbf%7Bv_n%7D%20%5Cend%7Bbmatrix%7D%20%5Cmathbf%20a%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Ba%7D%20=%20%5Cbegin%7Bbmatrix%7D%20a_1%20&amp;%20a_2%20&amp;%20%5Cdots%20&amp;%20a_n%20%5Cend%7Bbmatrix%7D%5ET"> is a column vector of scalars.</p>
<p>An example of a matrix with combinations of vectors is: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A%20=%0A%5Cbegin%7Bbmatrix%7D%0A1&amp;4&amp;7%5C%5C%0A2&amp;5&amp;8%5C%5C%0A3&amp;6&amp;9%5C%5C%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%5C%5C%0A2%5C%5C%0A3%5C%5C%0A%5Cend%7Bbmatrix%7D%5Cmathbf%7Be%7D_1%5E%5Ctext%7BT%7D+%0A%5Cbegin%7Bbmatrix%7D%0A4%5C%5C%0A5%5C%5C%0A6%5C%5C%0A%5Cend%7Bbmatrix%7D%5Cmathbf%7Be%7D_2%5E%5Ctext%7BT%7D+%0A%5Cbegin%7Bbmatrix%7D%0A7%5C%5C%0A8%5C%5C%0A9%5C%5C%0A%5Cend%7Bbmatrix%7D%5Cmathbf%7Be%7D_3%5E%5Ctext%7BT%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D_1,%20%5Cmathbf%7Be%7D_2,%20%5Cmathbf%7Be%7D_3"> are the standard basis vectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3">.</p>
</section>
<section id="matrix-addition" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="matrix-addition"><span class="header-section-number">2.2</span> Matrix addition</h3>
<p>The sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.</p>
<p>Given two <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D">, their sum <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D%20=%20%5Cmathbf%7BA%7D%20+%20%5Cmathbf%7BB%7D"> is defined by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ac_%7Bi,j%7D=a_%7Bi,j%7D+b_%7Bi,j%7D%E2%80%8B%0A"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%20m"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20j%20%5Cleq%20n">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%20%201%20&amp;%202%20%5C%5C%0A%20%203%20&amp;%204%20%5C%5C%0A%20%205%20&amp;%206%0A%5Cend%7Bbmatrix%7D%20+%0A%5Cbegin%7Bbmatrix%7D%0A%20%20-1%20&amp;%200%20%5C%5C%0A%20%202%20&amp;%20-3%20%5C%5C%0A%20%20-5%20&amp;%204%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A%20%200%20&amp;%202%20%5C%5C%0A%20%205%20&amp;%201%20%5C%5C%0A%20%200%20&amp;%2010%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="scalar-multiplication" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="scalar-multiplication"><span class="header-section-number">2.3</span> Scalar multiplication</h3>
<p>The product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.</p>
<p>Given a scalar <img src="https://latex.codecogs.com/png.latex?k"> and an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, their product <img src="https://latex.codecogs.com/png.latex?k%5Cmathbf%7BA%7D"> is defined by: <img src="https://latex.codecogs.com/png.latex?%0A(k%5Cmathbf%7BA%7D)_%7Bi,j%7D%20=%20k(a_%7Bi,j%7D)%0A"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%20m"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20j%20%5Cleq%20n">.</p>
<p>Example: <img src="https://latex.codecogs.com/png.latex?%0A2%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%20%5C%5C%0A5%20&amp;%206%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A2%20&amp;%204%20%5C%5C%0A6%20&amp;%208%20%5C%5C%0A10%20&amp;%2012%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="matrix-multiplication" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="matrix-multiplication"><span class="header-section-number">2.4</span> Matrix multiplication</h3>
<p>The product of two matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D"> is a matrix obtained by multiplying the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> by the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D">.</p>
<p>Given two matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D"> with dimensions <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> and <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20p">, respectively, their product <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D%20=%20%5Cmathbf%7BAB%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20p"> matrix defined by: <img src="https://latex.codecogs.com/png.latex?%0Ac_%7Bi,j%7D%20=%20%5Csum_%7Bk=1%7D%5En%20a_%7Bi,k%7Db_%7Bk,j%7D%0A"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%20m"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20j%20%5Cleq%20p">.</p>
<p>Example: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%20%5C%5C%0A5%20&amp;%206%0A%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%0A-1%20&amp;%200%20&amp;%202%20%5C%5C%0A2%20&amp;%20-3%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A3%20&amp;%20-6%20&amp;%204%20%5C%5C%0A5%20&amp;%20-12%20&amp;%2010%20%5C%5C%0A7%20&amp;%20-18%20&amp;%2016%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="matrix-multiplication-with-a-vector" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="matrix-multiplication-with-a-vector"><span class="header-section-number">2.5</span> Matrix Multiplication with a Vector</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> be a <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%201"> column vector. The matrix-vector product <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAx%7D"> is defined as:</p>
<p>$$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%5Cmathbf%7BAx%7D=%0A%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20%5Cdots%20&amp;%20a_%7B1n%7D%20%5C%5C%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20%5Cdots%20&amp;%20a_%7B2n%7D%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0Aa_%7Bm1%7D%20&amp;%20a_%7Bm2%7D%20&amp;%20%5Cdots%20&amp;%20a_%7Bmn%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%0Ax_1%20%5C%5C%20x_2%20%5C%5C%20%5Cvdots%20%5C%5C%20x_n%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7Dx_1%20&amp;%20a_%7B12%7Dx_2%20&amp;%20%5Cdots%20&amp;%20a_%7B1n%7Dx_n%20%5C%5C%0Aa_%7B21%7Dx_1%20&amp;%20a_%7B22%7Dx_2%20&amp;%20%5Cdots%20&amp;%20a_%7B2n%7Dx_n%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20%5C%5C%0Aa_%7Bm1%7Dx_1%20&amp;%20a_%7Bm2%7Dx_2%20&amp;%20%5Cdots%20&amp;%20a_%7Bmn%7Dx_n%0A%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation%7D"> $$</p>
<p>In other words, each entry of the resulting column vector is the dot product of the corresponding row of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and the column vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>For example, let</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A%20=%0A%5Cbegin%7Bbmatrix%7D%0A2%20&amp;%201%20%5C%5C%0A3%20&amp;%204%20%5C%5C%0A1%20&amp;%202%0A%5Cend%7Bbmatrix%7D%20%5Ctext%7B%20%20%7D%0A%5Cmathbf%20x=%0A%5Cbegin%7Bbmatrix%7D%0A%20%201%5C%5C-1%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>Then we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BAx%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A2%20&amp;%201%20%5C%5C%0A3%20&amp;%204%20%5C%5C%0A1%20&amp;%202%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%201%5C%5C-1%0A%5Cend%7Bbmatrix%7D%0A=%0A%5Cbegin%7Bbmatrix%7D%0A%20%201%5C%5C-5%5C%5C-1%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="transpose" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="transpose"><span class="header-section-number">2.6</span> Transpose</h3>
<p>The transpose of an <img src="https://latex.codecogs.com/png.latex?m%20x%20n"> matrix A, denoted by <img src="https://latex.codecogs.com/png.latex?A%5ET">, is the <img src="https://latex.codecogs.com/png.latex?n%20x%20m"> matrix obtained by interchanging the rows and columns of A. Formally, if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Ba_%7Bij%7D%5D"> is an m x n matrix, then its transpose <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET%20=%20%5Bb_%7Bij%7D%5D"> is an <img src="https://latex.codecogs.com/png.latex?n%20x%20m"> matrix where <img src="https://latex.codecogs.com/png.latex?b_%7Bij%7D"> = <img src="https://latex.codecogs.com/png.latex?a_%7Bji%7D"> for all <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j">. In other words, the element in the <img src="https://latex.codecogs.com/png.latex?i"> th row and <img src="https://latex.codecogs.com/png.latex?j"> th column of <img src="https://latex.codecogs.com/png.latex?A%5ET"> is equal to the element in the <img src="https://latex.codecogs.com/png.latex?j"> th row and ith column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>Given an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, its transpose <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m"> matrix defined by: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D_%7Bi,j%7D%5ET%20=%20%5Cmathbf%7BA%7D_%7Bj,i%7D%0A"></p>
<ul>
<li>When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is transposed, diagnoal entries(<img src="https://latex.codecogs.com/png.latex?a_%7Bii%7D">) do not change but off-diagnoal elements(<img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D%20%5C;%20i%20%5Cneq%20j">) change.</li>
<li>A column vector is tranposed into a row vector, and vice versa.</li>
<li>symmetric matrix: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BA%7D%5ET"></li>
</ul>
<p>Example:</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be the matrix <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%5C%5C%0A4%20&amp;%205%20&amp;%206%0A%5Cend%7Bbmatrix%7D%0A"> The transpose of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5ET">, is the matrix <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5ET%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%204%5C%5C%0A2%20&amp;%205%5C%5C%0A3%20&amp;%206%0A%5Cend%7Bbmatrix%7D%0A"></p>
<section id="properties" class="level4" data-number="2.6.1">
<h4 data-number="2.6.1" class="anchored" data-anchor-id="properties"><span class="header-section-number">2.6.1</span> Properties</h4>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20B"> be an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20p"> matrix, and let <img src="https://latex.codecogs.com/png.latex?c"> be a scalar. Then:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BA%7D%5ET)%5ET%20=%20%5Cmathbf%7BA%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BA%7D%20+%20%5Cmathbf%7BB%7D)%5ET%20=%20%5Cmathbf%7BA%7D%5ET%20+%20%5Cmathbf%7BB%7D%5ET"></li>
<li><img src="https://latex.codecogs.com/png.latex?(c%5Cmathbf%7BA%7D)%5ET%20=%20c%5Cmathbf%7BA%7D%5ET"></li>
<li><img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BAB%7D)%5ET%20=%20%5Cmathbf%7BB%7D%5ET%20%5Cmathbf%7BA%7D%5ET"></li>
<li><img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BABC%7D)%5ET%20=%20%5Cmathbf%7BC%7D%5ET%5Cmathbf%7BB%7D%5ET%20%5Cmathbf%7BA%7D%5ET"> , (cyclic properties)</li>
</ul>
</section>
</section>
<section id="linear-equations-of-a-matrix" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="linear-equations-of-a-matrix"><span class="header-section-number">2.7</span> Linear Equations of a Matrix</h3>
<p>Given an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> and an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%201"> vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, the matrix-vector product <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5Cmathbf%7Bx%7D"> is a linear combination of the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> with coefficients given by the entries of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">. The system of linear equations represented by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A%5Cmathbf%7Bx%7D=%5Cmathbf%7Bb%7D"> has a unique solution if and only if the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> are linearly independent.</p>
<p>A system of linear equations can be written in matrix form as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7B1n%7D%20%5C%5C%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7B2n%7D%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0Aa_%7Bm1%7D%20&amp;%20a_%7Bm2%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7Bmn%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0Ax_1%20%5C%5C%0Ax_2%20%5C%5C%0A%5Cvdots%20%5C%5C%0Ax_n%0A%5Cend%7Bbmatrix%7D=%0A%5Cbegin%7Bbmatrix%7D%0Ab_1%20%5C%5C%0Ab_2%20%5C%5C%0A%5Cvdots%20%5C%5C%0Ab_m%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> are the coefficients of the system, <img src="https://latex.codecogs.com/png.latex?x_i"> are the variables, and <img src="https://latex.codecogs.com/png.latex?b_j"> are the constants.</p>
<p>Here’s an example of a system of linear equations represented by a matrix: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A2x_1%20+%203x_2%20&amp;=%208%20%5C%5C%0A4x_1%20+%205x_2%20&amp;=%2013%0A%5Cend%7Balign*%7D%0A"></p>
<p>This can be written as the matrix equation <img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D=%5Cmathbf%7Bb%7D">, where $$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%20%20%5Cmathbf%7BA%7D%20=%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%202%20&amp;%203%5C%5C%0A%20%20%20%204%20&amp;%205%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%5Cmathbf%7Bx%7D%20=%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20x_1%5C%5C%0A%20%20%20%20x_2%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%0A%20%20%5Cmathbf%7Bb%7D%20=%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%208%5C%5C%0A%20%20%20%2013%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation%7D"> $$</p>
<p>The solution to this system can be found by computing the inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> (if it exists) and multiplying both sides of the equation by it:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bx%7D%20=%20%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7Bb%7D%0A"></p>
<p>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> does not have an inverse, then the system of equations may have either no solutions or infinitely many solutions.</p>
<p>Consider the following system of equations:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A2x_1%20+%203x_2%20&amp;=%205%20%5C%5C%0A4x_1%20-%20x_2%20&amp;=%202%0A%5Cend%7Baligned%7D%0A"></p>
<p>This can be written in matrix form as: $$</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%5Cbegin%7Bbmatrix%7D%0A2%20&amp;%203%20%5C%5C%0A4%20&amp;%20-1%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0Ax_1%20%5C%5C%0Ax_2%0A%5Cend%7Bbmatrix%7D=%0A%5Cbegin%7Bbmatrix%7D%0A5%20%5C%5C%0A2%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation%7D"> $$ where <img src="https://latex.codecogs.com/png.latex?a_%7B11%7D%20=%202,%20a_%7B12%7D%20=%203,%20a_%7B21%7D%20=%204,%20a_%7B22%7D%20=%20-1,%20x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2"> are the variables, and <img src="https://latex.codecogs.com/png.latex?b_1%20=%205,%20b_2%20=%202"> are the constants.</p>
</section>
<section id="determinant" class="level3" data-number="2.8">
<h3 data-number="2.8" class="anchored" data-anchor-id="determinant"><span class="header-section-number">2.8</span> Determinant</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> square matrix. The determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted by <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7BA%7D%7C"> or <img src="https://latex.codecogs.com/png.latex?%5Cdet(%5Cmathbf%7BA%7D)">, is a scalar value calculated as the sum of the products of the elements in any row or column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> with their corresponding cofactors, that is,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%5Cmathbf%7BA%7D%7C=%5Csum_%7Bi=1%7D%5E%7Bn%7Da_%7Bij%7DC_%7Bij%7D=%5Csum_%7Bj=1%7D%5E%7Bn%7Da_%7Bij%7DC_%7Bij%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> is the element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> in the <img src="https://latex.codecogs.com/png.latex?i">-th row and <img src="https://latex.codecogs.com/png.latex?j">-th column, and <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D"> is the cofactor of <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D">. The cofactor of <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D">, denoted by <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D">, is given by <img src="https://latex.codecogs.com/png.latex?(-1)%5E%7Bi+j%7D"> times the determinant of the <img src="https://latex.codecogs.com/png.latex?(n-1)%20%5Ctimes%20(n-1)"> matrix obtained by deleting the <img src="https://latex.codecogs.com/png.latex?i">-th row and <img src="https://latex.codecogs.com/png.latex?j">-th column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>The determinant of an <img src="https://latex.codecogs.com/png.latex?n%20x%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a scalar value denoted as <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7BA%7D%7C">. It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a <img src="https://latex.codecogs.com/png.latex?3%20x%203"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20="> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20a_%7B13%7D%20%5C%5C%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B32%7D%20&amp;%20a_%7B33%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%5Cmathbf%7BA%7D%7C%20=%20a_%7B11%7D%0A%5Cbegin%7Bvmatrix%7D%0Aa_%7B22%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B32%7D%20&amp;%20a_%7B33%7D%0A%5Cend%7Bvmatrix%7D%0A-%20a_%7B12%7D%0A%5Cbegin%7Bvmatrix%7D%0Aa_%7B21%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B33%7D%0A%5Cend%7Bvmatrix%7D%0A+%20a_%7B13%7D%0A%5Cbegin%7Bvmatrix%7D%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B32%7D%0A%5Cend%7Bvmatrix%7D%0A"></p>
<p>For example, consider the <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20&amp;%203%20%5C%5C%204%20&amp;%205%20&amp;%206%20%5C%5C%207%20&amp;%208%20&amp;%209%20%5Cend%7Bbmatrix%7D">. We can calculate the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> using any row or column. Let’s use the first column:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%5Cmathbf%7BA%7D%7C%20=%201%0A%5Cbegin%7Bvmatrix%7D%0A5%20&amp;%206%20%5C%5C%0A8%20&amp;%209%0A%5Cend%7Bvmatrix%7D%0A-%204%0A%5Cbegin%7Bvmatrix%7D%0A2%20&amp;%203%5C%5C%0A8%20&amp;%209%0A%5Cend%7Bvmatrix%7D%0A+%207%0A%5Cbegin%7Bvmatrix%7D%0A2%20&amp;%205%20%5C%5C%0A3%20&amp;%206%0A%5Cend%7Bvmatrix%7D%20=%200%0A"></p>
<p>Therefore, the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is zero.</p>
</section>
<section id="inverse" class="level3" data-number="2.9">
<h3 data-number="2.9" class="anchored" data-anchor-id="inverse"><span class="header-section-number">2.9</span> Inverse</h3>
<p>The inverse of a square matrix <img src="https://latex.codecogs.com/png.latex?A"> of size <img src="https://latex.codecogs.com/png.latex?n"> is a matrix <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> such that the product of <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> is the identity matrix <img src="https://latex.codecogs.com/png.latex?I_n">, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?A%20%5Ctimes%20A%5E%7B-1%7D%20=%20I_n">. If such a matrix exists, then <img src="https://latex.codecogs.com/png.latex?A"> is said to be invertible or non-singular.</p>
<p>The inverse of a square matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E%7B-1%7D"> and is defined as the unique matrix that satisfies the following equation: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BI%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D"> is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.</p>
<p>For example, consider the <img src="https://latex.codecogs.com/png.latex?2%5Ctimes%202"> matrix <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%0A%5Cend%7Bbmatrix%7D.%0A"> The inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%0A%5Cfrac%7B1%7D%7B-2%7D%0A%5Cbegin%7Bbmatrix%7D%0A4%20&amp;%20-2%20%5C%5C%0A-3%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A-2%20&amp;%201%20%5C%5C%0A%5Cfrac%7B3%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>We can verify that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BI%7D"> by computing:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A-2%20&amp;%201%20%5C%5C%0A%5Cfrac%7B3%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20%5C%5C%0A0%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7BI%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%0A%5Cbegin%7Bbmatrix%7D%20-2%20&amp;%201%20%5C%5C%0A%5Cfrac%7B3%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20%5C%5C%0A0%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7BI%7D%0A"></p>
<p>Let me give another example and A be a <img src="https://latex.codecogs.com/png.latex?3x3"> square matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20a_%7B13%7D%20%5C%5C%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B32%7D%20&amp;%20a_%7B33%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"> Then, the inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E%7B-1%7D">, is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cfrac%7B1%7D%7B%5Ctext%7Bdet%7D(%5Cmathbf%7BA%7D)%7D%5Cbegin%7Bbmatrix%7D%0Aa_%7B22%7Da_%7B33%7D-a_%7B23%7Da_%7B32%7D%20&amp;%20a_%7B13%7Da_%7B32%7D-a_%7B12%7Da_%7B33%7D%20&amp;%20a_%7B12%7Da_%7B23%7D-a_%7B13%7Da_%7B22%7D%20%5C%5C%0Aa_%7B23%7Da_%7B31%7D-a_%7B21%7Da_%7B33%7D%20&amp;%20a_%7B11%7Da_%7B33%7D-a_%7B13%7Da_%7B31%7D%20&amp;%20a_%7B13%7Da_%7B21%7D-a_%7B11%7Da_%7B23%7D%20%5C%5C%0Aa_%7B21%7Da_%7B32%7D-a_%7B22%7Da_%7B31%7D%20&amp;%20a_%7B12%7Da_%7B31%7D-a_%7B11%7Da_%7B32%7D%20&amp;%20a_%7B11%7Da_%7B22%7D-a_%7B12%7Da_%7B21%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?det(%5Cmathbf%7BA%7D)"> is the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>For example, let:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%20%5C%5C%0A0%20&amp;%201%20&amp;%204%20%5C%5C%0A5%20&amp;%206%20&amp;%200%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"> Then, <img src="https://latex.codecogs.com/png.latex?det(A)"> = -57, and the inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cfrac%7B1%7D%7B-57%7D%5Cbegin%7Bbmatrix%7D%0A-24%20&amp;%2018%20&amp;%205%20%5C%5C%0A20%20&amp;%20-15%20&amp;%20-4%20%5C%5C%0A-3%20&amp;%202%20&amp;%201%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>There are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.</p>
</section>
<section id="rank" class="level3" data-number="2.10">
<h3 data-number="2.10" class="anchored" data-anchor-id="rank"><span class="header-section-number">2.10</span> Rank</h3>
<p>The rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Brank%7D(%5Cmathbf%7BA%7D)">.</p>
<p>For example, consider the following matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%20%20%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%201%20&amp;%202%20&amp;%203%5C%5C%0A%20%20%20%204%20&amp;%205%20&amp;%206%20%5C%5C%0A%20%20%20%207%20&amp;%208%20&amp;%209%5C%5C%0A%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>The columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is 2.</p>
</section>
<section id="trace" class="level3" data-number="2.11">
<h3 data-number="2.11" class="anchored" data-anchor-id="trace"><span class="header-section-number">2.11</span> Trace</h3>
<p>The trace of a square matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Btr%7D(%5Cmathbf%7BA%7D)">, is defined as the sum of the diagonal elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. In other words, if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> matrix, then its trace is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathrm%7Btr%7D(%5Cmathbf%7BA%7D)=%5Csum_%7Bi=1%7D%5E%7Bn%7Da_%7Bij%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?a_%7Bii%7D"> denotes the <img src="https://latex.codecogs.com/png.latex?i"> th diagonal element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%20%202%20&amp;%203%20&amp;%201%20%5C%5C%0A%20%200%20&amp;%205%20&amp;%202%20%5C%5C%0A%20%201%20&amp;%201%20&amp;%204%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>Then, the trace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Btr%7D(%5Cmathbf%7BA%7D)%20=%202%20+%205%20+%204%20=%2011"></p>
</section>
<section id="eigenvalues-and-eigenvectors" class="level3" data-number="2.12">
<h3 data-number="2.12" class="anchored" data-anchor-id="eigenvalues-and-eigenvectors"><span class="header-section-number">2.12</span> Eigenvalues and Eigenvectors</h3>
<p>Let A be an <img src="https://latex.codecogs.com/png.latex?n%20%C3%97%20n"> square matrix. A scalar <img src="https://latex.codecogs.com/png.latex?%5Clambda"> is called an eigenvalue of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> if there exists a non-zero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> such that <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BAv%7D=%5Clambda%5Cmathbf%7Bv%7D%0A"></p>
<p>Such a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> is called an eigenvector corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda">.</p>
<p>Example:</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be the matrix</p>
<p>To find the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, we solve the characteristic equation <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdet%7D(%5Cmathbf%20A%20-%20%5Clambda%20%5Cmathbf%20I%20)%20=%200">, where I is the n × n identity matrix.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%5Ctext%7Bdet%7D(%5Cmathbf%20A%20-%20%5Clambda%20%5Cmathbf%20I%20)%0A%20%20&amp;=%0A%20%20%20%20%5Cbegin%7Bvmatrix%7D%0A%20%20%20%203%20-%20%5Clambda%20&amp;%201%20%5C%5C%0A%20%20%20%201%20&amp;%203%20-%20%5Clambda%0A%20%20%20%20%5Cend%7Bvmatrix%7D%20%5C%5C%0A%20%20&amp;=%0A%20%20(3%20-%20%5Clambda)(3%20-%20%5Clambda)%20-%201%20%5C%5C%0A%20%20&amp;=%20%5Clambda%5E2%20-%206%5Clambda%20+%208%20=%200%0A%5Cend%7Balign*%7D%0A"></p>
<p>Solving this quadratic equation gives us the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">: <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202"> and <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">.</p>
<p>To find the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202">, we solve the equation <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%20A%20-%202%20%5Cmathbf%20I)%5Cmathbf%7Bv%7D%20=%20%5Cmathbf%7B0%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20I"> is the <img src="https://latex.codecogs.com/png.latex?2%20%5Ctimes%202"> identity matrix.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20(%5Cmathbf%20A%20-%202%20%5Cmathbf%20I)%5Cmathbf%7Bv%7D%20=%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%201%20&amp;%201%20%5C%5C%0A%20%20%20%20%20%201%20&amp;%201%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20x%20%5C%5C%0A%20%20%20%20y%0A%20%20%5Cend%7Bbmatrix%7D%20=%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%200%20%5C%5C%0A%20%20%20%200%0A%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p>Solving this system of equations gives us the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202">: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_1%7D%20=%20%5Cbegin%7Bbmatrix%7D%20-1%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"></p>
<p>Similarly, for <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">, we solve the equation <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%20A%20-%204%5Cmathbf%20I)%5Cmathbf%7Bv%7D"> = <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D"> to get the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_2%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"></p>
<div class="cell">

</div>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/02.basic_matrix.html</guid>
  <pubDate>Thu, 30 Mar 2023 15:00:00 GMT</pubDate>
  <media:content url="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/images/chap02_09.PNG" medium="image"/>
</item>
<item>
  <title>Basics (3) - Special Matrices</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/03.basic_special_matrix.html</link>
  <description><![CDATA[ 



<section id="special-matrix" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="special-matrix"><span class="header-section-number">1</span> Special Matrix</h2>
<section id="square-matrix" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="square-matrix"><span class="header-section-number">1.1</span> Square Matrix</h3>
<p>A square matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a matrix with the same number of rows and columns, i.e., <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> matrix.</p>
<p>For example, the following is a <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203"> square matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%204%20&amp;%207%5C%5C%0A2%20&amp;%205%20&amp;%208%5C%5C%0A3%20&amp;%206%20&amp;%209%0A%5Cend%7Bbmatrix%7D%0A"></p>
<section id="properties" class="level4" data-number="1.1.1">
<h4 data-number="1.1.1" class="anchored" data-anchor-id="properties"><span class="header-section-number">1.1.1</span> Properties</h4>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be an <img src="https://latex.codecogs.com/png.latex?n%5Ctimes%20n"> square matrix. Then, the following properties hold:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is invertible if and only if <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdet%7D(%5Cmathbf%7BA%7D)%20%5Cneq%200">.</li>
<li>The trace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is defined as <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Btr%7D(%5Cmathbf%7BA%7D)%20=%20%5Csum_%7Bi=1%7D%5En%20a_%7Bii%7D">, where <img src="https://latex.codecogs.com/png.latex?a_%7Bii%7D"> is the <img src="https://latex.codecogs.com/png.latex?i">th diagonal element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is symmetric, then it has <img src="https://latex.codecogs.com/png.latex?n"> real eigenvalues and an orthonormal set of eigenvectors.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is diagonalizable, then <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> can be written as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BPDP%7D%5E%7B-1%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BP%7D"> is the matrix whose columns are the eigenvectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BD%7D"> is the diagonal matrix whose diagonal elements are the corresponding eigenvalues.</li>
<li>The transpose of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E%5Ctop">, is obtained by reflecting <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> across its main diagonal. That is, <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BA%7D%5E%5Ctop)%7Bij%7D%20=%20a%7Bji%7D">.</li>
</ul>
<p>Here’s an example of a <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203"> matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%204%20&amp;%207%5C%5C%0A2%20&amp;%205%20&amp;%208%5C%5C%0A3%20&amp;%206%20&amp;%209%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>With this matrix, we can see that <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdet%7D(%5Cmathbf%7BA%7D)%20=%200">, so <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is not invertible. The trace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Btr%7D(%5Cmathbf%7BA%7D)%20=%201%20+%205%20+%209%20=%2015">. Since <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is not symmetric, we cannot say that it has real eigenvalues and an orthonormal set of eigenvectors. However, we can check that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is diagonalizable, and we can find that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BPDP%7D%5E%7B-1%7D"> with</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0AP=%5Cbegin%7Bpmatrix%7D%0A-0.8252%20&amp;%20-0.2886%20&amp;%200.4848%5C%5C%0A-0.3779%20&amp;%20-0.7551%20&amp;%20-0.5375%5C%5C%0A0.2185%20&amp;%20-0.5800%20&amp;%200.7830%0A%5Cend%7Bpmatrix%7D,%20%5Ctext%7B%20%7D%20D=%5Cbegin%7Bpmatrix%7D%0A16.1168%20&amp;%200%20&amp;%200%5C%5C%0A0%20&amp;%20-1.1168%20&amp;%200%5C%5C%0A0%20&amp;%200%20&amp;%200%0A%5Cend%7Bpmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
</section>
</section>
<section id="diagonal-matrix" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="diagonal-matrix"><span class="header-section-number">1.2</span> Diagonal Matrix</h3>
<p>A diagonal matrix is a square matrix in which all the off-diagonal elements are zero. The diagonal elements can be any scalar value.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BD%7D%20=%20%5Cbegin%7Bpmatrix%7D%0Ad_%7B1%7D%20&amp;%200%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%0A0%20&amp;%20d_%7B2%7D%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%20d_%7B3%7D%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A0%20&amp;%200%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%20d_%7Bn%7D%0A%5Cend%7Bpmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BD%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> diagonal matrix with diagonal elements <img src="https://latex.codecogs.com/png.latex?d_1,%20d_2,%20%5Cldots,%20d_n">. An example of a <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203"> diagonal matrix is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BD%7D%20=%20%5Cbegin%7Bpmatrix%7D%0A2%20&amp;%200%20&amp;%200%20%5C%5C%0A0%20&amp;%20-1%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%204%0A%5Cend%7Bpmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<section id="properties-1" class="level4" data-number="1.2.1">
<h4 data-number="1.2.1" class="anchored" data-anchor-id="properties-1"><span class="header-section-number">1.2.1</span> Properties</h4>
<p>A diagonal matrix is a square matrix in which all the off-diagonal elements are zero, i.e., <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D%20=%200"> for <img src="https://latex.codecogs.com/png.latex?i%20%5Cneq%20j">. Some properties of a diagonal matrix are:</p>
<ul>
<li><p>For two diagnoal matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20E">, <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BDE%7D=%0A%5Cbegin%7Bpmatrix%7D%0Ad_%7B1%7De_%7B1%7D%20&amp;%200%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%0A0%20&amp;%20d_%7B2%7De_%7B2%7D%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%20d_%7B3%7De_%7B3%7D%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A0%20&amp;%200%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%20d_%7Bn%7De_%7Bn%7D%0A%5Cend%7Bpmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p></li>
<li><p>The determinant of a diagonal matrix is the product of its diagonal entries.</p></li>
<li><p>The trace of a diagonal matrix is the sum of its diagonal entries.</p></li>
<li><p>The inverse of a non-singular diagonal matrix is a diagonal matrix with the reciprocal of its diagonal entries as its diagonal entries.</p></li>
</ul>
<p>An example of a <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203"> diagonal matrix is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BD%7D%20=%20%5Cbegin%7Bpmatrix%7D%0A5%20&amp;%200%20&amp;%200%20%5C%5C%0A0%20&amp;%20-3%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%207%0A%5Cend%7Bpmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
</section>
</section>
<section id="identity-matrix" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="identity-matrix"><span class="header-section-number">1.3</span> Identity Matrix</h3>
<p>An identity matrix is a square matrix in which all the diagonal elements are equal to <img src="https://latex.codecogs.com/png.latex?1"> and all the off-diagonal elements are equal to 0. The notation for an identity matrix of size n is <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D_n">.</p>
<p>Here’s an example of a 3x3 identity matrix: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Cmathbf%7BI%7D_3%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20&amp;%200%20%5C%5C%0A0%20&amp;%201%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%201%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation%7D%0A"></p>
<section id="properties-2" class="level4" data-number="1.3.1">
<h4 data-number="1.3.1" class="anchored" data-anchor-id="properties-2"><span class="header-section-number">1.3.1</span> Properties</h4>
<p>An identity matrix, denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D">, is a square matrix with ones on the diagonal and zeros elsewhere.</p>
<p>For example, the <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203"> identity matrix is: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bpmatrix%7D%0A1%20&amp;%200%20&amp;%200%20%5C%5C%0A0%20&amp;%201%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%201%0A%5Cend%7Bpmatrix%7D%0A"></p>
<p>Some properties of an identity matrix include:</p>
<ul>
<li>Multiplying any matrix by an identity matrix results in the same matrix: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20%5Cmathbf%7BI%7D%20=%20%5Cmathbf%7BI%7D%20%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BA%7D">.</li>
<li>The product of any matrix and its corresponding inverse is an identity matrix: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BA%7D%5E%7B-1%7D%20%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BI%7D">.</li>
<li>The determinant of an identity matrix is 1: <img src="https://latex.codecogs.com/png.latex?%5Cdet(%5Cmathbf%7BI%7D)%20=%201">.</li>
<li>An identity matrix is symmetric: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D%20=%20%5Cmathbf%7BI%7D%5ET">.</li>
</ul>
</section>
</section>
<section id="symmetric-matrix" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="symmetric-matrix"><span class="header-section-number">1.4</span> Symmetric Matrix</h3>
<p>A symmetric matrix is a square matrix that is equal to its own transpose, i.e., <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BA%7D%5ET">. Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> matrix, then <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is symmetric if and only if <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D%20=%20a_%7Bji%7D"> for all <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j"> such that <img src="https://latex.codecogs.com/png.latex?1%20%5Cle%20i">, <img src="https://latex.codecogs.com/png.latex?j%20%5Cle%20n">.</p>
<p>Here’s an example of a symmetric matrix: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bpmatrix%7D%0A1%20&amp;%202%20&amp;%203%20%5C%5C%0A2%20&amp;%204%20&amp;%205%20%5C%5C%0A3%20&amp;%205%20&amp;%206%0A%5Cend%7Bpmatrix%7D%0A"></p>
<section id="properties-3" class="level4" data-number="1.4.1">
<h4 data-number="1.4.1" class="anchored" data-anchor-id="properties-3"><span class="header-section-number">1.4.1</span> Properties</h4>
<p>A symmetric matrix is a square matrix that is equal to its own transpose. Some properties of symmetric matrices include:</p>
<ul>
<li>The diagonal entries are real numbers.</li>
<li>The matrix is diagonalizable, meaning it can be expressed as a product of diagonal and orthogonal matrices.</li>
<li>The eigenvalues of a symmetric matrix are real numbers.</li>
<li>The eigenvectors corresponding to different eigenvalues are orthogonal.</li>
<li>The sum and difference of two symmetric matrices is also symmetric.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A%20=%20%5Cbegin%7Bpmatrix%7D%0A1%20&amp;%202%20&amp;%203%20%5C%5C%0A2%20&amp;%204%20&amp;%205%20%5C%5C%0A3%20&amp;%205%20&amp;%206%0A%5Cend%7Bpmatrix%7D%0A"></p>
</section>
</section>
<section id="idempotent-matrix" class="level3" data-number="1.5">
<h3 data-number="1.5" class="anchored" data-anchor-id="idempotent-matrix"><span class="header-section-number">1.5</span> Idempotent Matrix</h3>
<p>An idempotent matrix is a square matrix that when multiplied by itself yields itself. Formally, an idempotent matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BP%7D"> satisfies <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BP%7D%5E2%20=%20%5Cmathbf%7BP%7D">.</p>
<p>An example of an idempotent matrix is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Cmathbf%7BP%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%201%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation%7D%0A"></p>
<section id="properties-4" class="level4" data-number="1.5.1">
<h4 data-number="1.5.1" class="anchored" data-anchor-id="properties-4"><span class="header-section-number">1.5.1</span> Properties</h4>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E2%20=%20%5Cmathbf%20A">.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an idempotent matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI_n%7D-%5Cmathbf%7BA%7D"> is also an idempotent matrix. <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A(%5Cmathbf%7BI_n%7D-%5Cmathbf%7BA%7D)%5E2&amp;=(%5Cmathbf%7BI_n%7D-%5Cmathbf%7BA%7D)(%5Cmathbf%7BI_n%7D-%5Cmathbf%7BA%7D)%5C%5C%0A&amp;=%5Cmathbf%7BI_n%7D%5Cmathbf%7BI_n%7D-%5Cmathbf%7BI_n%7D%5Cmathbf%7BA%7D-%5Cmathbf%7BA%7D%5Cmathbf%7BI_n%7D-%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5C%5C%0A&amp;=%5Cmathbf%7BI_n%7D%5Cmathbf%7BI_n%7D-%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5C%5C%0A&amp;=%5Cmathbf%7BI_n%7D-%5Cmathbf%7BA%7D%0A%5Cend%7Baligned%7D%0A"></li>
<li>The determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is either 0 or 1.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is symmetric, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is idempotent if only if the eigenvalue of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is either <img src="https://latex.codecogs.com/png.latex?0"> or <img src="https://latex.codecogs.com/png.latex?1">.</li>
<li>The rank of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is equal to the trace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, which is the sum of the diagonal elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">.</li>
</ul>
</section>
</section>
<section id="one-matrix" class="level3" data-number="1.6">
<h3 data-number="1.6" class="anchored" data-anchor-id="one-matrix"><span class="header-section-number">1.6</span> One Matrix</h3>
<p>The one matrix, denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BJ%7D">, is a matrix in which every entry is equal to 1.</p>
<p>Here is an example of a 3x3 one matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BJ%7D%20=%0A%5Cbegin%7Bpmatrix%7D%0A1%20&amp;%201%20&amp;%201%20%5C%5C%0A1%20&amp;%201%20&amp;%201%20%5C%5C%0A1%20&amp;%201%20&amp;%201%0A%5Cend%7Bpmatrix%7D%0A"></p>
<section id="properties-5" class="level4" data-number="1.6.1">
<h4 data-number="1.6.1" class="anchored" data-anchor-id="properties-5"><span class="header-section-number">1.6.1</span> Properties</h4>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J"> is a square matrix</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J"> is symmetric</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J"> is idempotent</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J"> has rank 1 If A is any <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> matrix, then <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BAJ%7D%20=%20%5Cmathbf%7BJA%7D%20=%20trace(%5Cmathbf%7BA%7D)%5Cmathbf%7BJ%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Btrace%7D(%5Cmathbf%7BA%7D)"> is the sum of the diagonal elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J_%7Bm%5Ctimes%20n%7D"> can be represented as the product of two vectors, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%201_%7Bm%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%201_%7Bn%7D">, i.e., <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J_%7Bm%5Ctimes%20n%7D%20=%20%5Cmathbf%201_%7Bm%7D%20%5Cmathbf%201_%7Bn%7D%5ET"></li>
</ul>
</section>
<section id="applications" class="level4" data-number="1.6.2">
<h4 data-number="1.6.2" class="anchored" data-anchor-id="applications"><span class="header-section-number">1.6.2</span> Applications</h4>
<ul>
<li><p>calculate a sum using a <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%201_n"> vector for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x_n">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%201%5ET%5Cmathbf%20x%20=%5Csum_%7Bi=1%7D%5E%7Bn%7D1%5Ctimes%20x_i=x_1+x_2+%5Cdots+x_n%0A"></p></li>
<li><p>calculate a mean using a <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%201_n"> vector for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x_n">:</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbar%7Bx%7D=%5Cfrac%7B1%7D%7Bn%7D%5Cmathbf%201%5ET%5Cmathbf%20x%20=%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi=1%7D%5E%7Bn%7D1%5Ctimes%20x_i=%5Cfrac%7B1%7D%7Bn%7D(x_1+x_2+%5Cdots+x_n)%0A"></p>
<ul>
<li>calculate <img src="https://latex.codecogs.com/png.latex?n"> column sums of a dataset using a <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%201_m"> vector for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20X_%7Bm%5Ctimes%20n%7D">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbar%7B%5Cmathbf%20x%7D=%5Cfrac%7B1%7D%7Bm%7D%5Cmathbf%20X%5ET%5Cmathbf%201%0A"> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cbar%7B%5Cmathbf%20x%7D&amp;=%5Cfrac%7B1%7D%7Bm%7D%5Cmathbf%20X%5ET%5Cmathbf%201%20%5C%5C%0A&amp;=%5Cfrac%7B1%7D%7Bm%7D%0A%5Cbegin%7Bbmatrix%7D%0Ax_%7B11%7D%20&amp;%20x_%7B21%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7Bm1%7D%20%5C%5C%0Ax_%7B12%7D%20&amp;%20x_%7B22%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7Bm2%7D%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0Ax_%7B1n%7D%20&amp;%20x_%7B2n%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7Bmn%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A1_1%20%5C%5C%201_2%20%5C%5C%20%5Cvdots%20%5C%5C%201_m%0A%5Cend%7Bbmatrix%7D%5C%5C%0A&amp;=%5Cfrac%7B1%7D%7Bm%7D%0A%5Cbegin%7Bbmatrix%7D%0Ax_%7B11%7D%20+%20x_%7B21%7D%20+%20%5Ccdots%20+%20x_%7Bm1%7D%20%5C%5C%0Ax_%7B12%7D%20+%20x_%7B22%7D%20+%20%5Ccdots%20+%20x_%7Bm2%7D%20%5C%5C%0A%5Cvdots%20%20%5C%5C%0Ax_%7B1n%7D%20+%20x_%7B2n%7D%20+%20%5Ccdots%20+%20x_%7Bmn%7D%0A%5Cend%7Bbmatrix%7D%20%5C%5C%0A&amp;=%5Cfrac%7B1%7D%7Bm%7D%0A%5Cbegin%7Bbmatrix%7D%0A%5Csum_%7Bi=1%7D%5E%7Bm%20%7Dx_%7Bi1%7D%20%5C%5C%0A%5Csum_%7Bi=1%7D%5E%7Bm%20%7Dx_%7Bi2%7D%20%5C%5C%0A%5Cvdots%20%20%5C%5C%0A%5Csum_%7Bi=1%7D%5E%7Bm%20%7Dx_%7Bjn%7D%0A%5Cend%7Bbmatrix%7D%20%5C%5C%0A&amp;=%0A%5Cbegin%7Bbmatrix%7D%0A%5Cbar%7Bx%7D_%7B1%7D%20%5C%5C%0A%5Cbar%7Bx%7D_%7B2%7D%20%5C%5C%0A%5Cvdots%20%20%5C%5C%0A%5Cbar%7Bx%7D_%7Bn%7D%0A%5Cend%7Bbmatrix%7D%20%5C%5C%0A&amp;=%5Cbar%7B%5Cmathbf%20x%7D%0A%5Cend%7Baligned%7D%0A"></li>
</ul>
</section>
</section>
<section id="centering-matrix" class="level3" data-number="1.7">
<h3 data-number="1.7" class="anchored" data-anchor-id="centering-matrix"><span class="header-section-number">1.7</span> Centering Matrix</h3>
<p>A centering matrix is a square matrix that is used in multivariate statistical analysis to center data by subtracting the mean of each variable from each observation. The centering matrix is a symmetric and idempotent matrix, meaning that it is equal to its own transpose and that multiplying it by itself results in the same matrix. The centering matrix is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%20%20%5Cmathbf%20C%20=%20%5Cmathbf%20I%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Cmathbf%20J%0A%5Cend%7Bequation%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20I"> is the identity matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20J"> is a matrix of ones, and <img src="https://latex.codecogs.com/png.latex?m"> is the number of observations.</p>
<section id="properties-6" class="level4" data-number="1.7.1">
<h4 data-number="1.7.1" class="anchored" data-anchor-id="properties-6"><span class="header-section-number">1.7.1</span> Properties</h4>
<p>The centering matrix has the property that when it is multiplied by a data matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20X">, it centers the data by subtracting the mean of each column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20X"> from each element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20X">. The resulting matrix is called the centered data matrix with the mean equal to <img src="https://latex.codecogs.com/png.latex?0">. The centering matrix is often used in multivariate statistical analysis, such as principal component analysis, to transform the data into a new coordinate system where the variance of each variable is equal to its eigenvalue.</p>
<ul>
<li>A centering matrix is a square matrix.</li>
<li>The diagonal elements of a centering matrix are all equal and are given by <img src="https://latex.codecogs.com/png.latex?n%5E%7B-1%7D">, where <img src="https://latex.codecogs.com/png.latex?n"> is the size of the matrix.</li>
<li>The off-diagonal elements of a centering matrix are all equal and are given by <img src="https://latex.codecogs.com/png.latex?-n%5E%7B-1%7D">.</li>
<li>Multiplying a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> on the left by a centering matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20C"> is equivalent to subtracting the mean of the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> from each column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">.</li>
<li>Multiplying a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> on the right by a centering matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20C"> is equivalent to subtracting the mean of the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> from each row of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">.</li>
</ul>
<p>Here is an example of a centering matrix of size <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%20C%20=%20%5Cfrac%7B1%7D%7B3%7D%20%5Cbegin%7Bpmatrix%7D%0A2%20&amp;%20-1%20&amp;%20-1%20%5C%5C%0A-1%20&amp;%202%20&amp;%20-1%20%5C%5C%0A-1%20&amp;%20-1%20&amp;%202%0A%5Cend%7Bpmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
</section>
<section id="applications-1" class="level4" data-number="1.7.2">
<h4 data-number="1.7.2" class="anchored" data-anchor-id="applications-1"><span class="header-section-number">1.7.2</span> Applications</h4>
<ul>
<li><p>find a centered matrix, <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7B%5Cmathbf%20X%7D"> using a <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%201_m"> vector for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20X_%7Bm%5Ctimes%20n%7D">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20C%20=%20%5Cmathbf%20I%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Cmathbf%20J%0A"> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbf%20X&amp;=%0A%5Cbegin%7Bbmatrix%7D%0Ax_%7B11%7D%20&amp;%20x_%7B21%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7Bm1%7D%20%5C%5C%0Ax_%7B12%7D%20&amp;%20x_%7B22%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7Bm2%7D%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0Ax_%7B1n%7D%20&amp;%20x_%7B2n%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7Bmn%7D%0A%5Cend%7Bbmatrix%7D%20%5Ctext%7B%20%7D%20%5C%5C%0A%5Cmathbf%20%7B1%5Cbar%7Bx%7D%5ET%7D&amp;=%0A%5Cbegin%7Bbmatrix%7D%0A1%20%5C%5C%0A1%20%5C%5C%0A%5Cvdots%20%5C%5C%0A1%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%5Cbar%7Bx%7D_%7B1%7D%20&amp;%20%5Cbar%7Bx%7D_%7B2%7D%20&amp;%20%5Cdots%20&amp;%20%5Cbar%7Bx%7D_%7Bn%7D%0A%5Cend%7Bbmatrix%7D%5C%5C%0A&amp;=%5Cbegin%7Bbmatrix%7D%0A%5Cbar%7Bx%7D_%7B1%7D%20&amp;%20%5Cbar%7Bx%7D_%7B2%7D%20&amp;%20%5Cdots%20&amp;%20%5Cbar%7Bx%7D_%7Bn%7D%5C%5C%0A%5Cbar%7Bx%7D_%7B1%7D%20&amp;%20%5Cbar%7Bx%7D_%7B2%7D%20&amp;%20%5Cdots%20&amp;%20%5Cbar%7Bx%7D_%7Bn%7D%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%5C%5C%0A%5Cbar%7Bx%7D_%7B1%7D%20&amp;%20%5Cbar%7Bx%7D_%7B2%7D%20&amp;%20%5Cdots%20&amp;%20%5Cbar%7Bx%7D_%7Bn%7D%0A%5Cend%7Bbmatrix%7D%5C%5C%0A%5C%5C%0A%5Ctilde%7B%5Cmathbf%20X%7D&amp;=%5Cmathbf%20X%20-%7B1%7D_%7Bm%7D%5Cbar%7B%5Cmathbf%20x%7D%5ET%5C%5C%0A&amp;=%5Cmathbf%20X%20-%5Cmathbf%7B1%7D_%7Bm%7D(%5Cfrac%7B1%7D%7Bm%7D%5Cmathbf%20X%5ET%20%5Cmathbf%7B1%7D_m)%5ET%5C%5C%0A&amp;=%5Cmathbf%20X%20-%5Cmathbf%7B1%7D_%7Bm%7D%5Cmathbf%7B1%7D_m%5ET%5Cfrac%7B1%7D%7Bm%7D%5Cmathbf%20X%5C%5C%0A&amp;=(%5Cmathbf%20I%20-%5Cfrac%7B1%7D%7Bm%7D%5Cmathbf%20J)%5Cmathbf%20X%5C%5C%0A&amp;=%5Cmathbf%20C%20%5Cmathbf%20X%5C%5C%0A%5Cend%7Baligned%7D%0A"></p></li>
<li><p>represent a covariance matrix as a centered matrix form: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Coperatorname%7BCov%7D(%5Cmathbf%20X)&amp;=%5Coperatorname%7BE%7D(%5Cmathbf%20X-%5Coperatorname%7BE%7D(%5Cmathbf%20X))%5E2%5C%5C%0A&amp;=%5Coperatorname%7BE%7D(%5Cmathbf%20X-%5Coperatorname%7BE%7D(%5Cmathbf%20X))%5Coperatorname%7BE%7D(%5Cmathbf%20X-%5Coperatorname%7BE%7D(%5Cmathbf%20X))%5C%5C%0A&amp;=%5Cfrac%7B%5Csum_%7B%7D%5E%7B%7D(x_i-%5Cbar%7Bx_i%7D)(x_j-%5Cbar%7Bx_j%7D)%7D%7Bm-1%7D%5C%5C%0A&amp;=%5Cfrac%7B%5Ctilde%7B%5Cmathbf%7BX%7D%7D%5ET%5Ctilde%7B%5Cmathbf%7BX%7D%7D%7D%7Bm-1%7D%0A%5Cend%7Baligned%7D%0A"></p></li>
</ul>
</section>
</section>
<section id="positive-definite-matrix" class="level3" data-number="1.8">
<h3 data-number="1.8" class="anchored" data-anchor-id="positive-definite-matrix"><span class="header-section-number">1.8</span> Positive Definite Matrix</h3>
<p>A symmetric matrix <img src="https://latex.codecogs.com/png.latex?A"> is positive definite if and only if the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> is positive for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>To see why this is true, consider the eigenvalue decomposition of <img src="https://latex.codecogs.com/png.latex?A">, which can be written as <img src="https://latex.codecogs.com/png.latex?A%20=%20Q%20%5CLambda%20Q%5ET">, where <img src="https://latex.codecogs.com/png.latex?Q"> is an orthogonal matrix and <img src="https://latex.codecogs.com/png.latex?%5CLambda"> is a diagonal matrix containing the eigenvalues of <img src="https://latex.codecogs.com/png.latex?A">. Then, for any nonzero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbf%20x%5ET%20%5Cmathbf%20A%20%5Cmathbf%20x&amp;=%5Cmathbf%20x%5ET%20%5Cmathbf%20Q%20%5Cmathbf%20%5CLambda%20%5Cmathbf%20Q%5ET%20%5Cmathbf%20x%5C%5C%0A&amp;=(%5Cmathbf%20x%5ET%20%5Cmathbf%20Q)%5Cmathbf%20%5CLambda%20(%20%5Cmathbf%20Q%5ET%5Cmathbf%20x)%5C%5C%0A&amp;=%5Csum_%7Bi=1%7D%5E%7Bn%7D%20%5Clambda_iy_i%5E2%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?y_i%20=%20(%5Cmathbf%7Bx%7D%5ET%20Q)_i"> is the <img src="https://latex.codecogs.com/png.latex?i">th coordinate (i.e., a scalar value that represents the position of a point or a vector relative to a chosen basis) of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%5ET%20Q"> and <img src="https://latex.codecogs.com/png.latex?n"> is the dimension of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?A">. Note that since <img src="https://latex.codecogs.com/png.latex?Q"> is orthogonal, we have <img src="https://latex.codecogs.com/png.latex?Q%5ET%20Q%20=%20I">, so <img src="https://latex.codecogs.com/png.latex?y_i%20=%20%5Cmathbf%7Bq%7D_i%5ET%20%5Cmathbf%7Bx%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bq%7D_i"> is the <img src="https://latex.codecogs.com/png.latex?i">th column of <img src="https://latex.codecogs.com/png.latex?Q">. Therefore, the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)%20=%20%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> can be written in terms of the eigenvalues of <img src="https://latex.codecogs.com/png.latex?A"> and the coordinates of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> with respect to the eigenvectors of <img src="https://latex.codecogs.com/png.latex?A">.</p>
<p>Since <img src="https://latex.codecogs.com/png.latex?A"> is positive definite, we have <img src="https://latex.codecogs.com/png.latex?%5Clambda_i%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?i">, and so <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5En%20%5Clambda_i%20y_i%5E2%20%3E%200"> for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">. Therefore, the quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> is positive for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, which implies that <img src="https://latex.codecogs.com/png.latex?A"> is positive definite.</p>
<p>In other words, the positive definiteness of a symmetric matrix <img src="https://latex.codecogs.com/png.latex?A"> is equivalent to the positivity of the associated quadratic form <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)=%5Cmathbf%7Bx%7D%5ET%20A%20%5Cmathbf%7Bx%7D"> for all nonzero vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
<p>Therefore, a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is said to be positive definite if all of its eigenvalues are positive or equivalently, a symmetric matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is positive definite if left-multiplying and right-multiplying it by the same vector, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x"> always gives a positive number if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20x%5ET%20%5Cmathbf%20A%20%5Cmathbf%20x"></p>
<section id="properties-7" class="level4" data-number="1.8.1">
<h4 data-number="1.8.1" class="anchored" data-anchor-id="properties-7"><span class="header-section-number">1.8.1</span> Properties</h4>
<p>Properties of a positive definite matrix:</p>
<ul>
<li>All eigenvalues are positive.</li>
<li>The matrix is symmetric.</li>
<li>All principal submatrices have determinants that are positive.</li>
</ul>
<p>Example of a positive definite matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0AA%20=%20%5Cbegin%7Bpmatrix%7D%0A4%20&amp;%20-1%20&amp;%200%20%5C%5C%0A-1%20&amp;%205%20&amp;%20-1%20%5C%5C%0A0%20&amp;%20-1%20&amp;%202%0A%5Cend%7Bpmatrix%7D%0A%5Cend%7Bequation%7D%0A"></p>


</section>
</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/03.basic_special_matrix.html</guid>
  <pubDate>Thu, 30 Mar 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Basics (4) - Tensor</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/04.basic_tensor.html</link>
  <description><![CDATA[ 



<section id="tensor" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="tensor"><span class="header-section-number">1</span> Tensor</h2>
<p>A tensor is a mathematical object that generalizes vectors and matrices to higher dimensions. A tensor of order <img src="https://latex.codecogs.com/png.latex?n"> is an object that can be represented by a multidimensional array of <img src="https://latex.codecogs.com/png.latex?n"> indices. Each index can take on a range of values, which determine the dimensionality of the tensor along that axis.</p>
<p>For example, a rank-2 tensor (i.e., a matrix) can be represented as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A_%7Bij%7D%20%5Ctext%7B%20,%20%7D%20i=1,%5Cdots%20m%20%5Ctext%7B,%20%7D%20j=1,%5Cdots,n%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is the tensor, <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j"> are the indices, and <img src="https://latex.codecogs.com/png.latex?m"> and <img src="https://latex.codecogs.com/png.latex?n"> are the dimensions of the tensor along each axis. The entries of the tensor are given by <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D">.</p>
<p>A rank-3 tensor can be represented as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%20A_%7Bijk%7D%20%5Ctext%7B%20,%20%7D%20i=1,%5Cdots%20m%20%5Ctext%7B,%20%7D%20j=1,%5Cdots,n%5Ctext%7B,%20%7D%20k=1,%5Cdots,p%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is the tensor, <img src="https://latex.codecogs.com/png.latex?i">, <img src="https://latex.codecogs.com/png.latex?j">, and <img src="https://latex.codecogs.com/png.latex?k"> are the indices, and <img src="https://latex.codecogs.com/png.latex?m">, <img src="https://latex.codecogs.com/png.latex?n">, and <img src="https://latex.codecogs.com/png.latex?p"> are the dimensions of the tensor along each axis. The entries of the tensor are given by <img src="https://latex.codecogs.com/png.latex?A_%7Bijk%7D">.</p>
</section>
<section id="basic-tensor-operations" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="basic-tensor-operations"><span class="header-section-number">2</span> Basic Tensor Operations</h2>
<section id="section" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="section"><span class="header-section-number">2.1</span> </h3>


</section>
</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/04.basic_tensor.html</guid>
  <pubDate>Thu, 30 Mar 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Basics (3) - Special Matrices</title>
  <dc:creator>Kwangmin Kim</dc:creator>
  <link>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/04.linear_transformation.html</link>
  <description><![CDATA[ 



<section id="matrix" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="matrix"><span class="header-section-number">1</span> Matrix</h2>
<p>A matrix is a rectangular array of elements, usually real numbers, arranged in rows and columns. If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix, it can be represented as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%20%20a_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7B1n%7D%20%5C%5C%0A%20%20a_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7B2n%7D%20%5C%5C%0A%20%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A%20%20a_%7Bm1%7D%20&amp;%20a_%7Bm2%7D%20&amp;%20%5Ccdots%20&amp;%20a_%7Bmn%7D%0A%5Cend%7Bbmatrix%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> is the element in the <img src="https://latex.codecogs.com/png.latex?i">-th row and <img src="https://latex.codecogs.com/png.latex?j">-th column of the matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
</section>
<section id="basic-matrix-operations" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="basic-matrix-operations"><span class="header-section-number">2</span> Basic Matrix Operations</h2>
<section id="matrix-addition" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="matrix-addition"><span class="header-section-number">2.1</span> Matrix addition</h3>
<p>The sum of two matrices of the same size is a matrix of the same size obtained by adding corresponding entries.</p>
<p>Given two <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D">, their sum <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D%20=%20%5Cmathbf%7BA%7D%20+%20%5Cmathbf%7BB%7D"> is defined by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ac_%7Bi,j%7D=a_%7Bi,j%7D+b_%7Bi,j%7D%E2%80%8B%0A"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%20m"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20j%20%5Cleq%20n">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%20%201%20&amp;%202%20%5C%5C%0A%20%203%20&amp;%204%20%5C%5C%0A%20%205%20&amp;%206%0A%5Cend%7Bbmatrix%7D%20+%0A%5Cbegin%7Bbmatrix%7D%0A%20%20-1%20&amp;%200%20%5C%5C%0A%20%202%20&amp;%20-3%20%5C%5C%0A%20%20-5%20&amp;%204%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A%20%200%20&amp;%202%20%5C%5C%0A%20%205%20&amp;%201%20%5C%5C%0A%20%200%20&amp;%2010%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="scalar-multiplication" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="scalar-multiplication"><span class="header-section-number">2.2</span> Scalar multiplication</h3>
<p>The product of a scalar and a matrix is a matrix obtained by multiplying each entry of the matrix by the scalar.</p>
<p>Given a scalar <img src="https://latex.codecogs.com/png.latex?k"> and an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, their product <img src="https://latex.codecogs.com/png.latex?k%5Cmathbf%7BA%7D"> is defined by: <img src="https://latex.codecogs.com/png.latex?%0A(k%5Cmathbf%7BA%7D)_%7Bi,j%7D%20=%20k(a_%7Bi,j%7D)%0A"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%20m"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20j%20%5Cleq%20n">.</p>
<p>Example: <img src="https://latex.codecogs.com/png.latex?%0A2%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%20%5C%5C%0A5%20&amp;%206%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A2%20&amp;%204%20%5C%5C%0A6%20&amp;%208%20%5C%5C%0A10%20&amp;%2012%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="matrix-multiplication" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="matrix-multiplication"><span class="header-section-number">2.3</span> Matrix multiplication</h3>
<p>The product of two matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D"> is a matrix obtained by multiplying the rows of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> by the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D">.</p>
<p>Given two matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BB%7D"> with dimensions <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> and <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20p">, respectively, their product <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BC%7D%20=%20%5Cmathbf%7BAB%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20p"> matrix defined by: <img src="https://latex.codecogs.com/png.latex?%0Ac_%7Bi,j%7D%20=%20%5Csum_%7Bk=1%7D%5En%20a_%7Bi,k%7Db_%7Bk,j%7D%0A"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%20m"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20j%20%5Cleq%20p">.</p>
<p>Example: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%20%5C%5C%0A5%20&amp;%206%0A%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%0A-1%20&amp;%200%20&amp;%202%20%5C%5C%0A2%20&amp;%20-3%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A3%20&amp;%20-6%20&amp;%204%20%5C%5C%0A5%20&amp;%20-12%20&amp;%2010%20%5C%5C%0A7%20&amp;%20-18%20&amp;%2016%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="transpose" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="transpose"><span class="header-section-number">2.4</span> Transpose</h3>
<p>The transpose of an <img src="https://latex.codecogs.com/png.latex?m%20x%20n"> matrix A, denoted by <img src="https://latex.codecogs.com/png.latex?A%5ET">, is the <img src="https://latex.codecogs.com/png.latex?n%20x%20m"> matrix obtained by interchanging the rows and columns of A. Formally, if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Ba_%7Bij%7D%5D"> is an m x n matrix, then its transpose <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET%20=%20%5Bb_%7Bij%7D%5D"> is an <img src="https://latex.codecogs.com/png.latex?n%20x%20m"> matrix where <img src="https://latex.codecogs.com/png.latex?b_%7Bij%7D"> = <img src="https://latex.codecogs.com/png.latex?a_%7Bji%7D"> for all <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j">. In other words, the element in the <img src="https://latex.codecogs.com/png.latex?i"> th row and <img src="https://latex.codecogs.com/png.latex?j"> th column of <img src="https://latex.codecogs.com/png.latex?A%5ET"> is equal to the element in the <img src="https://latex.codecogs.com/png.latex?j"> th row and ith column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>Given an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, its transpose <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5ET"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m"> matrix defined by: $$ <em>{i,j}^T = </em>{j,i}</p>
<p>$$</p>
<ul>
<li>When <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is transposed, diagnoal entries(<img src="https://latex.codecogs.com/png.latex?a_%7Bii%7D">) do not change but off-diagnoal elements(<img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D%20%5C;%20i%20%5Cneq%20j">) change.</li>
<li>A column vector is tranposed into a row vector, and vice versa.</li>
<li>symmetric matrix: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BA%7D%5ET"></li>
</ul>
<p>Example:</p>
<p>Let A be the matrix <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%5C%5C%0A4%20&amp;%205%20&amp;%206%0A%5Cend%7Bbmatrix%7D%0A"> The transpose of A, denoted by A^T, is the matrix <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5ET%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%204%5C%5C%0A2%20&amp;%205%5C%5C%0A3%20&amp;%206%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="determinant" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="determinant"><span class="header-section-number">2.5</span> Determinant</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> square matrix. The determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted by <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7BA%7D%7C"> or <img src="https://latex.codecogs.com/png.latex?%5Cdet(%5Cmathbf%7BA%7D)">, is a scalar value calculated as the sum of the products of the elements in any row or column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> with their corresponding cofactors, that is,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%5Cmathbf%7BA%7D%7C=%5Csum_%7Bi=1%7D%5E%7Bn%7Da_%7Bij%7DC_%7Bij%7D=%5Csum_%7Bj=1%7D%5E%7Bn%7Da_%7Bij%7DC_%7Bij%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> is the element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> in the <img src="https://latex.codecogs.com/png.latex?i">-th row and <img src="https://latex.codecogs.com/png.latex?j">-th column, and <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D"> is the cofactor of <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D">. The cofactor of <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D">, denoted by <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D">, is given by <img src="https://latex.codecogs.com/png.latex?(-1)%5E%7Bi+j%7D"> times the determinant of the <img src="https://latex.codecogs.com/png.latex?(n-1)%20%5Ctimes%20(n-1)"> matrix obtained by deleting the <img src="https://latex.codecogs.com/png.latex?i">-th row and <img src="https://latex.codecogs.com/png.latex?j">-th column of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>The determinant of an <img src="https://latex.codecogs.com/png.latex?n%20x%20n"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a scalar value denoted as <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7BA%7D%7C">. It is defined as the sum of all possible products of n elements taken one from each row and one from each column, where the sign of each product alternates according to the position of the element in the matrix. For example, the determinant of a <img src="https://latex.codecogs.com/png.latex?3%20x%203"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20="> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20a_%7B13%7D%20%5C%5C%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B32%7D%20&amp;%20a_%7B33%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%5Cmathbf%7BA%7D%7C%20=%20a_%7B11%7D%0A%5Cbegin%7Bvmatrix%7D%0Aa_%7B22%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B32%7D%20&amp;%20a_%7B33%7D%0A%5Cend%7Bvmatrix%7D%0A-%20a_%7B12%7D%0A%5Cbegin%7Bvmatrix%7D%0Aa_%7B21%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B33%7D%0A%5Cend%7Bvmatrix%7D%0A+%20a_%7B13%7D%0A%5Cbegin%7Bvmatrix%7D%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B32%7D%0A%5Cend%7Bvmatrix%7D%0A"></p>
<p>For example, consider the <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%203"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20&amp;%203%20%5C%5C%204%20&amp;%205%20&amp;%206%20%5C%5C%207%20&amp;%208%20&amp;%209%20%5Cend%7Bbmatrix%7D">. We can calculate the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> using any row or column. Let’s use the first column:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7C%5Cmathbf%7BA%7D%7C%20=%201%0A%5Cbegin%7Bvmatrix%7D%0A5%20&amp;%206%20%5C%5C%0A8%20&amp;%209%0A%5Cend%7Bvmatrix%7D%0A-%204%0A%5Cbegin%7Bvmatrix%7D%0A2%20&amp;%203%5C%5C%0A8%20&amp;%209%0A%5Cend%7Bvmatrix%7D%0A+%207%0A%5Cbegin%7Bvmatrix%7D%0A2%20&amp;%205%20%5C%5C%0A3%20&amp;%206%0A%5Cend%7Bvmatrix%7D%20=%200%0A"></p>
<p>Therefore, the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is zero.</p>
</section>
<section id="inverse" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="inverse"><span class="header-section-number">2.6</span> Inverse</h3>
<p>The inverse of a square matrix <img src="https://latex.codecogs.com/png.latex?A"> of size <img src="https://latex.codecogs.com/png.latex?n"> is a matrix <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> such that the product of <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> is the identity matrix <img src="https://latex.codecogs.com/png.latex?I_n">, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?A%20%5Ctimes%20A%5E%7B-1%7D%20=%20I_n">. If such a matrix exists, then <img src="https://latex.codecogs.com/png.latex?A"> is said to be invertible or non-singular.</p>
<p>The inverse of a square matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E%7B-1%7D"> and is defined as the unique matrix that satisfies the following equation: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BI%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D"> is the identity matrix. Not all matrices have an inverse, and a matrix that has an inverse is called invertible or nonsingular. A matrix that does not have an inverse is called singular.</p>
<p>For example, consider the <img src="https://latex.codecogs.com/png.latex?2%5Ctimes%202"> matrix <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%0A%5Cend%7Bbmatrix%7D.%0A"> The inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%0A%5Cfrac%7B1%7D%7B-2%7D%0A%5Cbegin%7Bbmatrix%7D%0A4%20&amp;%20-2%20%5C%5C%0A-3%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A-2%20&amp;%201%20%5C%5C%0A%5Cfrac%7B3%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>We can verify that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BI%7D"> by computing:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A-2%20&amp;%201%20%5C%5C%0A%5Cfrac%7B3%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20%5C%5C%0A0%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7BI%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%0A%5Cbegin%7Bbmatrix%7D%20-2%20&amp;%201%20%5C%5C%0A%5Cfrac%7B3%7D%7B2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B2%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20%5C%5C%0A3%20&amp;%204%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20%5C%5C%0A0%20&amp;%201%0A%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7BI%7D%0A"></p>
<p>Let me give another example and A be a <img src="https://latex.codecogs.com/png.latex?3x3"> square matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0Aa_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20a_%7B13%7D%20%5C%5C%0Aa_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20a_%7B23%7D%20%5C%5C%0Aa_%7B31%7D%20&amp;%20a_%7B32%7D%20&amp;%20a_%7B33%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"> Then, the inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E%7B-1%7D">, is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cfrac%7B1%7D%7B%5Ctext%7Bdet%7D(%5Cmathbf%7BA%7D)%7D%5Cbegin%7Bbmatrix%7D%0Aa_%7B22%7Da_%7B33%7D-a_%7B23%7Da_%7B32%7D%20&amp;%20a_%7B13%7Da_%7B32%7D-a_%7B12%7Da_%7B33%7D%20&amp;%20a_%7B12%7Da_%7B23%7D-a_%7B13%7Da_%7B22%7D%20%5C%5C%0Aa_%7B23%7Da_%7B31%7D-a_%7B21%7Da_%7B33%7D%20&amp;%20a_%7B11%7Da_%7B33%7D-a_%7B13%7Da_%7B31%7D%20&amp;%20a_%7B13%7Da_%7B21%7D-a_%7B11%7Da_%7B23%7D%20%5C%5C%0Aa_%7B21%7Da_%7B32%7D-a_%7B22%7Da_%7B31%7D%20&amp;%20a_%7B12%7Da_%7B31%7D-a_%7B11%7Da_%7B32%7D%20&amp;%20a_%7B11%7Da_%7B22%7D-a_%7B12%7Da_%7B21%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?det(%5Cmathbf%7BA%7D)"> is the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>For example, let:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%20%5C%5C%0A0%20&amp;%201%20&amp;%204%20%5C%5C%0A5%20&amp;%206%20&amp;%200%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"> Then, <img src="https://latex.codecogs.com/png.latex?det(A)"> = -57, and the inverse of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cfrac%7B1%7D%7B-57%7D%5Cbegin%7Bbmatrix%7D%0A-24%20&amp;%2018%20&amp;%205%20%5C%5C%0A20%20&amp;%20-15%20&amp;%20-4%20%5C%5C%0A-3%20&amp;%202%20&amp;%201%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>There are several formulas for finding the inverse of a matrix such as Gauss-Jordan Elimination, Adjoint method, Cramer’s rule, Inverse formula, etc. This topic is going to be handled in the other blogs.</p>
</section>
<section id="rank" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="rank"><span class="header-section-number">2.7</span> Rank</h3>
<p>The rank of a matrix is the dimension of the vector space spanned by its columns (or rows). It is denoted by <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Brank%7D(%5Cmathbf%7BA%7D)">.</p>
<p>For example, consider the following matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%20%20%5Cmathbf%7BA%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%201%20&amp;%202%20&amp;%203%5C%5C%0A%20%20%20%204%20&amp;%205%20&amp;%206%20%5C%5C%0A%20%20%20%207%20&amp;%208%20&amp;%209%5C%5C%0A%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>The columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are linearly dependent since the third column is equal to the sum of the first two columns. Therefore, the dimension of the vector space spanned by the columns is 2, so the rank of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is 2.</p>
</section>
<section id="trace" class="level3" data-number="2.8">
<h3 data-number="2.8" class="anchored" data-anchor-id="trace"><span class="header-section-number">2.8</span> Trace</h3>
<p>The trace of a square matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Btr%7D(%5Cmathbf%7BA%7D)">, is defined as the sum of the diagonal elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. In other words, if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> matrix, then its trace is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathrm%7Btr%7D(%5Cmathbf%7BA%7D)=%5Csum_%7Bi=1%7D%5E%7Bn%7Da_%7Bij%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?a_%7Bii%7D"> denotes the <img src="https://latex.codecogs.com/png.latex?i"> th diagonal element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%20%202%20&amp;%203%20&amp;%201%20%5C%5C%0A%20%200%20&amp;%205%20&amp;%202%20%5C%5C%0A%20%201%20&amp;%201%20&amp;%204%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>Then, the trace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Btr%7D(%5Cmathbf%7BA%7D)%20=%202%20+%205%20+%204%20=%2011"></p>
</section>
<section id="eigenvalues-and-eigenvectors" class="level3" data-number="2.9">
<h3 data-number="2.9" class="anchored" data-anchor-id="eigenvalues-and-eigenvectors"><span class="header-section-number">2.9</span> Eigenvalues and Eigenvectors</h3>
<p>Let A be an <img src="https://latex.codecogs.com/png.latex?n%20%C3%97%20n"> square matrix. A scalar <img src="https://latex.codecogs.com/png.latex?%5Clambda"> is called an eigenvalue of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> if there exists a non-zero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> such that <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BAv%7D=%5Clambda%5Cmathbf%7Bv%7D%0A"></p>
<p>Such a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> is called an eigenvector corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda">.</p>
<p>Example:</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> be the matrix</p>
<p>To find the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">, we solve the characteristic equation <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdet%7D(%5Cmathbf%20A%20-%20%5Clambda%20%5Cmathbf%20I%20)%20=%200">, where I is the n × n identity matrix.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%5Ctext%7Bdet%7D(%5Cmathbf%20A%20-%20%5Clambda%20%5Cmathbf%20I%20)%0A%20%20&amp;=%0A%20%20%20%20%5Cbegin%7Bvmatrix%7D%0A%20%20%20%203%20-%20%5Clambda%20&amp;%201%20%5C%5C%0A%20%20%20%201%20&amp;%203%20-%20%5Clambda%0A%20%20%20%20%5Cend%7Bvmatrix%7D%20%5C%5C%0A%20%20&amp;=%0A%20%20(3%20-%20%5Clambda)(3%20-%20%5Clambda)%20-%201%20%5C%5C%0A%20%20&amp;=%20%5Clambda%5E2%20-%206%5Clambda%20+%208%20=%200%0A%5Cend%7Balign*%7D%0A"></p>
<p>Solving this quadratic equation gives us the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A">: <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202"> and <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">.</p>
<p>To find the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202">, we solve the equation <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%20A%20-%202%20%5Cmathbf%20I)%5Cmathbf%7Bv%7D%20=%20%5Cmathbf%7B0%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20I"> is the <img src="https://latex.codecogs.com/png.latex?2%20%5Ctimes%202"> identity matrix.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20(%5Cmathbf%20A%20-%202%20%5Cmathbf%20I)%5Cmathbf%7Bv%7D%20=%0A%20%20%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%20%201%20&amp;%201%20%5C%5C%0A%20%20%20%20%20%201%20&amp;%201%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20x%20%5C%5C%0A%20%20%20%20y%0A%20%20%5Cend%7Bbmatrix%7D%20=%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%200%20%5C%5C%0A%20%20%20%200%0A%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p>Solving this system of equations gives us the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20=%202">: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_1%7D%20=%20%5Cbegin%7Bbmatrix%7D%20-1%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"></p>
<p>Similarly, for <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">, we solve the equation <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%20A%20-%204%5Cmathbf%20I)%5Cmathbf%7Bv%7D"> = <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D"> to get the eigenvectors corresponding to <img src="https://latex.codecogs.com/png.latex?%5Clambda_2%20=%204">: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_2%7D%20=%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"></p>
</section>
<section id="singular-value-and-singluar-vectors" class="level3" data-number="2.10">
<h3 data-number="2.10" class="anchored" data-anchor-id="singular-value-and-singluar-vectors"><span class="header-section-number">2.10</span> Singular value and Singluar Vectors</h3>
<p>The singular value decomposition (SVD) of a matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> is a factorization of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%20A"> into the product of three matrices as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BU%7D%20%5Cmathbf%7B%5CSigma%7D%20%5Cmathbf%7BV%7D%5ET%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BU%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20m"> orthogonal matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CSigma%7D"> is an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20n"> rectangular diagonal matrix with non-negative real numbers on the diagonal, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> orthogonal matrix.</p>
<p>The diagonal entries of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CSigma%7D"> are called the singular values of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, denoted as <img src="https://latex.codecogs.com/png.latex?%5Csigma_1,%20%5Csigma_2,%20%5Cldots,%20%5Csigma_r"> (where <img src="https://latex.codecogs.com/png.latex?r"> is the rank of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">), and are arranged in descending order. The columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BU%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D"> are called the left and right singular vectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, respectively, and are orthonormal vectors.</p>
<p>For example, let <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> be a 3 by 2 matrix given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%0A%20%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%201%20&amp;%202%5C%5C%0A%20%20%20%203%20&amp;%204%5C%5C%0A%20%20%20%205%20&amp;%206%0A%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation*%7D%0A"></p>
<p>The SVD of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation*%7D%0A%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BU%7D%20%5Cmathbf%7B%5CSigma%7D%20%5Cmathbf%7BV%7D%5ET%20=%0A%5Cbegin%7Bbmatrix%7D%0A%20%20-0.23%20&amp;%20-0.53%20&amp;%20-0.81%5C%5C%0A%20%20-0.53%20&amp;%20-0.72%20&amp;%200.45%5C%5C%0A%20%20-0.81%20&amp;%200.45%20&amp;%20-0.38%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%209.53%20&amp;%200%5C%5C%0A%20%200%20&amp;%200.90%5C%5C%0A%20%200%20&amp;%200%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20-0.62%20&amp;%20-0.78%5C%5C%0A%20%20-0.78%20&amp;%200.62%0A%5Cend%7Bbmatrix%7D%5ET%0A%5Cend%7Bequation*%7D%0A"></p>
<p>where the left singular vectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BU%7D">, the right singular vectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are the columns of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D">, and the singular values of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> are the diagonal entries of <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%7B%5CSigma%7D">.</p>
<ul>
<li>연립 방정식을 행렬의 곱으로 나타내보기 <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bmatrix%7Dx_1+2y_1=4%5C%5C2x_1+5y_1=9%5Cend%7Bmatrix%7D%20%5Cquad%20%5Cquad%20%5Cquad%20%5Cbegin%7Bmatrix%7Dx_2+2y_2=3%5C%5C2x_2+5y_2=7%5Cend%7Bmatrix%7D"> <img src="https://latex.codecogs.com/png.latex?%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20%5C%5C%202%20&amp;%205%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20%5C%5C%20y_1%20&amp;%20y_2%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%204%20&amp;%209%20%5C%5C%203%20&amp;%207%20%5Cend%7Bbmatrix%7D"></li>
<li>중요한 사실(….당연한 사실?)
<ul>
<li>곱셈의 왼쪽 행렬의 열 수와, 오른쪽 행렬의 행 수가 같아야 가능함
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bm%20%5Ctimes%20n%7D%20%5Ctimes%20B_%7Bo%20%5Ctimes%20p%7D"> 에서 <img src="https://latex.codecogs.com/png.latex?n%20=%20o"> 여야 곱셈 성립</li>
</ul></li>
<li>곱셈의 결과 행렬의 크기 = 곱셈의 왼쪽 행렬의 행 수 <img src="https://latex.codecogs.com/png.latex?%5Ctimes"> 곱셈의 오른쪽 행렬의 열 수
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bm%20%5Ctimes%20n%7D%20%5Ctimes%20B_%7Bo%20%5Ctimes%20p%7D%20=%20C_%7Bm%20%5Ctimes%20p%7D"></li>
</ul></li>
<li>교환법칙(Commutative property)이 성립하지 않음
<ul>
<li><img src="https://latex.codecogs.com/png.latex?AB%20%5Cneq%20BA"></li>
</ul></li>
</ul></li>
<li>행렬 곱셈의 여러가지 관점
<ul>
<li>내적으로 바라보기 <img src="https://latex.codecogs.com/png.latex?%20A%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%5ET%7D%20%5C%5C%20%5Cmathbf%7Ba_2%5ET%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cmathbf%7Ba_m%5ET%7D%20%5Cend%7Bbmatrix%7D%20"> <img src="https://latex.codecogs.com/png.latex?%20AB%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%5ET%7D%20%5C%5C%20%5Cmathbf%7Ba_2%5ET%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cmathbf%7Ba_m%5ET%7D%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Bb_1%7D%20&amp;%20%5Cmathbf%7Bb_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Bb_m%7D%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%5ET%20b_1%7D%20&amp;%20%5Cmathbf%7Ba_1%5ET%20b_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Ba_1%5ET%20b_m%7D%20%5C%5C%20%5Cmathbf%7Ba_2%5ET%20b_1%7D%20&amp;%20%5Cmathbf%7Ba_2%5ET%20b_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Ba_2%5ET%20b_m%7D%20%5C%5C%20%5Cvdots%20%20&amp;%20%5Cvdots%20%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%20%5Cmathbf%7Ba_m%5ET%20b_1%7D%20&amp;%20%5Cmathbf%7Ba_m%5ET%20b_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Ba_m%5ET%20b_m%7D%20%5Cend%7Bbmatrix%7D"></li>
<li>rank-1 matrix의 합 (또 안가르쳐준 개념 먼저 사용중…..-_-) <img src="https://latex.codecogs.com/png.latex?AB%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%7D%20&amp;%20%5Cmathbf%7Ba_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Ba_m%7D%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Bb_1%5ET%7D%20%5C%5C%20%5Cmathbf%7Bb_2%5ET%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cmathbf%7Bb_m%5ET%7D%20%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7Ba_1%20b_1%5ET%7D%20+%20%5Cmathbf%7Ba_2%20b_2%5ET%7D%20+%20%5Ccdots%20+%20%5Cmathbf%7Ba_m%20b_m%5ET%7D"></li>
<li>column space로 바라보기 <img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%7D%20&amp;%20%5Cmathbf%7Ba_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Ba_m%7D%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%5C%5C%20%5Cvdots%20%5C%5C%20x_m%20%5Cend%7Bbmatrix%7D%20=%20%5Cmathbf%7Ba_1%7D%20x_1%20+%20%5Cmathbf%7Ba_2%7D%20x_2%20+%20%5Ccdots%20+%20%5Cmathbf%7Ba_m%7D%20x_m%20"> (스칼라배의 합)
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%200%20%5C%5C%200%20&amp;%201%20%5Cend%7Bbmatrix%7D"> 는 2차원 좌표평면의 모든 점을, <img src="https://latex.codecogs.com/png.latex?A=%5Cbegin%7Bbmatrix%7D%201%20&amp;%200%20&amp;%200%20%5C%5C%200%20&amp;%201%20&amp;%200%20%5C%5C%200%20&amp;%200%20&amp;%201%20%5Cend%7Bbmatrix%7D">은 3차원 좌표평면의 모든 점 표현 가능</li>
<li><img src="https://latex.codecogs.com/png.latex?AB%20=%20A%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Bb_1%7D%20&amp;%20%5Cmathbf%7Bb_2%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Bb_m%7D%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20A%20%5Cmathbf%7Bb_1%7D%20&amp;%20A%20%5Cmathbf%7Bb_2%7D%20&amp;%20%5Ccdots%20&amp;%20A%20%5Cmathbf%7Bb_m%7D%20%5Cend%7Bbmatrix%7D"></li>
<li>column space: A의 column vector로 만들 수 있는 부분 공간</li>
</ul></li>
<li>row space로 바라보기 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%5ET%7DA%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20&amp;%20x_2%20&amp;%20%5Ccdots%20&amp;%20x_m%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20%5Cmathbf%7Ba_1%5ET%7D%20%5C%5C%20%5Cmathbf%7Ba_2%5ET%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cmathbf%7Ba_m%5ET%7D%20%5Cend%7Bbmatrix%7D%20=%20x_1%20%5Cmathbf%7Ba_1%5ET%7D%20+%20x_2%20%5Cmathbf%7Ba_2%5ET%7D%20+%20%5Ccdots%20+%20x_m%20%5Cmathbf%7Ba_m%5ET%7D%20"></li>
</ul></li>
</ul>
</section>
</section>
<section id="열공간column-space" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="열공간column-space"><span class="header-section-number">3</span> 열공간(Column Space)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/g0eaDeVRdZk" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li>column space: column vector 들이 span 하는 space
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A">의 column space = <img src="https://latex.codecogs.com/png.latex?C(A)"> 또는 <img src="https://latex.codecogs.com/png.latex?range(A)"></li>
</ul></li>
<li>span: vector들의 linear combination 으로 나타낼 수 있는 모든 vector를 모은 집합
<ul>
<li>vector에 따라, 점일수도 선일수도 평면일 수도 있음</li>
<li>vector space를 이 vector들이 span하는 space → column space는 행렬의 열들이 span하는 space</li>
</ul></li>
<li>vector의 선형 결합(linear combination): vector에 스칼라배를 해서 더하는 것
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_1%7D"> 과 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_2%7D">의 linear combination으로 2차원 좌표평면 나타내기 <img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/images/chap02_09.PNG" class="img-fluid"></li>
</ul></li>
</ul>
</section>
<section id="선형-독립linear-independent" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="선형-독립linear-independent"><span class="header-section-number">4</span> 선형 독립(Linear Independent)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/mOOI4-BfjGQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>…and also see</p>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/9F4PZ_1orF0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li>선형 독립(linearly independent)인 vectors: (선형 결합을 통해) 더 고차원을 span할 수 있게 해줌</li>
<li>orthogonal 하면 independent
<ul>
<li>but independent해도 항상 orthogonal하지는 않음 (Independent &gt; Orthogonal)</li>
</ul></li>
<li>definition: <img src="https://latex.codecogs.com/png.latex?a_1%20%5Cmathbf%7Bv_1%7D%20+%20a_2%20%5Cmathbf%7Bv_2%7D%20+%20a_3%20%5Cmathbf%7Bv_3%7D%20%5Ccdots%20a_n%20%5Cmathbf%7Bv_n%7D%20=%20%5Cmathbf%7B0%7D"> 를 만족하는 <img src="https://latex.codecogs.com/png.latex?a_1,%20a_2,%20a_3,%20%5Ccdots%20a_n"> 이 <img src="https://latex.codecogs.com/png.latex?a_1%20=%20a_2%20=%20a_3%20=%20%5Ccdots%20=%20a_n%20=%200"> 밖에 없을때
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D">는 모든 elements가 <img src="https://latex.codecogs.com/png.latex?0">인 벡터</li>
<li>예: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%202%20%5C%5C%202%20%5Cend%7Bbmatrix%7D"> 는 <img src="https://latex.codecogs.com/png.latex?-2%20%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D%20+%201%20%5Cbegin%7Bbmatrix%7D%202%20%5C%5C%202%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%200%20%5Cend%7Bbmatrix%7D"> 이 되므로, linearly independent 하지 않음</li>
<li>independent한 vector 들의 수 = 표현할 수 있는 차원의 dimension</li>
</ul></li>
</ul>
</section>
<section id="기저basis" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="기저basis"><span class="header-section-number">5</span> 기저(basis)</h2>
<ul>
<li>주어진 vector space를 span하는 linearly independent한 vectors</li>
<li>어떤 공간을 이루는 필수적인 구성요소</li>
<li>orthogonal 하면 orthogonal basis</li>
<li>예: 2차원 좌표평면에 대해
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5Cend%7Bbmatrix%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"> : orthogonal basis</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5Cend%7Bbmatrix%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%201%20%5Cend%7Bbmatrix%7D"> : orthogonal 하지 않은 basis</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20%5C%5C%200%20%5Cend%7Bbmatrix%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%202%20%5C%5C%200%20%5Cend%7Bbmatrix%7D"> : linearly independent 하지 않으므로 basis 아님</li>
</ul></li>
</ul>
</section>
<section id="항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="항등행렬identity-matrix-역행렬inverse-matrix-대각행렬diagonal-matrix-직교행렬orthogonal-matrix"><span class="header-section-number">6</span> 항등행렬(identity matrix), 역행렬(inverse matrix), 대각행렬(Diagonal Matrix), 직교행렬(orthogonal matrix)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/XqOvyfMUAwA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="identity-matrix항등행렬" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="identity-matrix항등행렬"><span class="header-section-number">6.1</span> Identity matrix(항등행렬)</h3>
<ul>
<li>항등원: 임의의 원소에 대해 연산하면 자기 자신이 나오게 하는 원소 (by namu.wiki)
<ul>
<li>실수에서 곱셈의 항등원은 1</li>
</ul></li>
<li>행렬의 항등원: 항등행렬(<img src="https://latex.codecogs.com/png.latex?I">) <img src="https://latex.codecogs.com/png.latex?I%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%200%20&amp;%201%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%20%5Cvdots%20%20&amp;%20%5Cvdots%20%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%200%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%201%20%5Cend%7Bbmatrix%7D">
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bm%20%5Ctimes%20n%7D%20%5Ctimes%20I_%7Bn%20%5Ctimes%20n%20=%20n%7D%20=%20A_%7Bm%20%5Ctimes%20n%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?I_%7Bm%20%5Ctimes%20m%20=%20m%7D%20%5Ctimes%20A_%7Bm%20%5Ctimes%20n%7D%20=%20A_%7Bm%20%5Ctimes%20n%7D"></li>
</ul></li>
</ul>
</section>
<section id="sec-inv" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="sec-inv"><span class="header-section-number">6.2</span> Inverse matrix(역행렬)</h3>
<ul>
<li>역원: 연산 결과 항등원이 나오게 하는 연소
<ul>
<li>실수에서 곱셈의 역원은 지수의 부호가 반대인 역수 (by namu.wiki): <img src="https://latex.codecogs.com/png.latex?a%20%5Ctimes%20a%5E%7B-1%7D%20=%201"></li>
</ul></li>
<li>행렬의 역원: 역행렬(<img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D">) <img src="https://latex.codecogs.com/png.latex?A%20%5Ctimes%20A%5E%7B-1%7D%20=%20I%20,%20A%5E%7B-1%7D%20%5Ctimes%20A%20=%20I">
<ul>
<li>존재하지 않는 경우도 있음</li>
<li>존재하면 Invertible(nonsingular, nondegenerate) matrix라고 불림
<ul>
<li>존재하지 않으면 singular, degenerate라고 불림</li>
</ul></li>
<li>square matrix(정사각행렬, <img src="https://latex.codecogs.com/png.latex?m%20=%20n">)은 특수한 경우를 제외하면 역행렬이 항상 존재
<ul>
<li>역행렬이 존재하지 않는특수한 경우: (나중에 배울) determinant가 0인 경우</li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?m%20%5Cneq%20n">인 행렬의 경우에는 역행렬이 존재하지 않음
<ul>
<li>다만, 경우에 따라 <img src="https://latex.codecogs.com/png.latex?A%20%5Ctimes%20A%5E%7B-1%7D%20=%20I"> 를 만족하거나(right inverse), <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D%20%5Ctimes%20A%20=%20I">를 만족하는(left inverse)는 <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D">이 존재함</li>
</ul></li>
<li>연립 방정식을 matrix로 나타냈을 때, 역행렬을 이용해서 해를 찾을 수 있음 <img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D%20=%20%5Cmathbf%7Bb%7D%20%5CRightarrow%20A%5E%7B-1%7DA%5Cmathbf%7Bx%7D%20=%20A%5E%7B-1%7D%5Cmathbf%7Bb%7D%20%5CRightarrow%20I%5Cmathbf%7Bx%7D%20=%20A%5E%7B-1%7D%5Cmathbf%7Bb%7D%20%5CRightarrow%20%5Cmathbf%7Bx%7D%20=%20A%5E%7B-1%7D%5Cmathbf%7Bb%7D"></li>
</ul></li>
</ul>
</section>
<section id="diagonal-matrix대각행렬" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="diagonal-matrix대각행렬"><span class="header-section-number">6.3</span> Diagonal Matrix(대각행렬)</h3>
<ul>
<li>diagonal element(대각 원소)외의 모든 elements(off-diagonal elements)가 0인 matrix <img src="https://latex.codecogs.com/png.latex?%20D%20=%20Diag(%5Cmathbf%7Ba%7D)%20=%20%5Cbegin%7Bbmatrix%7D%20a_%7B1,1%7D%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%200%20&amp;%20a_%7B2,2%7D%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%20%5Cvdots%20%20&amp;%20%5Cvdots%20%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%200%20&amp;%200%20&amp;%20%5Ccdots%20&amp;%20a_%7Bn,n%7D%20%5Cend%7Bbmatrix%7D">
<ul>
<li>identity matrix는 diagonal matrix</li>
<li>diagnomal matrix는 symmetric matrix 이기도 함</li>
<li>보통은 square matrix에서 주로 사용됨
<ul>
<li>square matrix가 아니면서 diagonal matrix인 경우: rectangular diagonal matrix</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="orthogonal-matrix직교행렬-orthonomal-matrix" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="orthogonal-matrix직교행렬-orthonomal-matrix"><span class="header-section-number">6.4</span> Orthogonal matrix(직교행렬, orthonomal matrix)</h3>
<ul>
<li>행렬의 각 columns들이 orthonomal vectors (서로 수직하면서 unit vectors) <img src="https://latex.codecogs.com/png.latex?A%20A%5ET%20=%20A%5ET%20A%20=%20I"></li>
<li>identity matrix는 orthogonal matrix</li>
<li>square matrix에서만 정의됨</li>
<li>Orthogonal matrix인 <img src="https://latex.codecogs.com/png.latex?A">이면 <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D%20=%20A%5E%7BT%7D">
<ul>
<li>각 columns에서 자기 자신과의 내적 = 1, 다른 column과의 내적 = 0임</li>
</ul></li>
<li>complex matrix(복소수 행렬)에서는 unitary matrix라고 부름</li>
</ul>
</section>
</section>
<section id="계수rank" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="계수rank"><span class="header-section-number">7</span> 계수(Rank)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/HMST0Yc7EXE" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li>rank: 행렬이 가지는 independent한 columns의 수 = column space의 dimension = row space의 dimension</li>
<li>independent한 column의 수 = independent한 행의 수: <img src="https://latex.codecogs.com/png.latex?rank(A)%20=%20rank(A%5ET)">
<ul>
<li>proof: <a href="https://en.wikipedia.org/wiki/Rank_%28linear_algebra%29#Proofs_that_column_rank_=_row_rank">Wikipedia</a></li>
</ul></li>
<li>예: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20&amp;%203%20%5C%5C%200%20&amp;%200%20&amp;%200%20%5Cend%7Bbmatrix%7D%20%5CRightarrow%20rank=1"> <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bbmatrix%7D%201%20&amp;%200%20&amp;%201%20%5C%5C%200%20&amp;%201%20&amp;%201%20%5Cend%7Bbmatrix%7D%20%5CRightarrow%20rank=2"></li>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bm%20%5Ctimes%20n%7D"> 의 최대 랭크는 <img src="https://latex.codecogs.com/png.latex?min%5C%7Bm,n%5C%7D">
<ul>
<li><img src="https://latex.codecogs.com/png.latex?rank(A)%20%3C%20min%5C%7Bm,n%5C%7D"> 면 rank-deficient, <img src="https://latex.codecogs.com/png.latex?rank(A)%20=%20min%5C%7Bm,n%5C%7D">면 full (row/column) rank</li>
</ul></li>
</ul>
</section>
<section id="영공간null-space" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="영공간null-space"><span class="header-section-number">8</span> 영공간(Null space)</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/Eizc9TSRYMQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D=%20%5Cmathbf%7B0%7D"> 을 만족하는 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">의 집합
<ul>
<li>column space 관점에서 보기: <img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D%20=%20x_1%20%5Cmathbf%7Ba_1%7D%20+%20x_2%20%5Cmathbf%7Ba_2%7D%20+%20%5Ccdots%20+%20x_n%20%5Cmathbf%7Ba_n%7D%20=%20%5Cmathbf%7B0%7D"></li>
<li>null space에 항상 들어가는 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%200%20%5C%5C%200%20%5C%5C%20%5Cvdots%20%5C%5C%200%20%5Cend%7Bbmatrix%7D"> : trivial solution
<ul>
<li>모든 columns이 다 lienarly independent 하면, null space에는 위의 벡터 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D=%5Cmathbf%7B0%7D">하나 밖에 없음</li>
</ul></li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D=%5Cmathbf%7B0%7D"> 가 아닌 vector가 null space에 있으면, 스칼라배(constant <img src="https://latex.codecogs.com/png.latex?c">에 대해 <img src="https://latex.codecogs.com/png.latex?c%20%5Cmathbf%7Bx%7D">) 역시 null space에 포함됨</li>
<li>혼동 주의! null space는 column space의 일부가 아님
<ul>
<li>row vector의 차원이 null space가 존재하는 공간</li>
</ul></li>
<li>rank와 null space의 dimension의 합은 항상 matrix의 column의 수
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bm%20%5Ctimes%20n%7D">에 대해, <img src="https://latex.codecogs.com/png.latex?dim(N(A))%20=%20n%20-%20r">
<ul>
<li>모든 columns이 다 lienarly independent 하면 null space는 0차원(점)</li>
</ul></li>
<li>null space는 row space와 수직한 space
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A%5Cmathbf%7Bx%7D=%20%5Cmathbf%7B0%7D"> : 각각 모든 행과 내적해서 0, → 행들의 linear combination과 내적해도 0</li>
<li>rank는 row space의 dimension → row space의 dimension(<img src="https://latex.codecogs.com/png.latex?dim(R(A))">)과 null space의 dimension(<img src="https://latex.codecogs.com/png.latex?dim(N(A))">)의 합이 <img src="https://latex.codecogs.com/png.latex?n"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%5En%7D"> 공간에 표현: <img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/images/chap02_10.PNG" class="img-fluid">
<ul>
<li>겹친 점: 영벡터</li>
</ul></li>
</ul></li>
</ul></li>
<li>left null space: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%5ET%7D%20A%20=%20%5Cmathbf%7B0%5ET%7D"> 인 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">
<ul>
<li>위의 성질을 row에 대해 적용
<ul>
<li>m 차원에 놓인 벡터</li>
<li>dimension: <img src="https://latex.codecogs.com/png.latex?dim(N_L(A))%20=%20m%20-%20r"></li>
<li>column space와 수직: <img src="https://latex.codecogs.com/png.latex?dim(N_L(A))%20+dim(C(A))%20=%20m"></li>
</ul></li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?R(A)">에 있는 vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_r%7D"> 와 <img src="https://latex.codecogs.com/png.latex?N(A)">에 있는 vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_n%7D">에 대해:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_r%7D">에 <img src="https://latex.codecogs.com/png.latex?A">를 곱하면 column space로 감</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_n%7D">에 <img src="https://latex.codecogs.com/png.latex?A">를 곱하면 $</li>
<li><img src="https://latex.codecogs.com/png.latex?A(%5Cmathbf%7Bx_r%7D%20+%5Cmathbf%7Bx_n%7D)%20=%20A%5Cmathbf%7Bx_r%7D%20+%20A%5Cmathbf%7Bx_n%7D%20=%20A%5Cmathbf%7Bx_r%7D%20=%20%5Cmathbf%7Bb%7D"></li>
</ul></li>
</ul>
</section>
<section id="ax-b의-해의-수" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="ax-b의-해의-수"><span class="header-section-number">9</span> Ax = b의 해의 수</h2>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/nNI2TlD598c" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ul>
<li><p>full column rank 일때</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">가 column space(<img src="https://latex.codecogs.com/png.latex?C(A)">)안에 있으면 해가 하나</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">가 column space(<img src="https://latex.codecogs.com/png.latex?C(A)">)안에 없으면 해가 없음 <img src="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/images/chap02_11.PNG" class="img-fluid"></li>
</ul></li>
<li><p>full row rank 일때</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">는 항상 column space 안에 있음: 무한의 해를 가짐</li>
<li>임의의 특정한 해(particular solution) <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_p%7D">와 null space의 vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_n%7D">에 대해, <img src="https://latex.codecogs.com/png.latex?A(%5Cmathbf%7Bx_p%7D%20+%5Cmathbf%7Bx_n%7D)=%5Cmathbf%7Bb%7D">
<ul>
<li>즉, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx_p%7D%20+%5Cmathbf%7Bx_n%7D"> 도 해가 됨: complete solution
<ul>
<li>null space는 무한하므로, 해도 무한함</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>full rank 일때(square matrix): 해가 하나 존재 (<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20A%5E%7B-1%7D">$)</p></li>
<li><p>rank-deficient 일때</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">가 column space(<img src="https://latex.codecogs.com/png.latex?C(A)">)안에 있으면 무한한 해를 가짐</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">가 column space(<img src="https://latex.codecogs.com/png.latex?C(A)">)안에 없으면 해가 없음</li>
</ul></li>
</ul>


</section>

 ]]></description>
  <category>Mathematics</category>
  <guid>kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/04.linear_transformation.html</guid>
  <pubDate>Thu, 30 Mar 2023 15:00:00 GMT</pubDate>
  <media:content url="kmink3225.netlify.app/docs/blog/posts/Mathmatics/linear_algebra/images/chap02_09.PNG" medium="image"/>
</item>
</channel>
</rss>
