---
title: "Connection & Hook"
subtitle: docker_compose.yaml 해석, Postgres Container 올리기, Connection, Hook, bulk_load(), Custom Hook, Providers, Connection Type 
description: |
  template
categories:
  - Engineering
author: Kwangmin Kim
date: 05/01/2023
format: 
  html:
    page-layout: full
    code-fold: true
    toc: true
    number-sections: true
comments: 
  utterances: 
    repo: ./docs/comments
draft: False
---

<ul class="nav nav-pills" id="language-tab" role="tablist">
  <li class="nav-item" role="presentation">
    <button class="nav-link active" id="Korean-tab" data-bs-toggle="tab" data-bs-target="#Korean" type="button" role="tab" aria-controls="Korean" aria-selected="true">Korean</button>
  </li>
  <li class="nav-item" role="presentation">
    <button class="nav-link" id="English-tab" data-bs-toggle="tab" data-bs-target="#English" type="button" role="tab" aria-controls="knitr" aria-selected="false">English</button>
  </li>

<div class="tab-content" id="language-tabcontent">

<div class="tab-pane fade  show active" id="Korean" role="tabpanel" aria-labelledby="Korean-tab">

# Goal

* posgres DB를 container로 띄우기
* airflow의 connection & hook 설정

# Docker Compose Interpretation

* 목적
    * 1 개 이상의 도커 컨테이너 생성시 스크립트 하나로 컨테이너들의 설정을 관리할 수 있도록 해주는 Docker 의 기능 중 하나
    * 각 각의 container 설정 관리 뿐만 아니라 containers간의 연관관계 및 dependency까지 설정할 수 있다.
    * 일부 containers를 같은 network에서 띄워지는지도 설정할 수 있는 network 설정도 할 수 있다.
* 작성방법
    * 모든 설정은 docker_compose.yaml 파일에 컨테이너들의 설정 내용을 입력
* 도커컴포즈 서비스 시작
    * yaml 파일이 있는 위치에서 sudo docker compose up 명령 입력하면 docker_compose.yaml에 있는 모든 설정이 적용된다.
    * 기본적으로 Docker 서비스가 설치되어 있어야 함
* docker compose.yaml 파일의 구성
    * yaml 파일은 json 이나 xml 과 같이 **key, value 로 구성되며 계층적인 구조**를 가지며 파이썬처럼 들여쓰기 문법을 사용함. 들여쓰기 잘못하면 오류남.
    * 다시 말해서, json 이나 xml은 파이썬의 dictionary 같이 nested {key:value} structure로 작성할 수 있다.
* docker_compose.yaml 파일의 1 Level 내용

```markdown
version: '3.8' # yaml 파일의 버전 정보 옵션
x-airflow-common: # 'x-': Extention Fields(각 서비스 항목에 또는 container에  공통 적용될 항목들 정의)
services: # 컨테이너로 실행할 서비스 정의로 가장 신경써서 적어야할 부분
volumes:  # 컨테이너에 할당할 volume 정의
networks: # 컨테이너에 연결할 network 정의. 초기에는 level1에 networks 항목이 정의되어 있지 않아 정의해줘야함
```
  * 위의 내용에서 key값은  version, x-airflow-common, services, volumes, networks
  * networks: containers에 IP나 network정보를 할당해주기 위해 작성하는 항목


* x-airflow-common: 공통 지정할 항목을 &를 붙여서 지정

```markdown
x-airflow-common
    &airflow-common # 공통 지정 parameters 정의: image, environment, depends_on
    image: ${AIRFLOW_IMAGE:-apache/airflow:2.5.2}
    environment
        &airflow-common-env
        AIRFLOW__CORE__DEFAULT_TIMEZONE: 'Asia/Seoul'
        AIRFLOW__CORE__EXECUTOR: CeleryExecutor
        AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    depends_on # containers 실행 순서를 결정
        &airflow-common-depends-on # 공통 지정 parameters 정의: redis, postgres
        redis:
            condition: service_healthy
        postgres:
            condition: service_healthy
```
* &airflow-common는 image, environment, depends_on 인수를 미리 가지고 있음
* 공통 지정 항목1: &airflow-common 
    * 변수 또는 parameter: image, environment, depends_on
* 공통 지정 항목2: &airflow-common-env
    * 변수 또는 parameter: AIRFLOW__CORE__DEFAULT_TIMEZONE, AIRFLOW__CORE__EXECUTOR, AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
* 공통 지정 항목3: &airflow-common-depends-on
    * 변수 또는 parameter: redis, postgres
* 후차적인 스크립트에서 &airflow-common 를 호출 하면 아래의 내용이 모두 호출 됨

```markdown
&airflow-common # 공통 지정 parameters 정의: image, environment, depends_on
image: ${AIRFLOW_IMAGE:-apache/airflow:2.5.2}
environment
    &airflow-common-env
    AIRFLOW__CORE__DEFAULT_TIMEZONE: 'Asia/Seoul'
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
depends_on # containers 실행 순서를 결정
    &airflow-common-depends-on # 공통 지정 parameters 정의: redis, postgres
    redis:
        condition: service_healthy
    postgres:
        condition: service_healthy
```

* services: 컨테이너로 올릴(실행할) 서비스 지정
  * default postgres 서비스 말고 새로 지정할 postgres 서비스. 초기의 docker_compose.yaml파일에서는 없음
  ```markdown
  services:
     postgres:
       image: postgres:13
       environment:
         POSTGRES_USER: airflow
         POSTGRES_PASSWORD: airflow
         POSTGRES_DB: airflow
       volumes:
         - postgres-db-volume:/var/lib/postgresql/data
       healthcheck:
         test: ["CMD", "pg_isready", "-U", "airflow"]
         interval: 10s
         retries: 5
         start_period: 5s
      ports
          - 5432:5432
       restart: always
  ```
    * `image: postgres:13` 은 image는 postgre:13 버전의 image를 쓴다는 것이고 이 image가 local에 있으면 그대로 쓰고 없으면 인터넷에서 download됨. 
    * `environment:` 는 postgres OS에 설정할 환경 변수들
    * `volumes:` container와 연결할 local file system 경로를 의미
        * `postgres-db-volume:/var/lib/postgresql/data` `:`을 기준으로 왼쪽이 local file system의 경로 오른쪽이 연결할 container의 directory. 이 과정을 **mount** 시켰다라고 말함
        * `postgres-db-volume` 문서 제일 하단에 미리 만들어져 있음
        * container가 실행되었다가 (띄어졌다가) 꺼지면 (내려지면) 안에 있는 data들이 모두 사라지기 때문에 mount시키는 것이 필요함. 특히, DB container는 mount가 잘 됐는지 확인해야함
        * `postgres-db-volume:/var/lib/postgresql/data`는 postgresql의 data가 저장되는 directory를 local file system으로 연결시켜 놓은 것  
    * `healthcheck:`  container가 상태 꺼졌는지 켜졌는지 확인
    * `ports`: container에 접속하기 위해 공개할 port를 명시
        * `5432:5432` `:` 을 기준으로 왼쪽이 local에서 web에 접속할 port 번호고 오른쪽이 service가 갖고 있는 port번호. 다시 말해서, wsl 시스템안에 여러 컨테이너들이 있고 그 중 postgres 이미지가 깔려 있다면 postgres는 5432 port를 가지고 있는 상태이다. postgres 이미지에 접근하려면 wsl의 port를 통해서 접근해야하는데 그 wsl의 port가 5432로 지정된 것을 의미한다.
    * `restart: always` container가 죽으면 언제 새로 띄워주겠냐는 것이고 always니까 항상 새로 띄워줌 
  * redis 
  ```markdown
    redis:
      image: redis:latest
      expose:
        - 6379
      healthcheck:
        test: ["CMD", "redis-cli", "ping"]
        interval: 10s
        timeout: 30s
        retries: 50
        start_period: 30s
      restart: always
  ```
    * `expose: 6379` 이것 역시 port번호인데 외부와 연결될 때 사용되는 게 아니라 내부 다른 containers와 연결시 사용되는 port번호로 expose parameter로 공개 설정한다.
  * airflow-webserver 
  ```markdown
    airflow-webserver:
      <<: *airflow-common # &airflow-common의 공통 지정 parameters 호출: image, environment, depends_on
      command: webserver # container를 띄울 때 실행할 명령어
      ports:
        - "8080:8080"
      healthcheck:
        test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
        interval: 30s
        timeout: 10s
        retries: 5
        start_period: 30s
      restart: always
      depends_on: # 2번째 depends_on 선언이 되어 호출된 (<<: *airflow-common) &airflow-common의 depends_on 의 내용은 무시된다.
        <<: *airflow-common-depends-on # 공통 지정 parameters 호출: redis, postgres
        airflow-init:
          condition: service_completed_successfully
  ```
    * services: 1-level (x-airflow-common 같은 level)
        * airflow-webserver, airflow-scheduler, redis, postgres 등이 같은 level의 서비스 항목으로 열거 된다.
    * depends_on: containers를 띄우는 (실행하는) 순서를 설정하는 부분으로 위의 예시는
        * redis, postgres, airflow-init을 띄우고 나서 airflow-webserver를 띄우겠다는 것.
        * [redis, postgres, airflow-init]>>airflow-webserver
  * airflow-scheduler: # 1-level
  ```markdown
    airflow-scheduler: # 1-level
         <<: *airflow-common
         command: scheduler
         healthcheck:
           test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
           interval: 30s
           timeout: 10s
           retries: 5
           start_period: 30s
         restart: always
         depends_on:
           <<: *airflow-common-depends-on
           airflow-init:
             condition: service_completed_successfully
   ```
* volumes: 컨테이너와 연결하기 위한 볼륨 (공간) 정보
    ```markdown
    volumns:
        postgree-db-volume: #새로 만들 볼륨 이름

    ```
    * 볼륨에 대한 정보 확인하기
        1.볼륨 리스트 보기 : sudo docker volume ls (현재 만들어진 volumes 리스트와 volumne_id가 보임)
        2.볼륨 상세 보기 : sudo docker volume inspect {volume_id}
* networks: 컨테이너의 network 정보 구성
  ```markdown
  networks:
      network_custom: # 새로 만들 네트워크 이름
          driver: bridge
          ipam:
              driver: default
              config:
                  - subnet : 172.18.0.0/16 # 네트워크 IP의 주소값이 2^16 개, host가 가질 IP의 주소값은 2^16-2 만큼을 할당할 수 있다.
                    gateway: 172.18.0.1
  ```
  * config:
      - subnet : 172.18.0.0/16
      - gateway: 172.18.0.1
  주어진 서브넷은 172.18.0.0/16으로 주어졌는데 이 경우, 16비트의 네트워크 부분이 주소에 할당되어 있다. 네트워크 부분은 호스트를 식별하는데 사용되는 부분이 아닌 네트워크를 식별하는데 사용되는 부분이다. 이러한 설정에서는 네트워크 IP의 주소값이 2^16 (또는 65536) 개로 할당된다. 호스트는 네트워크에서 실제로 사용되는 장치 또는 시스템을 의미한다. 이 설정에서는 게이트웨이 주소를 172.18.0.1로 지정하여 해당 IP 주소를 게이트웨이로 사용한다. 따라서 호스트가 할당받을 수 있는 IP 주소값의 개수는 네트워크 IP 주소값에서 게이트웨이 주소를 제외한 개수이다. 즉, 2^16-2 (또는 65534) 개의 주소를 호스트에 할당할 수 있다.
  * 네트워크에 대한 정보 확인하기
      1. 네트워크 리스트 보기 : sudo docker network ls
      2. 네트워크 상세 보기 : sudo docker network inspect {network_id}

# Postgres 컨테이너 올리기

## Postgress 컨테이너 추가하기

* postgres_custom 이라는 이름의 컨테이너 서비스 추가하기
* networks를 만들어 컨테이너에 고정 IP 할당
* DBeaver로 postgres DB에 접속

```markdown
services:
  postgres_custom:
    image: postgres:13
    environment:
      POSTGRES_USER: kmkim
      POSTGRES_PASSWORD: kmkim
      POSTGRES_DB: kmkim
      TZ: Asia/Seoul
    volumes:
      - postgres-custom-db-volume:/var/lib/postgresql/data
    ports:
      - 5432:5432
    networks:
      network_custom: # 밑에서 정의한 network_custom 을 쓰겠다는 의미
        ipv4_address: 172.28.0.3 # 할당된 IP

networks:
  network_custom: 
    driver: bridge
    ipam:
        driver: default
        config:
            - subnet: 172.28.0.0/16 # 네트 워크 ID 주소 밑에 16개/host id 주소 밑에 16개를 할당하겠다는 의미
              gateway: 172.28.0.1 # default 네트워크 (172.18.0.0)가 쓰고 있지 않은 서브넷으로 구성
volumes:
  postgres-db-volume:
  postgres-custom-db-volume: #wsl의 어느 path에 mapping이 되어 있는지 확인할 것
```

## 컨테이너 고정 IP 할당하기

* 기본적으로 컨테이너들은 유동 IP를 지니며 (재기동시 IP 변경 가능)
* postgres DB에 접속하려면 고정 IP 필요
* 고정 IP를 할당하려면 networks를 만들어서 할당해야 함.
* networks 를 지정하지 않은 컨테이너들(airflow를 설치하면서 기본적으로 설치되는 containers)은 default network에 묶이게 됨
* 따라서 동일 네트워크에 두고 싶은 컨테이너들은 모두 동일 netwworks 할당 필요

```markdown
networks:
  network_custom: 
    driver: bridge
    ipam:
        driver: default
        config:
            - subnet: 172.28.0.0/16 # 네트 워크 ID 주소 밑에 16개/host id 주소 밑에 16개를 할당하겠다는 의미
              gateway: 172.28.0.1 # default 네트워크 (172.18.0.0)가 쓰고 있지 않은 서브넷으로 구성
```

* Postgres_custom 컨테이너 뿐만 아니라 다른 컨테이너에도 network_custom 할당하고 IP 부여
  ```markdown
  postgres_custom:  # 172.28.0.3
  postgres:  # 172.28.0.3 +  포트 노출 설정:(5431:5432)
  redis: # 172.28.0.5
  airflow-webserver: # 172.28.0.6
  airflow-scheduler: # 172.28.0.7
  airflow-worker: # 172.28.0.8
  airflow-triggerer: # 172.28.0.9
  airflow-init: # 172.28.0.10
  ```
    * postgres:  # airflow가 기본 메타DB로 쓰고 있는 postgress 컨테이너에 포트 번호 5431로 노출
* networks에 172.28.xxx.xxx 같이 172.28 대역을 준 이유
  * 아래와 같이 `sudo docker ps` 를 실행해 container list를 보고 container id를 확인 하여 `sudo docker inspect {container_id}` or `sudo docker inspect b739a3494646` 실행해보면 다음과 같은 정보를 볼 수 있다.
  ```markdown
   "NetworkSettings": {
            "Bridge": "",
            "SandboxID": "b1bda217ebe565bfcf64b3c52e7fbf47032821894db5d97ef9f8f85db5ee57d3",            "HairpinMode": false,
            "LinkLocalIPv6Address": "",
            "LinkLocalIPv6PrefixLen": 0,
            "Ports": {},
            "SandboxKey": "/var/run/docker/netns/b1bda217ebe5",
            "SecondaryIPAddresses": null,
            "SecondaryIPv6Addresses": null,
            "EndpointID": "",
            "Gateway": "", # custom-network로 지정해주기전엔 여기 172.19.0.1 로 되어 있었음
            "GlobalIPv6Address": "",
            "GlobalIPv6PrefixLen": 0,
            "IPAddress": "", # custom-network로 지정해주기전엔 여기 172.19.0.6 로 되어 있었음
            "IPPrefixLen": 0,
            "IPv6Gateway": "",
            "MacAddress": "",
            "Networks": {
                "airflow_network_custom": {
                    "IPAMConfig": {
                        "IPv4Address": "172.28.0.8"
                    },
                    "Links": null,
                    "Aliases": [
                        "airflow-airflow-worker-1",
                        "airflow-worker",
                        "b739a3494646"
                    ],
                    "NetworkID": "eb43aaa125bcf1aac0fd512057de947abafd9397dd0d51cf7f49582c8c7d5eb9",
                    "EndpointID": "",
                    "Gateway": "",
                    "IPAddress": "",
                    "IPPrefixLen": 0,
                    "IPv6Gateway": "",
                    "GlobalIPv6Address": "",
                    "GlobalIPv6PrefixLen": 0,
                    "MacAddress": "",
                    "DriverOpts": null
                }
            }
        }
  ```
  * 현재 설치되어 있는 networks list 보기 : `sudo docker network ls`
  ```markdown
  NETWORK ID     NAME                     DRIVER    SCOPE
  260163833c67   airflow_default          bridge    local
  eb43aaa125bc   airflow_network_custom   bridge    local
  1543a7e87603   bridge                   bridge    local
  9d0e4a4e52ce   host                     host      local
  5c1f555d034f   none                     null      local
  ```
  * worker container는 "NetworkID": "eb43aaa125bcf1aac0fd512057de947abafd9397dd0d51cf7f49582c8c7d5eb9" 에서 앞 부분이 eb43aaa125bc 이기 때문에 airflow_network_custom을 사용하는 것을 볼 수 있다. (networks 지정 전에는 default에 있음)
  * 원래 default network는 IP를 172.19.xxx.xxx 대역을 쓰기 때문에 network custom에서는 안전하게 172.29.xxx.xxx IP 주소가 충돌이 되지 않도록 29로 설정한다. 
* Volume 현황 보기 : `sudo docker volume ls`
  ```markdown
  DRIVER    VOLUME NAME
  local     1e0ab35524a66be9d849f574436e148479f9af7dd76c763cd4dac2ac147aba3c # docker가 알아서 만든 volume
  local     2f2593b44a32b1474400a100216ccc0e9658b99b1cc0c83ad993bc3bb387d4ba # docker가 알아서 만든 volume 
  local     4ae485474d6168c4b62c3bd6ba1b6cd130576d49e733314f3802e36ad34f47a2 # docker가 알아서 만든 volume
  local     4b80894138e5d2ed6b8d5a99723ac747e4325d215e04a81e7ec6254581758109 # docker가 알아서 만든 volume
  local     8bec2d1d658ea13617520f12f1d73981ee2cb7740bee4f3b10ce0aa1e565d0ee # docker가 알아서 만든 volume
  local     91a25038cbe4cec1757bb4ccb35b7a91d73000dfa501c45aa9ebebba351f4882 # docker가 알아서 만든 volume
  local     0315bc7a7513fc0480ae1b220b3f3022d1df134f21c31a3b05282960ea58b820 # docker가 알아서 만든 volume
  local     884de047d72dc84fcf02c7f2dba0c3c5ca6e4d2ef2eb7c2f471be32740ca6949 # docker가 알아서 만든 volume
  local     7300ab83ca5c136dbd95e2d969e5d7d8e09c285c169aaf7789d396d12a940b7a # docker가 알아서 만든 volume
  local     9619310c2f4f3e281b2bd49626cb7ffb157b97946e25bc2d84ea6a27b3842d7e # docker가 알아서 만든 volume
  local     94856658cd6a9ba830bdfd39a73fcf6737cdf82707454c3da2ea6f59c1599ce2 # docker가 알아서 만든 volume
  local     a9f0f6f5f1f4b83a4c1af47aef4ff8f0692cd21eed9db9cde77b08538ccd55a3 # docker가 알아서 만든 volume
  local     a97aa8ddd6bf1224517f37eaeee7425dbfcce1b144ec5c0f3a5bcab47cb80f69 # docker가 알아서 만든 volume
  local     ac91a112c4c03b110775e6ff6cdf8ae774f0376836501f96eb95130594038ec9 # docker가 알아서 만든 volume
  local     airflow_postgres-custom-db-volume # 내가 만든 volume
  local     airflow_postgres-db-volume # airflow가 postgres container를 실행하면서 만들었던 metaDB를 위한 volume
  local     b3b8ab88bbaa69adc798f5fbeebe75dd4d4e47843e9e2861922193695e614926 # docker가 알아서 만든 volume 
  local     b41f5d39b0778ca5efdc714a54ae103503cb8a96778cd9bdd19ecf5857e92e85 # docker가 알아서 만든 volume
  local     c5c7439b17427b11aceee49239ed8f3f4805a9c531cf8a3673a635b2f17cc3ec # docker가 알아서 만든 volume
  local     d31b1c160d8127fab58d1585a44b62498c4f0ae5c42962d68cd725cffe9fdd2d # docker가 알아서 만든 volume
  local     f928be82c57a041a2e02e43b81e6e1280b00c71b39a12bb05ff9a1dd9d1ddb32 # docker가 알아서 만든 volume
  ```
  * volume detail 보기 : `sudo docker volume inspect airflow_postgres-custom-db-volume`
  ```markdown
  [
    {
        "CreatedAt": "2023-07-01T10:38:26+09:00",
        "Driver": "local",
        "Labels": {
            "com.docker.compose.project": "airflow",
            "com.docker.compose.version": "2.18.1",
            "com.docker.compose.volume": "postgres-custom-db-volume"
        },
        "Mountpoint": "/var/lib/docker/volumes/airflow_postgres-custom-db-volume/_data",
        "Name": "airflow_postgres-custom-db-volume",
        "Options": null,
        "Scope": "local"
    }
  ]
  ```
  * volume의 위치: "/var/lib/docker/volumes/airflow_postgres-custom-db-volume/_data"
    * `sudo ls /var/lib/docker/volumes/airflow_postgres-custom-db-volume/_data` 실행하면 postgres container가 쓰고있는 file list를 확인할 수 있다.
    ```markdown
    PG_VERSION  global        pg_dynshmem  pg_ident.conf  pg_multixact  pg_replslot  pg_snapshots  pg_stat_tmp  pg_tblspc    pg_wal   postgresql.auto.conf  postmaster.opts   base  pg_commit_ts  pg_hba.conf  pg_logical     pg_notify     pg_serial    pg_stat       pg_subtrans  pg_twophase  pg_xact  postgresql.conf       postmaster.pid

    ```
## DB 접속하기

* [Dbeaver 설치 https://dbeaver.io/](https://dbeaver.io/)
* community versiono 설치하면 됨 >> windows installer download 받아 설치

### DBeaver에 Postgres 연결

![New Database Connection](../../../../../images/airflow/postgres_install1.PNG)
![Postgres Connection](../../../../../images/airflow/postgres_install2.PNG)

* 3번: localhost 자체가 local의 wsl을 의미하기 때문에 그대로 놔두면 됨
* 4번: port는 docker_compose.yaml에 설정된 port번호 자동으로 입력되서 나옴
* 5번: kmkim (docker_compose.yaml 설정대로 바꿔야함)
* 6번: kmkim (docker_compose.yaml 설정대로 바꿔야함)

### Airflow metaDB 연결

![Airflow MetaDB Connection](../../../../../images/airflow/airflow_metadb_connection.PNG)

* 1번: docker_compose.yaml 설정대로 5431이 나옴
* 2번: database 이름은 airflow
* 3번: username은 airflow
* 4번: password는 airflow

# Connection & Hook

## Postgres에 데이터 insert

```markdown
from airflow import DAG
import pendulum
from airflow.operators.python import PythonOperator

with DAG(
    dag_id='dags_python_with_postgres',
    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),
    schedule=None,
    catchup=False
) as dag:

    
    def insrt_postgres(ip, port, dbname, user, passwd, **kwargs):
        import psycopg2 # postgres DB에 접속해서 sql query 를 날리고 결과를 가지고 올수있게 해주는 library
        from contextlib import closing

        with closing(psycopg2.connect(host=ip, dbname=dbname, user=user, password=passwd, port=int(port))) as conn:
            with closing(conn.cursor()) as cursor:
                dag_id = kwargs.get('ti').dag_id
                task_id = kwargs.get('ti').task_id
                run_id = kwargs.get('ti').run_id
                msg = 'insrt 수행'
                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);'
                cursor.execute(sql,(dag_id,task_id,run_id,msg))
                conn.commit()

    insrt_postgres = PythonOperator(
        task_id='insrt_postgres',
        python_callable=insrt_postgres,
        op_args=['172.28.0.3', '5432', 'hjkim', 'hjkim', 'hjkim']
    )
        
    insrt_postgres
```

* `psycopg2.connect(host=ip, dbname=dbname, user=user, password=passwd, port=int(port))` : DB server와의 연결 (Session)
  * session: TCP/IP 기반의 connection
* conn session에서 sql을 이용한 구체적인 query 내용은 두 번째 with문에서 기술
  ```markdown
   with closing(conn.cursor()) as cursor:
                dag_id = kwargs.get('ti').dag_id # task instance object 에서 dag_id (property) 호출
                task_id = kwargs.get('ti').task_id # task instance object 에서 task_id (property) 호출
                run_id = kwargs.get('ti').run_id # task instance object 에서 run_id (property) 호출
                msg = 'insrt 수행'
                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);' 
                cursor.execute(sql,(dag_id,task_id,run_id,msg))
                conn.commit()
  ```
    * cursor: client(worker)와 DB서버(container) 사이의 session안에서 client에서 query를 날리고 DB서버로부터 결과를 가져와주는 object. 그러므로 cursor (=conn.cursor())가 없으면 query 수행을 못함
    * `cursor.execute(sql,(dag_id,task_id,run_id,msg))` 실제 sql 실행하는 부분
    * `py_opr_drct_insrt values` : 테이블 이름
    * `values (%s,%s,%s,%s)` 4개의 값 binding
* conn이 끝나면 첫 번째 with문의 closing()이 session을 닫아줌
* 위의 코드가 아래의 코드와 같은 내용임
  ```markdown
  conn=psycopg2.connect(...)
  ~~~
  conn.close()
  ```
* task 수행

```markdown
  insrt_postgres = PythonOperator(
        task_id='insrt_postgres',
        python_callable=insrt_postgres,
        op_args=['172.28.0.3', '5432', 'hjkim', 'hjkim', 'hjkim']
    )
        
    insrt_postgres
```
  * `def insrt_postgres(ip, port, dbname, user, passwd, **kwargs)` 에서 보듯이 
    * ip = '172.28.0.3'
    * port = '5422'
    * dbname = 'hjkim'
    * passwd = 'hjkim'
    * **kwargs = NULL
* postgres DB에 table 만들기
  * DBeaver Open >> kmkim databse 우클릭>> SQL editor >> New SQL Script >> `py_opr_drct_insrt` table 생성
  ```markdown
  create table py_opr_drct_insrt(
    dag_id varchar(100),
    task_id varchar(100),
    run_id varchar(100),
    msg text # 가변길이 type
  )
  ```
  * table 확인: kmkim >> Databases >> Schemas >> public

* DAG full example

```mardkown
from airflow import DAG
import pendulum
from airflow.operators.python import PythonOperator

with DAG(
    dag_id='dags_python_with_postgres',
    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),
    schedule=None,
    catchup=False
) as dag:

    
    def insrt_postgres(ip, port, dbname, user, passwd, **kwargs):
        import psycopg2
        from contextlib import closing

        with closing(psycopg2.connect(host=ip, dbname=dbname, user=user, password=passwd, port=int(port))) as conn:
            with closing(conn.cursor()) as cursor:
                dag_id = kwargs.get('ti').dag_id
                task_id = kwargs.get('ti').task_id
                run_id = kwargs.get('ti').run_id
                msg = 'insrt 수행'
                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);'
                cursor.execute(sql,(dag_id,task_id,run_id,msg))
                conn.commit()

    insrt_postgres = PythonOperator(
        task_id='insrt_postgres',
        python_callable=insrt_postgres,
        op_args=['172.28.0.3', '5432', 'hjkim', 'hjkim', 'hjkim']
    )
        
    insrt_postgres
```

## 문제점 및 해결방법

* 문제점
  * 접속정보 노출: postgres DB에 대한 User, Password 등
    * DAG을 열어볼 수 있는 사람이나 github에 접속할 수 있는 사람은 모두 볼 수 있음
  * 접속정보 변경시 대응 어려움
    * 만약 직접 접속하는 DAG이 수백개라면?
* 해결 방법
  * Variable 이용 (User, Password 등을 Variable에 등록하고 꺼내오기) - 번거롭기 때문에 권장하는 방식은 아님
  * Hook 이용 (Variable 등록 필요없음)

## Connection과 Hook의 개념

* Connection
  * Airflow UI 화면에서 등록한 커넥션 정보
* Hook의 개념
  * Airflow에서 외부 솔루션의 기능을 사용할 수 있도록 미리 구현된 메서드를 가진 클래스 
* Hook의 특징
  * Connection 정보를 통해 생성되는 객체로 Hook을 사용하기 위해선 먼저 connection을 등록해야한다.
    * 접속정보를 Connection을 통해 받아오므로 접속정보가 코드상 노출되지 않음
  * 특정 솔루션을 다룰 수 있는 메서드가 구현되어 있음.
  * 오퍼레이터나 센서와는 달리 Hook은 task를 만들어내지 못하므로 Custom 오퍼레이터 안에서나 Python 오퍼레이터 내 함수에서 사용됨

## Connection 등록

* airflow web ui >> admin >> connections >> plus button >>

| Connection_id   | conn-db-postgres-custom |
|-----------------|:------------------------|
| Connection_type | postgres                |
| Host            | 172.28.0.3              |
| Schema          | kmkim                   |
| Login           | kmkim                   |
| Password        | kmkim                   |
| Port            | 5432                    |

## Postgres Hook 명세 보기

* [airflow web service >> Providers >> apache.airflow.providers.postgres >> Python API >> airflow.providers.postgres.hooks.postgres >> get_conn() >> [source] >> def get_conn(self)](https://airflow.apache.org/docs/apache-airflow-providers-postgres/5.4.0/_modules/airflow/providers/postgres/hooks/postgres.html#PostgresHook.get_conn)
* 제공하는 함수의 source code를 잘 관찰하고 custom object(custom operator, custom sensor, custom hook 등)를 만드는 것에 익숙해져야 airflow를 잘 활용할 수 있다.

```markdown
def get_conn(self) -> connection:
        """Establishes a connection to a postgres database."""
        conn_id = getattr(self, self.conn_name_attr)
        conn = deepcopy(self.connection or self.get_connection(conn_id))

        # check for authentication via AWS IAM
        if conn.extra_dejson.get("iam", False):
            conn.login, conn.password, conn.port = self.get_iam_token(conn)

        conn_args = dict(
            host=conn.host,
            user=conn.login,
            password=conn.password,
            dbname=self.database or conn.schema,
            port=conn.port,
        )
        raw_cursor = conn.extra_dejson.get("cursor", False)
        if raw_cursor:
            conn_args["cursor_factory"] = self._get_cursor(raw_cursor)

        for arg_name, arg_val in conn.extra_dejson.items():
            if arg_name not in [
                "iam",
                "redshift",
                "cursor",
                "cluster-identifier",
                "aws_conn_id",
            ]:
                conn_args[arg_name] = arg_val

        self.conn = psycopg2.connect(**conn_args)
        return self.conn
```
* `conn = deepcopy(self.connection or self.get_connection(conn_id))` 를 보면 hook 클래스의 get_connection() method를 이용해 airflow web ui에서 입력했던 connection 입력값을 찾아내서 연결시켜줌

## Hook 이용하여 Postgres Insert

:::: {.columns}

::: {.column width="50%"}

```markdown
from airflow import DAG
import pendulum
from airflow.operators.python import PythonOperator

with DAG(
    dag_id='dags_python_with_postgres',
    start_date=pendulum.datetime(2023,4,1, tz='Asia/Seoul'),
    schedule=None,
    catchup=False
) as dag:

    
    def insrt_postgres(ip, port, dbname, user, passwd, **kwargs):
        import psycopg2
        from contextlib import closing

        with closing(psycopg2.connect(host=ip, dbname=dbname, user=user, password=passwd, port=int(port))) as conn:
            with closing(conn.cursor()) as cursor:
                dag_id = kwargs.get('ti').dag_id
                task_id = kwargs.get('ti').task_id
                run_id = kwargs.get('ti').run_id
                msg = 'insrt 수행'
                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);'
                cursor.execute(sql,(dag_id,task_id,run_id,msg))
                conn.commit()

    insrt_postgres = PythonOperator(
        task_id='insrt_postgres',
        python_callable=insrt_postgres,
        op_args=['172.28.0.3', '5432', 'hjkim', 'hjkim', 'hjkim']
    )
        
    insrt_postgres
```

:::

::: {.column width="50%"}

```markdown
from airflow import DAG
import pendulum
from airflow.operators.python import PythonOperator

with DAG(
        dag_id='dags_python_with_postgres_hook',
        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),
        schedule=None,
        catchup=False
) as dag:
    def insrt_postgres(postgres_conn_id, **kwargs):
        from airflow.providers.postgres.hooks.postgres import PostgresHook
        from contextlib import closing
        
        postgres_hook = PostgresHook(postgres_conn_id)
        with closing(postgres_hook.get_conn()) as conn:
            with closing(conn.cursor()) as cursor:
                dag_id = kwargs.get('ti').dag_id
                task_id = kwargs.get('ti').task_id
                run_id = kwargs.get('ti').run_id
                msg = 'hook insrt 수행'
                sql = 'insert into py_opr_drct_insrt values (%s,%s,%s,%s);'
                cursor.execute(sql, (dag_id, task_id, run_id, msg))
                conn.commit()

    insrt_postgres_with_hook = PythonOperator(
        task_id='insrt_postgres_with_hook',
        python_callable=insrt_postgres,
        op_kwargs={'postgres_conn_id':'conn-db-postgres-custom'}
    )
    insrt_postgres_with_hook

```
:::

::::

*  `op_args=['172.28.0.3', '5432', 'hjkim', 'hjkim', 'hjkim']` 와 같은 보안 사항이 오른 쪽 코드에서는 `op_kwargs={'postgres_conn_id':'conn-db-postgres-custom'}` 가려지게 된다.

# Postgres Hook 으로 bulk_load 하기

## Postgres Hook 명세 보기

* [airflow web service >> Providers >> apache.airflow.providers.postgres >> Python API >> airflow.providers.postgres.hooks.postgres >> get_conn() >> [source] >> def bulg_load(self, temp_file)[source],def bulk_dump(self, table: str, tmp_file: str),def copy_expert(self, sql: str, filename: str)](https://airflow.apache.org/docs/apache-airflow-providers-postgres/5.4.0/_modules/airflow/providers/postgres/hooks/postgres.html#PostgresHook.get_conn)

```markdown
[docs]    def bulk_load(self, table: str, tmp_file: str) -> None:
        """Loads a tab-delimited file into a database table"""
        self.copy_expert(f"COPY {table} FROM STDIN", tmp_file)

[docs]    def bulk_dump(self, table: str, tmp_file: str) -> None:
        """Dumps a database table into a tab-delimited file"""
        self.copy_expert(f"COPY {table} TO STDOUT", tmp_file)

[docs]    def copy_expert(self, sql: str, filename: str) -> None:
        """
        Executes SQL using psycopg2 copy_expert method.
        Necessary to execute COPY command without access to a superuser.

        Note: if this method is called with a "COPY FROM" statement and
        the specified input file does not exist, it creates an empty
        file and no data is loaded, but the operation succeeds.
        So if users want to be aware when the input file does not exist,
        they have to check its existence by themselves.
        """
        self.log.info("Running copy expert: %s, filename: %s", sql, filename)
        if not os.path.isfile(filename):
            with open(filename, "w"):
                pass

        with open(filename, "r+") as file:
            with closing(self.get_conn()) as conn:
                with closing(conn.cursor()) as cur:
                    cur.copy_expert(sql, file)
                    file.truncate(file.tell())
                    conn.commit()
```

```markdown
from airflow import DAG
import pendulum
from airflow.operators.python import PythonOperator
from hooks.custom_postgres_hook import CustomPostgresHook

with DAG(
        dag_id='dags_python_with_custom_hook_bulk_load',
        start_date=pendulum.datetime(2023, 4, 1, tz='Asia/Seoul'),
        schedule='0 7 * * *',
        catchup=False
) as dag:
    def insrt_postgres(postgres_conn_id, tbl_nm, file_nm, **kwargs):
        custom_postgres_hook = CustomPostgresHook(postgres_conn_id=postgres_conn_id)
        custom_postgres_hook.bulk_load(table_name=tbl_nm, file_name=file_nm, delimiter=',', is_header=True, is_replace=True)

    insrt_postgres = PythonOperator(
        task_id='insrt_postgres',
        python_callable=insrt_postgres,
        op_kwargs={'postgres_conn_id': 'conn-db-postgres-custom',
                   'tbl_nm':'TbCorona19CountStatus_bulk2',
                   'file_nm':'/opt/airflow/files/TbCorona19CountStatus/{{data_interval_end.in_timezone("Asia/Seoul") | ds_nodash}}/TbCorona19CountStatus.csv'}
    )
```

## Bulk_load 문제점 & 개선방안

* 문제점
  * Load 가능한 Delimiter는 Tab으로 고정되어 있음
  * Header까지 포함해서 업로드됨
  * 특수문자로 인해 파싱이 안될 경우 에러 발생
* 개선방안
  * Custom Hook 을 만들어서 Delimiter 유형을 입력받게 하고
  * Header 여부를 선택하게끔 하며
  * 특수문자를 제거하는 로직을 추가 후
  * sqlalchemy를 이용하여 Load 한다면? 그리고 테이블을 생성하면서 업로드할 수 있다면?

# Custom Hook 으로 bulk_load 하기

## BaseHook 명세 보기

* [Airflow Docs](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/hooks/base/index.html#airflow.hooks.base.BaseHook.get_connection_form_widgets)

## Custom Hook 개발

* 해야할 일
  * get_conn 메서드 구현하기
    * DB 와의 연결 세션 객체인 conn 을 리턴하도록 구현
      * (Airflow 에서 등록한 Connection 정보를 담은 conn 이 아님)
      * BaseHook 의 추상 메서드 , 자식 클래스에서 구현 필요
  * bulk_load 메서드 구현하기
    * 입맛대로 만들기

## Providers 더 보기

* 현재 설치되어 있는 Providers 패키지 확인
  * 웹의 Admin Providers 에서 확인 가능
* [설치 가능한 Providers 더 보기](https://airflow.apache.org/docs/apache-airflow-providers/)

## Connection Type 추가하기

* 만약 HDFS, Hive 에 대한 커넥션을 추가하고 싶다면
* 커넥션 타입 목록에 있는 대상은 이미 패키지 설치가 된 Providers 이며 Admin>>Providers 목록에서 설치된 대상 확인 가능
* Provider 검색 , py 라이브러리 설치 목록 확인
* [Airflow 이미지 Extend 방법 확인](https://airflow.apache.org/docs/docker-stack/build.html)
  * 이미지 Extend vs Custom 이미지 생성

  | Comparison          | image extend    | Custom Image Creation   |
  |---------------------|-----------------|:------------------------|
  | 간단히 생성 가능     | O               | X                       |
  | 빌드 시간            | 짧음            | 상대적으로 긺            |
  | 크기 최적화된 이미지 |  X              | O (약 20% 정도 사이즈 감소) |
  | 폐쇄망에서 구성 가능 |  X              | O                       |
  
* Dockerfile 만들기  

:::: {.columns}

::: {.column width="50%"}

```markdown
#> cd; cd airflow
#> mkdir -p custom_image/airflow
#> cd custom_image/airflow
#> vi Dockerfile
```

:::

::: {.column width="50%"}

```markdown

FROM apache/airflow:2.5.1
USER root
RUN apt get update \
  && apt-get install -y --no-install-recommends \
    gcc \
    heimdal-dev \ 
    g++ \
    libsasl2-dev \
  && apt-get autoremove -yqq -purge \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*
USER airflow
RUN pip install \
  apache airflow providers apache hdfs \
  apache airflow providers apache hive
RUN pip uninstall -y argparse
```
:::

::::

* Docker 이미지 생성하기

```markdown
#> pwd
/home/hjkim/airflow/custom_image/airflow
#> sudo docker build -t {image_name} .
```
* Docker 이미지 확인

```markdown
#> sudo docker image ls
```

* docker-compose.yaml 수정하기

```markdown
#> cd
#> vi docker-compose.yaml

version: '3.8'
x-airflow-common:
  &airflow-common
# In order to add …
  image: {image_name}
```

* docker compose (재) 기동
* Connection Type 에 추가 확인


</div>

<div class="tab-pane fade" id="English" role="tabpanel" aria-labelledby="English-tab">




</div>


# Go to Blog Content List

[Blog Content List](../../content_list.qmd)