---
title: Conditional Probability & Bayes' Rule
subtitle: Conditional Probability, Bayes' Rule, Bayesean Estimation, Naive Bayes
description: | 
  Probability for statistics, machine learning and deep learning.
categories:
  - Statistics
author: Kwangmin Kim
date: 02/05/2023
draft: false
format: 
  html:
    toc: true
    number-sections: True
    code-fold: true
    page-layout: full
execute: 
  warning: false
  message: false
---

<ul class="nav nav-pills" id="language-tab" role="tablist">
  <li class="nav-item" role="presentation">
    <button class="nav-link active" id="Korean-tab" data-bs-toggle="tab" data-bs-target="#Korean" type="button" role="tab" aria-controls="Korean" aria-selected="true">Korean</button>
  </li>
  <li class="nav-item" role="presentation">
    <button class="nav-link" id="English-tab" data-bs-toggle="tab" data-bs-target="#English" type="button" role="tab" aria-controls="knitr" aria-selected="false">English</button>
  </li>

<div class="tab-content" id="language-tabcontent">

<div class="tab-pane fade  show active" id="Korean" role="tabpanel" aria-labelledby="Korean-tab">

::: {#Korean .tab-pane .fade .show .active role="tabpanel" aria-labelledby="Korean-tab"}



## Conditional Probability

$$
P(A|B)=\frac{P(A\cap B)}{P(B)}
$$ {#eq-conditional_probability}

ì£¼ì‚¬ìœ„ë¥¼ ë˜ì§ˆ ë•Œ íŠ¹ì • ì£¼ì‚¬ìœ„ì˜ ëˆˆ (1~6)ì´ ë‚˜ì˜¬ í™•ë¥ ì€ $\frac{1}{6}$ ìœ¼ë¡œ ê°™ë‹¤ (eqaully likely)ë¼ê³  ê°€ì •í•  ë•Œ ì£¼ì‚¬ìœ„ì˜ ëˆˆì´ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ëª¨ë“  ì§‘í•© í‘œë³¸ ê³µê°„ $S$, íŠ¹ì • ì£¼ì‚¬ìœ„ì˜ ëˆˆì´ ë‚˜ì˜¤ëŠ” ì‚¬ê±´ $A$ ì— ëŒ€í•œ í™•ë¥ ì€ $\frac{n(A)}{n(S)}$ ì™€ ê°™ë‹¤. ì¡°ê±´ë¶€ í™•ë¥ ì€ 2ê°œ ì´ìƒì˜ ì‚¬ê±´ì— ëŒ€í•´ì„œ í•˜ë‚˜ì˜ ì‚¬ê±´ì´ ë‹¤ë¥¸ ì‚¬ê±´ì´ ë°œìƒí•  í™•ë¥ ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê°œë…ì„ ë§í•œë‹¤. ê°€ì¥ ê°„ë‹¨í•œ 2ê°œì˜ ì‚¬ê±´ $A, B$ ì— ëŒ€í•´ì„œ ì‚´í´ë³¼ ë•Œ ì¡°ê±´ë¶€ í™•ë¥ ì€ ìœ„ì˜ ì‹(@eq-conditional_probability)ê³¼ ê°™ë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ê±´ $A$ ëŠ”  ì£¼ì‚¬ìœ„ì˜ ëˆˆì´ 1ì´ ë‚˜ì˜¤ëŠ” ì‚¬ê±´, ì‚¬ê±´ $B$ ëŠ” ì£¼ì‚¬ìœ„ì˜ ëˆˆì´ 3ì´í•˜ê°€ ë‚˜ì˜¤ëŠ” ì‚¬ê±´ì´ë¼ê³  í–ˆì„ ë•Œ ì‚¬ê±´ $A$ ê°€ ì‚¬ê±´ $B$ ì˜ ë¶€ë¶„ ì§‘í•©ì´ë¯€ë¡œ ë‘ ì‚¬ê±´ì´ ì„œë¡œ ë…ë¦½ì´ ì•„ë‹ˆë‹¤. ì¦‰, $A \cap B $ ì‚¬ê±´ì—ì„œ ì£¼ì‚¬ìœ„ì˜ ëˆˆì´ 1 ë‚˜ì˜¤ëŠ” ê²½ìš° ë°–ì— ì—†ë‹¤. ì´ë ‡ê²Œ ì‚¬ê±´ $B$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ í˜¹ì€ $B$ ê°€ ë¨¼ì € ì¼ì–´ë‚¬ì„ ë•Œ 1ì´ ë‚˜ì˜¬ í™•ë¥ ì€ ë‹¬ë¼ì§€ê²Œ ëœë‹¤. ì¦‰, $A$ ì˜ sample space = $\{1,2,3,4,5,6\}$ ì´ê³  $A|B$ ì˜ sample space = $\{1,2,3\}$ ì´ ë˜ê¸° ë•Œë¬¸ì— $P(A) \not= P(A|B)$ ê°€ ëœë‹¤. 

ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ê³„ì‚°ì„ í•˜ê²Œ ë˜ë©´, $P(A)=\frac{1}{6}, P(B)=\frac{3}{6}, P(A\cap B)=\frac{1}{6}$ ì¼ ë•Œ, 

$$
P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{\frac{1}{6}}{\frac{3}{6}}=\frac{1}{3}
$$ 

ì¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 
## Bayes' Rule

Bayes' rule (ë² ì´ì¦ˆ ì •ë¦¬)ëŠ” prior probability(ì‚¬ì „ í™•ë¥ )ê³¼ posterior probability(ì‚¬í›„ í™•ë¥ )ì˜ ê´€ê³„ë¥¼ ì¡°ê±´ë¶€ í™•ë¥ ì„ ì´ìš©í•˜ì—¬ í™•ë¦½í•œ ê²ƒì´ë‹¤. ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ, 2ê°œì˜ ì‚¬ê±´ Aì™€ Bë¡œ í•œì •ì‹œì¼œ ìƒê°í•´ë´¤ì„ ë•Œ, ì¡°ê±´ë¶€ í™•ë¥  $P(A|B)$ ëŠ” ê° ì‚¬ê±´ì˜ í™•ë¥  $P(A), P(B), P(B|A)$ ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²Œì‚°ë  ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ ë² ì´ì¦ˆ ì •ë¦¬ëŠ” $P(B|A)$, $P(B|\overline{A})$, $P(A)$ ì˜ ì •ë³´ë¥¼ ì•Œê³ ìˆê±°ë‚˜ ê³„ì‚° ê°€ëŠ¥í•  ë•Œ ì•„ë˜ì™€ ê°™ì€ $P(A|B)$ ì˜ í™•ë¥ ì„ êµ¬í•  ìˆ˜ ìˆëŠ” ê³µì‹ì„ ì œê³µí•œë‹¤. 

$$
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
$$

* $P(A|B)$: posterior probability
* $P(A)$: prior probability
* $P(B|A)$: likelihood
* $P(B)$: marginal probability

ì´ ê³µì‹ì„ ì´í•´í•˜ê¸° ìœ„í•´ì„ , Law of Total Probability (ì „ í™•ë¥  ë²•ì¹™) ë˜ëŠ” Total Probability Rule (ì „ í™•ë¥  ì •ë¦¬)ì„ ì´í•´í•´ì•¼í•œë‹¤.

### Total Probability Rule

::: {#thm-general}

Let $A_1, A_2, ..., A_k$ be a set of mutually exclusive and exhaustive events. Let $A$ be a event and a partition of sample space $\Omega$, then 

$$
P(B)=\sum_{i=1}^{n}P(B|A_i)P(A_i)
$$

:::

::: columns
::: {.column width="50%"}
![Law of Total Probability Example - Two Events](law%20of%20total%20probability2.PNG)

[Source: Law of ToTal Probability with Proof](https://byjus.com/maths/total-probability-theorem/)
:::
::: {.column width="50%"}
![Law of Total Probability Example - Multiple Events](law%20of%20total%20probability.PNG)

[Source: MIT RES.6-012 Introduction to Probability, Spring 2018 - Youtube](https://www.youtube.com/watch?v=8odFouBR2wE)
:::
:::


$$
\begin{aligned}
P(B)&=P(B\cap A) + P(B\cap \overline A)\\
    &=P(B\cap A) + P(B\cap \overline A)\\
    &=P(B|A)P(A)+P(B|\overline A)P(\overline A)\\
\end{aligned}
$$

$$
\begin{aligned}
\therefore 
P(A|B)&=\frac{P(A \cap B)}{P(B)}\\
      &=\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\overline A)P(\overline A)}
\end{aligned}
$$

$$
P(A\cap B)=P(B|A)P(A)=P(A|B)P(B)
$$


### ì˜ˆì œ

[ì˜ìƒ ìë£Œ ](https://www.youtube.com/watch?v=Y4ecU7NkiEI&t=29s)
[ìŠ¬ê¸°ë¡œìš´ í†µê³„ìƒí™œ ë¸”ë¡œê·¸ ](https://statisticsplaybook.tistory.com/30)

ì˜ˆì œë¡œ ì•Œì•„ë³´ê¸° - ì´ˆì½œë¦¿ì„ ì¤€ ì½”ë‹ˆì˜ ë§ˆìŒ
í­ìˆ˜ëŠ” í‰ì†Œ ê´€ì‹¬ì´ ìˆë˜ ì½”ë‹ˆì—ê²Œì„œ ì´ˆì½œë¦¿ì„ ì„ ë¬¼ë°›ìŠµë‹ˆë‹¤. í­ìˆ˜ëŠ” ì´ˆì½œë¦¿ì„ ì¤€ ì½”ë‹ˆê°€ ë‚˜ë¥¼ ì¢‹ì•„í•˜ëŠ”ì§€ê°€ ê¶ê¸ˆí•˜ê¸° ë•Œë¬¸ì— ì´ê²ƒì„ í†µê³„ì ìœ¼ë¡œ í•´ë´…ë‹ˆë‹¤. ë¨¼ì € ìƒí™©ì„ ê°„ë‹¨íˆ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ì„œ ë‹¤ìŒ ë‘ ìƒí™©ì„ ê°€ì •í•©ë‹ˆë‹¤.

 

P(A) (ì‚¬ê±´ "í˜¸ê°"): ìƒëŒ€ë°©ì´ ë‚˜ë¥¼ ì¢‹ì•„í•œë‹¤. 

P(B) (ì‚¬ê±´ "ì´ˆì½œë¦¿"): ì´ˆì½œë¦¿ì„ ë°›ì•˜ë‹¤.

 
í­ìˆ˜ëŠ” ì‚¬ê±´ "í˜¸ê°"ì— ëŒ€í•œ í™•ë¥ , ì¦‰, ì½”ë‹ˆê°€ ë‚˜ë¥¼ ì¢‹ì•„í•  í™•ë¥ ì„ 50%, ì¢‹ì•„í•˜ì§€ ì•Šì„ í™•ë¥ ì„ 50%ë¡œ ê°€ì •í•©ë‹ˆë‹¤. ì¦‰, 
P(A)=0.5  ë¡œ í‘œí˜„ì´ ë  ê²ƒì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ìœ ëŠ” í­ìˆ˜ëŠ” ì´ˆì½œë¦¿ì„ ì¤€ ì½”ë‹ˆ ë§ˆìŒì— ëŒ€í•œ ì•„ë¬´ëŸ° ì •ë³´ê°€ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤!

**The Principle of Insufficient Reason(ì´ìœ ë¶ˆì¶©ë¶„ì˜ ì›ë¦¬): í•˜ë‚˜ì˜ ì‚¬ê±´ì„ ê¸°ëŒ€í• ë§Œí•œ ì–´ë–¤ ì´ìœ ê°€ ì—†ëŠ” ê²½ìš°ì—ëŠ” ê°€ëŠ¥í•œ ëª¨ë“  ì‚¬ê±´ì— ë™ì¼í•œ í™•ë¥ ì„ í• ë‹¹í•´ì•¼ í•œë‹¤ëŠ” ì›ì¹™.**

ì‰½ê²Œ ì´í•´í•˜ê¸° ìœ„í•˜ì—¬, ì£¼ë³€ì— 100ëª…ì˜ ì‚¬ëŒì´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ì´ìœ ë¶ˆì¶©ë¶„ì˜ ì›ë¦¬ì— ë”°ë¼ 50ëª…ì€ ëˆ„êµ°ê°€ë¡œë¶€í„° í˜¸ê°ì„ ë°›ê³  ìˆê³ , 50ëª…ì€ í˜¸ê°ì„ ì–»ê³  ìˆì§€ ì•Šê³  ìˆê² ë„¤ìš”!

 

ê·¸ë¦¬ê³  í­ìˆ˜ëŠ” ì¡°ì‚¬ë¥¼ í†µí•´ ë‘ ê°€ì§€ ì •ë³´ë¥¼ ì•Œê²Œë©ë‹ˆë‹¤.

* ì–´ë–¤ ì‚¬ëŒì´ ìƒëŒ€ë°©ì—ê²Œ í˜¸ê°ì´ ìˆì–´ì„œ ì´ˆì½œë¦¿ì„ ì¤„ í™•ë¥ ì€ 40%ì´ë‹¤. -> P(B|A)=0.4 
* ì–´ë–¤ ì‚¬ëŒì´ ìƒëŒ€ë°©ì—ê²Œ í˜¸ê°ì´ ì—†ì§€ë§Œ "ì˜ˆì˜ìƒ" ì´ˆì½œë¦¿ì„ ì¤„ í™•ë¥ ì€ 30%ì´ë‹¤. (ì–´ì¥ ê´€ë¦¬ ê·¸ë§Œã… ) -> P(B|Ac)=0.3
 

ì´ ì •ë³´ë¥¼ í† ëŒ€ë¡œ ì•„ë˜ ë‘ê°€ì§€ë¥¼ ìœ ì¶”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ìƒëŒ€ë°©ì—ê²Œ í˜¸ê°ì´ ìˆì§€ë§Œ ì´ˆì½œë¦¿ì„ ì£¼ì§€ ì•Šì„ í™•ë¥ ì€ 60%ì´ë‹¤. -> P(Bc|A)=0.6
í˜¸ê°ì´ ì—†ì–´ì„œ ì´ˆì½œë¦¿ì„ ì£¼ì§€ ì•Šì„ í™•ë¥ ì€ 70%ì´ë‹¤. -> P(Bc|Ac)=0.7

ì–»ì€ ì •ë³´ ì •ë¦¬í•´ë³´ê¸°
ìœ„ ë‚´ìš©ë“¤ì„ í† ëŒ€ë¡œ ìƒê°í•´ë³¼ ë•Œ, ìš°ë¦¬ëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

* ê°ì„ ì–»ê³  ìˆëŠ” 50ëª… ì¤‘ 40%ì¸ 20ëª…ì€ ì´ˆì½œë¦¿ì„ ë°›ìŠµë‹ˆë‹¤.
* ê°ì„ ì–»ê³  ìˆëŠ” 50ëª… ì¤‘ 60%ì¸ 30ëª…ì€ ì´ˆì½œë¦¿ì„ ë°›ì§€ ëª»í•©ë‹ˆë‹¤.
* ê°ì˜ ëŒ€ìƒì´ ì•„ë‹Œ 50ëª… ì¤‘ 30%ì¸ 15ëª…ì€ ì˜ˆì˜ìƒ ì¤€ ì´ˆì½œë¦¿ì„ ë°›ìŠµë‹ˆë‹¤.
* ê°ì˜ ëŒ€ìƒì´ ì•„ë‹Œ 50ëª… ì¤‘ 70%ì¸ 35ëª…ì€ ì´ˆì½œë¦¿ì„ ë°›ì§€ ëª»í•©ë‹ˆë‹¤.

ì, ì´ì œ í­ìˆ˜ê°€ ê¶ê¸ˆí•œ P(ì´ˆì½œë¦¿ì„ ë°›ì•˜ì„ ë•Œ, ì´ˆì½œë¦¿ì„ ì¤€ ì‚¬ëŒì´ ë‚˜ë¥¼ ì¢‹ì•„í•  í™•ë¥ )ì„ ìœ„ì—ì„œ ì•½ì†í•œ ê¸°í˜¸ë¡œ í‘œí˜„í•œë‹¤ë©´ 
P(A|B) ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ì´ê²ƒì€ í­ìˆ˜ê°€ ì¡°ì‚¬í•œ í™•ë¥ ,

$$
P(í˜¸ê°ì´ìˆì–´ì„œì´ˆì½œë¦¿ì„ì¤„í™•ë¥ )=P(B|A)
$$

ì—ì„œ ì¡°ê±´ê³¼ ê²°ê³¼ê°€ ë’¤ë°”ë€ ê²ƒì…ë‹ˆë‹¤. ì´ í™•ë¥ ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ì •ë¦¬í•´ë´…ì‹œë‹¤.
ì‚¬ê±´ "í˜¸ê°" : ìƒëŒ€ë°©ì´ ë‚˜ë¥¼ ì¢‹ì•„í•œë‹¤ -> P(A) = 50% ì´ê³  (ì´ìœ ë¶ˆì¶©ë¶„ì˜ ì›ë¦¬, ì‚¬ì „ê°€ì •!),
ì‚¬ê±´ "ì´ˆì½œë¦¿" : ì´ˆì½œë¦¿ì„ ë°›ì•˜ë‹¤ -> P(B) = (20+15)/100 = 35% (ì •ë¦¬í•œ ë…¸ë€ìƒ‰ ë¶€ë¶„)ì…ë‹ˆë‹¤.
ì–´ë–¤ ì‚¬ëŒì´ ìƒëŒ€ë°©ì—ê²Œ í˜¸ê°ì´ ìˆì–´ì„œ ì´ˆì½œë¦¿ì„ ì¤„ í™•ë¥ ì€ 40%ì´ë‹¤. -> P(B|A)=40% (í­ìˆ˜ê°€ ì¡°ì‚¬í•œ ì •ë³´)

 

ì´ê²ƒì„ ë² ì´ì¦ˆì •ë¦¬ì˜ ìˆ˜ì‹ì— ë„£ì–´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

 

P(A|B)=P(B|A)âˆ—P(A)/P(B)=(0.4âˆ—0.5)/0.35=0.57 

ì´ë¼ëŠ” í™•ë¥ ì´ ë‚˜ì˜µë‹ˆë‹¤. ë”°ë¼ì„œ, í­ìˆ˜ëŠ” ì²˜ìŒì˜ ì´ìœ ë¶ˆì¶©ë¶„ì˜ ì›ë¦¬ë¡œ ê°€ì •í–ˆë˜, ìƒëŒ€ë°©ì´ ë‚˜ë¥¼ ì¢‹ì•„í•  í™•ë¥  50%(ì‚¬ì „í™•ë¥ ) 
P(A)ë¥¼ 57%(ì‚¬í›„í™•ë¥ ) P(A|B) ë¡œ ì—…ë°ì´íŠ¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

 

ì´ë²ˆ ê¸€ì˜ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì§€ëŠ”, ë² ì´ì¦ˆ ì •ë¦¬ë¼ëŠ” ê²ƒì€ ìƒˆë¡œìš´ ì •ë³´ë¥¼ í†µí•´ ì‚¬ì „ í™•ë¥ ì„ ì—…ë°ì´íŠ¸ í•˜ì—¬ ì‚¬í›„ í™•ë¥ ì— ëŒ€í•œ ì •í™•ë„ë¥¼ ë†’ì—¬ê°€ëŠ” ê²ƒì´ë¼ëŠ” ì ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ëŸ¬í•œ ê°œë…ë“¤ì´ ì–´ë–»ê²Œ ë°ì´í„° ë¶„ì„ì— í™œìš©ë  ìˆ˜ ìˆëŠ”ì§€ í•œ ë²ˆ ìƒê°í•´ë³´ê³ ì í•©ë‹ˆë‹¤. 

 

í˜¹ì‹œ ë‚´ìš©ì— ì˜¤ë¥˜ê°€ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ ëŒ“ê¸€/í”¼ë“œë°± ë‹¬ì•„ì£¼ì„¸ìš” ğŸ† ì˜ëª»ëœ ë‚´ìš© ì•Œë ¤ì£¼ì‹œëŠ” ê²ƒ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ ğŸ‘©â€ğŸš€


## Generalized Bayes' Rule

::: {#thm-general}

Let $A_1, A_2, ..., A_k$ be a set of mutually exclusive and exhaustive events. Let $A$ be a event, then 

$$
P(A_i|B)=\frac{P(B|A_i)P(A_i)}{\sum_{j=1}^{k}P(B|A_i)P(A_i)}
$$

:::

::: {#thm-classic}

If in $N$ identical and independent repeated experiments, an event $A$ happens $n$ times, the the probability of $A$ is defined by
$$
P(A)=\lim_{N\to\infty}\frac{n}{N}
$$

:::

* The case of the sample space consisting of $N$ distinctive not equally likely elements,
* The case of the uncountable sample space 
*
*
*
*
*
*
*
*
*
*
*
:::
</div>

<div class="tab-pane fade" id="English" role="tabpanel" aria-labelledby="English-tab">

::: {#English .tab-pane .fade role="tabpanel" aria-labelledby="English-tab"}


Bayes' rule provides a formula how to calculate $P(A|B)$ if $P(B|A)$, $P(B|\overline{A})$, $P(A)$ are available
:::


</div>



## Blog Guide Map Link

* [Statistics Blog](../guide_map/index.qmd)
* [Engineering Blog](../../Engineering/guide_map/index.qmd)
* [Deep Learning Blog](../../DL/guide_map/index.qmd)
* [Machine Learning Blog](../../ML/guide_map/index.qmd)
* [Mathematics Blog](../../Mathmatics/guide_map/index.qmd)
* [Patent Blog](../../Patent/guide_map/index.qmd)
* [Validation Blog](../../Validation/guide_map/index.qmd)



